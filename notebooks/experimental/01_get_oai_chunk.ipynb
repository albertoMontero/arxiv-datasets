{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../arxiv_cs/')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from harvest import get_oai_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'verb': 'ListRecords', 'metadataPrefix': 'arXivRaw', 'from': '2019-10-01',\n",
    "                  'until': '2019-10-09', 'set': 'cs'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_oai_chunk(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<OAI-PMH xmlns=\"http://www.openarchives.org/OAI/2.0/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd\">\\n<responseDate>2019-11-05T14:25:40Z</responseDate>\\n<request verb=\"ListRecords\" until=\"2019-10-09\" from=\"2019-10-01\" metadataPrefix=\"arXivRaw\" set=\"cs\">http://export.arxiv.org/oai2</request>\\n<ListRecords>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1207.4664</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1207.4664</id><submitter>Alexander Mamonov V</submitter><version version=\"v1\"><date>Thu, 19 Jul 2012 13:39:08 GMT</date><size>11129kb</size><source_type>D</source_type></version><title>Quantitative photoacoustic imaging in radiative transport regime</title><authors>Alexander V. Mamonov, Kui Ren</authors><categories>physics.med-ph cs.NA math.NA physics.optics</categories><comments>40 pages, 13 figures</comments><msc-class>65N21</msc-class><journal-ref>Communications in Mathematical Sciences, 12(2):201-234, 2014</journal-ref><doi>10.4310/CMS.2014.v12.n2.a1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of quantitative photoacoustic tomography (QPAT) is to\\nreconstruct optical and thermodynamic properties of heterogeneous media from\\ndata of absorbed energy distribution inside the media. There have been\\nextensive theoretical and computational studies on the inverse problem in QPAT,\\nhowever, mostly in the diffusive regime. We present in this work some numerical\\nreconstruction algorithms for multi-source QPAT in the radiative transport\\nregime with energy data collected at either single or multiple wavelengths. We\\nshow that when the medium to be probed is non-scattering, explicit\\nreconstruction schemes can be derived to reconstruct the absorption and the\\nGruneisen coefficients. When data at multiple wavelengths are utilized, we can\\nreconstruct simultaneously the absorption, scattering and Gruneisen\\ncoefficients. We show by numerical simulations that the reconstructions are\\nstable.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1207.4933</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1207.4933</id><submitter>Nick McCullen</submitter><version version=\"v1\"><date>Fri, 20 Jul 2012 12:20:10 GMT</date><size>199kb</size></version><title>Multi-parameter models of innovation diffusion on complex networks</title><authors>Nicholas J. McCullen, Alastair M. Rucklidge, Catherine S. E. Bale, Tim\\n  J. Foxon, and William F. Gale</authors><categories>nlin.AO cs.MA cs.SI physics.soc-ph</categories><journal-ref>SIAM J. Applied Dynamical Systems Vol. 12, No. 1, pp. 515-532\\n  (2013)</journal-ref><doi>10.1137/120885371</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A model, applicable to a range of innovation diffusion applications with a\\nstrong peer to peer component, is developed and studied, along with methods for\\nits investigation and analysis. A particular application is to individual\\nhouseholds deciding whether to install an energy efficiency measure in their\\nhome. The model represents these individuals as nodes on a network, each with a\\nvariable representing their current state of adoption of the innovation. The\\nmotivation to adopt is composed of three terms, representing personal\\npreference, an average of each individual\\'s network neighbours\\' states and a\\nsystem average, which is a measure of the current social trend. The adoption\\nstate of a node changes if a weighted linear combination of these factors\\nexceeds some threshold. Numerical simulations have been carried out, computing\\nthe average uptake after a sufficient number of time-steps over many\\nrealisations at a range of model parameter values, on various network\\ntopologies, including random (Erdos-Renyi), small world (Watts-Strogatz) and\\n(Newman\\'s) highly clustered, community-based networks. An analytical and\\nprobabilistic approach has been developed to account for the observed\\nbehaviour, which explains the results of the numerical calculations.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1212.1710</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1212.1710</id><submitter>Vladimir Lerner S</submitter><version version=\"v1\"><date>Sun, 9 Dec 2012 19:39:10 GMT</date><size>893kb</size></version><version version=\"v2\"><date>Thu, 23 Jan 2014 19:45:48 GMT</date><size>923kb</size></version><version version=\"v3\"><date>Mon, 21 Aug 2017 18:33:03 GMT</date><size>1876kb</size></version><version version=\"v4\"><date>Tue, 21 Nov 2017 21:56:42 GMT</date><size>1938kb</size></version><version version=\"v5\"><date>Tue, 12 Dec 2017 03:29:56 GMT</date><size>1981kb</size></version><version version=\"v6\"><date>Wed, 10 Jan 2018 22:55:39 GMT</date><size>2014kb</size></version><version version=\"v7\"><date>Sun, 6 Oct 2019 18:18:31 GMT</date><size>1981kb</size></version><title>The information and its observer: external and internal information\\n  processes, information cooperation, and the origin of the observer intellect</title><authors>Vladimir S. Lerner</authors><categories>nlin.AO cs.IT math.IT</categories><comments>83 pages include 16 figures</comments><msc-class>58J65, 60J65, 93B52, 93E02, 93E15, 93E30</msc-class><acm-class>H.1.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The aim is formal principles of origin information and information process\\ncreating information observer self-creating information in interactive\\nobservations. The interactive phenomenon creates Yes-No actions of information\\nBits in its information observer. Information emerges from interacting random\\nfield of Kolmogorov probabilities, which link Kolmogorov 0-1 law probabilities\\nand Bayesian probabilities observing Markov diffusion process by probabilistic\\n0-1 impulses. Each No-0 action cuts maximum of impulse minimal entropy while\\nfollowing Yes-1 action transfers maxim between impulses performing dual\\nprinciple of converting process entropy to information. Merging Yes-No actions\\ngenerate microprocess within bordered impulse producing Bit with free\\ninformation when the microprocess probability approaches 1. Interacting bits\\nmemorize free information which attracts multiple Bits moving macroprocess self\\njoining triplet macrounits. Memorized information binds reversible microprocess\\nwith irreversible macroprocess. The observation converts cutting entropy to\\ninformation macrounits. Macrounits logically self-organize information networks\\nencoding the units in geometrical structures enclosing triplet code. Multiple\\nIN binds their ending triplets enclosing observer information cognition and\\nintelligence. The observer cognition assembles common units through multiple\\nattraction and resonances at forming IN triplet hierarchy which accept only\\nunits that recognizes each IN node. Maximal number of accepted triplet levels\\nin multiple IN measures the observer maximum comparative information\\nintelligence. The observation process carries probabilistic and certain wave\\nfunctions which self-organize the space hierarchical structures. These\\ninformation regularities create integral logic and intelligence self-requesting\\nneeded information.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1402.3735</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1402.3735</id><submitter>Dimitra Panagou</submitter><version version=\"v1\"><date>Sat, 15 Feb 2014 23:55:12 GMT</date><size>194kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 18 Feb 2014 12:49:21 GMT</date><size>194kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 3 Oct 2019 17:30:27 GMT</date><size>2950kb</size><source_type>D</source_type></version><title>Decentralized Goal Assignment and Safe Trajectory Generation in\\n  Multi-Robot Networks via Multiple Lyapunov Functions</title><authors>Dimitra Panagou, Matthew Turpin, Vijay Kumar</authors><categories>cs.MA cs.RO cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of decentralized goal assignment and\\ntrajectory generation for multi-robot networks when only local communication is\\navailable, and proposes an approach based on methods related to switched\\nsystems and set invariance. A family of Lyapunov-like functions is employed to\\nencode the (local) decision making among candidate goal assignments, under\\nwhich a group of connected agents chooses the assignment that results in the\\nshortest total distance to the goals. An additional family of Lyapunov-like\\nbarrier functions is activated in the case when the optimal assignment may lead\\nto colliding trajectories, maintaining thus system safety while preserving the\\nconvergence guarantees. The proposed switching strategies give rise to feedback\\ncontrol policies that are computationally efficient and scalable with the\\nnumber of agents, and therefore suitable for applications including\\nfirst-response deployment of robotic networks under limited information\\nsharing. The efficacy of the proposed method is demonstrated via simulation\\nresults and experiments with six ground robots.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1411.0782</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1411.0782</id><submitter>Seung Woo Shin</submitter><version version=\"v1\"><date>Tue, 4 Nov 2014 04:53:25 GMT</date><size>57kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 29 Apr 2017 18:22:00 GMT</date><size>278kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 10 Aug 2017 20:01:53 GMT</date><size>750kb</size><source_type>D</source_type></version><title>Verifying Chemical Reaction Network Implementations: A Pathway\\n  Decomposition Approach</title><authors>Seung Woo Shin, Chris Thachuk, Erik Winfree</authors><categories>cs.CE cs.ET cs.LO q-bio.MN</categories><comments>41 pages, 11 figures. Preliminary version appeared in VEMDP 2014, an\\n  affiliated workshop of CAV 2014. This version submitted to Theoretical\\n  Computer Science</comments><journal-ref>Theoretical Computer Science, Volume 765, 18 April 2019, Pages\\n  67-96</journal-ref><doi>10.1016/j.tcs.2017.10.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here we focus on the challenge of verifying the correctness of molecular\\nimplementations of abstract chemical reaction networks, where operation in a\\nwell-mixed &quot;soup&quot; of molecules is stochastic, asynchronous, concurrent, and\\noften involves multiple intermediate steps in the implementation, parallel\\npathways, and side reactions. This problem relates to the verification of Petri\\nnets, but existing approaches are not sufficient for providing a single\\nguarantee covering an infinite set of possible initial states (molecule counts)\\nand an infinite state space potentially explored by the system given any\\ninitial state. We address these issues by formulating a new theory of pathway\\ndecomposition that provides an elegant formal basis for comparing chemical\\nreaction network implementations, and we present an algorithm that computes\\nthis basis. Our theory naturally handles certain situations that commonly arise\\nin molecular implementations, such as what we call &quot;delayed choice,&quot; that are\\nnot easily accommodated by other approaches. We further show how pathway\\ndecomposition can be combined with weak bisimulation to handle a wider class\\nthat includes most currently known enzyme-free DNA implementation techniques.\\nWe anticipate that our notion of logical equivalence between chemical reaction\\nnetwork implementations will be valuable for other molecular implementations\\nsuch as biochemical enzyme systems, and perhaps even more broadly in\\nconcurrency theory.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1412.6761</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1412.6761</id><submitter>Masaki Nakanishi</submitter><version version=\"v1\"><date>Sun, 21 Dec 2014 10:41:03 GMT</date><size>472kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 12 Jul 2016 10:13:01 GMT</date><size>863kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 28 Mar 2018 02:06:47 GMT</date><size>608kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Wed, 25 Sep 2019 01:44:36 GMT</date><size>633kb</size><source_type>D</source_type></version><title>New results on classical and quantum counter automata</title><authors>Masaki Nakanishi, Abuzer Yakary{\\\\i}lmaz and Aida Gainutdinova</authors><categories>cs.FL cs.CC quant-ph</categories><comments>21 pages</comments><journal-ref>Discrete Mathematics &amp; Theoretical Computer Science, vol. 21 no.\\n  4, Automata, Logic and Semantics (September 27, 2019) dmtcs:5788</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that one-way quantum one-counter automaton with zero-error is more\\npowerful than its probabilistic counterpart on promise problems. Then, we\\nobtain a similar separation result between Las Vegas one-way probabilistic\\none-counter automaton and one-way deterministic one-counter automaton.\\n  We also obtain new results on classical counter automata regarding language\\nrecognition. It was conjectured that one-way probabilistic one blind-counter\\nautomata cannot recognize Kleene closure of equality language [A. Yakaryilmaz:\\nSuperiority of one-way and realtime quantum machines. RAIRO - Theor. Inf. and\\nApplic. 46(4): 615-641 (2012)]. We show that this conjecture is false, and also\\nshow several separation results for blind/non-blind counter automata.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1502.00353</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1502.00353</id><submitter>Sebastian Goncalves Dr</submitter><version version=\"v1\"><date>Mon, 2 Feb 2015 04:25:06 GMT</date><size>5880kb</size><source_type>D</source_type></version><title>Complex networks vulnerability to module-based attacks</title><authors>Bruno Requi\\\\~ao da Cunha, Juan Carlos Gonz\\\\\\'alez-Avella, and\\n  Sebasti\\\\\\'an Gon\\\\c{c}alves</authors><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>8 pages, 8 figures</comments><doi>10.1371/journal.pone.0142824</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the multidisciplinary field of Network Science, optimization of procedures\\nfor efficiently breaking complex networks is attracting much attention from\\npractical points of view. In this contribution we present a module-based method\\nto efficiently break complex networks. The procedure first identifies the\\ncommunities in which the network can be represented, then it deletes the nodes\\n(edges) that connect different modules by its order in the betweenness\\ncentrality ranking list. We illustrate the method by applying it to various\\nwell known examples of social, infrastructure, and biological networks. We show\\nthat the proposed method always outperforms vertex (edge) attacks which are\\nbased on the ranking of node (edge) degree or centrality, with a huge gain in\\nefficiency for some examples. Remarkably, for the US power grid, the present\\nmethod breaks the original network of 4941 nodes to many fragments smaller than\\n197 nodes (4% of the original size) by removing mere 164 nodes (~3%) identified\\nby the procedure. By comparison, any degree or centrality based procedure,\\ndeleting the same amount of nodes, removes only 22% of the original network,\\ni.e. more than 3800 nodes continue to be connected after that\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1506.03410</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1506.03410</id><submitter>Benjamin Falk</submitter><version version=\"v1\"><date>Wed, 10 Jun 2015 17:55:51 GMT</date><size>208kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 4 May 2016 18:14:39 GMT</date><size>139kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 19 Mar 2018 21:51:16 GMT</date><size>2686kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Wed, 10 Oct 2018 00:36:04 GMT</date><size>330kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Mon, 30 Sep 2019 21:35:19 GMT</date><size>170kb</size><source_type>D</source_type></version><version version=\"v6\"><date>Thu, 3 Oct 2019 14:04:32 GMT</date><size>170kb</size><source_type>D</source_type></version><title>Sparse Projection Oblique Randomer Forests</title><authors>Tyler M. Tomita, James Browne, Cencheng Shen, Jaewon Chung, Jesse L.\\n  Patsolic, Benjamin Falk, Jason Yim, Carey E. Priebe, Randal Burns, Mauro\\n  Maggioni, Joshua T. Vogelstein</authors><categories>stat.ML cs.LG</categories><comments>31 pages; submitted to Journal of Machine Learning Research for\\n  review</comments><msc-class>68T10</msc-class><acm-class>I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision forests, including Random Forests and Gradient Boosting Trees, have\\nrecently demonstrated state-of-the-art performance in a variety of machine\\nlearning settings. Decision forests are typically ensembles of axis-aligned\\ndecision trees; that is, trees that split only along feature dimensions. In\\ncontrast, many recent extensions to decision forests are based on axis-oblique\\nsplits. Unfortunately, these extensions forfeit one or more of the favorable\\nproperties of decision forests based on axis-aligned splits, such as robustness\\nto many noise dimensions, interpretability, or computational efficiency. We\\nintroduce yet another decision forest, called &quot;Sparse Projection Oblique\\nRandomer Forests&quot; (SPORF). SPORF uses very sparse random projections, i.e.,\\nlinear combinations of a small subset of features. SPORF significantly improves\\naccuracy over existing state-of-the-art algorithms on a standard benchmark\\nsuite for classification with &gt;100 problems of varying dimension, sample size,\\nand number of classes. To illustrate how SPORF addresses the limitations of\\nboth axis-aligned and existing oblique decision forest methods, we conduct\\nextensive simulated experiments. SPORF typically yields improved performance\\nover existing decision forests, while mitigating computational efficiency and\\nscalability and maintaining interpretability. SPORF can easily be incorporated\\ninto other ensemble methods such as boosting to obtain potentially similar\\ngains.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1511.02074</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1511.02074</id><submitter>Marcin Bienkowski</submitter><version version=\"v1\"><date>Fri, 6 Nov 2015 13:34:01 GMT</date><size>74kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 28 Dec 2015 17:23:01 GMT</date><size>75kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 18 Jul 2016 17:14:11 GMT</date><size>80kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Wed, 29 Nov 2017 09:30:03 GMT</date><size>69kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Tue, 1 Oct 2019 21:50:56 GMT</date><size>86kb</size><source_type>D</source_type></version><title>Dynamic Balanced Graph Partitioning</title><authors>Chen Avin and Marcin Bienkowski and Andreas Loukas and Maciej Pacut\\n  and Stefan Schmid</authors><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper initiates the study of the classic balanced graph partitioning\\nproblem from an online perspective: Given an arbitrary sequence of pairwise\\ncommunication requests between $n$ nodes, with patterns that may change over\\ntime, the objective is to service these requests efficiently by partitioning\\nthe nodes into $\\\\ell$ clusters, each of size $k$, such that frequently\\ncommunicating nodes are located in the same cluster. The partitioning can be\\nupdated dynamically by migrating nodes between clusters. The goal is to devise\\nonline algorithms which jointly minimize the amount of inter-cluster\\ncommunication and migration cost.\\n  The problem features interesting connections to other well-known online\\nproblems. For example, scenarios with $\\\\ell=2$ generalize online paging, and\\nscenarios with $k=2$ constitute a novel online variant of maximum matching. We\\npresent several lower bounds and algorithms for settings both with and without\\ncluster-size augmentation. In particular, we prove that any deterministic\\nonline algorithm has a competitive ratio of at least $k$, even with significant\\naugmentation. Our main algorithmic contributions are an $O(k\\n\\\\log{k})$-competitive deterministic algorithm for the general setting with\\nconstant augmentation, and a constant competitive algorithm for the maximum\\nmatching variant.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1511.05219</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1511.05219</id><submitter>James Zou</submitter><version version=\"v1\"><date>Mon, 16 Nov 2015 23:36:25 GMT</date><size>359kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 6 Oct 2016 04:53:06 GMT</date><size>496kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 8 Oct 2019 01:14:03 GMT</date><size>504kb</size><source_type>D</source_type></version><title>How much does your data exploration overfit? Controlling bias via\\n  information usage</title><authors>Daniel Russo and James Zou</authors><categories>stat.ML cs.LG</categories><comments>Accepted at IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern data is messy and high-dimensional, and it is often not clear a priori\\nwhat are the right questions to ask. Instead, the analyst typically needs to\\nuse the data to search for interesting analyses to perform and hypotheses to\\ntest. This is an adaptive process, where the choice of analysis to be performed\\nnext depends on the results of the previous analyses on the same data.\\nUltimately, which results are reported can be heavily influenced by the data.\\nIt is widely recognized that this process, even if well-intentioned, can lead\\nto biases and false discoveries, contributing to the crisis of reproducibility\\nin science. But while %the adaptive nature of exploration any data-exploration\\nrenders standard statistical theory invalid, experience suggests that different\\ntypes of exploratory analysis can lead to disparate levels of bias, and the\\ndegree of bias also depends on the particulars of the data set. In this paper,\\nwe propose a general information usage framework to quantify and provably bound\\nthe bias and other error metrics of an arbitrary exploratory analysis. We prove\\nthat our mutual information based bound is tight in natural settings, and then\\nuse it to give rigorous insights into when commonly used procedures do or do\\nnot lead to substantially biased estimation. Through the lens of information\\nusage, we analyze the bias of specific exploration procedures such as\\nfiltering, rank selection and clustering. Our general framework also naturally\\nmotivates randomization techniques that provably reduces exploration bias while\\npreserving the utility of the data analysis. We discuss the connections between\\nour approach and related ideas from differential privacy and blinded data\\nanalysis, and supplement our results with illustrative simulations.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1511.05732</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1511.05732</id><submitter>Akrati Saxena</submitter><version version=\"v1\"><date>Wed, 18 Nov 2015 11:04:23 GMT</date><size>181kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 6 Oct 2019 10:38:30 GMT</date><size>191kb</size><source_type>D</source_type></version><title>Estimating the Degree Centrality Ranking of a Node</title><authors>Akrati Saxena, Vaibhav Malik, S. R. S. Iyengar</authors><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex networks have gained more attention from the last few years. The size\\nof real-world complex networks, such as online social networks, WWW network,\\ncollaboration networks, is increasing exponentially with time. It is not\\nfeasible to collect the complete data and store and process it. In the present\\nwork, we propose a method to estimate the degree centrality rank of a node\\nwithout having the complete structure of the graph. The proposed algorithm uses\\nthe degree of a node and power-law exponent of the degree distribution to\\ncalculate the ranking. Simulation results on the Barabasi-Albert networks show\\nthat the average error in the estimated ranking is approximately $5\\\\%$ of the\\ntotal number of nodes.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1601.06672</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1601.06672</id><submitter>Jakub Mare\\\\v{c}ek</submitter><version version=\"v1\"><date>Mon, 25 Jan 2016 16:55:30 GMT</date><size>2542kb</size><source_type>D</source_type></version><title>Pricing Vehicle Sharing with Proximity Information</title><authors>Jakub Marecek, Robert Shorten, Jia Yuan Yu</authors><categories>math.OC cs.AI cs.MA</categories><doi>10.1109/ICBDSC.2016.7460378</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For vehicle sharing schemes, where drop-off positions are not fixed, we\\npropose a pricing scheme, where the price depends in part on the distance\\nbetween where a vehicle is being dropped off and where the closest shared\\nvehicle is parked. Under certain restrictive assumptions, we show that this\\npricing leads to a socially optimal spread of the vehicles within a region.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1602.08230</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1602.08230</id><submitter>Matthias Mnich</submitter><version version=\"v1\"><date>Fri, 26 Feb 2016 08:07:11 GMT</date><size>52kb</size></version><version version=\"v2\"><date>Thu, 2 Mar 2017 16:22:36 GMT</date><size>57kb</size></version><version version=\"v3\"><date>Sat, 5 Oct 2019 16:34:29 GMT</date><size>73kb</size></version><title>Stable Marriage with Covering Constraints: A Complete Computational\\n  Trichotomy</title><authors>Matthias Mnich and Ildik\\\\\\'o Schlotter</authors><categories>cs.DS</categories><comments>40 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider Stable Marriage with Covering Constraints (SMC): in this variant\\nof Stable Marriage, we distinguish a subset of women as well as a subset of\\nmen, and we seek a matching with fewest number of blocking pairs that matches\\nall of the distinguished people. We investigate how a set of natural\\nparameters, namely the maximum length of preference lists for men and women,\\nthe number of distinguished men and women, and the number of blocking pairs\\nallowed determine the computational tractability of this problem. Our main\\nresult is a complete complexity trichotomy that, for each choice of the studied\\nparameters, classifies SMC as polynomial-time solvable, NP-hard and\\nfixed-parameter tractable, or NP-hard and W[1]-hard. We also classify all cases\\nof one-sided constraints where only women may be distinguished.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1603.03205</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1603.03205</id><submitter>Davis Issac</submitter><version version=\"v1\"><date>Thu, 10 Mar 2016 10:12:11 GMT</date><size>25kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 11 Apr 2016 13:37:36 GMT</date><size>33kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 19 May 2016 12:50:55 GMT</date><size>33kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Fri, 11 Nov 2016 12:47:42 GMT</date><size>185kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Tue, 1 Oct 2019 17:28:44 GMT</date><size>192kb</size><source_type>D</source_type></version><title>Hadwiger\\'s Conjecture for squares of 2-Trees</title><authors>L. Sunil Chandran, Davis Issac, Sanming Zhou</authors><categories>math.CO cs.DM</categories><comments>18 pages, 3 figures</comments><journal-ref>European Journal of Combinatorics 76 (2019): 159-174</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hadwiger\\'s conjecture asserts that any graph contains a clique minor with\\norder no less than the chromatic number of the graph. We prove that this\\nwell-known conjecture is true for all graphs if and only if it is true for\\nsquares of split graphs. This observation implies that Hadwiger\\'s conjecture\\nfor squares of chordal graphs is as difficult as the general case, since\\nchordal graphs are a superclass of split graphs. Then we consider 2-trees which\\nare a subclass of each of planar graphs, 2-degenerate graphs and chordal\\ngraphs. We prove that Hadwiger\\'s conjecture is true for squares of $2$-trees.\\nWe achieve this by proving the following stronger result: for any $2$-tree $T$,\\nits square $T^2$ has a clique minor of order $\\\\chi(T^2)$ for which each branch\\nset induces a path, where $\\\\chi(T^2)$ is the chromatic number of $T^2$.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1603.06822</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1603.06822</id><submitter>Tony Huynh</submitter><version version=\"v1\"><date>Tue, 22 Mar 2016 15:14:38 GMT</date><size>13kb</size></version><version version=\"v2\"><date>Fri, 29 Apr 2016 11:57:55 GMT</date><size>14kb</size></version><version version=\"v3\"><date>Wed, 13 Jul 2016 22:53:55 GMT</date><size>15kb</size></version><version version=\"v4\"><date>Wed, 21 Dec 2016 16:24:25 GMT</date><size>15kb</size></version><version version=\"v5\"><date>Wed, 2 Oct 2019 06:49:27 GMT</date><size>19kb</size></version><title>The matroid secretary problem for minor-closed classes and random\\n  matroids</title><authors>Tony Huynh and Peter Nelson</authors><categories>math.CO cs.DM</categories><comments>15 pages, 0 figures</comments><msc-class>05B35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that for every proper minor-closed class $M$ of matroids\\nrepresentable over a prime field, there exists a constant-competitive matroid\\nsecretary algorithm for the matroids in $M$. This result relies on the\\nextremely powerful matroid minor structure theory being developed by Geelen,\\nGerards and Whittle.\\n  We also note that for asymptotically almost all matroids, the matroid\\nsecretary algorithm that selects a random basis, ignoring weights, is\\n$(2+o(1))$-competitive. In fact, assuming the conjecture that almost all\\nmatroids are paving, there is a $(1+o(1))$-competitive algorithm for almost all\\nmatroids.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1606.02087</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1606.02087</id><submitter>Pablo Martinez Olmos</submitter><version version=\"v1\"><date>Tue, 7 Jun 2016 10:25:52 GMT</date><size>847kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 13 Dec 2016 08:06:25 GMT</date><size>848kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 2 Oct 2019 09:45:15 GMT</date><size>2937kb</size></version><title>Continuous Transmission of Spatially-Coupled LDPC Code Chains</title><authors>Pablo M. Olmos, David G. M. Mitchell, Dmitri Truhachev, Daniel J.\\n  Costello Jr</authors><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1402.7170</comments><journal-ref>IEEE Transactions on Communications, December 2017. Pages 5097 -\\n  5109</journal-ref><doi>10.1109/TCOMM.2017.2737018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel encoding/transmission scheme called continuous chain (CC)\\ntransmission that is able to improve the finite-length performance of a system\\nusing spatially-coupled low-density parity-check (SC-LDPC) codes. In CC\\ntransmission, instead of transmitting a sequence of independent codewords from\\na terminated SC-LDPC code chain, we connect multiple chains in a layered\\nformat, where encoding, transmission, and decoding are now performed in a\\ncontinuous fashion. The connections between chains are created at specific\\npoints, chosen to improve the finite-length performance of the code structure\\nunder iterative decoding. We describe the design of CC schemes for different\\nSC-LDPC code ensembles constructed from protographs: a (J,K)-regular SC-LDPC\\ncode chain, a spatially-coupled repeat-accumulate (SC-RA) code, and a\\nspatially-coupled accumulate-repeat-jagged-accumulate (SC- ARJA) code. In all\\ncases, significant performance improvements are reported and, in addition, it\\nis shown that using CC transmission only requires a small increase in decoding\\ncomplexity and decoding delay with respect to a system employing a single\\nSC-LDPC code chain for transmission.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1606.05626</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1606.05626</id><submitter>Justin Yirka</submitter><version version=\"v1\"><date>Fri, 17 Jun 2016 19:09:38 GMT</date><size>31kb</size></version><version version=\"v2\"><date>Fri, 12 Jan 2018 16:20:56 GMT</date><size>39kb</size></version><version version=\"v3\"><date>Mon, 16 Sep 2019 20:49:22 GMT</date><size>57kb</size><source_type>D</source_type></version><title>The complexity of simulating local measurements on quantum systems</title><authors>Sevag Gharibian and Justin Yirka</authors><categories>quant-ph cond-mat.str-el cs.CC</categories><comments>38 pages. Significant reorganization and addition of background and\\n  clarifying material to improve readability (thank you to anonymous referees)</comments><journal-ref>Quantum 3, 189 (2019)</journal-ref><doi>10.22331/q-2019-09-30-189</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  An important task in quantum physics is the estimation of local quantities\\nfor ground states of local Hamiltonians. Recently, [Ambainis, CCC 2014] defined\\nthe complexity class P^QMA[log], and motivated its study by showing that the\\nphysical task of estimating the expectation value of a local observable against\\nthe ground state of a local Hamiltonian is P^QMA[log]-complete. In this paper,\\nwe continue the study of P^QMA[log], obtaining the following lower and upper\\nbounds.\\n  Lower bounds (hardness results): (1) The P^QMA[log]-completeness result of\\n[Ambainis, CCC 2014] requires O(log n)-local observables and Hamiltonians. We\\nshow that simulating even a single qubit measurement on ground states of\\n5-local Hamiltonians is P^QMA[log]-complete, resolving an open question of\\nAmbainis. (2) We formalize the complexity theoretic study of estimating\\ntwo-point correlation functions against ground states, and show that this task\\nis similarly P^QMA[log]-complete. (3) We identify a flaw in [Ambainis, CCC\\n2014] regarding a P^UQMA[log]-hardness proof for estimating spectral gaps of\\nlocal Hamiltonians. By introducing a &quot;query validation&quot; technique, we build on\\n[Ambainis, CCC 2014] to obtain P^UQMA[log]-hardness for estimating spectral\\ngaps under polynomial-time Turing reductions.\\n  Upper bounds (containment in complexity classes): P^QMA[log] is thought of as\\n&quot;slightly harder&quot; than QMA. We justify this formally by exploiting the\\nhierarchical voting technique of [Beigel, Hemachandra, Wechsung, SCT 1989] to\\nshow P^QMA[log] is in PP. This improves the containment QMA is in PP [Kitaev,\\nWatrous, STOC 2000].\\n  This work contributes a rigorous treatment of the subtlety involved in\\nstudying oracle classes in which the oracle solves a promise problem. This is\\nparticularly relevant for quantum complexity theory, where most natural classes\\nsuch as BQP and QMA are defined as promise classes.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1606.07282</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1606.07282</id><submitter>Irene C\\\\\\'ordoba</submitter><version version=\"v1\"><date>Thu, 23 Jun 2016 12:12:20 GMT</date><size>39kb</size></version><version version=\"v2\"><date>Fri, 29 Jul 2016 16:27:04 GMT</date><size>39kb</size></version><version version=\"v3\"><date>Thu, 14 Dec 2017 11:10:43 GMT</date><size>129kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Thu, 26 Sep 2019 08:35:01 GMT</date><size>111kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Wed, 2 Oct 2019 08:33:09 GMT</date><size>111kb</size><source_type>D</source_type></version><title>A review of Gaussian Markov models for conditional independence</title><authors>Irene C\\\\\\'ordoba, Concha Bielza and Pedro Larra\\\\~naga</authors><categories>stat.ME cs.AI stat.ML</categories><comments>Fix author signature</comments><doi>10.1016/j.jspi.2019.09.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov models lie at the interface between statistical independence in a\\nprobability distribution and graph separation properties. We review model\\nselection and estimation in directed and undirected Markov models with Gaussian\\nparametrization, emphasizing the main similarities and differences. These two\\nmodel classes are similar but not equivalent, although they share a common\\nintersection. We present the existing results from a historical perspective,\\ntaking into account the amount of literature existing from both the artificial\\nintelligence and statistics research communities, where these models were\\noriginated. We cover classical topics such as maximum likelihood estimation and\\nmodel selection via hypothesis testing, but also more modern approaches like\\nregularization and Bayesian methods. We also discuss how the Markov models\\nreviewed fit in the rich hierarchy of other, higher level Markov model classes.\\nFinally, we close the paper overviewing relaxations of the Gaussian assumption\\nand pointing out the main areas of application where these Markov models are\\nnowadays used.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1607.04789</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1607.04789</id><submitter>Thijs Laarhoven</submitter><version version=\"v1\"><date>Sat, 16 Jul 2016 18:59:19 GMT</date><size>258kb</size><source_type>D</source_type></version><title>Sieving for closest lattice vectors (with preprocessing)</title><authors>Thijs Laarhoven</authors><categories>cs.CR cs.CG</categories><comments>23rd Conference on Selected Areas in Cryptography (SAC), 2016</comments><journal-ref>23rd International Conference on Selected Areas in Cryptography\\n  (SAC), pp. 523-542, 2016</journal-ref><doi>10.1007/978-3-319-69453-5_28</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lattice-based cryptography has recently emerged as a prime candidate for\\nefficient and secure post-quantum cryptography. The two main hard problems\\nunderlying its security are the shortest vector problem (SVP) and the closest\\nvector problem (CVP). Various algorithms have been studied for solving these\\nproblems, and for SVP, lattice sieving currently dominates in terms of the\\nasymptotic time complexity: one can heuristically solve SVP in time\\n$2^{0.292d}$ in high dimensions $d$ [BDGL\\'16]. Although several SVP algorithms\\ncan also be used to solve CVP, it is not clear whether this also holds for\\nheuristic lattice sieving methods. The best time complexity for CVP is\\ncurrently $2^{0.377d}$ [BGJ\\'14].\\n  In this paper we revisit sieving algorithms for solving SVP, and study how\\nthese algorithms can be modified to solve CVP and its variants as well. Our\\nfirst method is aimed at solving one problem instance and minimizes the overall\\ntime complexity for a single CVP instance with a time complexity of\\n$2^{0.292d}$. Our second method minimizes the amortized time complexity for\\nseveral instances on the same lattice, at the cost of a larger preprocessing\\ncost. We can solve the closest vector problem with preprocessing (CVPP) with\\n$2^{0.636d}$ space and preprocessing, in $2^{0.136d}$ time, while the query\\ncomplexity can even be reduced to $2^{\\\\epsilon d}$ at the cost of preprocessing\\ntime and memory complexities of $(1/\\\\epsilon)^{O(d)}$.\\n  For easier variants of CVP, such as approximate CVP and bounded distance\\ndecoding (BDD), we further show how the preprocessing method achieves even\\nbetter complexities. For instance, we can solve approximate CVPP with large\\napproximation factors $k$ with polynomial-sized advice in polynomial time if $k\\n= \\\\Omega(\\\\sqrt{d/\\\\log d})$, heuristically closing the gap between the\\ndecision-CVPP result of [AR\\'04] and the search-CVPP result of [DRS\\'14].\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1608.00644</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1608.00644</id><submitter>Yangang Chen</submitter><version version=\"v1\"><date>Mon, 1 Aug 2016 23:51:18 GMT</date><size>1065kb</size></version><version version=\"v2\"><date>Mon, 5 Mar 2018 22:53:32 GMT</date><size>941kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 27 Sep 2019 05:55:05 GMT</date><size>941kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Mon, 7 Oct 2019 02:52:00 GMT</date><size>941kb</size><source_type>D</source_type></version><title>Monotone Mixed Finite Difference Scheme for Monge-Amp\\\\`ere Equation</title><authors>Yangang Chen, Justin W. L. Wan, Jessey Lin</authors><categories>math.NA cs.NA</categories><journal-ref>Journal of Scientific Computing, 76(3), pp.1839-1867 (2018)</journal-ref><doi>10.1007/s10915-018-0685-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a monotone mixed finite difference scheme for\\nsolving the two-dimensional Monge-Amp\\\\`ere equation. In order to accomplish\\nthis, we convert the Monge-Amp\\\\`ere equation to an equivalent\\nHamilton-Jacobi-Bellman (HJB) equation. Based on the HJB formulation, we apply\\nthe standard 7-point stencil discretization, which is second order accurate, to\\nthe grid points wherever monotonicity holds, and apply semi-Lagrangian wide\\nstencil discretization elsewhere to ensure monotonicity on the entire\\ncomputational domain. By dividing the admissible control set into six regions\\nand optimizing the sub-problem in each region, the computational cost of the\\noptimization problem at each grid point is reduced from $O(M^2)$ to $O(1)$ when\\nthe standard 7-point stencil discretization is applied and to $O(M)$ otherwise,\\nwhere the discretized control set is $M \\\\times M$. We prove that our numerical\\nscheme satisfies consistency, stability, monotonicity and strong comparison\\nprinciple, and hence is convergent to the viscosity solution of the\\nMonge-Amp\\\\`ere equation. In the numerical results, second order convergence\\nrate is achieved when the standard 7-point stencil discretization is applied\\nmonotonically on the entire computation domain, and up to order one convergence\\nis achieved otherwise. The proposed mixed scheme yields a smaller\\ndiscretization error and a faster convergence rate compared to the pure\\nsemi-Lagrangian wide stencil scheme.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1608.02619</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1608.02619</id><submitter>Sebastian Goncalves Dr</submitter><version version=\"v1\"><date>Mon, 8 Aug 2016 20:47:44 GMT</date><size>209kb</size><source_type>D</source_type></version><title>Performance of attack strategies on modular networks</title><authors>Bruno Requi\\\\~ao da Cunha and Sebasti\\\\\\'an Gon\\\\c{c}alves</authors><categories>physics.soc-ph cs.SI</categories><comments>14 pages, 4 figures, pre-print</comments><doi>10.1093/comnet/cnx015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vulnerabilities of complex networks have became a trend topic in complex\\nsystems recently due to its real world applications. Most real networks tend to\\nbe very fragile to high betweenness adaptive attacks. However, recent\\ncontributions have shown the importance of interconnected nodes in the\\nintegrity of networks and module-based attacks have appeared promising when\\ncompared to traditional malicious non-adaptive attacks. In the present work we\\ndeeply explore the trade-off associated with attack procedures, introducing a\\ngeneralized robustness measure and presenting an attack performance index that\\ntakes into account both robustness of the network against the attack and the\\nrun-time needed to obtained the list of targeted nodes for the attack. Besides,\\nwe introduce the concept of deactivation point aimed to mark the point at which\\nthe network stops to function properly. We then show empirically that\\nnon-adaptive module-based attacks perform better than high degree and\\nbetweenness adaptive attacks in networks with well defined community structures\\nand consequent high modularity.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1608.03368</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1608.03368</id><submitter>Arash Rafiey</submitter><version version=\"v1\"><date>Thu, 11 Aug 2016 04:53:53 GMT</date><size>77kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 22 Aug 2016 23:57:57 GMT</date><size>77kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 29 Dec 2016 02:53:32 GMT</date><size>157kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Mon, 7 Oct 2019 20:24:57 GMT</date><size>669kb</size><source_type>D</source_type></version><title>Bi-Arc Digraphs and Conservative Polymorphisms</title><authors>Pavol Hell and Akbar Rafiey and Arash Rafiey</authors><categories>cs.DS cs.CC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on the class of bi-arc digraphs, which are also known as\\nmin-orderable digraphs, or signed-interval digraphs.\\n  Bi-arc digraphs are a broad and natural class of digraphs generalizing\\ninterval graphs, perhaps the broadest generalization which still admits nice\\nalgorithms and characterizations. This is best seen when viewing them as\\nsigned-interval digraphs: they include interval graphs (corresponding to\\nprecisely reflexive symmetric digraphs), adjusted interval digraphs\\n(corresponding to precisely reflexive digraphs), complements of threshold\\ntolerance graphs (corresponding to precisely symmetric digraphs), and\\ntwo-directional orthogonal ray graphs (corresponding to precisely bipartite\\ndigraphs oriented from one part to the other).\\n  In the language of polymorphisms, they constitute precisely the class of\\ndigraphs that admit a conservative semi-lattice polymorphism, or a conservative\\nset polymorphism, or cyclic polymorphisms of all arities, or totally symmetric\\nconservative polymorphisms of all arities.\\n  We give an obstruction characterization of this class of digraphs.\\nSpecifically, we identify a digraph structure we call a strong circuit which is\\nan obstruction to min ordering, and prove that digraphs without strong circuits\\nadmit a min ordering (i.e., are bi-arc digraphs). This characterization yields\\na polynomial time recognition algorithm for min-orderable digraphs. The\\nexistence of a polynomial time algorithm for min-orderability was an open\\nproblem due to Bagan, Durand, Filiot, and Gauwin.\\n  We also discuss a generalization to k-arc digraphs, and corresponding\\nextended min orderings.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1608.03580</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1608.03580</id><submitter>Erik Waingarten</submitter><version version=\"v1\"><date>Thu, 11 Aug 2016 19:50:00 GMT</date><size>326kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 21 May 2017 16:57:47 GMT</date><size>334kb</size><source_type>D</source_type></version><title>Optimal Hashing-based Time-Space Trade-offs for Approximate Near\\n  Neighbors</title><authors>Alexandr Andoni and Thijs Laarhoven and Ilya Razenshteyn and Erik\\n  Waingarten</authors><categories>cs.DS cs.CC cs.CG cs.IR</categories><comments>62 pages, 5 figures; a merger of arXiv:1511.07527 [cs.DS] and\\n  arXiv:1605.02701 [cs.DS], which subsumes both of the preprints. New version\\n  contains more elaborated proofs and fixed some typos</comments><doi>10.1137/1.9781611974782.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  [See the paper for the full abstract.]\\n  We show tight upper and lower bounds for time-space trade-offs for the\\n$c$-Approximate Near Neighbor Search problem. For the $d$-dimensional Euclidean\\nspace and $n$-point datasets, we develop a data structure with space $n^{1 +\\n\\\\rho_u + o(1)} + O(dn)$ and query time $n^{\\\\rho_q + o(1)} + d n^{o(1)}$ for\\nevery $\\\\rho_u, \\\\rho_q \\\\geq 0$ such that: \\\\begin{equation} c^2 \\\\sqrt{\\\\rho_q} +\\n(c^2 - 1) \\\\sqrt{\\\\rho_u} = \\\\sqrt{2c^2 - 1}. \\\\end{equation}\\n  This is the first data structure that achieves sublinear query time and\\nnear-linear space for every approximation factor $c &gt; 1$, improving upon\\n[Kapralov, PODS 2015]. The data structure is a culmination of a long line of\\nwork on the problem for all space regimes; it builds on Spherical\\nLocality-Sensitive Filtering [Becker, Ducas, Gama, Laarhoven, SODA 2016] and\\ndata-dependent hashing [Andoni, Indyk, Nguyen, Razenshteyn, SODA 2014] [Andoni,\\nRazenshteyn, STOC 2015].\\n  Our matching lower bounds are of two types: conditional and unconditional.\\nFirst, we prove tightness of the whole above trade-off in a restricted model of\\ncomputation, which captures all known hashing-based approaches. We then show\\nunconditional cell-probe lower bounds for one and two probes that match the\\nabove trade-off for $\\\\rho_q = 0$, improving upon the best known lower bounds\\nfrom [Panigrahy, Talwar, Wieder, FOCS 2010]. In particular, this is the first\\nspace lower bound (for any static data structure) for two probes which is not\\npolynomially smaller than the one-probe bound. To show the result for two\\nprobes, we establish and exploit a connection to locally-decodable codes.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1609.01100</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1609.01100</id><submitter>Yariv Aizenbud</submitter><version version=\"v1\"><date>Mon, 5 Sep 2016 11:08:34 GMT</date><size>1100kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 14:48:05 GMT</date><size>4517kb</size><source_type>D</source_type></version><title>A max-cut approach to heterogeneity in cryo-electron microscopy</title><authors>Yariv Aizenbud and Yoel Shkolnisky</authors><categories>cs.CV math.OC q-bio.BM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of cryo-electron microscopy has made astounding advancements in the\\npast few years, mainly due to advancements in electron detectors\\' technology.\\nYet, one of the key open challenges of the field remains the processing of\\nheterogeneous data sets, produced from samples containing particles at several\\ndifferent conformational states. For such data sets, the algorithms must\\ninclude some classification procedure to identify homogeneous groups within the\\ndata, so that the images in each group correspond to the same underlying\\nstructure. The fundamental importance of the heterogeneity problem in\\ncryo-electron microscopy has drawn many research efforts, and resulted in\\nsignificant progress in classification algorithms for heterogeneous data sets.\\nWhile these algorithms are extremely useful and effective in practice, they\\nlack rigorous mathematical analysis and performance guarantees.\\n  In this paper, we attempt to make the first steps towards rigorous\\nmathematical analysis of the heterogeneity problem in cryo-electron microscopy.\\nTo that end, we present an algorithm for processing heterogeneous data sets,\\nand prove accuracy and stability bounds for it. We also suggest an extension of\\nthis algorithm that combines the classification and reconstruction steps. We\\ndemonstrate it on simulated data, and compare its performance to the\\nstate-of-the-art algorithm in RELION.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1610.06497</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1610.06497</id><submitter>Giovanni Luca Ciampaglia</submitter><version version=\"v1\"><date>Thu, 20 Oct 2016 16:57:36 GMT</date><size>2070kb</size><source_type>D</source_type></version><title>Information Overload in Group Communication: From Conversation to\\n  Cacophony in the Twitch Chat</title><authors>Azadeh Nematzadeh, Giovanni Luca Ciampaglia, Yong-Yeol Ahn, Alessandro\\n  Flammini</authors><categories>cs.SI cs.HC physics.soc-ph</categories><comments>25 pages, 8 figures</comments><journal-ref>Nematzadeh et al. 2019. R. Soc. open sci. 6: 191412</journal-ref><doi>10.1098/rsos.191412</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online communication channels, especially social web platforms, are rapidly\\nreplacing traditional ones. Online platforms allow users to overcome physical\\nbarriers, enabling worldwide participation. However, the power of online\\ncommunication bears an important negative consequence --- we are exposed to too\\nmuch information to process. Too many participants, for example, can turn\\nonline public spaces into noisy, overcrowded fora where no meaningful\\nconversation can be held. Here we analyze a large dataset of public chat logs\\nfrom Twitch, a popular video streaming platform, in order to examine how\\ninformation overload affects online group communication. We measure structural\\nand textual features of conversations such as user output, interaction, and\\ninformation content per message across a wide range of information loads. Our\\nanalysis reveals the existence of a transition from a conversational state to a\\ncacophony --- a state of overload with lower user participation, more\\ncopy-pasted messages, and less information per message. These results hold both\\non average and at the individual level for the majority of users. This study\\nprovides a quantitative basis for further studies of the social effects of\\ninformation overload, and may guide the design of more resilient online\\ncommunication systems.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1610.07703</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1610.07703</id><submitter>Ilya Safro</submitter><version version=\"v1\"><date>Tue, 25 Oct 2016 01:50:24 GMT</date><size>720kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 15 Oct 2017 04:06:39 GMT</date><size>887kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 4 Oct 2019 14:37:41 GMT</date><size>377kb</size><source_type>D</source_type></version><title>Scalable Dynamic Topic Modeling with Clustered Latent Dirichlet\\n  Allocation (CLDA)</title><authors>Chris Gropp, Alexander Herzog, Ilya Safro, Paul W. Wilson, Amy W. Apon</authors><categories>cs.IR stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topic modeling, a method for extracting the underlying themes from a\\ncollection of documents, is an increasingly important component of the design\\nof intelligent systems enabling the sense-making of highly dynamic and diverse\\nstreams of text data. Traditional methods such as Dynamic Topic Modeling (DTM)\\ndo not lend themselves well to direct parallelization because of dependencies\\nfrom one time step to another. In this paper, we introduce and empirically\\nanalyze Clustered Latent Dirichlet Allocation (CLDA), a method for extracting\\ndynamic latent topics from a collection of documents. Our approach is based on\\ndata decomposition in which the data is partitioned into segments, followed by\\ntopic modeling on the individual segments. The resulting local models are then\\ncombined into a global solution using clustering. The decomposition and\\nresulting parallelization leads to very fast runtime even on very large\\ndatasets. Our approach furthermore provides insight into how the composition of\\ntopics changes over time and can also be applied using other data partitioning\\nstrategies over any discrete features of the data, such as geographic features\\nor classes of users. In this paper CLDA is applied successfully to seventeen\\nyears of NIPS conference papers (2,484 documents and 3,280,697 words),\\nseventeen years of computer science journal abstracts (533,560 documents and\\n32,551,540 words), and to forty years of the PubMed corpus (4,025,978 documents\\nand 273,853,980 words).\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1611.00502</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1611.00502</id><submitter>John Livieratos</submitter><version version=\"v1\"><date>Wed, 2 Nov 2016 08:33:13 GMT</date><size>12kb</size></version><version version=\"v2\"><date>Mon, 15 Jan 2018 00:06:17 GMT</date><size>28kb</size></version><version version=\"v3\"><date>Wed, 7 Nov 2018 18:03:30 GMT</date><size>21kb</size></version><version version=\"v4\"><date>Tue, 1 Oct 2019 22:51:38 GMT</date><size>37kb</size></version><title>Directed Lov\\\\\\'asz Local Lemma and Shearer\\'s Lemma</title><authors>Lefteris Kirousis, John Livieratos and Kostas I. Psaromiligkos</authors><categories>math.CO cs.DM</categories><comments>23 pages, 1 figure. Revised, as it will appear in Springer\\'s journal:\\n  Annals of Mathematics and Artificial Intelligence</comments><msc-class>68W20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Moser and Tardos (2010) gave an algorithmic proof of the lopsided Lov\\\\\\'asz\\nlocal lemma (LLL) in the variable framework, where each of the undesirable\\nevents is assumed to depend on a subset of a collection of independent random\\nvariables. For the proof, they define a notion of a lopsided dependency between\\nthe events suitable for this framework. In this work, we strengthen this\\nnotion, defining a novel directed notion of dependency and prove LLL for the\\ncorresponding graph. We show that this graph can be strictly sparser (thus the\\nsufficient condition for LLL weaker) compared with graphs that correspond to\\nother extant lopsided versions of dependency. Thus, in a sense, we address the\\nproblem &quot;find other simple local conditions for the constraints (in the\\nvariable framework) that advantageously translate to some abstract lopsided\\ncondition&quot; posed by Szegedy (2013). We also give an example where our notion of\\ndependency graph gives better results than the classical Shearer lemma.\\nFinally, we prove Shearer\\'s lemma for the dependency graph we define. For the\\nproofs, we perform a direct probabilistic analysis that yields an exponentially\\nsmall upper bound for the probability of the algorithm that searches for the\\ndesired assignment to the variables not to return a correct answer within $n$\\nsteps. In contrast, the method of proof that became known as the entropic\\nmethod, gives an estimate of only the expectation of the number of steps until\\nthe algorithm returns a correct answer, unless the probabilities are tinkered\\nwith.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1701.02161</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1701.02161</id><submitter>Martin Vohralik</submitter><version version=\"v1\"><date>Mon, 9 Jan 2017 13:00:28 GMT</date><size>51kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 20 Aug 2018 20:06:52 GMT</date><size>376kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 30 Sep 2019 15:06:03 GMT</date><size>371kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Fri, 4 Oct 2019 11:39:15 GMT</date><size>371kb</size><source_type>D</source_type></version><title>Stable broken $H^1$ and $\\\\bf H(\\\\mathrm{div})$ polynomial extensions for\\n  polynomial-degree-robust potential and flux reconstruction in three space\\n  dimensions</title><authors>Alexandre Ern and Martin Vohral\\\\\\'ik</authors><categories>math.NA cs.NA</categories><journal-ref>Mathematics of Computation, 2019</journal-ref><doi>10.1090/mcom/3482</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study extensions of piecewise polynomial data prescribed on faces and\\npossibly in elements of a patch of simplices sharing a vertex. In the $H^1$\\nsetting, we look for functions whose jumps across the faces are prescribed,\\nwhereas in the ${\\\\bf H}(\\\\mathrm{div})$ setting, the normal component jumps and\\nthe piecewise divergence are prescribed. We show stability in the sense that\\nthe minimizers over piecewise polynomial spaces of the same degree as the data\\nare subordinate in the broken energy norm to the minimizers over the whole\\nbroken $H^1$ and ${\\\\bf H}(\\\\mathrm{div})$ spaces. Our proofs are constructive\\nand yield constants independent of the polynomial degree. One particular\\napplication of these results is in a posteriori error analysis, where the\\npresent results justify polynomial-degree-robust efficiency of potential and\\nflux reconstructions.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1701.08711</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1701.08711</id><submitter>Yan Chi Vinci Chow</submitter><version version=\"v1\"><date>Mon, 30 Jan 2017 17:14:25 GMT</date><size>415kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 1 Feb 2017 17:41:35 GMT</date><size>416kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 14 Feb 2017 16:36:38 GMT</date><size>578kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Wed, 13 Feb 2019 08:34:32 GMT</date><size>1318kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Tue, 8 Oct 2019 16:25:45 GMT</date><size>689kb</size><source_type>D</source_type></version><title>Predicting Auction Price of Vehicle License Plate with Deep Recurrent\\n  Neural Network</title><authors>Vinci Chow</authors><categories>cs.CL cs.LG q-fin.EC stat.ML</categories><doi>10.1016/j.eswa.2019.113008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Chinese societies, superstition is of paramount importance, and vehicle\\nlicense plates with desirable numbers can fetch very high prices in auctions.\\nUnlike other valuable items, license plates are not allocated an estimated\\nprice before auction. I propose that the task of predicting plate prices can be\\nviewed as a natural language processing (NLP) task, as the value depends on the\\nmeaning of each individual character on the plate and its semantics. I\\nconstruct a deep recurrent neural network (RNN) to predict the prices of\\nvehicle license plates in Hong Kong, based on the characters on a plate. I\\ndemonstrate the importance of having a deep network and of retraining.\\nEvaluated on 13 years of historical auction prices, the deep RNN\\'s predictions\\ncan explain over 80 percent of price variations, outperforming previous models\\nby a significant margin. I also demonstrate how the model can be extended to\\nbecome a search engine for plates and to provide estimates of the expected\\nprice distribution.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1702.05951</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1702.05951</id><submitter>Havana Rika</submitter><version version=\"v1\"><date>Mon, 20 Feb 2017 12:42:07 GMT</date><size>1151kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 31 Jul 2017 12:37:48 GMT</date><size>1356kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 4 Oct 2019 19:01:22 GMT</date><size>1216kb</size><source_type>D</source_type></version><title>Refined Vertex Sparsifiers of Planar Graphs</title><authors>Robert Krauthgamer and Havana (Inbal) Rika</authors><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the following version of cut sparsification. Given a large\\nedge-weighted network $G$ with $k$ terminal vertices, compress it into a\\nsmaller network $H$ with the same terminals, such that every minimum terminal\\ncut in $H$ approximates the corresponding one in $G$, up to a factor $q\\\\geq 1$\\nthat is called the quality. (The case $q=1$ is known also as a mimicking\\nnetwork). We provide new insights about the structure of minimum terminal cuts,\\nleading to new results for cut sparsifiers of planar graphs. Our first\\ncontribution identifies a subset of the minimum terminal cuts, which we call\\nelementary, that generates all the others. Consequently, $H$ is a cut\\nsparsifier if and only if it preserves all the elementary terminal cuts (up to\\nthis factor $q$). This structural characterization lead to improved bounds on\\nthe size of $H$. For example, it improve the bound of mimicking-network size\\nfor planar graphs into a near-optimal one. Our second and main contribution is\\nto refine the known bounds in terms of $\\\\gamma=\\\\gamma(G)$, which is defined as\\nthe minimum number of faces that are incident to all the terminals in a planar\\ngraph $G$. We prove that the number of elementary terminal cuts is\\n$O((2k/\\\\gamma)^{2\\\\gamma})$ (compared to $O(2^k)$ terminal cuts), and\\nfurthermore obtain a mimicking-network of size $O(\\\\gamma 2^{2\\\\gamma} k^4)$,\\nwhich is near-optimal as a function of $\\\\gamma$. In the analysis we break the\\nelementary terminal cuts into fragments, and count them carefully. Our third\\ncontribution is a duality between cut sparsification and distance\\nsparsification for certain planar graphs, when the sparsifier $H$ is required\\nto be a minor of $G$. This duality connects problems that were previously\\nstudied separately, implying new results, new proofs of known results, and\\nequivalences between open gaps.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1703.04679</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1703.04679</id><submitter>Thomas Ranner</submitter><version version=\"v1\"><date>Tue, 14 Mar 2017 19:30:22 GMT</date><size>219kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 19:21:26 GMT</date><size>1765kb</size><source_type>D</source_type></version><title>A unified theory for continuous in time evolving finite element space\\n  approximations to partial differential equations in evolving domains</title><authors>Charles M. Elliott and Thomas Ranner</authors><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a unified theory for continuous in time finite element\\ndiscretisations of partial differential equations posed in evolving domains\\nincluding the consideration of equations posed on evolving surfaces and bulk\\ndomains as well coupled surface bulk systems. We use an abstract variational\\nsetting with time dependent function spaces and abstract time dependent finite\\nelement spaces. Optimal a priori bounds are shown under usual assumptions on\\nperturbations of bilinear forms and approximation properties of the abstract\\nfinite element spaces. The abstract theory is applied to evolving finite\\nelements in both flat and curved spaces. Evolving bulk and surface\\nisoparametric finite element spaces defined on evolving triangulations are\\ndefined and developed. These spaces are used to define approximations to\\nparabolic equations in general domains for which the abstract theory is shown\\nto apply. Numerical experiments are described which confirm the rates of\\nconvergence.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1703.05502</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1703.05502</id><submitter>Evgeny Burnaev</submitter><version version=\"v1\"><date>Thu, 16 Mar 2017 08:28:11 GMT</date><size>1852kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 19:56:14 GMT</date><size>2158kb</size><source_type>D</source_type></version><title>Steganographic Generative Adversarial Networks</title><authors>Denis Volkhonskiy and Ivan Nazarov and Evgeny Burnaev</authors><categories>cs.MM cs.CR cs.CV stat.AP</categories><comments>15 pages, 10 figures, 5 tables, Workshop on Adversarial Training\\n  (NIPS 2016, Barcelona, Spain)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Steganography is collection of methods to hide secret information (&quot;payload&quot;)\\nwithin non-secret information &quot;container&quot;). Its counterpart, Steganalysis, is\\nthe practice of determining if a message contains a hidden payload, and\\nrecovering it if possible. Presence of hidden payloads is typically detected by\\na binary classifier. In the present study, we propose a new model for\\ngenerating image-like containers based on Deep Convolutional Generative\\nAdversarial Networks (DCGAN). This approach allows to generate more\\nsetganalysis-secure message embedding using standard steganography algorithms.\\nExperiment results demonstrate that the new model successfully deceives the\\nsteganography analyzer, and for this reason, can be used in steganographic\\napplications.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1703.06463</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1703.06463</id><submitter>Lennard Kamenski</submitter><version version=\"v1\"><date>Sun, 19 Mar 2017 16:32:59 GMT</date><size>63kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 24 Feb 2019 19:38:43 GMT</date><size>67kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sat, 5 Oct 2019 17:40:03 GMT</date><size>72kb</size><source_type>D</source_type></version><title>Conditioning of implicit Runge-Kutta integration for finite element\\n  approximation of linear diffusion equations on anisotropic meshes</title><authors>Weizhang Huang, Lennard Kamenski, Jens Lang</authors><categories>math.NA cs.NA</categories><comments>Accepted manuscript. \\\\copyright 2019. Licensed under CC-BY-NC-ND 4.0\\n  (https://creativecommons.org/licenses/by-nc-nd/4.0)</comments><msc-class>65M60, 65M50, 65F35, 65F15</msc-class><acm-class>G.1.3; G.1.8</acm-class><journal-ref>J. Comput. Appl. Math. (2019) 112497</journal-ref><doi>10.1016/j.cam.2019.112497</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The conditioning of implicit Runge-Kutta (RK) integration for linear finite\\nelement approximation of diffusion equations on general anisotropic meshes is\\ninvestigated. Bounds are established for the condition number of the resulting\\nlinear system with and without diagonal preconditioning for the implicit Euler\\nand general implicit RK methods. Two solution strategies are considered for the\\nlinear system resulting from general implicit RK integration: the simultaneous\\nsolution (the system is solved as a whole) and a successive solution which\\nfollows the commonly used implementation of implicit RK methods to first\\ntransform the system into smaller systems using the Jordan normal form of the\\nRK matrix and then solve them successively.\\n  For the simultaneous solution in case of a positive semidefinite symmetric\\npart of the RK coefficient matrix and for the successive solution it is shown\\nthat . If the smallest eigenvalue of the symmetric part of the RK coefficient\\nmatrix is negative and the simultaneous solution strategy is used, an upper\\nbound on the time step is given so that the system matrix is positive definite.\\n  The obtained bounds for the condition number have explicit geometric\\ninterpretations and take the interplay between the diffusion matrix and the\\nmesh geometry into full consideration. They show that there are three\\nmesh-dependent factors that can affect the conditioning: the number of\\nelements, the mesh nonuniformity measured in the Euclidean metric, and the mesh\\nnonuniformity with respect to the inverse of the diffusion matrix. They also\\nreveal that the preconditioning using the diagonal of the system matrix, the\\nmass matrix, or the lumped mass matrix can effectively eliminate the effects of\\nthe mesh nonuniformity measured in the Euclidean metric. Numerical examples are\\ngiven.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1704.01691</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1704.01691</id><submitter>Chunting Zhou</submitter><version version=\"v1\"><date>Thu, 6 Apr 2017 02:36:56 GMT</date><size>863kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 17 Aug 2017 03:22:23 GMT</date><size>863kb</size><source_type>D</source_type></version><title>Multi-space Variational Encoder-Decoders for Semi-supervised Labeled\\n  Sequence Transduction</title><authors>Chunting Zhou and Graham Neubig</authors><categories>cs.CL cs.LG</categories><comments>Accepted by ACL 2017</comments><journal-ref>ACL 2017</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Labeled sequence transduction is a task of transforming one sequence into\\nanother sequence that satisfies desiderata specified by a set of labels. In\\nthis paper we propose multi-space variational encoder-decoders, a new model for\\nlabeled sequence transduction with semi-supervised learning. The generative\\nmodel can use neural networks to handle both discrete and continuous latent\\nvariables to exploit various features of data. Experiments show that our model\\nprovides not only a powerful supervised framework but also can effectively take\\nadvantage of the unlabeled data. On the SIGMORPHON morphological inflection\\nbenchmark, our model outperforms single-model state-of-art results by a large\\nmargin for the majority of languages.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1704.01914</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1704.01914</id><submitter>Dmitriy Zhuk</submitter><version version=\"v1\"><date>Thu, 6 Apr 2017 16:27:29 GMT</date><size>17kb</size><source_type>D</source_type></version><version version=\"v10\"><date>Sun, 29 Sep 2019 19:51:40 GMT</date><size>147kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 14 May 2017 21:06:40 GMT</date><size>56kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 1 Jun 2017 17:28:13 GMT</date><size>55kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Tue, 13 Jun 2017 20:31:08 GMT</date><size>55kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Sun, 25 Jun 2017 11:15:29 GMT</date><size>57kb</size><source_type>D</source_type></version><version version=\"v6\"><date>Thu, 3 Aug 2017 05:11:01 GMT</date><size>58kb</size><source_type>D</source_type></version><version version=\"v7\"><date>Mon, 18 Dec 2017 17:44:49 GMT</date><size>62kb</size><source_type>D</source_type></version><version version=\"v8\"><date>Tue, 20 Feb 2018 19:15:26 GMT</date><size>63kb</size><source_type>D</source_type></version><version version=\"v9\"><date>Wed, 6 Mar 2019 11:00:41 GMT</date><size>63kb</size><source_type>D</source_type></version><title>A Proof of CSP Dichotomy Conjecture</title><authors>Dmitriy Zhuk</authors><categories>cs.CC</categories><comments>The whole paper was rewritten</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many natural combinatorial problems can be expressed as constraint\\nsatisfaction problems. This class of problems is known to be NP-complete in\\ngeneral, but certain restrictions on the form of the constraints can ensure\\ntractability. The standard way to parameterize interesting subclasses of the\\nconstraint satisfaction problem is via finite constraint languages. The main\\nproblem is to classify those subclasses that are solvable in polynomial time\\nand those that are NP-complete. It was conjectured that if a constraint\\nlanguage has a weak near unanimity polymorphism then the corresponding\\nconstraint satisfaction problem is tractable, otherwise it is NP-complete.\\n  In the paper we present an algorithm that solves Constraint Satisfaction\\nProblem in polynomial time for constraint languages having a weak near\\nunanimity polymorphism, which proves the remaining part of the conjecture.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1704.08412</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1704.08412</id><submitter>Danielle Gonzalez</submitter><version version=\"v1\"><date>Thu, 27 Apr 2017 02:22:26 GMT</date><size>127kb</size><source_type>D</source_type></version><title>A Large-Scale Study on the Usage of Testing Patterns that Address\\n  Maintainability Attributes (Patterns for Ease of Modification, Diagnoses, and\\n  Comprehension)</title><authors>Danielle Gonzalez, Joanna C.S. Santos, Andrew Popovich, Mehdi\\n  Mirakhorli, Mei Nagappan</authors><categories>cs.SE</categories><comments>Mining Software Repositories (MSR) 2017 Research Track</comments><journal-ref>017 IEEE/ACM 14th International Conference on Mining Software\\n  Repositories (MSR), Buenos Aires, 2017, pp. 391-401</journal-ref><doi>10.1109/MSR.2017.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Test case maintainability is an important concern, especially in open source\\nand distributed development environments where projects typically have high\\ncontributor turnover with varying backgrounds and experience, and where code\\nownership changes often. Similar to design patterns, patterns for unit testing\\npromote maintainability quality attributes such as ease of diagnoses,\\nmodifiability, and comprehension. In this paper, we report the results of a\\nlarge-scale study on the usage of four xUnit testing patterns which can be used\\nto satisfy these maintainability attributes. This is a first-of-its-kind study\\nwhich developed automated techniques to investigate these issues across 82,447\\nopen source projects, and the findings provide more insight into testing\\npractices in open source projects. Our results indicate that only 17% of\\nprojects had test cases, and from the 251 testing frameworks we studied, 93 of\\nthem were being used. We found 24% of projects with test files implemented\\npatterns that could help with maintainability, while the remaining did not use\\nthese patterns. Multiple qualitative analyses indicate that usage of patterns\\nwas an ad-hoc decision by individual developers, rather than motivated by the\\ncharacteristics of the project, and that developers sometimes used alternative\\ntechniques to address maintainability concerns.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1705.07565</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1705.07565</id><submitter>Xin Dong</submitter><version version=\"v1\"><date>Mon, 22 May 2017 05:54:37 GMT</date><size>139kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 9 Nov 2017 23:50:55 GMT</date><size>727kb</size><source_type>D</source_type></version><title>Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain\\n  Surgeon</title><authors>Xin Dong, Shangyu Chen, Sinno Jialin Pan</authors><categories>cs.NE cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How to develop slim and accurate deep neural networks has become crucial for\\nreal- world applications, especially for those employed in embedded systems.\\nThough previous work along this research line has shown some promising results,\\nmost existing methods either fail to significantly compress a well-trained deep\\nnetwork or require a heavy retraining process for the pruned deep network to\\nre-boost its prediction performance. In this paper, we propose a new layer-wise\\npruning method for deep neural networks. In our proposed method, parameters of\\neach individual layer are pruned independently based on second order\\nderivatives of a layer-wise error function with respect to the corresponding\\nparameters. We prove that the final prediction performance drop after pruning\\nis bounded by a linear combination of the reconstructed errors caused at each\\nlayer. Therefore, there is a guarantee that one only needs to perform a light\\nretraining process on the pruned network to resume its original prediction\\nperformance. We conduct extensive experiments on benchmark datasets to\\ndemonstrate the effectiveness of our pruning method compared with several\\nstate-of-the-art baseline methods.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1706.04902</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1706.04902</id><submitter>Sebastian Ruder</submitter><version version=\"v1\"><date>Thu, 15 Jun 2017 14:46:56 GMT</date><size>1827kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 18 Oct 2017 10:44:06 GMT</date><size>2430kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 30 May 2019 08:59:16 GMT</date><size>2623kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Sun, 6 Oct 2019 10:01:48 GMT</date><size>2666kb</size><source_type>D</source_type></version><title>A Survey Of Cross-lingual Word Embedding Models</title><authors>Sebastian Ruder, Ivan Vuli\\\\\\'c, Anders S{\\\\o}gaard</authors><categories>cs.CL cs.LG</categories><comments>Published in Journal of Artificial Intelligence Research</comments><journal-ref>JAIR 65 (2019) 569-631</journal-ref><doi>10.1613/jair.1.11640</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cross-lingual representations of words enable us to reason about word meaning\\nin multilingual contexts and are a key facilitator of cross-lingual transfer\\nwhen developing natural language processing models for low-resource languages.\\nIn this survey, we provide a comprehensive typology of cross-lingual word\\nembedding models. We compare their data requirements and objective functions.\\nThe recurring theme of the survey is that many of the models presented in the\\nliterature optimize for the same objectives, and that seemingly different\\nmodels are often equivalent modulo optimization strategies, hyper-parameters,\\nand such. We also discuss the different ways cross-lingual word embeddings are\\nevaluated, as well as future challenges and research horizons.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1706.05148</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1706.05148</id><submitter>Bin Dai</submitter><version version=\"v1\"><date>Fri, 16 Jun 2017 05:31:10 GMT</date><size>422kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 23 Jun 2017 10:32:13 GMT</date><size>425kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 27 Nov 2017 09:12:44 GMT</date><size>472kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Wed, 4 Apr 2018 11:09:42 GMT</date><size>503kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Mon, 7 Oct 2019 06:10:39 GMT</date><size>98kb</size><source_type>D</source_type></version><title>Hidden Talents of the Variational Autoencoder</title><authors>Bin Dai and Yu Wang and John Aston and Gang Hua and David Wipf</authors><categories>cs.LG</categories><journal-ref>The Journal of Machine Learning Research, Volume 19 Issue 1,\\n  January 2018 Pages 1573-1614</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variational autoencoders (VAE) represent a popular, flexible form of deep\\ngenerative model that can be stochastically fit to samples from a given random\\nprocess using an information-theoretic variational bound on the true underlying\\ndistribution. Once so-obtained, the model can be putatively used to generate\\nnew samples from this distribution, or to provide a low-dimensional latent\\nrepresentation of existing samples. While quite effective in numerous\\napplication domains, certain important mechanisms which govern the behavior of\\nthe VAE are obfuscated by the intractable integrals and resulting stochastic\\napproximations involved. Moreover, as a highly non-convex model, it remains\\nunclear exactly how minima of the underlying energy relate to original design\\npurposes. We attempt to better quantify these issues by analyzing a series of\\ntractable special cases of increasing complexity. In doing so, we unveil\\ninteresting connections with more traditional dimensionality reduction models,\\nas well as an intrinsic yet underappreciated propensity for robustly dismissing\\nsparse outliers when estimating latent manifolds. With respect to the latter,\\nwe demonstrate that the VAE can be viewed as the natural evolution of recent\\nrobust PCA models, capable of learning nonlinear manifolds of unknown dimension\\nobscured by gross corruptions.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1706.07473</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1706.07473</id><submitter>Pierre Lairez</submitter><version version=\"v1\"><date>Thu, 22 Jun 2017 19:44:28 GMT</date><size>44kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 19 Dec 2018 14:50:16 GMT</date><size>81kb</size></version><version version=\"v3\"><date>Sat, 28 Sep 2019 19:27:19 GMT</date><size>81kb</size></version><title>Computing the homology of basic semialgebraic sets in weak exponential\\n  time</title><authors>Peter B\\\\&quot;urgisser, Felipe Cucker, Pierre Lairez</authors><categories>cs.CG math.AG math.AT</categories><msc-class>14P10</msc-class><journal-ref>Journal of the ACM 66(1), Article 5, 2018</journal-ref><doi>10.1145/3275242</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe and analyze an algorithm for computing the homology (Betti\\nnumbers and torsion coefficients) of basic semialgebraic sets which works in\\nweak exponential time. That is, out of a set of exponentially small measure in\\nthe space of data the cost of the algorithm is exponential in the size of the\\ndata. All algorithms previously proposed for this problem have a complexity\\nwhich is doubly exponential (and this is so for almost all data).\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1706.08818</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1706.08818</id><submitter>Roswitha Bammer</submitter><version version=\"v1\"><date>Tue, 27 Jun 2017 12:39:10 GMT</date><size>138kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 13 Feb 2019 15:38:34 GMT</date><size>480kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 2 Jul 2019 12:57:37 GMT</date><size>1950kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Tue, 1 Oct 2019 12:48:14 GMT</date><size>1641kb</size><source_type>D</source_type></version><title>Gabor frames and deep scattering networks in audio processing</title><authors>Roswitha Bammer, Monika D\\\\&quot;orfler and Pavol Harar</authors><categories>cs.SD cs.LG</categories><comments>26 pages, 8 figures, 4 tables. Repository for reproducibility:\\n  https://gitlab.com/hararticles/gs-gt . Keywords: machine learning; scattering\\n  transform; Gabor transform; deep learning; time-frequency analysis; CNN.\\n  Accepted and published after peer revision</comments><journal-ref>Axioms 2019, 8(4), 106</journal-ref><doi>10.3390/axioms8040106</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces Gabor scattering, a feature extractor based on Gabor\\nframes and Mallat\\'s scattering transform. By using a simple signal model for\\naudio signals specific properties of Gabor scattering are studied. It is shown\\nthat for each layer, specific invariances to certain signal characteristics\\noccur. Furthermore, deformation stability of the coefficient vector generated\\nby the feature extractor is derived by using a decoupling technique which\\nexploits the contractivity of general scattering networks. Deformations are\\nintroduced as changes in spectral shape and frequency modulation. The\\ntheoretical results are illustrated by numerical examples and experiments.\\nNumerical evidence is given by evaluation on a synthetic and a &quot;real&quot; data set,\\nthat the invariances encoded by the Gabor scattering transform lead to higher\\nperformance in comparison with just using Gabor transform, especially when few\\ntraining samples are available.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1707.02877</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1707.02877</id><submitter>Adrien Laurent</submitter><version version=\"v1\"><date>Mon, 10 Jul 2017 14:28:11 GMT</date><size>218kb</size></version><version version=\"v2\"><date>Tue, 1 May 2018 13:25:24 GMT</date><size>275kb</size></version><version version=\"v3\"><date>Fri, 5 Apr 2019 12:42:43 GMT</date><size>275kb</size></version><version version=\"v4\"><date>Mon, 1 Jul 2019 11:54:23 GMT</date><size>270kb</size></version><title>Exotic aromatic B-series for the study of long time integrators for a\\n  class of ergodic SDEs</title><authors>Adrien Laurent and Gilles Vilmart</authors><categories>math.NA cs.NA</categories><comments>33 pages</comments><msc-class>60H35, 37M25, 65L06, 41A58</msc-class><journal-ref>Math. Comp. 89 (2020), 169-202</journal-ref><doi>10.1090/mcom/3455</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new algebraic framework based on a modification (called\\nexotic) of aromatic Butcher-series for the systematic study of the accuracy of\\nnumerical integrators for the invariant measure of a class of ergodic\\nstochastic differential equations (SDEs) with additive noise. The proposed\\nanalysis covers Runge-Kutta type schemes including the cases of partitioned\\nmethods and postprocessed methods. We also show that the introduced exotic\\naromatic B-series satisfy an isometric equivariance property.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1708.02966</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1708.02966</id><submitter>Meena Jagadeesan</submitter><version version=\"v1\"><date>Wed, 9 Aug 2017 18:32:42 GMT</date><size>16kb</size></version><version version=\"v2\"><date>Mon, 30 Sep 2019 02:57:29 GMT</date><size>93kb</size></version><title>Simple Analysis of Sparse, Sign-Consistent JL</title><authors>Meena Jagadeesan</authors><categories>cs.DS math.PR q-bio.NC</categories><comments>Appeared at RANDOM 2019; this is the full version with some\\n  additional appendices</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Allen-Zhu, Gelashvili, Micali, and Shavit construct a sparse, sign-consistent\\nJohnson-Lindenstrauss distribution, and prove that this distribution yields an\\nessentially optimal dimension for the correct choice of sparsity. However,\\ntheir analysis of the upper bound on the dimension and sparsity requires a\\ncomplicated combinatorial graph-based argument similar to Kane and Nelson\\'s\\nanalysis of sparse JL. We present a simple, combinatorics-free analysis of\\nsparse, sign-consistent JL that yields the same dimension and sparsity upper\\nbounds as the original analysis. Our analysis also yields dimension/sparsity\\ntradeoffs, which were not previously known.\\n  As with previous proofs in this area, our analysis is based on applying\\nMarkov\\'s inequality to the pth moment of an error term that can be expressed as\\na quadratic form of Rademacher variables. Interestingly, we show that, unlike\\nin previous work in the area, the traditionally used Hanson-Wright bound is not\\nstrong enough to yield our desired result. Indeed, although the Hanson-Wright\\nbound is known to be optimal for gaussian degree-2 chaos, it was already shown\\nto be suboptimal for Rademachers. Surprisingly, we are able to show a simple\\nmoment bound for quadratic forms of Rademachers that is sufficiently tight to\\nachieve our desired result, which given the ubiquity of moment and tail bounds\\nin theoretical computer science, is likely to be of broader interest.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1708.04321</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1708.04321</id><submitter>Surya Prasath</submitter><version version=\"v1\"><date>Mon, 14 Aug 2017 20:52:35 GMT</date><size>847kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 18 Jun 2019 19:58:50 GMT</date><size>851kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sun, 29 Sep 2019 16:27:25 GMT</date><size>851kb</size><source_type>D</source_type></version><title>Distance and Similarity Measures Effect on the Performance of K-Nearest\\n  Neighbor Classifier -- A Review</title><authors>V. B. Surya Prasath, Haneen Arafat Abu Alfeilat, Ahmad B. A. Hassanat,\\n  Omar Lasassmeh, Ahmad S. Tarawneh, Mahmoud Bashir Alhasanat, Hamzeh S. Eyal\\n  Salman</authors><categories>cs.LG cs.AI</categories><comments>39 pages, 6 figures, 17 tables, revised text and added extra\\n  experiments</comments><doi>10.1089/big.2018.0175</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The K-nearest neighbor (KNN) classifier is one of the simplest and most\\ncommon classifiers, yet its performance competes with the most complex\\nclassifiers in the literature. The core of this classifier depends mainly on\\nmeasuring the distance or similarity between the tested examples and the\\ntraining examples. This raises a major question about which distance measures\\nto be used for the KNN classifier among a large number of distance and\\nsimilarity measures available? This review attempts to answer this question\\nthrough evaluating the performance (measured by accuracy, precision and recall)\\nof the KNN using a large number of distance measures, tested on a number of\\nreal-world datasets, with and without adding different levels of noise. The\\nexperimental results show that the performance of KNN classifier depends\\nsignificantly on the distance used, and the results showed large gaps between\\nthe performances of different distances. We found that a recently proposed\\nnon-convex distance performed the best when applied on most datasets comparing\\nto the other tested distances. In addition, the performance of the KNN with\\nthis top performing distance degraded only about $20\\\\%$ while the noise level\\nreaches $90\\\\%$, this is true for most of the distances used as well. This means\\nthat the KNN classifier using any of the top $10$ distances tolerate noise to a\\ncertain degree. Moreover, the results show that some distances are less\\naffected by the added noise comparing to other distances.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1708.07904</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1708.07904</id><submitter>Romeil Sandhu</submitter><version version=\"v1\"><date>Fri, 25 Aug 2017 22:17:38 GMT</date><size>1246kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 29 Aug 2018 12:47:45 GMT</date><size>1246kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 7 Oct 2019 00:51:07 GMT</date><size>1750kb</size><source_type>D</source_type></version><title>Characterizing Distances of Networks on the Tensor Manifold</title><authors>Bipul Islam, Ji Liu, Romeil Sandhu</authors><categories>cs.SY</categories><comments>This paper is accepted at 8th International Conference on Complex\\n  Networks 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At the core of understanding dynamical systems is the ability to maintain and\\ncontrol the systems behavior that includes notions of robustness,\\nheterogeneity, or regime-shift detection. Recently, to explore such functional\\nproperties, a convenient representation has been to model such dynamical\\nsystems as a weighted graph consisting of a finite, but very large number of\\ninteracting agents. This said, there exists very limited relevant statistical\\ntheory that is able cope with real-life data, i.e., how does perform analysis\\nand/or statistics over a family of networks as opposed to a specific network or\\nnetwork-to-network variation. Here, we are interested in the analysis of\\nnetwork families whereby each network represents a point on an underlying\\nstatistical manifold. To do so, we explore the Riemannian structure of the\\ntensor manifold developed by Pennec previously applied to Diffusion Tensor\\nImaging (DTI) towards the problem of network analysis. In particular, while\\nthis note focuses on Pennec definition of geodesics amongst a family of\\nnetworks, we show how it lays the foundation for future work for developing\\nmeasures of network robustness for regime-shift detection. We conclude with\\nexperiments highlighting the proposed distance on synthetic networks and an\\napplication towards biological (stem-cell) systems.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1708.09427</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1708.09427</id><submitter>Li Shen</submitter><version version=\"v1\"><date>Wed, 30 Aug 2017 18:46:16 GMT</date><size>1421kb</size></version><version version=\"v2\"><date>Fri, 6 Oct 2017 14:40:06 GMT</date><size>1492kb</size></version><version version=\"v3\"><date>Thu, 19 Oct 2017 02:03:40 GMT</date><size>1469kb</size></version><version version=\"v4\"><date>Sat, 22 Sep 2018 16:10:10 GMT</date><size>660kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Mon, 31 Dec 2018 23:23:07 GMT</date><size>851kb</size><source_type>D</source_type></version><title>Deep Learning to Improve Breast Cancer Early Detection on Screening\\n  Mammography</title><authors>Li Shen, Laurie R. Margolies, Joseph H. Rothstein, Eugene Fluder,\\n  Russell B. McBride, Weiva Sieh</authors><categories>cs.CV cs.AI stat.ML</categories><comments>Major modification with an additional figure and new results</comments><journal-ref>Scientific Reports, volume 9, Article number: 12495 (2019)</journal-ref><doi>10.1038/s41598-019-48995-4</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The rapid development of deep learning, a family of machine learning\\ntechniques, has spurred much interest in its application to medical imaging\\nproblems. Here, we develop a deep learning algorithm that can accurately detect\\nbreast cancer on screening mammograms using an &quot;end-to-end&quot; training approach\\nthat efficiently leverages training datasets with either complete clinical\\nannotation or only the cancer status (label) of the whole image. In this\\napproach, lesion annotations are required only in the initial training stage,\\nand subsequent stages require only image-level labels, eliminating the reliance\\non rarely available lesion annotations. Our all convolutional network method\\nfor classifying screening mammograms attained excellent performance in\\ncomparison with previous methods. On an independent test set of digitized film\\nmammograms from Digital Database for Screening Mammography (DDSM), the best\\nsingle model achieved a per-image AUC of 0.88, and four-model averaging\\nimproved the AUC to 0.91 (sensitivity: 86.1%, specificity: 80.1%). On a\\nvalidation set of full-field digital mammography (FFDM) images from the\\nINbreast database, the best single model achieved a per-image AUC of 0.95, and\\nfour-model averaging improved the AUC to 0.98 (sensitivity: 86.7%, specificity:\\n96.1%). We also demonstrate that a whole image classifier trained using our\\nend-to-end approach on the DDSM digitized film mammograms can be transferred to\\nINbreast FFDM images using only a subset of the INbreast data for fine-tuning\\nand without further reliance on the availability of lesion annotations. These\\nfindings show that automatic deep learning methods can be readily trained to\\nattain high accuracy on heterogeneous mammography platforms, and hold\\ntremendous promise for improving clinical tools to reduce false positive and\\nfalse negative screening mammography results.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1709.04403</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1709.04403</id><submitter>Mehmet Emir Koksal</submitter><version version=\"v1\"><date>Wed, 13 Sep 2017 16:16:00 GMT</date><size>855kb</size></version><title>Explicit Commutativity Conditions for Second-order Linear Time-Varying\\n  Systems with Non-Zero Initial Conditions</title><authors>Mehmet Emir Koksal</authors><categories>cs.SY</categories><comments>20 PAGES, 7 F\\\\.IGURES</comments><msc-class>93C05, 93C15, 93A30</msc-class><journal-ref>Archives of Control Sciences, vol. 29, no. 3, pp. 413-432, 2019</journal-ref><doi>10.24425/acs.2019.XXXXXX</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the explicit commutativitiy conditions for second-order linear\\ntime-varying systems have been appeared in some literature, these are all for\\ninitially relaxed systems. This paper presents explicit necessary and\\nsufficient commutativity conditions for commutativity of second-order linear\\ntime-varying systems with non-zero initial conditions. It has appeared\\ninteresting that the second requirement for the commutativity of non-relaxed\\nsystems plays an important role on the commutativity conditions when non-zero\\ninitial conditions exist. Another highlight is that the commutativity of\\nswitched systems is considered and spoiling of commutativity at the switching\\ninstants is illustrated for the first time. The simulation results support the\\ntheory developed in the paper.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1709.05386</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1709.05386</id><submitter>Mehmet Emir Koksal</submitter><version version=\"v1\"><date>Fri, 15 Sep 2017 20:12:00 GMT</date><size>251kb</size><source_type>D</source_type></version><title>Decomposition of Third Order Linear Time-Varying System into its Second\\n  and First Order Commutative Pairs</title><authors>Mehmet Emir Koksal and Ali Yakar</authors><categories>cs.SY</categories><comments>17 pages, 5 figures</comments><msc-class>34H05, 49K15, 93B52, 93C15</msc-class><journal-ref>Circuits, Systems, and Signal Processing, vol. 38, no. 10, pp.\\n  4446-4464, 2019</journal-ref><doi>10.1007/s00034-019-01075-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decomposition is a common tool for synthesis of many physical systems. It is\\nalso used for analyzing large scale systems which then known as tearing and\\nreconstruction. On the other hand, commutativity of cascade connected systems\\nhave gained a grade deal of interest and its possible benefits have been\\npointed out on the literature. In this paper, the necessary and sufficient\\nconditions for decomposition of any third order linear time-varying system as a\\ncommutative pair of first and second order systems of which parameters are also\\nexplicitly expressed, are investigated. Further, additional requirements in\\ncase of non-zero initial conditions are derived. This paper highlights the\\ndirect formulas for realization of any third order linear time-varying system\\nas a series (cascade) connection of first and second order subsystems. This\\nseries connection is commutative so that it is independent from the sequence of\\nsubsystems in the connection. Hence, the convenient sequence can be decided by\\nconsidering the overall performance of the system when the sensitivity,\\ndisturbance and robustness effects are considered. Realization covers transient\\nresponses as well as steady state responses.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1709.05481</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1709.05481</id><submitter>Mehmet Emir Koksal</submitter><version version=\"v1\"><date>Sat, 16 Sep 2017 08:50:29 GMT</date><size>472kb</size></version><title>Transitivity of Commutativity for Second-Order Linear Time-Varying\\n  Analog Systems</title><authors>Mehmet Emir Koksal</authors><categories>cs.SY</categories><comments>26 pages, 4 figures</comments><msc-class>93C05, 93C15, 93A30</msc-class><journal-ref>Circuits, Systems, and Signal Processing, vol. 38, no. 3, pp.\\n  1385-1395, 2019</journal-ref><doi>10.1007/s00034-018-0911-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  After reviewing commutativity of second-order linear time-varying analog\\nsystems, the inverse commutativity conditions are derived for these systems by\\nconsidering non-zero initial conditions. On the base of these conditions, the\\ntransitivity property is studied for second order linear time-varying unrelaxed\\nanalog systems. It is proven that this property is always valid for such\\nsystems when their initial states are zero; when non-zero initial states are\\npresent, it is shown that the validity of transitivity does not require any\\nmore conditions and it is still valid. Throughout the study it is assumed that\\nthe subsystems considered can not be obtained from each other by any\\nfeed-forward and feed-back structure. The results are well validated by MATLAB\\nsimulations.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1710.00763</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1710.00763</id><submitter>Doris Jung-Lin Lee</submitter><version version=\"v1\"><date>Mon, 2 Oct 2017 16:31:24 GMT</date><size>2609kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 3 Oct 2017 02:06:10 GMT</date><size>2609kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 24 Apr 2018 17:56:33 GMT</date><size>5750kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Tue, 9 Oct 2018 15:53:41 GMT</date><size>3199kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Sun, 30 Dec 2018 03:38:58 GMT</date><size>3903kb</size><source_type>D</source_type></version><version version=\"v6\"><date>Tue, 16 Jul 2019 19:51:27 GMT</date><size>5218kb</size><source_type>D</source_type></version><version version=\"v7\"><date>Thu, 3 Oct 2019 19:19:02 GMT</date><size>5217kb</size><source_type>D</source_type></version><title>You can\\'t always sketch what you want: Understanding Sensemaking in\\n  Visual Query Systems</title><authors>Doris Jung-Lin Lee, John Lee, Tarique Siddiqui, Jaewoo Kim, Karrie\\n  Karahalios, Aditya Parameswaran</authors><categories>cs.DB cs.HC</categories><comments>Accepted for presentation at IEEE VAST 2019, to be held October 20-25\\n  in Vancouver, Canada. Paper will also be published in a special issue of IEEE\\n  Transactions on Visualization and Computer Graphics (TVCG) IEEE VIS\\n  (InfoVis/VAST/SciVis) 2019 ACM 2012 CCS - Human-centered computing,\\n  Visualization, Visualization design and evaluation methods</comments><doi>10.1109/TVCG.2019.2934666</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual query systems (VQSs) empower users to interactively search for line\\ncharts with desired visual patterns, typically specified using intuitive\\nsketch-based interfaces. Despite decades of past work on VQSs, these efforts\\nhave not translated to adoption in practice, possibly because VQSs are largely\\nevaluated in unrealistic lab-based settings. To remedy this gap in adoption, we\\ncollaborated with experts from three diverse domains---astronomy, genetics, and\\nmaterial science---via a year-long user-centered design process to develop a\\nVQS that supports their workflow and analytical needs, and evaluate how VQSs\\ncan be used in practice. Our study results reveal that ad-hoc sketch-only\\nquerying is not as commonly used as prior work suggests, since analysts are\\noften unable to precisely express their patterns of interest. In addition, we\\ncharacterize three essential sensemaking processes supported by our enhanced\\nVQS. We discover that participants employ all three processes, but in different\\nproportions, depending on the analytical needs in each domain. Our findings\\nsuggest that all three sensemaking processes must be integrated in order to\\nmake future VQSs useful for a wide range of analytical inquiries.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1710.01408</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1710.01408</id><submitter>Mohammed Yousefhussien</submitter><version version=\"v1\"><date>Tue, 3 Oct 2017 22:35:25 GMT</date><size>7783kb</size><source_type>D</source_type></version><title>A Fully Convolutional Network for Semantic Labeling of 3D Point Clouds</title><authors>Mohammed Yousefhussien, David J. Kelbe, Emmett J. Ientilucci and Carl\\n  Salvaggio</authors><categories>cs.CV cs.LG stat.ML</categories><doi>10.1016/j.isprsjprs.2018.03.018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When classifying point clouds, a large amount of time is devoted to the\\nprocess of engineering a reliable set of features which are then passed to a\\nclassifier of choice. Generally, such features - usually derived from the\\n3D-covariance matrix - are computed using the surrounding neighborhood of\\npoints. While these features capture local information, the process is usually\\ntime-consuming, and requires the application at multiple scales combined with\\ncontextual methods in order to adequately describe the diversity of objects\\nwithin a scene. In this paper we present a 1D-fully convolutional network that\\nconsumes terrain-normalized points directly with the corresponding spectral\\ndata,if available, to generate point-wise labeling while implicitly learning\\ncontextual features in an end-to-end fashion. Our method uses only the\\n3D-coordinates and three corresponding spectral features for each point.\\nSpectral features may either be extracted from 2D-georeferenced images, as\\nshown here for Light Detection and Ranging (LiDAR) point clouds, or extracted\\ndirectly for passive-derived point clouds,i.e. from muliple-view imagery. We\\ntrain our network by splitting the data into square regions, and use a pooling\\nlayer that respects the permutation-invariance of the input points. Evaluated\\nusing the ISPRS 3D Semantic Labeling Contest, our method scored second place\\nwith an overall accuracy of 81.6%. We ranked third place with a mean F1-score\\nof 63.32%, surpassing the F1-score of the method with highest accuracy by\\n1.69%. In addition to labeling 3D-point clouds, we also show that our method\\ncan be easily extended to 2D-semantic segmentation tasks, with promising\\ninitial results.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1710.05269</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1710.05269</id><submitter>Zhaoji Zhang</submitter><version version=\"v1\"><date>Sun, 15 Oct 2017 03:22:58 GMT</date><size>1434kb</size></version><title>Sparse Message Passing Based Preamble Estimation for Crowded M2M\\n  Communications</title><authors>Zhaoji Zhang, Ying Li, Lei Liu, and Huimei Han</authors><categories>cs.IT math.IT</categories><comments>submitted to ICC 2018 with 6 pages and 4 figures</comments><journal-ref>2018 IEEE International Conference on Communications (ICC), Kansas\\n  City, MO, 2018, pp. 1-6</journal-ref><doi>10.1109/ICC.2018.8422285</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the massive number of devices in the M2M communication era, new\\nchallenges have been brought to the existing random-access (RA) mechanism, such\\nas severe preamble collisions and resource block (RB) wastes. To address these\\nproblems, a novel sparse message passing (SMP) algorithm is proposed, based on\\na factor graph on which Bernoulli messages are updated. The SMP enables an\\naccurate estimation on the activity of the devices and the identity of the\\npreamble chosen by each active device. Aided by the estimation, the RB\\nefficiency for the uplink data transmission can be improved, especially among\\nthe collided devices. In addition, an analytical tool is derived to analyze the\\niterative evolution and convergence of the SMP algorithm. Finally, numerical\\nsimulations are provided to verify the validity of our analytical results and\\nthe significant improvement of the proposed SMP on estimation error rate even\\nwhen preamble collision occurs.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1710.05465</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1710.05465</id><submitter>Cathy Wu</submitter><version version=\"v1\"><date>Mon, 16 Oct 2017 01:51:51 GMT</date><size>5151kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 04:17:06 GMT</date><size>5196kb</size><source_type>D</source_type></version><title>Flow: A Modular Learning Framework for Autonomy in Traffic</title><authors>Cathy Wu, Aboudy Kreidieh, Kanaad Parvate, Eugene Vinitsky, Alexandre\\n  M Bayen</authors><categories>cs.AI cs.RO cs.SY</categories><comments>13 pages, 7 figures; major revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid development of autonomous vehicles (AVs) holds vast potential for\\ntransportation systems through improved safety, efficiency, and access to\\nmobility. However, due to numerous technical, political, and human factors\\nchallenges, new methodologies are needed to design vehicles and transportation\\nsystems for these positive outcomes. This article tackles important technical\\nchallenges arising from the partial adoption of autonomy (hence termed mixed\\nautonomy, to involve both AVs and human-driven vehicles): partial control,\\npartial observation, complex multi-vehicle interactions, and the sheer variety\\nof traffic settings represented by real-world networks. To enable the study of\\nthe full diversity of traffic settings, we first propose to decompose traffic\\ncontrol tasks into modules, which may be configured and composed to create new\\ncontrol tasks of interest. These modules include salient aspects of traffic\\ncontrol tasks: networks, actors, control laws, metrics, initialization, and\\nadditional dynamics. Second, we study the potential of model-free deep\\nReinforcement Learning (RL) methods to address the complexity of traffic\\ndynamics. The resulting modular learning framework is called Flow. Using Flow,\\nwe create and study a variety of mixed-autonomy settings, including\\nsingle-lane, multi-lane, and intersection traffic. In all cases, the learned\\ncontrol law exceeds human driving performance (measured by system-level\\nvelocity) by at least 40% with only 5-10% adoption of AVs. In the case of\\npartially-observed single-lane traffic, we show that a low-parameter neural\\nnetwork control law can eliminate commonly observed stop-and-go traffic. In\\nparticular, the control laws surpass all known model-based controllers,\\nachieving near-optimal performance across a wide spectrum of vehicle densities\\n(even with a memoryless control law) and generalizing to out-of-distribution\\nvehicle densities.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1710.06578</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1710.06578</id><submitter>Masashi Okada Dr</submitter><version version=\"v1\"><date>Wed, 18 Oct 2017 04:01:33 GMT</date><size>5638kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 9 Mar 2018 09:37:13 GMT</date><size>3774kb</size><source_type>D</source_type></version><title>Acceleration of Gradient-based Path Integral Method for Efficient\\n  Optimal and Inverse Optimal Control</title><authors>Masashi Okada, Tadahiro Taniguchi</authors><categories>cs.SY</categories><comments>ICRA2018 camera ready version</comments><doi>10.1109/ICRA.2018.8463164</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with a new accelerated path integral method, which\\niteratively searches optimal controls with a small number of iterations. This\\nstudy is based on the recent observations that a path integral method for\\nreinforcement learning can be interpreted as gradient descent. This observation\\nalso applies to an iterative path integral method for optimal control, which\\nsets a convincing argument for utilizing various optimization methods for\\ngradient descent, such as momentum-based acceleration, step-size adaptation and\\ntheir combination. We introduce these types of methods to the path integral and\\ndemonstrate that momentum-based methods, like Nesterov Accelerated Gradient and\\nAdam, can significantly improve the convergence rate to search for optimal\\ncontrols in simulated control systems. We also demonstrate that the accelerated\\npath integral could improve the performance on model predictive control for\\nvarious vehicle navigation tasks. Finally, we represent this accelerated path\\nintegral method as a recurrent network, which is the accelerated version of the\\npreviously proposed path integral networks (PI-Net). We can train the\\naccelerated PI-Net more efficiently for inverse optimal control with less RAM\\nthan the original PI-Net.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1710.08262</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1710.08262</id><submitter>Marco Savi</submitter><version version=\"v1\"><date>Mon, 23 Oct 2017 13:33:20 GMT</date><size>1656kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 16 Nov 2018 09:00:21 GMT</date><size>3102kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 17 Apr 2019 11:50:45 GMT</date><size>3103kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Tue, 30 Apr 2019 13:28:20 GMT</date><size>3097kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Mon, 30 Sep 2019 15:05:41 GMT</date><size>3098kb</size><source_type>D</source_type></version><title>Impact of Processing-Resource Sharing on the Placement of Chained\\n  Virtual Network Functions</title><authors>Marco Savi, Massimo Tornatore, Giacomo Verticale</authors><categories>cs.NI</categories><comments>Accepted for publication in IEEE Transactions on Cloud Computing</comments><doi>10.1109/TCC.2019.2914387</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network Function Virtualization (NFV) provides higher flexibility for network\\noperators and reduces the complexity in network service deployment. Using NFV,\\nVirtual Network Functions (VNF) can be located in various network nodes and\\nchained together in a Service Function Chain (SFC) to provide a specific\\nservice. Consolidating multiple VNFs in a smaller number of locations would\\nallow decreasing capital expenditures. However, excessive consolidation of VNFs\\nmight cause additional latency penalties due to processing-resource sharing,\\nand this is undesirable, as SFCs are bounded by service-specific latency\\nrequirements. In this paper, we identify two different types of penalties\\n(referred as &quot;costs&quot;) related to the processingresource sharing among multiple\\nVNFs: the context switching costs and the upscaling costs. Context switching\\ncosts arise when multiple CPU processes (e.g., supporting different VNFs) share\\nthe same CPU and thus repeated loading/saving of their context is required.\\nUpscaling costs are incurred by VNFs requiring multi-core implementations,\\nsince they suffer a penalty due to the load-balancing needs among CPU cores.\\nThese costs affect how the chained VNFs are placed in the network to meet the\\nperformance requirement of the SFCs. We evaluate their impact while considering\\nSFCs with different bandwidth and latency requirements in a scenario of VNF\\nconsolidation.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1710.08338</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1710.08338</id><submitter>Mehdi Samiee</submitter><version version=\"v1\"><date>Sun, 15 Oct 2017 20:49:56 GMT</date><size>9945kb</size></version><title>A Unified Spectral Method for FPDEs with Two-sided Derivatives; A Fast\\n  Solver</title><authors>M. Samiee, M. Zayernouri. Mark M. Meerschaert</authors><categories>cs.CE math.NA</categories><journal-ref>https://doi.org/10.1016/j.jcp.2018.02.014</journal-ref><doi>10.1016/j.jcp.2018.02.014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a unified Petrov-Galerkin spectral method for a class of\\nfractional partial differential equations with two-sided derivatives and\\nconstant coefficients of the form $ _{0}{\\\\mathcal{D}}_{t}^{2\\\\tau}u^{} +\\n\\\\sum_{i=1}^{d}$ $[c_{l_i}$ $_{a_i}{\\\\mathcal{D}}_{x_i}^{2\\\\mu_i} u^{} +c_{r_i}$\\n$_{x_i}{\\\\mathcal{D}}_{b_i}^{2\\\\mu_i}$ $u^{} ] +$ $\\\\gamma$ $u^{} = \\\\sum_{j=1}^{d}\\n[ \\\\kappa_{l_j}$ $_{a_j}{\\\\mathcal{D}}_{x_j}^{2\\\\nu_j} u^{}$ $+\\\\kappa_{r_j}$\\n$_{x_j}{\\\\mathcal{D}}_{b_j}^{2\\\\nu_j}$ $u^{} ]$ $+ f$, where $2\\\\tau \\\\in (0,2)$,\\n$2\\\\mu_i \\\\in (0,1)$ and $2\\\\nu_j \\\\in (1,2)$, in a ($1+d$)-dimensional\\n\\\\textit{space-time} hypercube, $d = 1, 2, 3, \\\\cdots$, subject to homogeneous\\nDirichlet initial/boundary conditions. We employ the eigenfunctions of the\\nfractional Sturm-Liouville eigen-problems of the first kind in\\n\\\\cite{zayernouri2013fractional}, called \\\\textit{Jacobi poly-fractonomial}s, as\\ntemporal bases, and the eigen-functions of the boundary-value problem of the\\nsecond kind as temporal test functions. Next, we construct our spatial\\nbasis/test functions using Legendre polynomials, yielding mass matrices being\\nindependent of the spatial fractional orders ($\\\\mu_i, \\\\, \\\\nu_j, \\\\, i,\\n\\\\,j=1,2,\\\\cdots,d$). Furthermore, we formulate a novel unified fast linear\\nsolver for the resulting high-dimensional linear system based on the solution\\nof generalized eigen-problem of spatial mass matrices with respect to the\\ncorresponding stiffness matrices, hence, making the complexity of the problem\\noptimal, i.e., $\\\\mathcal{O}(N^{d+2})$. We carry out several numerical test\\ncases to examine the CPU time and convergence rate of the method. The\\ncorresponding stability and error analysis of the Petrov-Galerkin method are\\ncarried out in \\\\cite{samiee2016Unified2}.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1710.09000</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1710.09000</id><submitter>Christopher Griffin</submitter><version version=\"v1\"><date>Tue, 24 Oct 2017 21:52:21 GMT</date><size>855kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 15:41:18 GMT</date><size>593kb</size><source_type>D</source_type></version><title>Control Problems with Vanishing Lie Bracket Arising from Complete Odd\\n  Circulant Evolutionary Games</title><authors>Christopher Griffin and James Fan</authors><categories>math.OC cs.GT</categories><comments>24 pages, 8 figures</comments><msc-class>91A22, 92A15, 34D45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an optimal control problem arising from a generalization of\\nrock-paper-scissors in which the number of strategies may be selected from any\\npositive odd number greater than 1 and in which the payoff to the winner is\\ncontrolled by a control variable $\\\\gamma$. Using the replicator dynamics as the\\nequations of motion, we show that a quasi-linearization of the problem admits a\\nspecial optimal control form in which explicit dynamics for the controller can\\nbe identified. We show that all optimal controls must satisfy a specific second\\norder differential equation parameterized by the number of strategies in the\\ngame. We show that as the number of strategies increases, a limiting case\\nadmits a closed form for the open-loop optimal control. In performing our\\nanalysis we show necessary conditions on an optimal control problem that allow\\nthis analytic approach to function.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1710.09010</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1710.09010</id><submitter>Justin Hsu</submitter><version version=\"v1\"><date>Tue, 24 Oct 2017 22:30:15 GMT</date><size>66kb</size></version><version version=\"v2\"><date>Mon, 19 Feb 2018 16:04:04 GMT</date><size>746kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 16 Jul 2018 09:58:48 GMT</date><size>107kb</size><source_type>D</source_type></version><title>Approximate Span Liftings</title><authors>Tetsuya Sato, Gilles Barthe, Marco Gaboardi, Justin Hsu, Shin-ya\\n  Katsumata</authors><categories>cs.PL cs.LO</categories><doi>10.1109/LICS.2019.8785668</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop new abstractions for reasoning about relaxations of differential\\nprivacy: R\\\\\\'enyi differential privacy, zero-concentrated differential privacy,\\nand truncated concentrated differential privacy, which express different bounds\\non statistical divergences between two output probability distributions. In\\norder to reason about such properties compositionally, we introduce approximate\\nspan-lifting, a novel construction extending the approximate relational lifting\\napproaches previously developed for standard differential privacy to a more\\ngeneral class of divergences, and also to continuous distributions. As an\\napplication, we develop a program logic based on approximate span-liftings\\ncapable of proving relaxations of differential privacy and other statistical\\ndivergence properties.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1710.09867</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1710.09867</id><submitter>Felix Hill Mr</submitter><version version=\"v1\"><date>Thu, 26 Oct 2017 18:48:20 GMT</date><size>3008kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 17:43:34 GMT</date><size>3676kb</size><source_type>D</source_type></version><title>Understanding Early Word Learning in Situated Artificial Agents</title><authors>Felix Hill, Stephen Clark, Karl Moritz Hermann, Phil Blunsom</authors><categories>cs.CL cs.AI cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural network-based systems can now learn to locate the referents of words\\nand phrases in images, answer questions about visual scenes, and execute\\nsymbolic instructions as first-person actors in partially-observable worlds. To\\nachieve this so-called grounded language learning, models must overcome\\nchallenges that infants face when learning their first words. While it is\\nnotable that models with no meaningful prior knowledge overcome these\\nobstacles, researchers currently lack a clear understanding of how they do so,\\na problem that we attempt to address in this paper. For maximum control and\\ngenerality, we focus on a simple neural network-based language learning agent,\\ntrained via policy-gradient methods, which can interpret single-word\\ninstructions in a simulated 3D world. Whilst the goal is not to explicitly\\nmodel infant word learning, we take inspiration from experimental paradigms in\\ndevelopmental psychology and apply some of these to the artificial agent,\\nexploring the conditions under which established human biases and learning\\neffects emerge. We further propose a novel method for visualising semantic\\nrepresentations in the agent.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1710.10738</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1710.10738</id><submitter>Cunlai Pu</submitter><version version=\"v1\"><date>Mon, 30 Oct 2017 01:50:54 GMT</date><size>159kb</size></version><version version=\"v2\"><date>Sat, 8 Sep 2018 09:25:50 GMT</date><size>183kb</size></version><version version=\"v3\"><date>Tue, 8 Oct 2019 11:40:25 GMT</date><size>195kb</size></version><title>Node similarity distribution of complex networks and its application in\\n  link prediction</title><authors>Cunlai Pu, Jie Li, Jian Wang, and Tony Q. S. Quek</authors><categories>cs.SI physics.soc-ph</categories><comments>10 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the years, quantifying the similarity of nodes has been a hot topic in\\ncomplex networks, yet little has been known about the distributions of\\nnode-similarity. In this paper, we consider a typical measure of\\nnode-similarity called the common neighbor based similarity (CNS). By means of\\nthe generating function, we propose a general framework for calculating the CNS\\ndistributions of node sets in various complex networks. In particular, we show\\nthat for the Erd\\\\&quot;{o}s-R\\\\\\'{e}nyi (ER) random network, the CNS distribution of\\nnode sets of any particular size obeys the Poisson law. We also connect the\\nnode-similarity distribution to the link prediction problem. We found that the\\nperformance of link prediction depends solely on the CNS distributions of the\\nconnected and unconnected node pairs in the network. Furthermore, we derive\\ntheoretical solutions of two key evaluation metrics in link prediction: i)\\nprecision and ii) area under the receiver operating characteristic curve (AUC).\\nWe show that for any link prediction method, if the similarity distributions of\\nthe connected and unconnected node pairs are identical, the AUC will be $0.5$.\\nThe theoretical solutions are elegant alternatives of the traditional\\nexperimental evaluation methods with nevertheless much lower computational\\ncost.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1711.02123</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1711.02123</id><submitter>Dena Asta</submitter><version version=\"v1\"><date>Mon, 6 Nov 2017 19:15:52 GMT</date><size>55kb</size></version><version version=\"v2\"><date>Tue, 1 Oct 2019 19:42:11 GMT</date><size>48kb</size></version><title>Consistency of Maximum Likelihood for Continuous-Space Network Models</title><authors>Cosma Rohilla Shalizi and Dena Asta</authors><categories>math.ST cs.SI physics.soc-ph stat.TH</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network analysis needs tools to infer distributions over graphs of arbitrary\\nsize from a single graph. Assuming the distribution is generated by a\\ncontinuous latent space model which obeys certain natural symmetry and\\nsmoothness properties, we establish three levels of consistency for\\nnon-parametric maximum likelihood inference as the number of nodes grows: (i)\\nthe estimated locations of all nodes converge in probability on their true\\nlocations; (ii) the distribution over locations in the latent space converges\\non the true distribution; and (iii) the distribution over graphs of arbitrary\\nsize converges.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1711.02880</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1711.02880</id><submitter>Celine Comte</submitter><version version=\"v1\"><date>Wed, 8 Nov 2017 09:38:57 GMT</date><size>187kb</size></version><version version=\"v2\"><date>Tue, 19 Dec 2017 07:38:46 GMT</date><size>187kb</size></version><version version=\"v3\"><date>Wed, 2 Oct 2019 08:18:45 GMT</date><size>225kb</size></version><title>Performance of Balanced Fairness in Resource Pools: A Recursive Approach</title><authors>Thomas Bonald (LINCS), C\\\\\\'eline Comte (LINCS), Fabien Mathieu (LINCS)</authors><categories>cs.NI</categories><proxy>ccsd</proxy><journal-ref>Proceedings of the ACM on Measurement and Analysis of Computing\\n  Systems , ACM, 2017, 1 (2), pp.1-25. \\\\&amp;\\\\#x27E8;10.1145/3154500\\\\&amp;\\\\#x27E9</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the performance of a pool of servers is crucial for proper\\ndimensioning. One of the main challenges is to take into account the complex\\ninteractions between servers that are pooled to process jobs. In particular, a\\njob can generally not be processed by any server of the cluster due to various\\nconstraints like data locality. In this paper, we represent these constraints\\nby some assignment graph between jobs and servers. We present a recursive\\napproach to computing performance metrics like mean response times when the\\nserver capacities are shared according to balanced fairness. While the\\ncomputational cost of these formulas can be exponential in the number of\\nservers in the worst case, we illustrate their practical interest by\\nintroducing broad classes of pool structures that can be exactly analyzed in\\npolynomial time. This extends considerably the class of models for which\\nexplicit performance metrics are accessible.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1711.05775</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1711.05775</id><submitter>Li Shen</submitter><version version=\"v1\"><date>Wed, 15 Nov 2017 19:52:27 GMT</date><size>1347kb</size></version><title>End-to-end Training for Whole Image Breast Cancer Diagnosis using An All\\n  Convolutional Design</title><authors>Li Shen</authors><categories>cs.CV</categories><comments>Accepted poster at NIPS 2017 Workshop on Machine Learning for Health\\n  (https://ml4health.github.io/2017/)</comments><journal-ref>Scientific Reports, volume 9, Article number: 12495 (2019)</journal-ref><doi>10.1038/s41598-019-48995-4</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  We develop an end-to-end training algorithm for whole-image breast cancer\\ndiagnosis based on mammograms. It requires lesion annotations only at the first\\nstage of training. After that, a whole image classifier can be trained using\\nonly image level labels. This greatly reduced the reliance on lesion\\nannotations. Our approach is implemented using an all convolutional design that\\nis simple yet provides superior performance in comparison with the previous\\nmethods. On DDSM, our best single-model achieves a per-image AUC score of 0.88\\nand three-model averaging increases the score to 0.91. On INbreast, our best\\nsingle-model achieves a per-image AUC score of 0.96. Using DDSM as benchmark,\\nour models compare favorably with the current state-of-the-art. We also\\ndemonstrate that a whole image model trained on DDSM can be easily transferred\\nto INbreast without using its lesion annotations and using only a small amount\\nof training data. Code availability: https://github.com/lishen/end2end-all-conv\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1711.06077</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1711.06077</id><submitter>Yochai Blau</submitter><version version=\"v1\"><date>Thu, 16 Nov 2017 13:22:30 GMT</date><size>2001kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 16 Mar 2018 09:14:05 GMT</date><size>2673kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 3 Oct 2019 16:16:13 GMT</date><size>4382kb</size><source_type>D</source_type></version><title>The Perception-Distortion Tradeoff</title><authors>Yochai Blau and Tomer Michaeli</authors><categories>cs.CV</categories><comments>CVPR 2018 (long oral presentation), see talk at:\\n  https://youtu.be/_aXbGqdEkjk?t=39m43s</comments><journal-ref>Proceedings of the IEEE Conference on Computer Vision and Pattern\\n  Recognition, pp. 6228-6237, 2018</journal-ref><doi>10.1109/CVPR.2018.00652</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image restoration algorithms are typically evaluated by some distortion\\nmeasure (e.g. PSNR, SSIM, IFC, VIF) or by human opinion scores that quantify\\nperceived perceptual quality. In this paper, we prove mathematically that\\ndistortion and perceptual quality are at odds with each other. Specifically, we\\nstudy the optimal probability for correctly discriminating the outputs of an\\nimage restoration algorithm from real images. We show that as the mean\\ndistortion decreases, this probability must increase (indicating worse\\nperceptual quality). As opposed to the common belief, this result holds true\\nfor any distortion measure, and is not only a problem of the PSNR or SSIM\\ncriteria. We also show that generative-adversarial-nets (GANs) provide a\\nprincipled way to approach the perception-distortion bound. This constitutes\\ntheoretical support to their observed success in low-level vision tasks. Based\\non our analysis, we propose a new methodology for evaluating image restoration\\nmethods, and use it to perform an extensive comparison between recent\\nsuper-resolution algorithms.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1711.07042</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1711.07042</id><submitter>Kieran Greer Dr</submitter><version version=\"v1\"><date>Sun, 19 Nov 2017 16:24:26 GMT</date><size>705kb</size></version><version version=\"v2\"><date>Fri, 18 Jan 2019 08:26:31 GMT</date><size>375kb</size></version><version version=\"v3\"><date>Thu, 3 Oct 2019 18:09:07 GMT</date><size>0kb</size><source_type>I</source_type></version><title>An Improved Oscillating-Error Classifier with Branching</title><authors>Kieran Greer</authors><categories>cs.LG stat.ML</categories><comments>This paper is now out of date. You should read \\'An Improved Batch\\n  Classifier with Bands and Dimensions\\', arXiv:1811.02617, instead</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper extends the earlier work on an oscillating error correction\\ntechnique. Specifically, it extends the design to include further corrections,\\nby adding new layers to the classifier through a branching method. This\\ntechnique is still consistent with earlier work and also neural networks in\\ngeneral. With this extended design, the classifier can now achieve the high\\nlevels of accuracy reported previously.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1712.00716</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1712.00716</id><submitter>Qing Qu</submitter><version version=\"v1\"><date>Sun, 3 Dec 2017 06:04:25 GMT</date><size>2573kb</size></version><version version=\"v2\"><date>Wed, 28 Aug 2019 03:30:06 GMT</date><size>5643kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sun, 6 Oct 2019 02:55:12 GMT</date><size>5604kb</size><source_type>D</source_type></version><title>Convolutional Phase Retrieval via Gradient Descent</title><authors>Qing Qu, Yuqian Zhang, Yonina C. Eldar, and John Wright</authors><categories>stat.CO cs.IT cs.NA math.IT math.OC stat.ML</categories><comments>64 pages , 9 figures, appeared in NeurIPS 2017. Accepted at IEEE\\n  Transactions on Information Theory. This is the final (minor) update: fixed\\n  typos and grammar issues</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the convolutional phase retrieval problem, of recovering an unknown\\nsignal $\\\\mathbf x \\\\in \\\\mathbb C^n $ from $m$ measurements consisting of the\\nmagnitude of its cyclic convolution with a given kernel $\\\\mathbf a \\\\in \\\\mathbb\\nC^m $. This model is motivated by applications such as channel estimation,\\noptics, and underwater acoustic communication, where the signal of interest is\\nacted on by a given channel/filter, and phase information is difficult or\\nimpossible to acquire. We show that when $\\\\mathbf a$ is random and the number\\nof observations $m$ is sufficiently large, with high probability $\\\\mathbf x$\\ncan be efficiently recovered up to a global phase shift using a combination of\\nspectral initialization and generalized gradient descent. The main challenge is\\ncoping with dependencies in the measurement operator. We overcome this\\nchallenge by using ideas from decoupling theory, suprema of chaos processes and\\nthe restricted isometry property of random circulant matrices, and recent\\nanalysis of alternating minimization methods.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1712.01272</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1712.01272</id><submitter>Thanh Nguyen</submitter><version version=\"v1\"><date>Mon, 4 Dec 2017 02:16:03 GMT</date><size>672kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 10 Feb 2018 02:00:51 GMT</date><size>879kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 7 Jun 2018 12:03:50 GMT</date><size>1195kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Wed, 12 Sep 2018 08:22:24 GMT</date><size>2472kb</size></version><version version=\"v5\"><date>Tue, 25 Jun 2019 12:23:47 GMT</date><size>4210kb</size></version><version version=\"v6\"><date>Sun, 6 Oct 2019 11:19:50 GMT</date><size>4221kb</size></version><title>Layer-wise Learning of Stochastic Neural Networks with Information\\n  Bottleneck</title><authors>Thanh T. Nguyen, Jaesik Choi</authors><categories>cs.LG</categories><comments>published in Entropy journal</comments><journal-ref>Entropy 2019, 21(10), 976; https://doi.org/10.3390/e21100976</journal-ref><doi>10.3390/e21100976</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information Bottleneck (IB) is a generalization of rate-distortion theory\\nthat naturally incorporates compression and relevance trade-offs for learning.\\nThough the original IB has been extensively studied, there has not been much\\nunderstanding of multiple bottlenecks which better fit in the context of neural\\nnetworks. In this work, we propose Information Multi-Bottlenecks (IMBs) as an\\nextension of IB to multiple bottlenecks which has a direct application to\\ntraining neural networks by considering layers as multiple bottlenecks and\\nweights as parameterized encoders and decoders. We show that the multiple\\noptimality of IMB is not simultaneously achievable for stochastic encoders. We\\nthus propose a simple compromised scheme of IMB which in turn generalizes\\nmaximum likelihood estimate (MLE) principle in the context of stochastic neural\\nnetworks. We demonstrate the effectiveness of IMB on classification tasks and\\nadversarial robustness in MNIST and CIFAR10.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1712.03158</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1712.03158</id><submitter>Thijs Laarhoven</submitter><version version=\"v1\"><date>Fri, 8 Dec 2017 16:26:22 GMT</date><size>209kb</size><source_type>D</source_type></version><title>Graph-based time-space trade-offs for approximate near neighbors</title><authors>Thijs Laarhoven</authors><categories>cs.DS cs.CC cs.CG cs.CR cs.IR</categories><comments>26 pages, 4 figures</comments><journal-ref>34th International Symposium on Computational Geometry (SoCG), pp.\\n  57:1-57:14, 2018</journal-ref><doi>10.4230/LIPIcs.SoCG.2018.57</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We take a first step towards a rigorous asymptotic analysis of graph-based\\napproaches for finding (approximate) nearest neighbors in high-dimensional\\nspaces, by analyzing the complexity of (randomized) greedy walks on the\\napproximate near neighbor graph. For random data sets of size $n = 2^{o(d)}$ on\\nthe $d$-dimensional Euclidean unit sphere, using near neighbor graphs we can\\nprovably solve the approximate nearest neighbor problem with approximation\\nfactor $c &gt; 1$ in query time $n^{\\\\rho_q + o(1)}$ and space $n^{1 + \\\\rho_s +\\no(1)}$, for arbitrary $\\\\rho_q, \\\\rho_s \\\\geq 0$ satisfying \\\\begin{align} (2c^2 -\\n1) \\\\rho_q + 2 c^2 (c^2 - 1) \\\\sqrt{\\\\rho_s (1 - \\\\rho_s)} \\\\geq c^4. \\\\end{align}\\nGraph-based near neighbor searching is especially competitive with hash-based\\nmethods for small $c$ and near-linear memory, and in this regime the asymptotic\\nscaling of a greedy graph-based search matches the recent optimal hash-based\\ntrade-offs of Andoni-Laarhoven-Razenshteyn-Waingarten [SODA\\'17]. We further\\nstudy how the trade-offs scale when the data set is of size $n =\\n2^{\\\\Theta(d)}$, and analyze asymptotic complexities when applying these results\\nto lattice sieving.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1712.05548</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1712.05548</id><submitter>Paul Rosen</submitter><version version=\"v1\"><date>Fri, 15 Dec 2017 06:06:57 GMT</date><size>1867kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 19 Apr 2019 22:11:43 GMT</date><size>8854kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 31 Jul 2019 23:54:22 GMT</date><size>9596kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Fri, 4 Oct 2019 18:20:35 GMT</date><size>9597kb</size><source_type>D</source_type></version><title>Persistent Homology Guided Force-Directed Graph Layouts</title><authors>Ashley Suh, Mustafa Hajij, Bei Wang, Carlos Scheidegger, Paul Rosen</authors><categories>cs.GR cs.SI</categories><doi>10.1109/TVCG.2019.2934802</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphs are commonly used to encode relationships among entities, yet their\\nabstractness makes them difficult to analyze. Node-link diagrams are popular\\nfor drawing graphs, and force-directed layouts provide a flexible method for\\nnode arrangements that use local relationships in an attempt to reveal the\\nglobal shape of the graph. However, clutter and overlap of unrelated structures\\ncan lead to confusing graph visualizations. This paper leverages the persistent\\nhomology features of an undirected graph as derived information for interactive\\nmanipulation of force-directed layouts. We first discuss how to efficiently\\nextract 0-dimensional persistent homology features from both weighted and\\nunweighted undirected graphs. We then introduce the interactive persistence\\nbarcode used to manipulate the force-directed graph layout. In particular, the\\nuser adds and removes contracting and repulsing forces generated by the\\npersistent homology features, eventually selecting the set of persistent\\nhomology features that most improve the layout. Finally, we demonstrate the\\nutility of our approach across a variety of synthetic and real datasets.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1712.07438</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1712.07438</id><submitter>Richard Gerum</submitter><version version=\"v1\"><date>Wed, 20 Dec 2017 12:13:44 GMT</date><size>4269kb</size><source_type>D</source_type></version><title>CameraTransform: a Scientific Python Package for Perspective Camera\\n  Corrections</title><authors>Richard Gerum, Sebastian Richter, Alexander Winterl, Ben Fabry, Daniel\\n  Zitterbart</authors><categories>cs.MS</categories><comments>8 pages, 5 figures</comments><doi>10.1016/j.softx.2019.100333</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific applications often require an exact reconstruction of object\\npositions and distances from digital images. Therefore, the images need to be\\ncorrected for perspective distortions. We present \\\\textit{CameraTransform}, a\\npython package that performs a perspective image correction whereby the height,\\ntilt/roll angle and heading of the camera can be automatically obtained from\\nthe images if additional information such as GPS coordinates or object sizes\\nare provided. We present examples of images of penguin colonies that are\\nrecorded with stationary cameras and from a helicopter.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1712.09952</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1712.09952</id><submitter>Joanna Piotrowska</submitter><version version=\"v1\"><date>Thu, 28 Dec 2017 17:48:30 GMT</date><size>422kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 13 May 2019 14:17:39 GMT</date><size>443kb</size><source_type>D</source_type></version><title>Spectral Methods in the Presence of Discontinuities</title><authors>Joanna Piotrowska, Jonah M. Miller, Erik Schnetter</authors><categories>cs.NA gr-qc physics.comp-ph</categories><comments>20 pages, 18 figures</comments><report-no>LA-UR-17-31492</report-no><journal-ref>Journal of Computational Physics 390 (2019) 527-547</journal-ref><doi>10.1016/j.jcp.2019.03.048</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral methods provide an elegant and efficient way of numerically solving\\ndifferential equations of all kinds. For smooth problems, truncation error for\\nspectral methods vanishes exponentially in the infinity norm and $L_2$-norm.\\nHowever, for non-smooth problems, convergence is significantly worse---the\\n$L_2$-norm of the error for a discontinuous problem will converge at a\\nsub-linear rate and the infinity norm will not converge at all. We explore and\\nimprove upon a post-processing technique---optimally convergent mollifiers---to\\nrecover exponential convergence from a poorly-converging spectral\\nreconstruction of non-smooth data. This is an important first step towards\\nusing these techniques for simulations of realistic systems.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1801.00132</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1801.00132</id><submitter>Won-Yong Shin</submitter><version version=\"v1\"><date>Sat, 30 Dec 2017 13:22:29 GMT</date><size>1392kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 10 Jun 2018 03:49:03 GMT</date><size>446kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 12 Jun 2018 08:58:08 GMT</date><size>446kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Tue, 9 Apr 2019 04:32:51 GMT</date><size>577kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Thu, 3 Oct 2019 13:52:59 GMT</date><size>629kb</size><source_type>D</source_type></version><title>Community Detection in Partially Observable Social Networks</title><authors>Cong Tran, Won-Yong Shin, Andreas Spitz</authors><categories>cs.SI cs.LG physics.soc-ph</categories><comments>14 pages, 7 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The discovery of community structures in social networks has gained\\nsignificant attention since it is a fundamental problem in understanding the\\nnetworks\\' topology and functions. However, most social network data are\\ncollected from partially observable networks with both missing nodes and edges.\\nIn this paper, we address a new problem of detecting overlapping community\\nstructures in the context of such an incomplete network, where communities in\\nthe network are allowed to overlap since nodes belong to multiple communities\\nat once. To solve this problem, we introduce KroMFac, a new framework that\\nconducts community detection via regularized nonnegative matrix factorization\\n(NMF) based on the Kronecker graph model. Specifically, from an interred\\nKronecker generative parameter metrix, we first estimate the missing part of\\nthe network. As our major contribution to the proposed framework, to improve\\ncommunity detection accuracy, we then characterize and select influential nodes\\n(which tend to have high degrees) by ranking, and add them to the existing\\ngraph. Finally, we uncover the community structures by solving the regularized\\nNMF-aided optimization problem in terms of maximizing the likelihood of the\\nunderlying graph. Furthermore, adopting normalized mutual information (NMI), we\\nempirically show superiority of our KroMFac approach over two baseline schemes\\nby using both synthetic and real-world networks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1801.02961</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1801.02961</id><submitter>Milad Zafar Nezhad</submitter><version version=\"v1\"><date>Sat, 6 Jan 2018 23:17:24 GMT</date><size>1681kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 14:01:32 GMT</date><size>1104kb</size><source_type>D</source_type></version><title>Representation Learning with Autoencoders for Electronic Health Records:\\n  A Comparative Study</title><authors>Najibesadat Sadati, Milad Zafar Nezhad, Ratna Babu Chinnam, Dongxiao\\n  Zhu</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasing volume of Electronic Health Records (EHR) in recent years provides\\ngreat opportunities for data scientists to collaborate on different aspects of\\nhealthcare research by applying advanced analytics to these EHR clinical data.\\nA key requirement however is obtaining meaningful insights from high\\ndimensional, sparse and complex clinical data. Data science approaches\\ntypically address this challenge by performing feature learning in order to\\nbuild more reliable and informative feature representations from clinical data\\nfollowed by supervised learning. In this paper, we propose a predictive\\nmodeling approach based on deep learning based feature representations and word\\nembedding techniques. Our method uses different deep architectures (stacked\\nsparse autoencoders, deep belief network, adversarial autoencoders and\\nvariational autoencoders) for feature representation in higher-level\\nabstraction to obtain effective and robust features from EHRs, and then build\\nprediction models on top of them. Our approach is particularly useful when the\\nunlabeled data is abundant whereas labeled data is scarce. We investigate the\\nperformance of representation learning through a supervised learning approach.\\nOur focus is to present a comparative study to evaluate the performance of\\ndifferent deep architectures through supervised learning and provide insights\\nin the choice of deep feature representation techniques. Our experiments\\ndemonstrate that for small data sets, stacked sparse autoencoder demonstrates a\\nsuperior generality performance in prediction due to sparsity regularization\\nwhereas variational autoencoders outperform the competing approaches for large\\ndata sets due to its capability of learning the representation distribution.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1801.04886</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1801.04886</id><submitter>Khaza Anuarul Hoque</submitter><version version=\"v1\"><date>Thu, 11 Jan 2018 19:00:22 GMT</date><size>1629kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 12 Mar 2018 20:38:27 GMT</date><size>1629kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 8 Oct 2019 17:11:31 GMT</date><size>1629kb</size><source_type>D</source_type></version><title>Dependability modeling and optimization of triple modular redundancy\\n  partitioning for SRAM-based FPGAs</title><authors>Khaza Anuarul Hoque, Otmane Ait Mohamed, Yvon Savaria</authors><categories>cs.DC cs.AR cs.LO</categories><comments>Published in Reliability Engineering &amp; System Safety Volume 182,\\n  February 2019, Pages 107-119</comments><journal-ref>Reliability Engineering &amp; System Safety Volume 182, February 2019,\\n  Pages 107-119</journal-ref><doi>10.1016/j.ress.2018.10.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SRAM-based FPGAs are popular in the aerospace industry for their field\\nprogrammability and low cost. However, they suffer from cosmic\\nradiation-induced Single Event Upsets (SEUs). Triple Modular Redundancy (TMR)\\nis a well-known technique to mitigate SEUs in FPGAs that is often used with\\nanother SEU mitigation technique known as configuration scrubbing. Traditional\\nTMR provides protection against a single fault at a time, while partitioned TMR\\nprovides improved reliability and availability. In this paper, we present a\\nmethodology to analyze TMR partitioning at early design stage using\\nprobabilistic model checking. The proposed formal model can capture both single\\nand multiple-cell upset scenarios, regardless of any assumption of equal\\npartition sizes. Starting with a high-level description of a design, a Markov\\nmodel is constructed from the Data Flow Graph (DFG) using a specified number of\\npartitions, a component characterization library and a user defined scrub rate.\\nSuch a model and exhaustive analysis captures all the considered failures and\\nrepairs possible in the system within the radiation environment. Various\\nreliability and availability properties are then verified automatically using\\nthe PRISM model checker exploring the relationship between the scrub frequency\\nand the number of TMR partitions required to meet the design requirements.\\nAlso, the reported results show that based on a known voter failure rate, it is\\npossible to find an optimal number of partitions at early design stages using\\nour proposed method.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1801.05365</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1801.05365</id><submitter>Pramuditha Perera</submitter><version version=\"v1\"><date>Tue, 16 Jan 2018 17:01:48 GMT</date><size>2066kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 16 May 2019 16:40:12 GMT</date><size>2614kb</size><source_type>D</source_type></version><title>Learning Deep Features for One-Class Classification</title><authors>Pramuditha Perera, Vishal M. Patel</authors><categories>cs.CV</categories><comments>Accepted to appear in Transactions in Image Processing</comments><doi>10.1109/TIP.2019.2917862</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a deep learning-based solution for the problem of feature learning\\nin one-class classification. The proposed method operates on top of a\\nConvolutional Neural Network (CNN) of choice and produces descriptive features\\nwhile maintaining a low intra-class variance in the feature space for the given\\nclass. For this purpose two loss functions, compactness loss and\\ndescriptiveness loss are proposed along with a parallel CNN architecture. A\\ntemplate matching-based framework is introduced to facilitate the testing\\nprocess. Extensive experiments on publicly available anomaly detection, novelty\\ndetection and mobile active authentication datasets show that the proposed Deep\\nOne-Class (DOC) classification method achieves significant improvements over\\nthe state-of-the-art.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1801.06447</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1801.06447</id><submitter>Omid Taghizadeh M.Sc.</submitter><version version=\"v1\"><date>Wed, 10 Jan 2018 17:35:03 GMT</date><size>754kb</size><source_type>D</source_type></version><title>Environment-Aware Minimum-Cost Wireless Backhaul Network Planning with\\n  Full-Duplex Links</title><authors>Omid Taghizadeh, Praveen Sirvi, Santosh Narasimha, Jose Angel Leon\\n  Calvo, Rudolf Mathar</authors><categories>cs.NI math.OC</categories><comments>Submuitted to IEEE for publication</comments><doi>10.1109/JSYST.2019.2893537</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we address the joint design of the wireless backhauling network\\ntopology as well as the frequency/power allocation on the wireless links, where\\nnodes are capable of full-duplex (FD) operation. The proposed joint design\\nenables the coexistence of multiple wireless links at the same channel,\\nresulting in an enhanced spectral efficiency. Moreover, it enables the usage of\\nFD capability when/where it is gainful. In this regard, a\\nmixed-integer-linear-program (MILP) is proposed, aiming at a minimum cost\\ndesign for the wireless backhaul network, considering the required rate demand\\nat each base station. Moreover, a re-tunning algorithm is proposed which reacts\\nto the slight changes in the network condition, e.g., channel attenuation or\\nrate demand, by adjusting the transmit power at the wireless links. In this\\nregard, a successive inner approximation (SIA)- based design is proposed, where\\nin each step a convex subproblem is solved. Numerical simulations show a\\nreduction in the overall network cost via the utilization of the proposed\\ndesigns, thanks to the coexistence of multiple wireless links on the same\\nchannel due to the FD capability.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1801.10280</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1801.10280</id><submitter>Robert Kenny</submitter><version version=\"v1\"><date>Wed, 31 Jan 2018 02:32:51 GMT</date><size>49kb</size></version><version version=\"v2\"><date>Sun, 29 Sep 2019 14:39:58 GMT</date><size>53kb</size></version><version version=\"v3\"><date>Tue, 1 Oct 2019 04:55:02 GMT</date><size>53kb</size></version><title>Dugundji systems and a retract characterization of effective\\n  zero-dimensionality</title><authors>Robert Kenny</authors><categories>cs.LO math.LO</categories><comments>33 pages, major revised version, intended for postproceedings of CCC\\n  2017</comments><msc-class>03D78 (Primary), 12J25, 54B20 (Secondary)</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper (as in [Ken15]), we consider an effective version of the\\ncharacterization of (separable metric) spaces as zero-dimensional iff every\\nnonempty closed subset is a retract of the space (actually, we have proved a\\nrelative result for closed (zero-dimensional) subspaces of a fixed space). This\\nuses (in the converse direction) local compactness &amp; bilocated sets as in\\n[Ken15], but in the forward direction the newer version has a simpler proof and\\nno compactness assumption. Furthermore, the proof of the forward implication\\nrelates to so-called Dugundji systems: we elaborate both a general construction\\nof such systems for a proper nonempty closed subspace (using a computable form\\nof countable paracompactness), and modifications to make the sets pairwise\\ndisjoint if the subspace is zero-dimensional, or to avoid the restriction to\\nproper subspaces. In a different direction, a second theorem applies in\\n$p$-adic analysis the ideas of the first theorem to compute a more general form\\nof retraction, given a Dugundji system (possibly without disjointness).\\n  Finally, we complement the mentioned effective retract characterization of\\nzero-dimensional subspaces by improving to equivalence the implications (or\\nWeihrauch reductions in some cases), for closed at-most-zero-dimensional\\nsubsets with some negative information, among separate conditions of\\ncomputability of operations $N,M,B,S$ introduced in [Ken15,\\\\S 4] and\\ncorresponding to vanishing large inductive dimension, vanishing small inductive\\ndimension, existence of a countable basis of relatively clopen sets, and the\\nreduction principle for sequences of open sets. Thus, similarly to the robust\\nnotion of effective zero-dimensionality of computable metric spaces in [Ken15],\\nthere is a robust notion of `uniform effective zero-dimensionality\\' for a\\nrepresented pointclass consisting of at-most-zero-dimensional closed subsets.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1801.10355</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1801.10355</id><submitter>Alan JiaXiang Guo</submitter><version version=\"v1\"><date>Wed, 31 Jan 2018 08:57:10 GMT</date><size>2686kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 7 Aug 2018 07:50:02 GMT</date><size>2812kb</size><source_type>D</source_type></version><title>A CNN-based Spatial Feature Fusion Algorithm for Hyperspectral Imagery\\n  Classification</title><authors>Alan J.X. Guo, Fei Zhu</authors><categories>cs.CV</categories><doi>10.1109/TGRS.2019.2911993</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The shortage of training samples remains one of the main obstacles in\\napplying the artificial neural networks (ANN) to the hyperspectral images\\nclassification. To fuse the spatial and spectral information, pixel patches are\\noften utilized to train a model, which may further aggregate this problem. In\\nthe existing works, an ANN model supervised by center-loss (ANNC) was\\nintroduced. Training merely with spectral information, the ANNC yields\\ndiscriminative spectral features suitable for the subsequent classification\\ntasks. In this paper, a CNN-based spatial feature fusion (CSFF) algorithm is\\nproposed, which allows a smart fusion of the spatial information to the\\nspectral features extracted by ANNC. As a critical part of CSFF, a CNN-based\\ndiscriminant model is introduced to estimate whether two paring pixels belong\\nto the same class. At the testing stage, by applying the discriminant model to\\nthe pixel-pairs generated by the test pixel and its neighbors, the local\\nstructure is estimated and represented as a customized convolutional kernel.\\nThe spectral-spatial feature is obtained by a convolutional operation between\\nthe estimated kernel and the corresponding spectral features within a\\nneighborhood. At last, the label of the test pixel is predicted by classifying\\nthe resulting spectral-spatial feature. Without increasing the number of\\ntraining samples or involving pixel patches at the training stage, the CSFF\\nframework achieves the state-of-the-art by declining $20\\\\%-50\\\\%$ classification\\nfailures in experiments on three well-known hyperspectral images.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1802.00197</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1802.00197</id><submitter>Jens Markus Melenk</submitter><version version=\"v1\"><date>Thu, 1 Feb 2018 08:54:13 GMT</date><size>49kb</size></version><version version=\"v2\"><date>Fri, 12 Apr 2019 13:35:41 GMT</date><size>47kb</size></version><version version=\"v3\"><date>Fri, 4 Oct 2019 14:49:34 GMT</date><size>47kb</size></version><title>On commuting $p$-version projection-based interpolation on tetrahedra</title><authors>Jens Markus Melenk and Claudio Rojik</authors><categories>math.NA cs.NA</categories><journal-ref>Math. Comp.89 (2019), pp. 45-87</journal-ref><doi>10.1090/mcom/3454</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On the reference tetrahedron $\\\\widehat K$, we define three projection-based\\ninterpolation operators on $H^2(\\\\widehat K)$, ${\\\\mathbf H}^1(\\\\widehat\\nK,\\\\operatorname{\\\\mathbf{curl}})$, and ${\\\\mathbf H}^1(\\\\widehat\\nK,\\\\operatorname{div})$. These operators are projections onto space of\\npolynomials, they have the commuting diagram property and feature the optimal\\nconvergence rate as the polynomial degree increases in $H^{1-s}(\\\\widehat K)$,\\n${\\\\mathbf H}^{-s}(\\\\widehat K,\\\\operatorname{\\\\mathbf{curl}})$, ${\\\\mathbf\\nH}^{-s}(\\\\widehat K,\\\\operatorname{div})$ for $0 \\\\leq s \\\\leq 1$.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1802.00745</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1802.00745</id><submitter>Hugo Jair  Escalante</submitter><version version=\"v1\"><date>Fri, 2 Feb 2018 16:02:55 GMT</date><size>2185kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 15 Oct 2018 14:48:26 GMT</date><size>4533kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sun, 29 Sep 2019 03:24:32 GMT</date><size>4533kb</size><source_type>D</source_type></version><title>Explaining First Impressions: Modeling, Recognizing, and Explaining\\n  Apparent Personality from Videos</title><authors>Hugo Jair Escalante, Heysem Kaya, Albert Ali Salah, Sergio Escalera,\\n  Yagmur Gucluturk, Umut Guclu, Xavier Baro, Isabelle Guyon, Julio Jacques\\n  Junior, Meysam Madadi, Stephane Ayache, Evelyne Viegas, Furkan Gurpinar,\\n  Achmadnoer Sukma Wicaksana, Cynthia C. S. Liem, Marcel A. J. van Gerven, Rob\\n  van Lier</authors><categories>cs.CV</categories><comments>Preprint submitted to TAC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Explainability and interpretability are two critical aspects of decision\\nsupport systems. Within computer vision, they are critical in certain tasks\\nrelated to human behavior analysis such as in health care applications. Despite\\ntheir importance, it is only recently that researchers are starting to explore\\nthese aspects. This paper provides an introduction to explainability and\\ninterpretability in the context of computer vision with an emphasis on looking\\nat people tasks. Specifically, we review and study those mechanisms in the\\ncontext of first impressions analysis. To the best of our knowledge, this is\\nthe first effort in this direction. Additionally, we describe a challenge we\\norganized on explainability in first impressions analysis from video. We\\nanalyze in detail the newly introduced data set, the evaluation protocol, and\\nsummarize the results of the challenge. Finally, derived from our study, we\\noutline research opportunities that we foresee will be decisive in the near\\nfuture for the development of the explainable computer vision field.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1802.01697</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1802.01697</id><submitter>Yao-Yuan Yang</submitter><version version=\"v1\"><date>Mon, 5 Feb 2018 21:27:59 GMT</date><size>171kb</size></version><version version=\"v2\"><date>Mon, 7 Oct 2019 07:27:59 GMT</date><size>402kb</size><source_type>D</source_type></version><title>Deep Learning with a Rethinking Structure for Multi-label Classification</title><authors>Yao-Yuan Yang, Yi-An Lin, Hong-Min Chu and Hsuan-Tien Lin</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-label classification (MLC) is an important class of machine learning\\nproblems that come with a wide spectrum of applications, each demanding a\\npossibly different evaluation criterion. When solving the MLC problems, we\\ngenerally expect the learning algorithm to take the hidden correlation of the\\nlabels into account to improve the prediction performance. Extracting the\\nhidden correlation is generally a challenging task. In this work, we propose a\\nnovel deep learning framework to better extract the hidden correlation with the\\nhelp of the memory structure within recurrent neural networks. The memory\\nstores the temporary guesses on the labels and effectively allows the framework\\nto rethink about the goodness and correlation of the guesses before making the\\nfinal prediction. Furthermore, the rethinking process makes it easy to adapt to\\ndifferent evaluation criteria to match real-world application needs. In\\nparticular, the framework can be trained in an end-to-end style with respect to\\nany given MLC evaluation criteria. The end-to-end design can be seamlessly\\ncombined with other deep learning techniques to conquer challenging MLC\\nproblems like image tagging. Experimental results across many real-world data\\nsets justify that the rethinking framework indeed improves MLC performance\\nacross different evaluation criteria and leads to superior performance over\\nstate-of-the-art MLC algorithms.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1802.02311</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1802.02311</id><submitter>Jinmiao Huang</submitter><version version=\"v1\"><date>Wed, 7 Feb 2018 05:23:21 GMT</date><size>1939kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 8 Jun 2019 16:35:12 GMT</date><size>2253kb</size><source_type>D</source_type></version><title>An Empirical Evaluation of Deep Learning for ICD-9 Code Assignment using\\n  MIMIC-III Clinical Notes</title><authors>Jinmiao Huang, Cesar Osorio, Luke Wicent Sy</authors><categories>cs.CL</categories><doi>10.1016/j.cmpb.2019.05.024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background and Objective: Code assignment is of paramount importance in many\\nlevels in modern hospitals, from ensuring accurate billing process to creating\\na valid record of patient care history. However, the coding process is tedious\\nand subjective, and it requires medical coders with extensive training. This\\nstudy aims to evaluate the performance of deep-learning-based systems to\\nautomatically map clinical notes to ICD-9 medical codes. Methods: The\\nevaluations of this research are focused on end-to-end learning methods without\\nmanually defined rules. Traditional machine learning algorithms, as well as\\nstate-of-the-art deep learning methods such as Recurrent Neural Networks and\\nConvolution Neural Networks, were applied to the Medical Information Mart for\\nIntensive Care (MIMIC-III) dataset. An extensive number of experiments was\\napplied to different settings of the tested algorithm. Results: Findings showed\\nthat the deep learning-based methods outperformed other conventional machine\\nlearning methods. From our assessment, the best models could predict the top 10\\nICD-9 codes with 0.6957 F1 and 0.8967 accuracy and could estimate the top 10\\nICD-9 categories with 0.7233 F1 and 0.8588 accuracy. Our implementation also\\noutperformed existing work under certain evaluation metrics. Conclusion: A set\\nof standard metrics was utilized in assessing the performance of ICD-9 code\\nassignment on MIMIC-III dataset. All the developed evaluation tools and\\nresources are available online, which can be used as a baseline for further\\nresearch.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1802.03462</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1802.03462</id><submitter>Zhichuang Sun</submitter><version version=\"v1\"><date>Fri, 9 Feb 2018 21:59:47 GMT</date><size>548kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 1 May 2019 17:27:56 GMT</date><size>635kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 1 Oct 2019 13:17:56 GMT</date><size>546kb</size><source_type>D</source_type></version><title>OAT: Attesting Operation Integrity of Embedded Devices</title><authors>Zhichuang Sun, Bo Feng, Long Lu, Somesh Jha</authors><categories>cs.CR</categories><comments>13 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the wide adoption of IoT/CPS systems, embedded devices(IoT frontends)\\nbecome increasingly connected and mission-critical, which in turn has attracted\\nadvanced attacks (e.g., control-flow hijacks and data-only attacks).\\nUnfortunately, IoT backends are unable to detect if such attacks have happened\\nwhile receiving data, service requests, or operation status from IoT devices.\\nAs a result, currently, IoT backends are forced to blindly trust the IoT\\ndevices that they interact with.\\n  To fill this void, we first formulate a new security property for embedded\\ndevices, called &quot;Operation Execution Integrity&quot; or OEI. We then design and\\nbuild a system, OAT, that enables remote OEI attestation for ARM-based\\nbare-metal embedded devices. Our formulation of OEI captures the integrity of\\nboth control flow and critical data involved in an operation execution.\\nTherefore, satisfying OEI entails that an operation execution is free of\\nunexpected control and data manipulations, which existing attestation methods\\ncannot check. Our design of OAT strikes a balance between prover\\'s constraints\\n(embedded devices\\' limited computing power and storage) and verifier\\'s\\nrequirements(complete verifiability and forensic assistance). OAT uses a new\\ncontrol-flow measurement scheme, which enables light-weight and space-efficient\\ncollection of measurements (97% space reduction from the trace-based approach).\\nOAT performs the remote control-flow verification through abstract execution,\\nwhich is fast and deterministic. OAT also features lightweight integrity\\nchecking for critical data (74% fewer instrumentation needed than previous\\nwork). Our security analysis shows that OAT allows remote verifiers or IoT\\nbackends to detect both control-flow hijacks and data-only attacks that affect\\nthe execution of operations on IoT devices. In our evaluation using real\\nembedded programs, OAT incurs a runtime overhead of 2.7%.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1802.03518</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1802.03518</id><submitter>Maur\\\\\\'icio Pamplona Segundo</submitter><version version=\"v1\"><date>Sat, 10 Feb 2018 04:16:36 GMT</date><size>3211kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 20 Mar 2019 18:03:28 GMT</date><size>3437kb</size><source_type>D</source_type></version><title>Hydra: an Ensemble of Convolutional Neural Networks for Geospatial Land\\n  Classification</title><authors>Rodrigo Minetto, Mauricio Pamplona Segundo, Sudeep Sarkar</authors><categories>cs.CV</categories><comments>12 pages, 14 figures, 5 tables</comments><doi>10.1109/TGRS.2019.2906883</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe in this paper Hydra, an ensemble of convolutional neural networks\\n(CNN) for geospatial land classification. The idea behind Hydra is to create an\\ninitial CNN that is coarsely optimized but provides a good starting pointing\\nfor further optimization, which will serve as the Hydra\\'s body. Then, the\\nobtained weights are fine-tuned multiple times with different augmentation\\ntechniques, crop styles, and classes weights to form an ensemble of CNNs that\\nrepresent the Hydra\\'s heads. By doing so, we prompt convergence to different\\nendpoints, which is a desirable aspect for ensembles. With this framework, we\\nwere able to reduce the training time while maintaining the classification\\nperformance of the ensemble. We created ensembles for our experiments using two\\nstate-of-the-art CNN architectures, ResNet and DenseNet. We have demonstrated\\nthe application of our Hydra framework in two datasets, FMOW and NWPU-RESISC45,\\nachieving results comparable to the state-of-the-art for the former and the\\nbest reported performance so far for the latter. Code and CNN models are\\navailable at https://github.com/maups/hydra-fmow\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1802.04987</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1802.04987</id><submitter>Luca Pappalardo</submitter><version version=\"v1\"><date>Wed, 14 Feb 2018 08:43:43 GMT</date><size>2178kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 16 Feb 2018 12:22:23 GMT</date><size>2513kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 25 Jan 2019 13:48:06 GMT</date><size>3071kb</size><source_type>D</source_type></version><title>PlayeRank: data-driven performance evaluation and player ranking in\\n  soccer via a machine learning approach</title><authors>Luca Pappalardo and Paolo Cintia and Paolo Ferragina and Emanuele\\n  Massucco and Dino Pedreschi and Fosca Giannotti</authors><categories>stat.AP cs.AI</categories><journal-ref>PlayeRank: Data-driven Performance Evaluation and Player Ranking\\n  in Soccer via a Machine Learning Approach. ACM Trans. Intell. Syst. Technol.\\n  10, 5, Article 59 (September 2019), 27 pages</journal-ref><doi>10.1145/3343172</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of evaluating the performance of soccer players is attracting the\\ninterest of many companies and the scientific community, thanks to the\\navailability of massive data capturing all the events generated during a match\\n(e.g., tackles, passes, shots, etc.). Unfortunately, there is no consolidated\\nand widely accepted metric for measuring performance quality in all of its\\nfacets. In this paper, we design and implement PlayeRank, a data-driven\\nframework that offers a principled multi-dimensional and role-aware evaluation\\nof the performance of soccer players. We build our framework by deploying a\\nmassive dataset of soccer-logs and consisting of millions of match events\\npertaining to four seasons of 18 prominent soccer competitions. By comparing\\nPlayeRank to known algorithms for performance evaluation in soccer, and by\\nexploiting a dataset of players\\' evaluations made by professional soccer\\nscouts, we show that PlayeRank significantly outperforms the competitors. We\\nalso explore the ratings produced by {\\\\sf PlayeRank} and discover interesting\\npatterns about the nature of excellent performances and what distinguishes the\\ntop players from the others. At the end, we explore some applications of\\nPlayeRank -- i.e. searching players and player versatility --- showing its\\nflexibility and efficiency, which makes it worth to be used in the design of a\\nscalable platform for soccer analytics.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1802.06940</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1802.06940</id><submitter>Alexander Semenov</submitter><version version=\"v1\"><date>Tue, 20 Feb 2018 02:47:41 GMT</date><size>79kb</size></version><title>Using Automatic Generation of Relaxation Constraints to Improve the\\n  Preimage Attack on 39-step MD4</title><authors>Irina Gribanova and Alexander Semenov</authors><categories>cs.AI</categories><comments>This paper was submitted to MIPRO 2018 as a conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we construct preimage attack on the truncated variant of the\\nMD4 hash function. Specifically, we study the MD4-39 function defined by the\\nfirst 39 steps of the MD4 algorithm. We suggest a new attack on MD4-39, which\\ndevelops the ideas proposed by H. Dobbertin in 1998. Namely, the special\\nrelaxation constraints are introduced in order to simplify the equations\\ncorresponding to the problem of finding a preimage for an arbitrary MD4-39 hash\\nvalue. The equations supplemented with the relaxation constraints are then\\nreduced to the Boolean Satisfiability Problem (SAT) and solved using the\\nstate-of-the-art SAT solvers. We show that the effectiveness of a set of\\nrelaxation constraints can be evaluated using the black-box function of a\\nspecial kind. Thus, we suggest automatic method of relaxation constraints\\ngeneration by applying the black-box optimization to this function. The\\nproposed method made it possible to find new relaxation constraints that\\ncontribute to a SAT-based preimage attack on MD4-39 which significantly\\noutperforms the competition.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1803.00345</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1803.00345</id><submitter>Oliver Braganza</submitter><version version=\"v1\"><date>Thu, 1 Mar 2018 12:43:09 GMT</date><size>4090kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 4 Oct 2018 10:59:24 GMT</date><size>3170kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 8 Apr 2019 11:33:28 GMT</date><size>4391kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Wed, 2 Oct 2019 14:22:36 GMT</date><size>4350kb</size><source_type>D</source_type></version><title>Proxyeconomics, the inevitable corruption of proxy-based competition</title><authors>Oliver Braganza</authors><categories>physics.soc-ph cs.GT q-fin.GN</categories><comments>This is a work in progress uploaded for sharing and feedback.\\n  Depending on contributions even authorships may change</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When society maintains a competitive system to promote an abstract goal,\\ncompetition by necessity relies on imperfect proxy measures. For instance\\nprofit is used to measure value to consumers, patient volumes to measure\\nhospital performance, or the Journal Impact Factor to measure scientific value.\\nHere we note that \\\\textit{any proxy measure in a competitive societal system\\nbecomes a target for the competitors, promoting corruption of the measure},\\nsuggesting a general applicability of what is best known as Campbell\\'s or\\nGoodhart\\'s Law. Indeed, prominent voices have argued that the scientific\\nreproducibility crisis or inaction to the threat of global warming represent\\ninstances of such competition induced corruption. Moreover, competing\\nindividuals often report that competitive pressures limit their ability to act\\naccording to the societal goal, suggesting lock-in. However, despite the\\nprofound implications, we lack a coherent theory of such a process. Here we\\npropose such a theory, formalized as an agent based model, integrating insights\\nfrom complex systems theory, contest theory, behavioral economics and cultural\\nevolution theory. The model reproduces empirically observed patterns at\\nmultiple levels. It further suggests that any system is likely to converge\\ntowards an equilibrium level of corruption determined by i) the information\\ncaptured in the proxy and ii) the strength of an intrinsic incentive towards\\nthe societal goal. Overall, the theory offers mechanistic insight to subjects\\nas diverse as the scientific reproducibility crisis and the threat of global\\nwarming.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1803.02869</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1803.02869</id><submitter>Tamal Dey</submitter><version version=\"v1\"><date>Wed, 7 Mar 2018 20:43:21 GMT</date><size>422kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 25 Sep 2018 21:22:32 GMT</date><size>420kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 3 Oct 2019 20:35:56 GMT</date><size>430kb</size><source_type>D</source_type></version><title>Computing Bottleneck Distance for Multi-parameter Interval Decomposable\\n  Persistence Modules</title><authors>Tamal K. Dey and Cheng Xin</authors><categories>cs.CG</categories><comments>This is the n-parameter extension of the conference paper that\\n  appeared in SoCG 2018 (which was only for 2-parameter case)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computation of the interleaving distance between persistence modules is a\\ncentral task in topological data analysis. For $1$-parameter persistence\\nmodules, thanks to the isometry theorem, this can be done by computing the\\nbottleneck distance with known efficient algorithms. The question is open for\\nmost $n$-parameter persistence modules, $n&gt;1$, because of the well recognized\\ncomplications of the indecomposables. Here, we consider a reasonably\\ncomplicated class called {\\\\em $n$-parameter interval decomposable} modules\\nwhose indecomposables may have a description of non-constant complexity. We\\npresent a polynomial time algorithm to compute the bottleneck distance for\\nthese modules from indecomposables, which bounds the interleaving distance from\\nabove, and give another algorithm to compute a new distance called {\\\\em\\ndimension distance} that bounds it from below. An earlier version of this paper\\nconsidered only the $2$-parameter interval decomposable\\nmodules~\\\\cite{DeyCheng18}.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1803.04610</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1803.04610</id><submitter>Phil Ammirato</submitter><version version=\"v1\"><date>Tue, 13 Mar 2018 03:56:36 GMT</date><size>5241kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 2 Jul 2018 23:31:15 GMT</date><size>5634kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 17 Jul 2018 17:42:05 GMT</date><size>5636kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Wed, 28 Nov 2018 17:48:26 GMT</date><size>1546kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Sat, 27 Jul 2019 15:10:47 GMT</date><size>1546kb</size><source_type>D</source_type></version><version version=\"v6\"><date>Tue, 1 Oct 2019 15:32:03 GMT</date><size>1551kb</size><source_type>D</source_type></version><title>Target Driven Instance Detection</title><authors>Phil Ammirato, Cheng-Yang Fu, Mykhailo Shvets, Jana Kosecka, Alexander\\n  C. Berg</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While state-of-the-art general object detectors are getting better and\\nbetter, there are not many systems specifically designed to take advantage of\\nthe instance detection problem. For many applications, such as household\\nrobotics, a system may need to recognize a few very specific instances at a\\ntime. Speed can be critical in these applications, as can the need to recognize\\npreviously unseen instances. We introduce a Target Driven Instance\\nDetector(TDID), which modifies existing general object detectors for the\\ninstance recognition setting. TDID not only improves performance on instances\\nseen during training, with a fast runtime, but is also able to generalize to\\ndetect novel instances.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1803.05945</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1803.05945</id><submitter>Gabriel Dario Alvarado Barrios</submitter><version version=\"v1\"><date>Thu, 15 Mar 2018 18:53:41 GMT</date><size>4007kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 14 May 2019 13:27:12 GMT</date><size>1028kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 27 Sep 2019 23:32:40 GMT</date><size>3008kb</size></version><title>Analog simulator of integro-differential equations with classical\\n  memristors</title><authors>G. Alvarado Barrios, J. C. Retamal, E. Solano and M. Sanz</authors><categories>cs.ET cs.NE quant-ph</categories><journal-ref>Scientific Reports 9, 12928 (2019)</journal-ref><doi>10.1038/s41598-019-49204-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An analog computer makes use of continuously changeable quantities of a\\nsystem, such as its electrical, mechanical, or hydraulic properties, to solve a\\ngiven problem. While these devices are usually computationally more powerful\\nthan their digital counterparts, they suffer from analog noise which does not\\nallow for error control. We will focus on analog computers based on active\\nelectrical networks comprised of resistors, capacitors, and operational\\namplifiers which are capable of simulating any linear ordinary differential\\nequation. However, the class of nonlinear dynamics they can solve is limited.\\nIn this work, by adding memristors to the electrical network, we show that the\\nanalog computer can simulate a large variety of linear and nonlinear\\nintegro-differential equations by carefully choosing the conductance and the\\ndynamics of the memristor state variable. To the best of our knowledge, this is\\nthe first time that circuits based on memristors are proposed for simulations.\\nWe study the performance of these analog computers by simulating\\nintegro-differential models related to fluid dynamics, nonlinear Volterra\\nequations for population growth, and quantum models describing non-Markovian\\nmemory effects, among others. Finally, we perform stability tests by\\nconsidering imperfect analog components, obtaining robust solutions with up to\\n$13\\\\%$ relative error for relevant timescales.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1803.06046</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1803.06046</id><submitter>Ali Devran Kara</submitter><version version=\"v1\"><date>Fri, 16 Mar 2018 01:04:42 GMT</date><size>51kb</size></version><version version=\"v2\"><date>Mon, 19 Mar 2018 23:59:21 GMT</date><size>41kb</size></version><version version=\"v3\"><date>Sat, 25 Aug 2018 10:04:54 GMT</date><size>49kb</size></version><version version=\"v4\"><date>Tue, 28 Aug 2018 12:18:24 GMT</date><size>44kb</size></version><version version=\"v5\"><date>Wed, 7 Aug 2019 16:13:54 GMT</date><size>56kb</size></version><version version=\"v6\"><date>Sun, 29 Sep 2019 16:54:31 GMT</date><size>195kb</size></version><title>Robustness to incorrect system models in stochastic control</title><authors>Ali Devran Kara and Serdar Y\\\\&quot;uksel</authors><categories>cs.SY</categories><comments>Conference version to appear at the 2018 IEEE CDC with title\\n  &quot;Robustness to Incorrect System Models in Stochastic Control and Application\\n  to Data-Driven Learning&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In stochastic control applications, typically only an ideal model (controlled\\ntransition kernel) is assumed and the control design is based on the given\\nmodel, raising the problem of performance loss due to the mismatch between the\\nassumed model and the actual model. Toward this end, we study continuity\\nproperties of discrete-time stochastic control problems with respect to system\\nmodels (i.e., controlled transition kernels) and robustness of optimal control\\npolicies designed for incorrect models applied to the true system. We study\\nboth fully observed and partially observed setups under an infinite horizon\\ndiscounted expected cost criterion. We show that continuity and robustness\\ncannot be established under weak and setwise convergences of transition kernels\\nin general, but that the expected induced cost is robust under total variation.\\nBy imposing further assumptions on the measurement models and on the kernel\\nitself (such as continuous convergence), we show that the optimal cost can be\\nmade continuous under weak convergence of transition kernels as well. Using\\nthese continuity properties, we establish convergence results and error bounds\\ndue to mismatch that occurs by the application of a control policy which is\\ndesigned for an incorrectly estimated system model to a true model, thus\\nestablishing positive and negative results on robustness.Compared to the\\nexisting literature, we obtain strictly refined robustness results that are\\napplicable even when the incorrect models can be investigated under weak\\nconvergence and setwise convergence criteria (with respect to a true model), in\\naddition to the total variation criteria. These entail positive implications on\\nempirical learning in (data-driven) stochastic control since often system\\nmodels are learned through empirical training data where typically weak\\nconvergence criterion applies but stronger convergence criteria do not.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1803.08134</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1803.08134</id><submitter>Qing Tian</submitter><version version=\"v1\"><date>Wed, 21 Mar 2018 20:52:32 GMT</date><size>1628kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 18 Nov 2018 01:20:41 GMT</date><size>3588kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sun, 2 Dec 2018 02:53:34 GMT</date><size>3588kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Wed, 1 May 2019 15:28:54 GMT</date><size>306kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Mon, 7 Oct 2019 08:35:54 GMT</date><size>5688kb</size><source_type>D</source_type></version><title>Task-specific Deep LDA pruning of neural networks</title><authors>Qing Tian, Tal Arbel, James J. Clark</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With deep learning\\'s success, a limited number of popular deep nets have been\\nwidely adopted for various vision tasks. However, this usually results in\\nunnecessarily high complexities and possibly many features of low task utility.\\nIn this paper, we address this problem by introducing a task-dependent deep\\npruning framework based on Fisher\\'s Linear Discriminant Analysis (LDA). The\\napproach can be applied to convolutional, fully-connected, and module-based\\ndeep network structures, in all cases leveraging the high decorrelation of\\nneuron motifs found in the pre-decision layer and cross-layer deconv\\ndependency. Moreover, we examine our approach\\'s potential in network\\narchitecture search for specific tasks and analyze the influence of our pruning\\non model robustness to noises and adversarial attacks. Experimental results on\\ndatasets of generic objects, as well as domain specific tasks (CIFAR100,\\nAdience, and LFWA) illustrate our framework\\'s superior performance over\\nstate-of-the-art pruning approaches and fixed compact nets (e.g. SqueezeNet,\\nMobileNet). The proposed method successfully maintains comparable accuracies\\neven after discarding most parameters (98%-99% for VGG16, up to 82% for the\\nalready compact InceptionNet) and with significant FLOP reductions (83% for\\nVGG16, up to 64% for InceptionNet). Through pruning, we can also derive\\nsmaller, but more accurate and more robust models suitable for the task.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1803.08190</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1803.08190</id><submitter>Geonho Cha</submitter><version version=\"v1\"><date>Thu, 22 Mar 2018 01:22:50 GMT</date><size>6641kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 04:44:03 GMT</date><size>6643kb</size><source_type>D</source_type></version><title>Deep Pose Consensus Networks</title><authors>Geonho Cha, Minsik Lee, Jungchan Cho, Songhwai Oh</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of estimating a 3D human pose from a\\nsingle image, which is important but difficult to solve due to many reasons,\\nsuch as self-occlusions, wild appearance changes, and inherent ambiguities of\\n3D estimation from a 2D cue. These difficulties make the problem ill-posed,\\nwhich have become requiring increasingly complex estimators to enhance the\\nperformance. On the other hand, most existing methods try to handle this\\nproblem based on a single complex estimator, which might not be good solutions.\\nIn this paper, to resolve this issue, we propose a\\nmultiple-partial-hypothesis-based framework for the problem of estimating 3D\\nhuman pose from a single image, which can be fine-tuned in an end-to-end\\nfashion. We first select several joint groups from a human joint model using\\nthe proposed sampling scheme, and estimate the 3D poses of each joint group\\nseparately based on deep neural networks. After that, they are aggregated to\\nobtain the final 3D poses using the proposed robust optimization formula. The\\noverall procedure can be fine-tuned in an end-to-end fashion, resulting in\\nbetter performance. In the experiments, the proposed framework shows the\\nstate-of-the-art performances on popular benchmark data sets, namely Human3.6M\\nand HumanEva, which demonstrate the effectiveness of the proposed framework.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1803.11266</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1803.11266</id><submitter>Patrick Schratz</submitter><version version=\"v1\"><date>Thu, 29 Mar 2018 21:48:11 GMT</date><size>4057kb</size><source_type>D</source_type></version><title>Performance evaluation and hyperparameter tuning of statistical and\\n  machine-learning models using spatial data</title><authors>Patrick Schratz, Jannes Muenchow, Eugenia Iturritxa, Jakob Richter,\\n  Alexander Brenning</authors><categories>stat.ML cs.LG stat.ME</categories><journal-ref>Ecological Modelling Volume 406, 24 August 2019, Pages 109-120</journal-ref><doi>10.1016/j.ecolmodel.2019.06.002</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Machine-learning algorithms have gained popularity in recent years in the\\nfield of ecological modeling due to their promising results in predictive\\nperformance of classification problems. While the application of such\\nalgorithms has been highly simplified in the last years due to their\\nwell-documented integration in commonly used statistical programming languages\\nsuch as R, there are several practical challenges in the field of ecological\\nmodeling related to unbiased performance estimation, optimization of algorithms\\nusing hyperparameter tuning and spatial autocorrelation. We address these\\nissues in the comparison of several widely used machine-learning algorithms\\nsuch as Boosted Regression Trees (BRT), k-Nearest Neighbor (WKNN), Random\\nForest (RF) and Support Vector Machine (SVM) to traditional parametric\\nalgorithms such as logistic regression (GLM) and semi-parametric ones like\\ngeneralized additive models (GAM). Different nested cross-validation methods\\nincluding hyperparameter tuning methods are used to evaluate model performances\\nwith the aim to receive bias-reduced performance estimates. As a case study the\\nspatial distribution of forest disease Diplodia sapinea in the Basque Country\\nin Spain is investigated using common environmental variables such as\\ntemperature, precipitation, soil or lithology as predictors. Results show that\\nGAM and RF (mean AUROC estimates 0.708 and 0.699) outperform all other methods\\nin predictive accuracy. The effect of hyperparameter tuning saturates at around\\n50 iterations for this data set. The AUROC differences between the bias-reduced\\n(spatial cross-validation) and overoptimistic (non-spatial cross-validation)\\nperformance estimates of the GAM and RF are 0.167 (24%) and 0.213 (30%),\\nrespectively. It is recommended to also use spatial partitioning for\\ncross-validation hyperparameter tuning of spatial data.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1803.11410</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1803.11410</id><submitter>Amnon Drory</submitter><version version=\"v1\"><date>Fri, 30 Mar 2018 11:06:43 GMT</date><size>1202kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 13:49:57 GMT</date><size>1987kb</size><source_type>D</source_type></version><title>The Resistance to Label Noise in K-NN and CNN Depends on its\\n  Concentration</title><authors>Amnon Drory, Oria Ratzon, Shai Avidan, Raja Giryes</authors><categories>cs.LG cs.CV cs.NE stat.ML</categories><comments>None</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the multi-class classification performance of K-Nearest\\nNeighbors (K-NN) and Convolutional Neural Networks (CNNs) in the presence of\\nlabel noise. We first show empirically that a CNN\\'s prediction for a given test\\nsample depends on the labels of the training samples in its local neighborhood.\\nThis motivates us to derive a realizable analytic expression that approximates\\nthe multi-class K-NN classification error in the presence of label noise, which\\nis of independent importance. We then suggest that the expression for K-NN may\\nserve as a first-order approximation for the CNN error. Finally, we demonstrate\\nempirically the proximity of the developed expression to the observed\\nperformance of K-NN and CNN classifiers. Our results may explain the already\\nobserved surprising resistance of CNNs to some types of label noise. In\\nparticular, it charcterizes an important factor in this resistance, by showing\\nthat the more concentrated the noise is (in the data), the greater the\\ndegration in performance.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1804.00175</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1804.00175</id><submitter>Yi Li</submitter><version version=\"v1\"><date>Sat, 31 Mar 2018 14:02:25 GMT</date><size>4737kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 25 Apr 2018 16:28:50 GMT</date><size>7202kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 14 Mar 2019 13:25:49 GMT</date><size>4357kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Wed, 2 Oct 2019 00:54:47 GMT</date><size>4384kb</size><source_type>D</source_type></version><title>DeepIM: Deep Iterative Matching for 6D Pose Estimation</title><authors>Yi Li, Gu Wang, Xiangyang Ji, Yu Xiang, Dieter Fox</authors><categories>cs.CV cs.RO</categories><comments>submitted to IJCV, update results on YCB_Video, add depth-based\\n  results</comments><doi>10.1007/s11263-019-01250-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating the 6D pose of objects from images is an important problem in\\nvarious applications such as robot manipulation and virtual reality. While\\ndirect regression of images to object poses has limited accuracy, matching\\nrendered images of an object against the observed image can produce accurate\\nresults. In this work, we propose a novel deep neural network for 6D pose\\nmatching named DeepIM. Given an initial pose estimation, our network is able to\\niteratively refine the pose by matching the rendered image against the observed\\nimage. The network is trained to predict a relative pose transformation using\\nan untangled representation of 3D location and 3D orientation and an iterative\\ntraining process. Experiments on two commonly used benchmarks for 6D pose\\nestimation demonstrate that DeepIM achieves large improvements over\\nstate-of-the-art methods. We furthermore show that DeepIM is able to match\\npreviously unseen objects.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1804.02969</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1804.02969</id><submitter>Tomas Kliegr</submitter><version version=\"v1\"><date>Mon, 9 Apr 2018 13:28:56 GMT</date><size>168kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 10 Apr 2018 06:31:38 GMT</date><size>168kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 27 Jun 2018 06:43:29 GMT</date><size>54kb</size></version><version version=\"v4\"><date>Thu, 3 Oct 2019 08:44:37 GMT</date><size>62kb</size></version><title>A review of possible effects of cognitive biases on interpretation of\\n  rule-based machine learning models</title><authors>Tom\\\\\\'a\\\\v{s} Kliegr, \\\\v{S}t\\\\v{e}p\\\\\\'an Bahn\\\\\\'ik, Johannes F\\\\&quot;urnkranz</authors><categories>stat.ML cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the interpretability of machine learning models is often equated with\\ntheir mere syntactic comprehensibility, we think that interpretability goes\\nbeyond that, and that human interpretability should also be investigated from\\nthe point of view of cognitive science. In particular, the goal of this paper\\nis to discuss to what extent cognitive biases may affect human understanding of\\ninterpretable machine learning models, in particular of logical rules\\ndiscovered from data. Twenty cognitive biases are covered, as are possible\\ndebiasing techniques that can be adopted by designers of machine learning\\nalgorithms and software. Our review transfers results obtained in cognitive\\npsychology to the domain of machine learning, aiming to bridge the current gap\\nbetween these two areas. It needs to be followed by empirical studies\\nspecifically focused on the machine learning domain.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1804.03596</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1804.03596</id><submitter>Xinghao Ding</submitter><version version=\"v1\"><date>Tue, 10 Apr 2018 15:43:48 GMT</date><size>6643kb</size><source_type>D</source_type></version><title>A Deep Information Sharing Network for Multi-contrast Compressed Sensing\\n  MRI Reconstruction</title><authors>Liyan Sun, Zhiwen Fan, Yue Huang, Xinghao Ding and John Paisley</authors><categories>cs.CV</categories><comments>13 pages, 16 figures, 3 tables</comments><doi>10.1109/TIP.2019.2925288</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multi-contrast magnetic resonance imaging (MRI), compressed sensing theory\\ncan accelerate imaging by sampling fewer measurements within each contrast. The\\nconventional optimization-based models suffer several limitations: strict\\nassumption of shared sparse support, time-consuming optimization and &quot;shallow&quot;\\nmodels with difficulties in encoding the rich patterns hiding in massive MRI\\ndata. In this paper, we propose the first deep learning model for\\nmulti-contrast MRI reconstruction. We achieve information sharing through\\nfeature sharing units, which significantly reduces the number of parameters.\\nThe feature sharing unit is combined with a data fidelity unit to comprise an\\ninference block. These inference blocks are cascaded with dense connections,\\nwhich allows for information transmission across different depths of the\\nnetwork efficiently. Our extensive experiments on various multi-contrast MRI\\ndatasets show that proposed model outperforms both state-of-the-art\\nsingle-contrast and multi-contrast MRI methods in accuracy and efficiency. We\\nshow the improved reconstruction quality can bring great benefits for the later\\nmedical image analysis stage. Furthermore, the robustness of the proposed model\\nto the non-registration environment shows its potential in real MRI\\napplications.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1804.04171</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1804.04171</id><submitter>Christoph H. Lampert</submitter><version version=\"v1\"><date>Wed, 11 Apr 2018 19:05:51 GMT</date><size>5847kb</size><source_type>D</source_type></version><title>KS(conf ): A Light-Weight Test if a ConvNet Operates Outside of Its\\n  Specifications</title><authors>R\\\\\\'emy Sun and Christoph H. Lampert</authors><categories>stat.ML cs.LG</categories><doi>10.1007/978-3-030-12939-2_18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer vision systems for automatic image categorization have become\\naccurate and reliable enough that they can run continuously for days or even\\nyears as components of real-world commercial applications. A major open problem\\nin this context, however, is quality control. Good classification performance\\ncan only be expected if systems run under the specific conditions, in\\nparticular data distributions, that they were trained for. Surprisingly, none\\nof the currently used deep network architectures has a built-in functionality\\nthat could detect if a network operates on data from a distribution that it was\\nnot trained for and potentially trigger a warning to the human users. In this\\nwork, we describe KS(conf), a procedure for detecting such outside of the\\nspecifications operation. Building on statistical insights, its main step is\\nthe applications of a classical Kolmogorov-Smirnov test to the distribution of\\npredicted confidence values. We show by extensive experiments using ImageNet,\\nAwA2 and DAVIS data on a variety of ConvNets architectures that KS(conf)\\nreliably detects out-of-specs situations. It furthermore has a number of\\nproperties that make it an excellent candidate for practical deployment: it is\\neasy to implement, adds almost no overhead to the system, works with all\\nnetworks, including pretrained ones, and requires no a priori knowledge about\\nhow the data distribution could change.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1804.06496</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1804.06496</id><submitter>Baosen Zhang</submitter><version version=\"v1\"><date>Tue, 17 Apr 2018 22:50:51 GMT</date><size>480kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 19:44:52 GMT</date><size>616kb</size><source_type>D</source_type></version><title>A Capacity-Price Game for Uncertain Renewables Resources</title><authors>Pan Li, Shreyas Sekar, Baosen Zhang</authors><categories>math.OC cs.GT</categories><comments>Appears in IEEE Transactions on Sustainable Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Renewable resources are starting to constitute a growing portion of the total\\ngeneration mix of the power system. A key difference between renewables and\\ntraditional generators is that many renewable resources are managed by\\nindividuals, especially in the distribution system. In this paper, we study the\\ncapacity investment and pricing problem, where multiple renewable producers\\ncompete in a decentralized market. It is known that most deterministic capacity\\ngames tend to result in very inefficient equilibria, even when there are a\\nlarge number of similar players. In contrast, we show that due to the inherent\\nrandomness of renewable resources, the equilibria in our capacity game becomes\\nefficient as the number of players grows and coincides with the centralized\\ndecision from the social planner\\'s problem. This result provides a new\\nperspective on how to look at the positive influence of randomness in a game\\nframework as well as its contribution to resource planning, scheduling, and\\nbidding. We validate our results by simulation studies using real world data.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1804.10462</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1804.10462</id><submitter>Abdolrahim Kadkhodamohammadi</submitter><version version=\"v1\"><date>Fri, 27 Apr 2018 12:14:40 GMT</date><size>4903kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 15:15:51 GMT</date><size>4932kb</size><source_type>D</source_type></version><title>A generalizable approach for multi-view 3D human pose regression</title><authors>Abdolrahim Kadkhodamohammadi, Nicolas Padoy</authors><categories>cs.CV</categories><comments>The supplementary video is available at https://youtu.be/Cx_kTRzqqzA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the significant improvement in the performance of monocular pose\\nestimation approaches and their ability to generalize to unseen environments,\\nmulti-view (MV) approaches are often lagging behind in terms of accuracy and\\nare specific to certain datasets. This is mainly due to the fact that (1)\\ncontrary to real world single-view (SV) datasets, MV datasets are often\\ncaptured in controlled environments to collect precise 3D annotations, which do\\nnot cover all real world challenges, and (2) the model parameters are learned\\nfor specific camera setups. To alleviate these problems, we propose a two-stage\\napproach to detect and estimate 3D human poses, which separates SV pose\\ndetection from MV 3D pose estimation. This separation enables us to utilize\\neach dataset for the right task, i.e. SV datasets for constructing robust pose\\ndetection models and MV datasets for constructing precise MV 3D regression\\nmodels. In addition, our 3D regression approach only requires 3D pose data and\\nits projections to the views for building the model, hence removing the need\\nfor collecting annotated data from the test setup. Our approach can therefore\\nbe easily generalized to a new environment by simply projecting 3D poses into\\n2D during training according to the camera setup used at test time. As 2D poses\\nare collected at test time using a SV pose detector, which might generate\\ninaccurate detections, we model its characteristics and incorporate this\\ninformation during training. We demonstrate that incorporating the detector\\'s\\ncharacteristics is important to build a robust 3D regression model and that the\\nresulting regression model generalizes well to new MV environments. Our\\nevaluation results show that our approach achieves competitive results on the\\nHuman3.6M dataset and significantly improves results on a MV clinical dataset\\nthat is the first MV dataset generated from live surgery recordings.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1804.10738</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1804.10738</id><submitter>Alexander Rusciano</submitter><version version=\"v1\"><date>Sat, 28 Apr 2018 03:57:45 GMT</date><size>12kb</size></version><version version=\"v2\"><date>Tue, 1 Oct 2019 18:30:23 GMT</date><size>16kb</size></version><title>A Riemannian Corollary of Helly\\'s Theorem</title><authors>Alexander Rusciano</authors><categories>math.MG cs.DS</categories><msc-class>52A01</msc-class><acm-class>F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a notion of halfspace for Hadamard manifolds that is natural in\\nthe context of convex optimization. For this notion of halfspace, we generalize\\na classic result of Gr\\\\&quot;unbaum, which itself is a corollary of Helly\\'s theorem.\\nNamely, given a probability distribution on the manifold, there is a point for\\nwhich all halfspaces based at this point have at least $\\\\frac{1}{n+1}$ of the\\nmass. As an application, the gradient oracle complexity of convex optimization\\nis polynomial in the parameters defining the problem.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1804.11116</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1804.11116</id><submitter>Jean-Simon Lemay</submitter><version version=\"v1\"><date>Mon, 30 Apr 2018 10:53:59 GMT</date><size>25kb</size></version><version version=\"v2\"><date>Tue, 4 Dec 2018 11:06:01 GMT</date><size>37kb</size></version><version version=\"v3\"><date>Tue, 26 Mar 2019 15:03:46 GMT</date><size>37kb</size></version><version version=\"v4\"><date>Mon, 5 Aug 2019 18:15:16 GMT</date><size>44kb</size></version><version version=\"v5\"><date>Thu, 3 Oct 2019 09:48:23 GMT</date><size>44kb</size></version><title>Lifting Coalgebra Modalities and $\\\\mathsf{MELL}$ Model Structure to\\n  Eilenberg-Moore Categories</title><authors>Jean-Simon Pacaud Lemay</authors><categories>cs.LO</categories><comments>An extend abstract version of this paper appears in the conference\\n  proceedings of the 3rd International Conference on Formal Structures for\\n  Computation and Deduction (FSCD 2018), under the title &quot;Lifting Coalgebra\\n  Modalities and $\\\\mathsf{MELL}$ Model Structure to Eilenberg-Moore Categories&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A categorical model of the multiplicative and exponential fragments of\\nintuitionistic linear logic ($\\\\mathsf{MELL}$), known as a \\\\emph{linear\\ncategory}, is a symmetric monoidal closed category with a monoidal coalgebra\\nmodality (also known as a linear exponential comonad). Inspired by Blute and\\nScott\\'s work on categories of modules of Hopf algebras as models of linear\\nlogic, we study categories of algebras of monads (also known as Eilenberg-Moore\\ncategories) as models of $\\\\mathsf{MELL}$. We define a $\\\\mathsf{MELL}$ lifting\\nmonad on a linear category as a Hopf monad -- in the Brugui{\\\\`e}res, Lack, and\\nVirelizier sense -- with a special kind of mixed distributive law over the\\nmonoidal coalgebra modality. As our main result, we show that the linear\\ncategory structure lifts to the category of algebras of $\\\\mathsf{MELL}$ lifting\\nmonads. We explain how groups in the category of coalgebras of the monoidal\\ncoalgebra modality induce $\\\\mathsf{MELL}$ lifting monads and provide a source\\nfor such groups from enrichment over abelian groups. Along the way we also\\ndefine mixed distributive laws of symmetric comonoidal monads over symmetric\\nmonoidal comonads and lifting differential category structure.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1804.11270</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1804.11270</id><submitter>Murilo Marques Marinho</submitter><version version=\"v1\"><date>Mon, 30 Apr 2018 15:13:47 GMT</date><size>8654kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 21 Nov 2018 10:00:40 GMT</date><size>9044kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 24 Jun 2019 14:16:51 GMT</date><size>1499kb</size></version><title>Dynamic Active Constraints for Surgical Robots using Vector Field\\n  Inequalities</title><authors>Murilo M. Marinho, Bruno V. Adorno, Kanako Harada and Mamoru Mitsuishi</authors><categories>cs.RO</categories><comments>Accepted on T-RO 2019, 19 Pages</comments><journal-ref>IEEE Transactions on Robotics, vol. 35, no. 5, pp. 1166-1185, Oct.\\n  2019</journal-ref><doi>10.1109/TRO.2019.2920078</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robotic assistance allows surgeons to perform dexterous and tremor-free\\nprocedures, but robotic aid is still underrepresented in procedures with\\nconstrained workspaces, such as deep brain neurosurgery and endonasal surgery.\\nIn these procedures, surgeons have restricted vision to areas near the surgical\\ntooltips, which increases the risk of unexpected collisions between the shafts\\nof the instruments and their surroundings. In this work, our\\nvector-field-inequalities method is extended to provide dynamic\\nactive-constraints to any number of robots and moving objects sharing the same\\nworkspace. The method is evaluated with experiments and simulations in which\\nrobot tools have to avoid collisions autonomously and in real-time, in a\\nconstrained endonasal surgical environment. Simulations show that with our\\nmethod the combined trajectory error of two robotic systems is optimal.\\nExperiments using a real robotic system show that the method can autonomously\\nprevent collisions between the moving robots themselves and between the robots\\nand the environment. Moreover, the framework is also successfully verified\\nunder teleoperation with tool-tissue interactions.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1805.00178</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1805.00178</id><submitter>Rui Wang</submitter><version version=\"v1\"><date>Tue, 1 May 2018 04:09:09 GMT</date><size>95kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 19 Oct 2018 00:12:50 GMT</date><size>0kb</size><source_type>I</source_type></version><version version=\"v3\"><date>Thu, 3 Oct 2019 02:45:00 GMT</date><size>133kb</size><source_type>D</source_type></version><title>Dynamic Sentence Sampling for Efficient Training of Neural Machine\\n  Translation</title><authors>Rui Wang, Masao Utiyama, and Eiichiro Sumita</authors><categories>cs.CL</categories><comments>Revised version of ACL-2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional Neural machine translation (NMT) involves a fixed training\\nprocedure where each sentence is sampled once during each epoch. In reality,\\nsome sentences are well-learned during the initial few epochs; however, using\\nthis approach, the well-learned sentences would continue to be trained along\\nwith those sentences that were not well learned for 10-30 epochs, which results\\nin a wastage of time. Here, we propose an efficient method to dynamically\\nsample the sentences in order to accelerate the NMT training. In this approach,\\na weight is assigned to each sentence based on the measured difference between\\nthe training costs of two iterations. Further, in each epoch, a certain\\npercentage of sentences are dynamically sampled according to their weights.\\nEmpirical results based on the NIST Chinese-to-English and the WMT\\nEnglish-to-German tasks depict that the proposed method can significantly\\naccelerate the NMT training and improve the NMT performance.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1805.02089</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1805.02089</id><submitter>Marco Bressan</submitter><version version=\"v1\"><date>Sat, 5 May 2018 17:36:45 GMT</date><size>98kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 27 Jul 2018 12:54:07 GMT</date><size>35kb</size></version><version version=\"v3\"><date>Tue, 26 Feb 2019 16:59:28 GMT</date><size>99kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Mon, 7 Oct 2019 14:58:21 GMT</date><size>314kb</size><source_type>D</source_type></version><title>Faster subgraph counting in sparse graphs</title><authors>Marco Bressan</authors><categories>cs.CC</categories><comments>Extended version of a work appeared at IPEC 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental graph problem asks to compute the number of induced copies of a\\n$k$-node pattern graph $H$ in an $n$-node graph $G$. The fastest algorithm to\\ndate is still the 35-years-old algorithm by Ne\\\\v{s}et\\\\v{r}il and Poljak [31],\\nwith running time $f(k) \\\\cdot O(n^{\\\\omega\\\\lfloor\\\\frac{k}{3}\\\\rfloor + 2})$ where\\n$\\\\omega \\\\le 2.373$ is the matrix multiplication exponent. In this work we show\\nthat, if one takes into account the degeneracy $d$ of $G$, then the picture\\nbecomes substantially richer and leads to faster algorithms when $G$ is\\nsufficiently sparse. More precisely, after introducing a novel notion of graph\\nwidth, the \\\\emph{DAG-treewidth}, we prove what follows. If $H$ has\\nDAG-treewidth $\\\\tau(H)$ and $G$ has degeneracy $d$, then the induced copies of\\n$H$ in $G$ can be counted in time $f(d,k) \\\\cdot \\\\tilde{O}(n^{\\\\tau(H)})$; and,\\nunder the Exponential Time Hypothesis, no algorithm can solve the problem in\\ntime $f(d,k) \\\\cdot n^{o(\\\\tau(H)/\\\\ln \\\\tau(H))}$ for all $H$. This result\\ncharacterises the complexity of counting subgraphs in a $d$-degenerate graph.\\nDeveloping bounds on $\\\\tau(H)$, then, we obtain natural generalisations of\\nclassic results and faster algorithms for sparse graphs. For example, when\\n$d=O(\\\\operatorname{poly}\\\\log(n))$ we can count the induced copies of any $H$ in\\ntime $f(k) \\\\cdot\\\\tilde{O}(n^{\\\\lfloor \\\\frac{k}{4} \\\\rfloor + 2})$, beating the\\nNe\\\\v{s}et\\\\v{r}il-Poljak algorithm by essentially a cubic factor in $n$.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1805.02412</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1805.02412</id><submitter>Oscar Defrain</submitter><version version=\"v1\"><date>Mon, 7 May 2018 09:34:22 GMT</date><size>121kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 13:45:34 GMT</date><size>140kb</size><source_type>D</source_type></version><title>Neighborhood inclusions for minimal dominating sets enumeration: linear\\n  and polynomial delay algorithms in $P_7$-free and $P_8$-free chordal graphs</title><authors>Oscar Defrain and Lhouari Nourine</authors><categories>cs.DM cs.DS math.CO</categories><comments>16 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [M. M. Kant\\\\\\'e, V. Limouzy, A. Mary, and L. Nourine. On the enumeration of\\nminimal dominating sets and related notions. SIAM Journal on Discrete\\nMathematics, 28(4):1916-1929, 2014] the authors give an $O(n+m)$ delay\\nalgorithm based on neighborhood inclusions for the enumeration of minimal\\ndominating sets in split and $P_6$-free chordal graphs. In this paper, we\\ninvestigate generalizations of this technique to $P_k$-free chordal graphs for\\nlarger integers $k$. In particular, we give $O(n+m)$ and $O(n^3\\\\cdot m)$ delays\\nalgorithms in the classes of $P_7$-free and $P_8$-free chordal graphs. As for\\n$P_k$-free chordal graphs for $k\\\\geq 9$, we give evidence that such a technique\\nis inefficient as a key step of the algorithm, namely the irredundant extension\\nproblem, becomes NP-complete.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1805.02566</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1805.02566</id><submitter>Hyoukjun Kwon</submitter><version version=\"v1\"><date>Fri, 4 May 2018 15:36:44 GMT</date><size>2791kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 13 Sep 2018 17:09:00 GMT</date><size>1488kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 4 Feb 2019 17:53:17 GMT</date><size>7055kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Sun, 8 Sep 2019 01:02:40 GMT</date><size>7875kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Tue, 1 Oct 2019 23:41:20 GMT</date><size>7874kb</size><source_type>D</source_type></version><title>Understanding Reuse, Performance, and Hardware Cost of DNN Dataflows: A\\n  Data-Centric Approach</title><authors>Hyoukjun Kwon, Prasanth Chatarasi, Michael Pellauer, Angshuman\\n  Parashar, Vivek Sarkar, Tushar Krishna</authors><categories>cs.DC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The data partitioning and scheduling strategies used by DNN accelerators to\\nleverage reuse and perform staging are known as dataflow, and they directly\\nimpact the performance and energy efficiency of DNN accelerator designs. An\\naccelerator microarchitecture dictates the dataflow(s) that can be employed to\\nexecute a layer or network. Selecting an optimal dataflow for a layer shape can\\nhave a large impact on utilization and energy efficiency, but there is a lack\\nof understanding on the choices and consequences of dataflows, and of tools and\\nmethodologies to help architects explore the co-optimization design space. In\\nthis work, we first introduce a set of data-centric directives to concisely\\nspecify the space of DNN dataflows in a compilerfriendly form. We then show how\\nthese directives can be analyzed to infer various forms of reuse and to exploit\\nthem using hardware capabilities. We codify this analysis into an analytical\\ncost model, MAESTRO (Modeling Accelerator Efficiency via Spatio-Temporal Reuse\\nand Occupancy), that estimates various cost-benefit tradeoffs of a dataflow\\nincluding execution time and energy efficiency for a DNN model and hardware\\nconfiguration. We demonstrate the use of MAESTRO to drive a hardware design\\nspace exploration (DSE) experiment, which searches across 480M designs to\\nidentify 2.5M valid designs at an average rate of 0.17M designs per second,\\nincluding Pareto-optimal throughput- and energy-optimized design points.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1805.04141</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1805.04141</id><submitter>Assia Benbihi</submitter><version version=\"v1\"><date>Thu, 10 May 2018 19:14:06 GMT</date><size>3649kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 6 Oct 2019 19:34:31 GMT</date><size>679kb</size><source_type>D</source_type></version><title>Semi-Supervised Domain Adaptation with Representation Learning for\\n  Semantic Segmentation across Time</title><authors>Assia Benbihi, Matthieu Geist, C\\\\\\'edric Pradalier</authors><categories>cs.CV</categories><journal-ref>Neural Information Processing - 26th International Conference,\\n  {ICONIP} 2019, Sydney, Australia, December 12-15, 2019, Proceedings,</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning generates state-of-the-art semantic segmentation provided that\\na large number of images together with pixel-wise annotations are available. To\\nalleviate the expensive data collection process, we propose a semi-supervised\\ndomain adaptation method for the specific case of images with similar semantic\\ncontent but different pixel distributions. A network trained with supervision\\non a past dataset is finetuned on the new dataset to conserve its features\\nmaps. The domain adaptation becomes a simple regression between feature maps\\nand does not require annotations on the new dataset. This method reaches\\nperformances similar to classic transfer learning on the PASCAL VOC dataset\\nwith synthetic transformations.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1805.04288</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1805.04288</id><submitter>Xiu-Shen Wei</submitter><version version=\"v1\"><date>Fri, 11 May 2018 09:24:15 GMT</date><size>2126kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 21 Jun 2019 08:03:07 GMT</date><size>11065kb</size><source_type>D</source_type></version><title>Piecewise classifier mappings: Learning fine-grained learners for novel\\n  categories with few examples</title><authors>Xiu-Shen Wei, Peng Wang, Lingqiao Liu, Chunhua Shen, Jianxin Wu</authors><categories>cs.CV</categories><comments>Accepted by IEEE TIP</comments><doi>10.1109/TIP.2019.2924811</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humans are capable of learning a new fine-grained concept with very little\\nsupervision, \\\\emph{e.g.}, few exemplary images for a species of bird, yet our\\nbest deep learning systems need hundreds or thousands of labeled examples. In\\nthis paper, we try to reduce this gap by studying the fine-grained image\\nrecognition problem in a challenging few-shot learning setting, termed few-shot\\nfine-grained recognition (FSFG). The task of FSFG requires the learning systems\\nto build classifiers for novel fine-grained categories from few examples (only\\none or less than five). To solve this problem, we propose an end-to-end\\ntrainable deep network which is inspired by the state-of-the-art fine-grained\\nrecognition model and is tailored for the FSFG task.\\n  Specifically, our network consists of a bilinear feature learning module and\\na classifier mapping module: while the former encodes the discriminative\\ninformation of an exemplar image into a feature vector, the latter maps the\\nintermediate feature into the decision boundary of the novel category. The key\\nnovelty of our model is a &quot;piecewise mappings&quot; function in the classifier\\nmapping module, which generates the decision boundary via learning a set of\\nmore attainable sub-classifiers in a more parameter-economic way. We learn the\\nexemplar-to-classifier mapping based on an auxiliary dataset in a meta-learning\\nfashion, which is expected to be able to generalize to novel categories. By\\nconducting comprehensive experiments on three fine-grained datasets, we\\ndemonstrate that the proposed method achieves superior performance over the\\ncompeting baselines.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1805.05732</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1805.05732</id><submitter>Adel Hafiane</submitter><version version=\"v1\"><date>Tue, 15 May 2018 12:41:48 GMT</date><size>4743kb</size><source_type>D</source_type></version><title>Robust Adaptive Median Binary Pattern for noisy texture classification\\n  and retrieval</title><authors>Mohammad Alkhatib and Adel Hafiane</authors><categories>cs.CV</categories><doi>10.1109/TIP.2019.2916742</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Texture is an important cue for different computer vision tasks and\\napplications. Local Binary Pattern (LBP) is considered one of the best yet\\nefficient texture descriptors. However, LBP has some notable limitations,\\nmostly the sensitivity to noise. In this paper, we address these criteria by\\nintroducing a novel texture descriptor, Robust Adaptive Median Binary Pattern\\n(RAMBP). RAMBP based on classification process of noisy pixels, adaptive\\nanalysis window, scale analysis and image regions median comparison. The\\nproposed method handles images with high noisy textures, and increases the\\ndiscriminative properties by capturing microstructure and macrostructure\\ntexture information. The proposed method has been evaluated on popular texture\\ndatasets for classification and retrieval tasks, and under different high noise\\nconditions. Without any train or prior knowledge of noise type, RAMBP achieved\\nthe best classification compared to state-of-the-art techniques. It scored more\\nthan $90\\\\%$ under $50\\\\%$ impulse noise densities, more than $95\\\\%$ under\\nGaussian noised textures with standard deviation $\\\\sigma = 5$, and more than\\n$99\\\\%$ under Gaussian blurred textures with standard deviation $\\\\sigma = 1.25$.\\nThe proposed method yielded competitive results and high performance as one of\\nthe best descriptors in noise-free texture classification. Furthermore, RAMBP\\nshowed also high performance for the problem of noisy texture retrieval\\nproviding high scores of recall and precision measures for textures with high\\nlevels of noise.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1805.05838</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1805.05838</id><submitter>Tribhuvanesh Orekondy</submitter><version version=\"v1\"><date>Tue, 15 May 2018 15:12:45 GMT</date><size>4661kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 12:02:07 GMT</date><size>4333kb</size><source_type>D</source_type></version><title>Gradient-Leaks: Understanding and Controlling Deanonymization in\\n  Federated Learning</title><authors>Tribhuvanesh Orekondy, Seong Joon Oh, Yang Zhang, Bernt Schiele, Mario\\n  Fritz</authors><categories>cs.CR cs.AI cs.CV cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Federated Learning (FL) systems are gaining popularity as a solution to\\ntraining Machine Learning (ML) models from large-scale user data collected on\\npersonal devices (e.g., smartphones) without their raw data leaving the device.\\nAt the core of FL is a network of anonymous user devices sharing minimal\\ntraining information (model parameter deltas) computed locally on personal\\ndata. However, the degree to which user-specific information is encoded in the\\nmodel deltas is poorly understood. In this paper, we identify model deltas\\nencode subtle variations in which users capture and generate data. The\\nvariations provide a powerful statistical signal, allowing an adversary to\\neffectively deanonymize participating devices using a limited set of auxiliary\\ndata. We analyze resulting deanonymization attacks on diverse tasks on\\nreal-world (anonymized) user-generated data across a range of closed- and\\nopen-world scenarios. We study various strategies to mitigate the risks of\\ndeanonymization. As random perturbation methods do not offer convincing\\noperating points, we propose data-augmentation strategies which introduces\\nadversarial biases in device data and thereby, offer substantial protection\\nagainst deanonymization threats with little effect on utility.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1805.06989</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1805.06989</id><submitter>Jo\\\\~ao Leit\\\\~ao</submitter><version version=\"v1\"><date>Thu, 17 May 2018 23:12:40 GMT</date><size>144kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 07:39:13 GMT</date><size>342kb</size><source_type>D</source_type></version><title>Towards Enabling Novel Edge-Enabled Applications</title><authors>Jo\\\\~ao Leit\\\\~ao and Pedro \\\\\\'Akos Costa and Maria Cec\\\\\\'ilia Gomes and\\n  Nuno Pregui\\\\c{c}a</authors><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Edge computing has emerged as a distributed computing paradigm to overcome\\npractical scalability limits of cloud computing. The main principle of edge\\ncomputing is to leverage on computational resources outside of the cloud for\\nperforming computations closer to data sources, avoiding unnecessary data\\ntransfers to the cloud and enabling faster responses for clients.\\n  While this paradigm has been successfully employed to improve response times\\nin some contexts, mostly by having clients perform pre-processing and/or\\nfiltering of data, or by leveraging on distributed caching infrastructures, we\\nargue that the combination of edge and cloud computing has the potential to\\nenable novel applications. However, to do so, some significant research\\nchallenges have to be tackled by the computer science community. In this paper,\\nwe discuss different edge resources and their potential use, motivated by\\nenvisioned use cases. We then discuss concrete research challenges that are in\\nthe critical path towards realizing our edge vision. We conclude by proposing a\\nresearch agenda to allow the full exploitation of the potential for the\\nemerging hybrid cloud/edge paradigm.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1805.07239</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1805.07239</id><submitter>Alexander Semenov</submitter><version version=\"v1\"><date>Thu, 17 May 2018 09:18:14 GMT</date><size>1360kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 18 Mar 2019 08:42:30 GMT</date><size>1880kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 19 Mar 2019 05:21:43 GMT</date><size>1880kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Fri, 4 Oct 2019 03:02:26 GMT</date><size>1871kb</size><source_type>D</source_type></version><title>Translation of Algorithmic Descriptions of Discrete Functions to SAT\\n  with Applications to Cryptanalysis Problems</title><authors>Alexander Semenov, Ilya Otpuschennikov, Irina Gribanova, Oleg Zaikin,\\n  Stepan Kochemazov</authors><categories>cs.LO cs.AI cs.CR</categories><comments>Revised version of the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present paper, we propose a technology for translating algorithmic\\ndescriptions of discrete functions to SAT. The proposed technology is aimed at\\napplications in algebraic cryptanalysis. We describe how cryptanalysis problems\\nare reduced to SAT in such a way that it should be perceived as natural by the\\ncryptographic community. In~the theoretical part of the paper we justify the\\nmain principles of general reduction to SAT for discrete functions from a class\\ncontaining the majority of functions employed in cryptography. Then, we\\ndescribe the Transalg software tool developed based on these principles with\\nSAT-based cryptanalysis specifics in mind. We demonstrate the results of\\napplications of Transalg to construction of a number of attacks on various\\ncryptographic functions. Some of the corresponding attacks are state of the\\nart. We compare the functional capabilities of the proposed tool with that of\\nother domain-specific software tools which can be used to reduce cryptanalysis\\nproblems to SAT, and also with the CBMC system widely employed in symbolic\\nverification. The paper also presents vast experimental data, obtained using\\nthe SAT solvers that took first places at the SAT competitions in the recent\\nseveral years.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1805.07440</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1805.07440</id><submitter>Linnan Wang</submitter><version version=\"v1\"><date>Fri, 18 May 2018 20:57:41 GMT</date><size>1797kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 19 Dec 2018 07:47:47 GMT</date><size>7903kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 25 Feb 2019 06:50:28 GMT</date><size>7903kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Wed, 2 Oct 2019 01:04:05 GMT</date><size>7590kb</size><source_type>D</source_type></version><title>AlphaX: eXploring Neural Architectures with Deep Neural Networks and\\n  Monte Carlo Tree Search</title><authors>Linnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian, Rodrigo Fonseca</authors><categories>cs.LG cs.AI cs.CV cs.DC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present AlphaX, a fully automated agent that designs complex neural\\narchitectures from scratch. AlphaX explores the search space with a distributed\\nMonte Carlo Tree Search (MCTS) and a Meta-Deep Neural Network (DNN). MCTS\\nguides transfer learning and intrinsically improves the search efficiency by\\ndynamically balancing the exploration and exploitation at fine-grained states,\\nwhile Meta-DNN predicts the network accuracy to guide the search, and to\\nprovide an estimated reward to speed up the rollout. As the search progresses,\\nAlphaX also generates the training data for Meta-DNN. So, the learning of\\nMeta-DNN is end-to-end. In 8 GPU days, AlphaX found an architecture that\\nreaches 97.88\\\\% top-1 accuracy on CIFAR-10, and 75.5\\\\% top-1 accuracy on\\nImageNet. We also evaluate AlphaX on a large scale NAS dataset for\\nreproducibility. On NASBench-101, AlphaX also demonstrates 3x and 2.8x speedup\\nover \\\\textit{Random Search} and \\\\textit{Regularized Evolution} in finding the\\nglobal optimum. Finally, we show the searched architecture improves a variety\\nof vision applications from Neural Style Transfer, to Image Captioning and\\nObject Detection.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1805.07816</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1805.07816</id><submitter>Jiefeng Chen</submitter><version version=\"v1\"><date>Sun, 20 May 2018 19:37:54 GMT</date><size>190kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 25 May 2018 02:46:43 GMT</date><size>190kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 20 Nov 2018 18:30:38 GMT</date><size>7334kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Tue, 16 Apr 2019 02:02:16 GMT</date><size>662kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Thu, 3 Oct 2019 15:43:21 GMT</date><size>7324kb</size><source_type>D</source_type></version><title>Towards Understanding Limitations of Pixel Discretization Against\\n  Adversarial Attacks</title><authors>Jiefeng Chen, Xi Wu, Vaibhav Rastogi, Yingyu Liang, Somesh Jha</authors><categories>cs.CR cs.LG cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wide adoption of artificial neural networks in various domains has led to an\\nincreasing interest in defending adversarial attacks against them.\\nPreprocessing defense methods such as pixel discretization are particularly\\nattractive in practice due to their simplicity, low computational overhead, and\\napplicability to various systems. It is observed that such methods work well on\\nsimple datasets like MNIST, but break on more complicated ones like ImageNet\\nunder recently proposed strong white-box attacks. To understand the conditions\\nfor success and potentials for improvement, we study the pixel discretization\\ndefense method, including more sophisticated variants that take into account\\nthe properties of the dataset being discretized. Our results again show poor\\nresistance against the strong attacks. We analyze our results in a theoretical\\nframework and offer strong evidence that pixel discretization is unlikely to\\nwork on all but the simplest of the datasets. Furthermore, our arguments\\npresent insights why some other preprocessing defenses may be insecure.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1805.08244</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1805.08244</id><submitter>Jing An</submitter><version version=\"v1\"><date>Mon, 21 May 2018 18:24:20 GMT</date><size>174kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 16 Oct 2018 16:28:31 GMT</date><size>174kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 27 Sep 2019 20:08:55 GMT</date><size>1002kb</size><source_type>D</source_type></version><title>Stochastic modified equations for the asynchronous stochastic gradient\\n  descent</title><authors>Jing An, Jianfeng Lu, Lexing Ying</authors><categories>stat.ML cs.LG</categories><comments>Final version. To appear in Information and Inference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a stochastic modified equations (SME) for modeling the\\nasynchronous stochastic gradient descent (ASGD) algorithms. The resulting SME\\nof Langevin type extracts more information about the ASGD dynamics and\\nelucidates the relationship between different types of stochastic gradient\\nalgorithms. We show the convergence of ASGD to the SME in the continuous time\\nlimit, as well as the SME\\'s precise prediction to the trajectories of ASGD with\\nvarious forcing terms. As an application of the SME, we propose an optimal\\nmini-batching strategy for ASGD via solving the optimal control problem of the\\nassociated SME.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1805.10238</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1805.10238</id><submitter>Marco Camurri</submitter><version version=\"v1\"><date>Fri, 25 May 2018 16:30:55 GMT</date><size>5481kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 31 May 2018 12:00:17 GMT</date><size>4308kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 12 Jun 2018 15:01:34 GMT</date><size>5617kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Wed, 7 Nov 2018 13:42:05 GMT</date><size>6601kb</size><source_type>D</source_type></version><title>Heuristic Planning for Rough Terrain Locomotion in Presence of External\\n  Disturbances and Variable Perception Quality</title><authors>Michele Focchi, Romeo Orsolino, Marco Camurri, Victor Barasuol, Carlos\\n  Mastalli, Darwin G. Caldwell, Claudio Semini</authors><categories>cs.RO</categories><comments>20 pages, 24 figures</comments><doi>10.1007/978-3-030-22327-4_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quality of the visual feedback can vary significantly on a legged robot\\nthat is meant to traverse unknown and unstructured terrains. The map of the\\nenvironment, acquired with online state-of-the-art algorithms, often degrades\\nafter a few steps, due to sensing inaccuracies, slippage and unexpected\\ndisturbances. When designing locomotion algorithms, this degradation can result\\nin planned trajectories that are not consistent with the reality, if not dealt\\nproperly. In this work, we propose a heuristic-based planning approach that\\nenables a quadruped robot to successfully traverse a significantly rough\\nterrain (e.g., stones up to 10 cm of diameter), in absence of visual feedback.\\nWhen available, the approach allows also to exploit the visual feedback (e.g.,\\nto enhance the stepping strategy) in multiple ways, according to the quality of\\nthe 3D map. The proposed framework also includes reflexes, triggered in\\nspecific situations, and the possibility to estimate online an unknown\\ntime-varying disturbance and compensate for it. We demonstrate the\\neffectiveness of the approach with experiments performed on our quadruped robot\\nHyQ (85 kg), traversing different terrains, such as: ramps, rocks, bricks,\\npallets and stairs. We also demonstrate the capability to estimate and\\ncompensate for disturbances, showing the robot walking up a ramp while pulling\\na cart attached to its back.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1805.11251</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1805.11251</id><submitter>Shinsaku Sakaue</submitter><version version=\"v1\"><date>Tue, 29 May 2018 05:35:08 GMT</date><size>669kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 30 May 2018 03:55:27 GMT</date><size>667kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 19 Mar 2019 08:01:13 GMT</date><size>371kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Tue, 21 May 2019 04:08:42 GMT</date><size>2018kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Thu, 3 Oct 2019 03:16:52 GMT</date><size>4062kb</size><source_type>D</source_type></version><title>On Maximization of Weakly Modular Functions: Guarantees of Multi-stage\\n  Algorithms, Tractability, and Hardness</title><authors>Shinsaku Sakaue</authors><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maximization of {\\\\it non-submodular} functions appears in various scenarios,\\nand many previous works studied it based on some measures that quantify the\\ncloseness to being submodular. On the other hand, many practical non-submodular\\nfunctions are actually close to being {\\\\it modular}, which has been utilized in\\nfew studies. In this paper, we study cardinality-constrained maximization of\\n{\\\\it weakly modular} functions, whose closeness to being modular is measured by\\n{\\\\it submodularity} and {\\\\it supermodularity ratios}, and reveal what we can\\nand cannot do by using the weak modularity. We first show that guarantees of\\nmulti-stage algorithms can be proved with the weak modularity, which generalize\\nand improve some existing results, and experiments confirm their effectiveness.\\nWe then show that weakly modular maximization is {\\\\it fixed-parameter\\ntractable} under certain conditions; as a byproduct, we provide a new\\ntime--accuracy trade-off for $\\\\ell_0$-constrained minimization. We finally\\nprove that, even if objective functions are weakly modular, no polynomial-time\\nalgorithms can improve the existing approximation guarantees achieved by the\\ngreedy algorithm.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1805.11589</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1805.11589</id><submitter>Angelica I. Aviles-Rivero</submitter><version version=\"v1\"><date>Tue, 29 May 2018 17:12:33 GMT</date><size>4077kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 25 Jan 2019 20:49:29 GMT</date><size>6780kb</size><source_type>D</source_type></version><title>Mirror, Mirror, on the Wall, Who\\'s Got the Clearest Image of Them All? -\\n  A Tailored Approach to Single Image Reflection Removal</title><authors>Daniel Heydecker, Georg Maierhofer, Angelica I. Aviles-Rivero, Qingnan\\n  Fan, Dongdong Chen, Carola-Bibiane Sch\\\\&quot;onlieb and Sabine S\\\\&quot;usstrunk</authors><categories>cs.CV</categories><doi>10.1109/TIP.2019.2923559</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Removing reflection artefacts from a single image is a problem of both\\ntheoretical and practical interest, which still presents challenges because of\\nthe massively ill-posed nature of the problem. In this work, we propose a\\ntechnique based on a novel optimisation problem. Firstly, we introduce a simple\\nuser interaction scheme, which helps minimise information loss in\\nreflection-free regions. Secondly, we introduce an $H^2$ fidelity term, which\\npreserves fine detail while enforcing global colour similarity. We show that\\nthis combination allows us to mitigate some major drawbacks of the existing\\nmethods for reflection removal. We demonstrate, through numerical and visual\\nexperiments, that our method is able to outperform the state-of-the-art methods\\nand compete with recent deep-learning approaches.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1805.11722</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1805.11722</id><submitter>Joao Victor De Carvalho Evangelista</submitter><version version=\"v1\"><date>Tue, 29 May 2018 21:42:35 GMT</date><size>306kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 20:36:45 GMT</date><size>2614kb</size><source_type>D</source_type></version><title>Fairness and Sum-Rate Maximization via Joint Channel and Power\\n  Allocation in Uplink SCMA Networks</title><authors>Joao V.C. Evangelista, Zeeshan Sattar, Georges Kaddoum, Anas Chaaban</authors><categories>cs.IT math.IT</categories><comments>This work has been submitted to the IEEE for possible publication.\\n  Copyright may be transferred without notice, after which this version may no\\n  longer be accessible</comments><doi>10.1109/TWC.2019.2939820</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider a sparse code multiple access uplink system, where\\n$J$ users simultaneously transmit data over $K$ subcarriers, such that $J &gt; K$,\\nwith a constraint on the power transmitted by each user. To jointly optimize\\nthe subcarrier assignment and the transmitted power per subcarrier, two new\\niterative algorithms are proposed, the first one aims to maximize the sum-rate\\n(Max-SR) of the network, while the second aims to maximize the fairness\\n(Max-Min). In both cases, the optimization problem is of the mixed-integer\\nnonlinear programming (MINLP) type, with non-convex objective functions, which\\nare generally not tractable. We prove that both joint allocation problems are\\nNP-hard. To address these issues, we employ a variant of the block successive\\nupper-bound minimization (BSUM) \\\\cite{razaviyayn.2013} framework, obtaining\\npolynomial-time approximation algorithms to the original problem. Moreover, we\\nevaluate the algorithms\\' robustness against outdated channel state information\\n(CSI), present an analysis of the convergence of the algorithms, and a\\ncomparison of the sum-rate and Jain\\'s fairness index of the novel algorithms\\nwith three other algorithms proposed in the literature. The Max-SR algorithm\\noutperforms the others in the sum-rate sense, while the Max-Min outperforms\\nthem in the fairness sense.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1805.12471</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1805.12471</id><submitter>Alex Warstadt</submitter><version version=\"v1\"><date>Thu, 31 May 2018 13:52:06 GMT</date><size>347kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 11 Sep 2018 03:34:37 GMT</date><size>349kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 1 Oct 2019 18:41:05 GMT</date><size>881kb</size><source_type>D</source_type></version><title>Neural Network Acceptability Judgments</title><authors>Alex Warstadt, Amanpreet Singh, Samuel R. Bowman</authors><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the ability of artificial neural networks to judge\\nthe grammatical acceptability of a sentence, with the goal of testing their\\nlinguistic competence. We introduce the Corpus of Linguistic Acceptability\\n(CoLA), a set of 10,657 English sentences labeled as grammatical or\\nungrammatical from published linguistics literature. As baselines, we train\\nseveral recurrent neural network models on acceptability classification, and\\nfind that our models outperform unsupervised models by Lau et al (2016) on\\nCoLA. Error-analysis on specific grammatical phenomena reveals that both Lau et\\nal.\\'s models and ours learn systematic generalizations like subject-verb-object\\norder. However, all models we test perform far below human level on a wide\\nrange of grammatical constructions.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1806.00458</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1806.00458</id><submitter>Tianyi Lin</submitter><version version=\"v1\"><date>Fri, 1 Jun 2018 17:29:34 GMT</date><size>896kb</size></version><version version=\"v2\"><date>Wed, 18 Sep 2019 05:15:27 GMT</date><size>979kb</size></version><version version=\"v3\"><date>Wed, 2 Oct 2019 01:27:00 GMT</date><size>840kb</size></version><title>Improved Sample Complexity for Stochastic Compositional Variance Reduced\\n  Gradient</title><authors>Tianyi Lin, Chenyou Fan, Mengdi Wang and Michael I. Jordan</authors><categories>math.OC cs.LG</categories><comments>Correct some typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convex composition optimization is an emerging topic that covers a wide range\\nof applications arising from stochastic optimal control, reinforcement learning\\nand multi-stage stochastic programming. Existing algorithms suffer from\\nunsatisfactory sample complexity and practical issues since they ignore the\\nconvexity structure in the algorithmic design. In this paper, we develop a new\\nstochastic compositional variance-reduced gradient algorithm with the sample\\ncomplexity of $O((m+n)\\\\log(1/\\\\epsilon)+1/\\\\epsilon^3)$ where $m+n$ is the total\\nnumber of samples. Our algorithm is near-optimal as the dependence on $m+n$ is\\noptimal up to a logarithmic factor. Experimental results on real-world datasets\\ndemonstrate the effectiveness and efficiency of the new algorithm.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1806.00860</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1806.00860</id><submitter>David Aristoff</submitter><version version=\"v1\"><date>Sun, 3 Jun 2018 19:36:32 GMT</date><size>989kb</size></version><version version=\"v2\"><date>Wed, 5 Sep 2018 21:43:49 GMT</date><size>561kb</size></version><version version=\"v3\"><date>Sat, 8 Jun 2019 23:39:27 GMT</date><size>462kb</size></version><version version=\"v4\"><date>Wed, 2 Oct 2019 01:35:47 GMT</date><size>415kb</size></version><title>Optimizing weighted ensemble sampling of steady states</title><authors>David Aristoff and Daniel M. Zuckerman</authors><categories>math.NA cs.NA</categories><comments>28 pages, 5 figures</comments><msc-class>65C05, 65C20, 65C40, 65Y05, 82C80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose parameter optimization techniques for weighted ensemble sampling\\nof Markov chains in the steady-state regime. Weighted ensemble consists of\\nreplicas of a Markov chain, each carrying a weight, that are periodically\\nresampled according to their weights inside of each of a number of bins that\\npartition state space. We derive, from first principles, strategies for\\noptimizing the choices of weighted ensemble parameters, in particular the\\nchoice of bins and the number of replicas to maintain in each bin. In a simple\\nnumerical example, we compare our new strategies with more traditional ones and\\nwith direct Monte Carlo.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1806.03957</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1806.03957</id><submitter>Aleksandr Chuklin</submitter><version version=\"v1\"><date>Mon, 11 Jun 2018 13:25:23 GMT</date><size>934kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 24 Jun 2019 09:41:51 GMT</date><size>412kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 4 Jul 2019 10:21:55 GMT</date><size>412kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Wed, 2 Oct 2019 14:18:34 GMT</date><size>412kb</size><source_type>D</source_type></version><title>Prosody Modifications for Question-Answering in Voice-Only Settings</title><authors>Aleksandr Chuklin, Aliaksei Severyn, Johanne Trippas, Enrique\\n  Alfonseca, Hanna Silen and Damiano Spina</authors><categories>cs.CL cs.HC</categories><comments>Shorter version of this paper was accepted to CLEF\\'2019, Lugano,\\n  Switzerland. The final authenticated version is available online at\\n  https://doi.org/10.1007/978-3-030-28577-7_12</comments><acm-class>H.3.3; H.5.2</acm-class><journal-ref>Lecture Notes in Computer Science, vol 11696 CLEF 2019</journal-ref><doi>10.1007/978-3-030-28577-7_12</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Many popular form factors of digital assistants---such as Amazon Echo, Apple\\nHomepod, or Google Home---enable the user to hold a conversation with these\\nsystems based only on the speech modality. The lack of a screen presents unique\\nchallenges. To satisfy the information need of a user, the presentation of the\\nanswer needs to be optimized for such voice-only interactions. In this paper,\\nwe propose a task of evaluating the usefulness of audio transformations (i.e.,\\nprosodic modifications) for voice-only question answering. We introduce a\\ncrowdsourcing setup where we evaluate the quality of our proposed modifications\\nalong multiple dimensions corresponding to the informativeness, naturalness,\\nand ability of the user to identify key parts of the answer. We offer a set of\\nprosodic modifications that highlight potentially important parts of the answer\\nusing various acoustic cues. Our experiments show that some of these prosodic\\nmodifications lead to better comprehension at the expense of only slightly\\ndegraded naturalness of the audio.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1806.04482</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1806.04482</id><submitter>Andrea Beck</submitter><version version=\"v1\"><date>Sun, 10 Jun 2018 17:40:51 GMT</date><size>678kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 13 Jun 2018 11:56:38 GMT</date><size>919kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 15 Jun 2018 14:24:15 GMT</date><size>920kb</size><source_type>D</source_type></version><title>Deep Neural Networks for Data-Driven Turbulence Models</title><authors>Andrea D. Beck, David G. Flad, Claus-Dieter Munz</authors><categories>cs.CE physics.flu-dyn</categories><doi>10.1016/j.jcp.2019.108910</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present a novel data-based approach to turbulence modelling\\nfor Large Eddy Simulation (LES) by artificial neural networks. We define the\\nexact closure terms including the discretization operators and generate\\ntraining data from direct numerical simulations of decaying homogeneous\\nisotropic turbulence. We design and train artificial neural networks based on\\nlocal convolution filters to predict the underlying unknown non-linear mapping\\nfrom the coarse grid quantities to the closure terms without a priori\\nassumptions. All investigated networks are able to generalize from the data and\\nlearn approximations with a cross correlation of up to 47% and even 73% for the\\ninner elements, leading to the conclusion that the current training success is\\ndata-bound. We further show that selecting both the coarse grid primitive\\nvariables as well as the coarse grid LES operator as input features\\nsignificantly improves training results. Finally, we construct a stable and\\naccurate LES model from the learned closure terms. Therefore, we translate the\\nmodel predictions into a data-adaptive, pointwise eddy viscosity closure and\\nshow that the resulting LES scheme performs well compared to current state of\\nthe art approaches. This work represents the starting point for further\\nresearch into data-driven, universal turbulence models.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1806.04743</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1806.04743</id><submitter>Pablo de Castro</submitter><version version=\"v1\"><date>Tue, 12 Jun 2018 20:08:53 GMT</date><size>852kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 11 Oct 2018 12:41:56 GMT</date><size>339kb</size><source_type>D</source_type></version><title>INFERNO: Inference-Aware Neural Optimisation</title><authors>Pablo de Castro and Tommaso Dorigo</authors><categories>stat.ML cs.LG hep-ex physics.data-an stat.ME</categories><comments>Code available at https://github.com/pablodecm/paper-inferno .\\n  Version updates: - v2: fixed typos, improve text, link to code and a better\\n  synthetic experiment</comments><doi>10.1016/j.cpc.2019.06.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex computer simulations are commonly required for accurate data\\nmodelling in many scientific disciplines, making statistical inference\\nchallenging due to the intractability of the likelihood evaluation for the\\nobserved data. Furthermore, sometimes one is interested on inference drawn over\\na subset of the generative model parameters while taking into account model\\nuncertainty or misspecification on the remaining nuisance parameters. In this\\nwork, we show how non-linear summary statistics can be constructed by\\nminimising inference-motivated losses via stochastic gradient descent such they\\nprovided the smallest uncertainty for the parameters of interest. As a use\\ncase, the problem of confidence interval estimation for the mixture coefficient\\nin a multi-dimensional two-component mixture model (i.e. signal vs background)\\nis considered, where the proposed technique clearly outperforms summary\\nstatistics based on probabilistic classification, which are a commonly used\\nalternative but do not account for the presence of nuisance parameters.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1806.04899</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1806.04899</id><submitter>Yijun Bian</submitter><version version=\"v1\"><date>Wed, 13 Jun 2018 08:58:49 GMT</date><size>3491kb</size></version><version version=\"v2\"><date>Sat, 16 Jun 2018 03:40:07 GMT</date><size>3491kb</size></version><version version=\"v3\"><date>Mon, 30 Sep 2019 12:22:24 GMT</date><size>84kb</size></version><title>Ensemble Pruning based on Objection Maximization with a General\\n  Distributed Framework</title><authors>Yijun Bian and Yijun Wang and Yaqiang Yao and Huanhuan Chen</authors><categories>cs.LG cs.AI stat.ML</categories><comments>Accepted by TNNLS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ensemble pruning, selecting a subset of individual learners from an original\\nensemble, alleviates the deficiencies of ensemble learning on the cost of time\\nand space. Accuracy and diversity serve as two crucial factors while they\\nusually conflict with each other. To balance both of them, we formalize the\\nensemble pruning problem as an objection maximization problem based on\\ninformation entropy. Then we propose an ensemble pruning method including a\\ncentralized version and a distributed version, in which the latter is to speed\\nup the former. At last, we extract a general distributed framework for ensemble\\npruning, which can be widely suitable for most of the existing ensemble pruning\\nmethods and achieve less time consuming without much accuracy degradation.\\nExperimental results validate the efficiency of our framework and methods,\\nparticularly concerning a remarkable improvement of the execution speed,\\naccompanied by gratifying accuracy performance.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1806.05063</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1806.05063</id><submitter>Ruming Zhang</submitter><version version=\"v1\"><date>Tue, 12 Jun 2018 08:21:02 GMT</date><size>150kb</size></version><version version=\"v2\"><date>Mon, 30 Sep 2019 10:07:23 GMT</date><size>59kb</size></version><title>Numerical methods for scattering problems from multi-layers with\\n  different periodicities</title><authors>Ruming Zhang</authors><categories>math.NA cs.NA</categories><comments>arXiv admin note: text overlap with arXiv:1805.11484</comments><msc-class>35P25, 78M10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a numerical method to solve scattering problems\\nwith multi-periodic layers with different periodicities. The main tool applied\\nin this paper is the Bloch transform. With this method, the problem is written\\ninto an equivalent coupled family of quasi-periodic problems. As the Bloch\\ntransform is only defined for one fixed period, the inhomogeneous layer with\\nanother period is simply treated as a non-periodic one. First, we approximate\\nthe refractive index by a periodic one where its period is an integer multiple\\nof the fixed period, and it is decomposed by finite number of quasi-periodic\\nfunctions. Then the coupled system is reduced into a simplified formulation. A\\nconvergent finite element method is proposed for the numerical solution, and\\nthe numerical method has been applied to several numerical experiments. At the\\nend of this paper, relative errors of the numerical solutions will be shown to\\nillustrate the convergence of the numerical algorithm.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1806.06064</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1806.06064</id><submitter>Mark Hallen</submitter><version version=\"v1\"><date>Wed, 30 May 2018 04:49:44 GMT</date><size>2732kb</size><source_type>D</source_type></version><title>Protein Design by Algorithm</title><authors>Mark A. Hallen and Bruce R. Donald</authors><categories>cs.CE cs.DS q-bio.BM</categories><journal-ref>Communications of the ACM, October 2019, Vol. 62 No. 10, Pages\\n  76-84</journal-ref><doi>10.1145/3338124</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review algorithms for protein design in general. Although these algorithms\\nhave a rich combinatorial, geometric, and mathematical structure, they are\\nalmost never covered in computer science classes. Furthermore, many of these\\nalgorithms admit provable guarantees of accuracy, soundness, complexity,\\ncompleteness, optimality, and approximation bounds. The algorithms represent a\\ndelicate and beautiful balance between discrete and continuous computation and\\nmodeling, analogous to that which is seen in robotics, computational geometry,\\nand other fields in computational science. Finally, computer scientists may be\\nunaware of the almost direct impact of these algorithms for predicting and\\nintroducing molecular therapies that have gone in a short time from mathematics\\nto algorithms to software to predictions to preclinical testing to clinical\\ntrials. Indeed, the overarching goal of these algorithms is to enable the\\ndevelopment of new therapeutics that might be impossible or too expensive to\\ndiscover using experimental methods. Thus the potential impact of these\\nalgorithms on individual, community, and global health has the potential to be\\nquite significant.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1806.06091</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1806.06091</id><submitter>Mark Velednitsky</submitter><version version=\"v1\"><date>Fri, 15 Jun 2018 18:42:49 GMT</date><size>8kb</size></version><version version=\"v2\"><date>Thu, 21 Jun 2018 03:51:48 GMT</date><size>8kb</size></version><version version=\"v3\"><date>Thu, 18 Oct 2018 03:52:39 GMT</date><size>9kb</size></version><version version=\"v4\"><date>Sat, 12 Jan 2019 21:02:20 GMT</date><size>0kb</size><source_type>I</source_type></version><version version=\"v5\"><date>Wed, 2 Oct 2019 15:03:47 GMT</date><size>25kb</size></version><title>Solving $(k-1)$-Stable Instances of k-Terminal Cut with Isolating Cuts</title><authors>Mark Velednitsky</authors><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The k-Terminal Cut problem, also known as the Multiway Cut problem, is\\ndefined on an edge-weighted graph with $k$ distinct vertices called\\n&quot;terminals.&quot; The goal is to remove a minimum weight collection of edges from\\nthe graph such that there is no path between any pair of terminals. The problem\\nis NP-hard.\\n  Isolating cuts are minimum cuts that separate one terminal from the rest. The\\nunion of all the isolating cuts, except the largest, is a\\n$(2-2/k)$-approximation to the optimal k-Terminal Cut. This is the only\\ncurrently-known approximation algorithm for k-Terminal Cut which does not\\nrequire solving a linear program.\\n  An instance of k-Terminal Cut is $\\\\gamma$-stable if edges in the cut can be\\nmultiplied by up to $\\\\gamma$ without changing the unique optimal solution. In\\nthis paper, we show that, in any $(k-1)$-stable instance of k-Terminal Cut, the\\nsource sets of the isolating cuts are the source sets of the unique optimal\\nsolution of that k-Terminal Cut instance. We conclude that the\\n$(2-2/k)$-approximation algorithm returns the optimal solution on\\n$(k-1)$-stable instances. Ours is the first result showing that this\\n$(2-2/k)$-approximation is an exact optimization algorithm on a special class\\nof graphs.\\n  We also show that our $(k-1)$-stability result is tight. We construct\\n$(k-1-\\\\epsilon)$-stable instances of the k-Terminal Cut problem which only have\\ntrivial isolating cuts: that is, the source set of the isolating cuts for each\\nterminal is just the terminal itself. Thus, the $(2-2/k)$-approximation does\\nnot return an optimal solution.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1806.06732</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1806.06732</id><submitter>Ye Zhang</submitter><version version=\"v1\"><date>Mon, 18 Jun 2018 14:30:56 GMT</date><size>1461kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 02:47:38 GMT</date><size>3971kb</size></version><title>Damped second order flow applied to image denoising</title><authors>George Baravdish, Olof Svensson, M{\\\\aa}rten Gulliksson and Ye Zhang</authors><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a new image denoising model: the damped flow\\n(DF), which is a second order nonlinear evolution equation associated with a\\nclass of energy functionals of image. The existence, uniqueness and\\nregularization property of DF are proven. For the numerical implementation,\\nbased on the St\\\\&quot;{o}rmer-Verlet method, a discrete damped flow, SV-DDF, is\\ndeveloped. The convergence of SV-DDF is studied as well. Several numerical\\nexperiments, as well as a comparison with other methods, are provided to\\ndemonstrate the feasibility and effectiveness of the SV-DDF.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1806.06766</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1806.06766</id><submitter>Sinho Chewi</submitter><version version=\"v1\"><date>Mon, 18 Jun 2018 15:19:05 GMT</date><size>205kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 20:34:58 GMT</date><size>86kb</size></version><title>Matching Observations to Distributions: Efficient Estimation via\\n  Sparsified Hungarian Algorithm</title><authors>Sinho Chewi, Forest Yang, Avishek Ghosh, Abhay Parekh, Kannan\\n  Ramchandran</authors><categories>cs.DS cs.SY</categories><comments>8 pages, 1 figure; to appear in the 57th Annual Allerton Conference\\n  on Communication, Control, and Computing</comments><msc-class>68W20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose we are given observations, where each observation is drawn\\nindependently from one of $k$ known distributions. The goal is to match each\\nobservation to the distribution from which it was drawn. We observe that the\\nmaximum likelihood estimator (MLE) for this problem can be computed using\\nweighted bipartite matching, even when $n$, the number of observations per\\ndistribution, exceeds one. This is achieved by instantiating $n$ duplicates of\\neach distribution node. However, in the regime where the number of observations\\nper distribution is much larger than the number of distributions, the Hungarian\\nmatching algorithm for computing the weighted bipartite matching requires\\n$\\\\mathcal O(n^3)$ time. We introduce a novel randomized matching algorithm that\\nreduces the runtime to $\\\\tilde{\\\\mathcal O}(n^2)$ by sparsifying the original\\ngraph, returning the exact MLE with high probability. Next, we give statistical\\njustification for using the MLE by bounding the excess risk of the MLE, where\\nthe loss is defined as the negative log-likelihood. We test these bounds for\\nthe case of isotropic Gaussians with equal covariances and whose means are\\nseparated by a distance $\\\\eta$, and find (1) that $\\\\gg \\\\log k$ separation\\nsuffices to drive the proportion of mismatches of the MLE to 0, and (2) that\\nthe expected fraction of mismatched observations goes to zero at rate $\\\\mathcal\\nO({(\\\\log k)}^2/\\\\eta^2)$.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1806.08238</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1806.08238</id><submitter>Niranjan Saikumar</submitter><version version=\"v1\"><date>Wed, 20 Jun 2018 11:44:13 GMT</date><size>2624kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 24 Dec 2018 12:59:17 GMT</date><size>4206kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 1 Oct 2019 16:51:35 GMT</date><size>4207kb</size><source_type>D</source_type></version><title>Development of Robust Fractional-Order Reset Control</title><authors>Linda Chen, Niranjan Saikumar and S. Hassan HosseinNia</authors><categories>cs.SY</categories><comments>arXiv admin note: text overlap with arXiv:1805.10037</comments><doi>10.1109/TCST.2019.2913534</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a framework for the combination of robust fractional order\\nCRONE control with non-linear reset is given for both first and second\\ngeneration CRONE control. General design rules are derived and presented for\\nthese CRONE reset controllers. Within this framework, fractional order control\\nallows for better tuning of the open-loop responses on the one hand. On the\\nother, reset control enables a reduction in phase lag and a corresponding\\nincrease in phase margin compared to linear control for similar open loop gain\\nprofile. Hence, the combination of the two control methods can provide\\nwell-tuned open-loop responses that can overcome the fundamental linear control\\nlimitation of Bode\\'s gain-phase relationship. Moreover, as established\\nloop-shaping concepts are used in the controller design, CRONE reset can be\\nhighly compatible with the industry. The designed CRONE reset controllers are\\nvalidated on a one degree-of-freedom Lorentz-actuated precision positioning\\nstage. On this setup, CRONE reset control is shown to provide better tracking\\nperformance compared to linear CRONE control, which is in agreement with the\\npredicted performance improvement.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1806.08663</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1806.08663</id><submitter>Jinfeng Zhang</submitter><version version=\"v1\"><date>Mon, 11 Jun 2018 23:34:59 GMT</date><size>616kb</size></version><version version=\"v2\"><date>Tue, 11 Dec 2018 18:06:53 GMT</date><size>0kb</size><source_type>I</source_type></version><version version=\"v3\"><date>Mon, 7 Oct 2019 18:06:39 GMT</date><size>616kb</size></version><title>Simulation Study on a New Peer Review Approach</title><authors>Albert Steppi, Jinchan Qu, Minjing Tao, Tingting Zhao, Xiaodong Pang,\\n  Jinfeng Zhang</authors><categories>cs.DL cs.AI cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing volume of scientific publications and grant proposals has\\ngenerated an unprecedentedly high workload to scientific communities.\\nConsequently, review quality has been decreasing and review outcomes have\\nbecome less correlated with the real merits of the papers and proposals. A\\nnovel distributed peer review (DPR) approach has recently been proposed to\\naddress these issues. The new approach assigns principal investigators (PIs)\\nwho submitted proposals (or papers) to the same program as reviewers. Each PI\\nreviews and ranks a small number (such as seven) of other PIs\\' proposals. The\\nindividual rankings are then used to estimate a global ranking of all proposals\\nusing the Modified Borda Count (MBC). In this study, we perform simulation\\nstudies to investigate several parameters important for the decision making\\nwhen adopting this new approach. We also propose a new method called\\nConcordance Index-based Global Ranking (CIGR) to estimate global ranking from\\nindividual rankings. An efficient simulated annealing algorithm is designed to\\nsearch the optimal Concordance Index (CI). Moreover, we design a new balanced\\nreview assignment procedure, which can result in significantly better\\nperformance for both MBC and CIGR methods. We found that CIGR performs better\\nthan MBC when the review quality is relatively high. As review quality and\\nreview difficulty are tightly correlated, we constructed a boundary in the\\nspace of review quality vs review difficulty that separates the CIGR-superior\\nand MBC-superior regions. Finally, we propose a multi-stage DPR strategy based\\non CIGR, which has the potential to substantially improve the overall review\\nperformance while reducing the review workload.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1806.08681</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1806.08681</id><submitter>Hendrik Schawe</submitter><version version=\"v1\"><date>Fri, 22 Jun 2018 14:12:41 GMT</date><size>293kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 18 Jul 2019 15:20:02 GMT</date><size>537kb</size><source_type>D</source_type></version><title>Replica Symmetry and Replica Symmetry Breaking for the Traveling\\n  Salesperson Problem</title><authors>Hendrik Schawe, Jitesh Kumar Jha, Alexander K. Hartmann</authors><categories>cond-mat.dis-nn cond-mat.stat-mech cs.CC</categories><comments>9 pages, 5 figures, 2 table</comments><journal-ref>Phys. Rev. E 100, 032135 (2019)</journal-ref><doi>10.1103/PhysRevE.100.032135</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the energy landscape of the Traveling Salesperson problem (TSP)\\nusing exact ground states and a novel linear programming approach to generate\\nexcited states with closely defined properties. We look at four different\\nensembles, notably the classic finite dimensional Euclidean TSP and the\\nmean-field-like (1,2)-TSP, which has its origin directly in the mapping of the\\nHamiltonian circuit problem on the TSP. Our data supports previous conjectures\\nthat the Euclidean TSP does not show signatures of replica symmetry breaking\\nneither in two nor in higher dimension. On the other hand the (1,2)-TSP\\nexhibits some signature which does not exclude broken replica symmetry, making\\nit a candidate for further studies in the future.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1806.10337</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1806.10337</id><submitter>Felipe Garcia-Sanchez</submitter><version version=\"v1\"><date>Wed, 27 Jun 2018 08:29:38 GMT</date><size>1122kb</size></version><version version=\"v2\"><date>Mon, 7 Oct 2019 12:43:37 GMT</date><size>1042kb</size></version><title>Skyrmion Logic System for Large-Scale Reversible Computation</title><authors>Maverick Chauwin, Xuan Hu, Felipe Garcia-Sanchez, Neilesh Betrabet,\\n  Alexandru Paler, Christoforos Moutafis and Joseph S. Friedman</authors><categories>cond-mat.mes-hall cs.ET</categories><comments>24 pages, 7 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational reversibility is necessary for quantum computation and inspires\\nthe development of computing systems in which information carriers are\\nconserved as they flow through a circuit. While conservative logic provides an\\nexciting vision for reversible computing with no energy dissipation, the large\\ndimensions of information carriers in previous realizations detract from the\\nsystem efficiency, and nanoscale conservative logic remains elusive. We\\ntherefore propose a non-volatile reversible computing system in which the\\ninformation carriers are magnetic skyrmions, topologically-stable magnetic\\nwhirls. These nanoscale quasiparticles interact with one another via the\\nspin-Hall and skyrmion-Hall effects as they propagate through ferromagnetic\\nnanowires structured to form cascaded conservative logic gates. These logic\\ngates can be directly cascaded in large-scale systems that perform complex\\nlogic functions, with signal integrity provided by clocked synchronization\\nstructures. The feasibility of the proposed system is demonstrated through\\nmicromagnetic simulations of Boolean logic gates, a Fredkin gate, and a\\ncascaded full adder. As skyrmions can be transported in a pipelined and\\nnon-volatile manner at room temperature without the motion of any physical\\nparticles, this skyrmion logic system has the potential to deliver scalable\\nhigh-speed low-power reversible Boolean and quantum computing.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1806.11015</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1806.11015</id><submitter>Irene C\\\\\\'ordoba</submitter><version version=\"v1\"><date>Thu, 28 Jun 2018 14:44:22 GMT</date><size>1279kb</size><source_type>D</source_type></version><title>Bayesian optimization of the PC algorithm for learning Gaussian Bayesian\\n  networks</title><authors>Irene C\\\\\\'ordoba, Eduardo C. Garrido-Merch\\\\\\'an, Daniel\\n  Hern\\\\\\'andez-Lobato, Concha Bielza, Pedro Larra\\\\~naga</authors><categories>cs.LG stat.ML</categories><journal-ref>Lecture Notes in Artificial Intelligence (CAEPIA 2018),\\n  11160:44:54, 2018</journal-ref><doi>10.1007/978-3-030-00374-6_5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The PC algorithm is a popular method for learning the structure of Gaussian\\nBayesian networks. It carries out statistical tests to determine absent edges\\nin the network. It is hence governed by two parameters: (i) The type of test,\\nand (ii) its significance level. These parameters are usually set to values\\nrecommended by an expert. Nevertheless, such an approach can suffer from human\\nbias, leading to suboptimal reconstruction results. In this paper we consider a\\nmore principled approach for choosing these parameters in an automatic way. For\\nthis we optimize a reconstruction score evaluated on a set of different\\nGaussian Bayesian networks. This objective is expensive to evaluate and lacks a\\nclosed-form expression, which means that Bayesian optimization (BO) is a\\nnatural choice. BO methods use a model to guide the search and are hence able\\nto exploit smoothness properties of the objective surface. We show that the\\nparameters found by a BO method outperform those found by a random search\\nstrategy and the expert recommendation. Importantly, we have found that an\\noften overlooked statistical test provides the best over-all reconstruction\\nresults.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1806.11500</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1806.11500</id><submitter>Ben London</submitter><version version=\"v1\"><date>Fri, 29 Jun 2018 16:01:34 GMT</date><size>20kb</size></version><version version=\"v2\"><date>Tue, 30 Oct 2018 21:47:31 GMT</date><size>21kb</size></version><version version=\"v3\"><date>Thu, 29 Aug 2019 23:29:11 GMT</date><size>135kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Mon, 30 Sep 2019 18:42:25 GMT</date><size>135kb</size><source_type>D</source_type></version><title>Bayesian Counterfactual Risk Minimization</title><authors>Ben London and Ted Sandler</authors><categories>cs.LG stat.ML</categories><comments>Extended version of the paper published at the 2019 International\\n  Conference on Machine Learning (ICML). Contains some additional citations;\\n  fewer deferred proofs; and slightly more detailed analysis. Latest revision\\n  fixes a few punctuation issues</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a Bayesian view of counterfactual risk minimization (CRM) for\\noffline learning from logged bandit feedback. Using PAC-Bayesian analysis, we\\nderive a new generalization bound for the truncated inverse propensity score\\nestimator. We apply the bound to a class of Bayesian policies, which motivates\\na novel, potentially data-dependent, regularization technique for CRM.\\nExperimental results indicate that this technique outperforms standard $L_2$\\nregularization, and that it is competitive with variance regularization while\\nbeing both simpler to implement and more computationally efficient.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1806.11536</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1806.11536</id><submitter>Amirhossein Reisizadeh</submitter><version version=\"v1\"><date>Fri, 29 Jun 2018 17:02:54 GMT</date><size>134kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 8 Nov 2018 23:27:11 GMT</date><size>900kb</size></version><version version=\"v3\"><date>Fri, 2 Aug 2019 01:10:39 GMT</date><size>5096kb</size><source_type>D</source_type></version><title>An Exact Quantized Decentralized Gradient Descent Algorithm</title><authors>Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ramtin\\n  Pedarsani</authors><categories>cs.LG cs.DC math.OC stat.ML</categories><doi>10.1109/TSP.2019.2932876</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of decentralized consensus optimization, where the\\nsum of $n$ smooth and strongly convex functions are minimized over $n$\\ndistributed agents that form a connected network. In particular, we consider\\nthe case that the communicated local decision variables among nodes are\\nquantized in order to alleviate the communication bottleneck in distributed\\noptimization. We propose the Quantized Decentralized Gradient Descent (QDGD)\\nalgorithm, in which nodes update their local decision variables by combining\\nthe quantized information received from their neighbors with their local\\ninformation. We prove that under standard strong convexity and smoothness\\nassumptions for the objective function, QDGD achieves a vanishing mean solution\\nerror under customary conditions for quantizers. To the best of our knowledge,\\nthis is the first algorithm that achieves vanishing consensus error in the\\npresence of quantization noise. Moreover, we provide simulation results that\\nshow tight agreement between our derived theoretical convergence rate and the\\nnumerical results.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1807.00939</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1807.00939</id><submitter>Sheikh Rabiul Islam</submitter><version version=\"v1\"><date>Mon, 2 Jul 2018 04:21:10 GMT</date><size>521kb</size></version><version version=\"v2\"><date>Tue, 21 Aug 2018 16:00:02 GMT</date><size>1396kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 7 Nov 2018 20:04:31 GMT</date><size>2998kb</size><source_type>D</source_type></version><title>Mining Illegal Insider Trading of Stocks: A Proactive Approach</title><authors>Sheikh Rabiul Islam, Sheikh Khaled Ghafoor, William Eberle</authors><categories>q-fin.ST cs.LG stat.ML</categories><comments>Accepted in IEEE BigData 2018</comments><journal-ref>2018 IEEE International Conference on Big Data (Big Data)</journal-ref><doi>10.1109/BigData.2018.8622303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Illegal insider trading of stocks is based on releasing non-public\\ninformation (e.g., new product launch, quarterly financial report, acquisition\\nor merger plan) before the information is made public. Detecting illegal\\ninsider trading is difficult due to the complex, nonlinear, and non-stationary\\nnature of the stock market. In this work, we present an approach that detects\\nand predicts illegal insider trading proactively from large heterogeneous\\nsources of structured and unstructured data using a deep-learning based\\napproach combined with discrete signal processing on the time series data. In\\naddition, we use a tree-based approach that visualizes events and actions to\\naid analysts in their understanding of large amounts of unstructured data.\\nUsing existing data, we have discovered that our approach has a good success\\nrate in detecting illegal insider trading patterns.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1807.02037</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1807.02037</id><submitter>Tung D. Le</submitter><version version=\"v1\"><date>Thu, 5 Jul 2018 14:56:39 GMT</date><size>87kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 06:54:46 GMT</date><size>87kb</size><source_type>D</source_type></version><title>TFLMS: Large Model Support in TensorFlow by Graph Rewriting</title><authors>Tung D. Le, Haruki Imai, Yasushi Negishi and Kiyokuni Kawachiya</authors><categories>cs.LG cs.AI stat.ML</categories><comments>A new version of TFLMS was published at ISMM 2019\\n  (https://dl.acm.org/citation.cfm?id=3329984)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While accelerators such as GPUs have limited memory, deep neural networks are\\nbecoming larger and will not fit with the memory limitation of accelerators for\\ntraining. We propose an approach to tackle this problem by rewriting the\\ncomputational graph of a neural network, in which swap-out and swap-in\\noperations are inserted to temporarily store intermediate results on CPU\\nmemory. In particular, we first revise the concept of a computational graph by\\ndefining a concrete semantics for variables in a graph. We then formally show\\nhow to derive swap-out and swap-in operations from an existing graph and\\npresent rules to optimize the graph. To realize our approach, we developed a\\nmodule in TensorFlow, named TFLMS. TFLMS is published as a pull request in the\\nTensorFlow repository for contributing to the TensorFlow community. With TFLMS,\\nwe were able to train ResNet-50 and 3DUnet with 4.7x and 2x larger batch size,\\nrespectively. In particular, we were able to train 3DUNet using images of size\\nof $192^3$ for image segmentation, which, without TFLMS, had been done only by\\ndividing the images to smaller images, which affects the accuracy.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1807.02790</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1807.02790</id><submitter>Dmitry Gribanov</submitter><version version=\"v1\"><date>Sun, 8 Jul 2018 09:27:33 GMT</date><size>135kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Jul 2018 06:25:54 GMT</date><size>136kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 7 Oct 2019 12:52:27 GMT</date><size>134kb</size><source_type>D</source_type></version><title>On the complexity of quasiconvex integer minimization problem</title><authors>A. Yu. Chirkov, D. V. Gribanov, D. S. Malyshev, P. M. Pardalos, S. I.\\n  Veselov, N. Yu. Zolotykh</authors><categories>math.OC cs.CC</categories><comments>Some new proofs have been added. Some fixes are done</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the class of quasiconvex functions and its proper\\nsubclass of conic functions. The integer minimization problem of these\\nfunctions is considered in the paper, assuming that an optimized function is\\ndefined by the comparison oracle. We will show that there is no a polynomial\\nalgorithm on $\\\\log R$ to optimize quasiconvex functions in the ball of integer\\nradius $R$ using only the comparison oracle. On the other hand, if an optimized\\nfunction is conic, then we show that there is a polynomial on $\\\\log R$\\nalgorithm. We also present an exponential on the dimension lower bound for the\\noracle complexity of the conic function integer optimization problem.\\nAdditionally, we give examples of known problems that can be polynomially\\nreduced to the minimization problem of functions in our classes.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1807.03422</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1807.03422</id><submitter>Jian-Jia Weng</submitter><version version=\"v1\"><date>Mon, 9 Jul 2018 23:29:23 GMT</date><size>3954kb</size></version><version version=\"v2\"><date>Sat, 14 Jul 2018 19:35:13 GMT</date><size>3954kb</size></version><version version=\"v3\"><date>Mon, 30 Sep 2019 22:35:53 GMT</date><size>3170kb</size></version><title>Capacity of Two-Way Channels with Symmetry Properties</title><authors>Jian-Jia Weng, Lin Song, Fady Alajaji, and Tam\\\\\\'as Linder</authors><categories>cs.IT math.IT</categories><comments>56 pages, 8 figures, a missing condition in Corollary 4 and Theorem 9\\n  added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we make use of channel symmetry properties to determine the\\ncapacity region of three types of two-way networks: (a) two-user memoryless\\ntwo-way channels (TWCs), (b) two-user TWCs with memory, and (c) three-user\\nmultiaccess/degraded broadcast (MA/DB) TWCs. For each network, symmetry\\nconditions under which a Shannon-type random coding inner bound (under\\nindependent non-adaptive inputs) is tight are given. For two-user memoryless\\nTWCs, prior results are substantially generalized by viewing a TWC as two\\ninteracting state-dependent one-way channels. The capacity of symmetric TWCs\\nwith memory, whose outputs are functions of the inputs and independent\\nstationary and ergodic noise processes, is also obtained. Moreover, various\\nchannel symmetry properties under which the Shannon-type inner bound is tight\\nare identified for three-user MA/DB TWCs. The results not only enlarge the\\nclass of symmetric TWCs whose capacity region can be exactly determined but\\nalso imply that interactive adaptive coding, not improving capacity, is\\nunnecessary for such channels.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1807.03566</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1807.03566</id><submitter>Irene C\\\\\\'ordoba</submitter><version version=\"v1\"><date>Tue, 10 Jul 2018 10:59:37 GMT</date><size>278kb</size><source_type>D</source_type></version><title>A modelling language for the effective design of Java annotations</title><authors>Irene C\\\\\\'ordoba and Juan de Lara</authors><categories>cs.PL cs.SE</categories><comments>6 pages, 6 figures, 2015 conference</comments><acm-class>D.3.2; D.3.4; D.2.3</acm-class><journal-ref>Proceedings of the 30th Annual ACM Symposium on Applied Computing\\n  (SAC 2015), 2087-2092, 2015</journal-ref><doi>10.1145/2695664.2695717</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a new modelling language for the effective design of\\nJava annotations. Since their inclusion in the 5th edition of Java, annotations\\nhave grown from a useful tool for the addition of meta-data to play a central\\nrole in many popular software projects. Usually they are conceived as sets with\\ndependency and integrity constraints within them; however, the native support\\nprovided by Java for expressing this design is very limited. To overcome its\\ndeficiencies and make explicit the rich conceptual model which lies behind a\\nset of annotations, we propose a domain-specific modelling language. The\\nproposal has been implemented as an Eclipse plug-in, including an editor and an\\nintegrated code generator that synthesises annotation processors. The language\\nhas been tested using a real set of annotations from the Java Persistence API\\n(JPA). It has proven to cover a greater scope with respect to other related\\nwork in different shared areas of application.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1807.03602</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1807.03602</id><submitter>Jianlong Wu</submitter><version version=\"v1\"><date>Tue, 10 Jul 2018 13:01:48 GMT</date><size>3120kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 6 May 2019 14:10:05 GMT</date><size>3875kb</size><source_type>D</source_type></version><title>Essential Tensor Learning for Multi-view Spectral Clustering</title><authors>Jianlong Wu, Zhouchen Lin, Hongbin Zha</authors><categories>cs.CV</categories><comments>Accepted by IEEE Transactions on Image Processing</comments><doi>10.1109/TIP.2019.2916740</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-view clustering attracts much attention recently, which aims to take\\nadvantage of multi-view information to improve the performance of clustering.\\nHowever, most recent work mainly focus on self-representation based subspace\\nclustering, which is of high computation complexity. In this paper, we focus on\\nthe Markov chain based spectral clustering method and propose a novel essential\\ntensor learning method to explore the high order correlations for multi-view\\nrepresentation. We first construct a tensor based on multi-view transition\\nprobability matrices of the Markov chain. By incorporating the idea from robust\\nprinciple component analysis, tensor singular value decomposition (t-SVD) based\\ntensor nuclear norm is imposed to preserve the low-rank property of the\\nessential tensor, which can well capture the principle information from\\nmultiple views. We also employ the tensor rotation operator for this task to\\nbetter investigate the relationship among views as well as reduce the\\ncomputation complexity. The proposed method can be efficiently optimized by the\\nalternating direction method of multipliers~(ADMM). Extensive experiments on\\nsix real world datasets corresponding to five different applications show that\\nour method achieves superior performance over other state-of-the-art methods.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1807.04931</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1807.04931</id><submitter>Jin Wu</submitter><version version=\"v1\"><date>Fri, 13 Jul 2018 06:30:16 GMT</date><size>19kb</size></version><title>Convexity Analysis of Optimization Framework of Attitude Determination\\n  from Vector Observations</title><authors>Jin Wu, Zebo Zhou, Min Song</authors><categories>cs.SY</categories><journal-ref>IEEE CODIT 2019</journal-ref><doi>10.1109/CoDIT.2019.8820652</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In the past several years, there have been several representative attitude\\ndetermination methods developed using derivative-based optimization algorithms.\\nOptimization techniques e.g. gradient-descent algorithm (GDA), Gauss-Newton\\nalgorithm (GNA), Levenberg-Marquadt algorithm (LMA) suffer from local optimum\\nin real engineering practices. A brief discussion on the convexity of this\\nproblem is presented recently \\\\cite{Ahmed2012} stating that the problem is\\nneither convex nor concave. In this paper, we give analytic proofs on this\\nproblem. The results reveal that the target loss function is convex in the\\ncommon practice of quaternion normalization, which leads to non-existence of\\nlocal optimum.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1807.05597</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1807.05597</id><submitter>Marcus Scheunemann</submitter><version version=\"v1\"><date>Sun, 15 Jul 2018 19:15:41 GMT</date><size>2305kb</size><source_type>D</source_type></version><title>Deep Learning for Semantic Segmentation on Minimal Hardware</title><authors>Sander G. van Dijk, Marcus M. Scheunemann</authors><categories>cs.LG cs.CV cs.RO stat.ML</categories><comments>12 pages, 5 figures, RoboCup International Symposium 2018</comments><acm-class>I.2.9; I.2.6; I.2.10</acm-class><doi>10.1007/978-3-030-27544-0_29</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning has revolutionised many fields, but it is still challenging to\\ntransfer its success to small mobile robots with minimal hardware.\\nSpecifically, some work has been done to this effect in the RoboCup humanoid\\nfootball domain, but results that are performant and efficient and still\\ngenerally applicable outside of this domain are lacking. We propose an approach\\nconceptually different from those taken previously. It is based on semantic\\nsegmentation and does achieve these desired properties. In detail, it is being\\nable to process full VGA images in real-time on a low-power mobile processor.\\nIt can further handle multiple image dimensions without retraining, it does not\\nrequire specific domain knowledge for achieving a high frame rate and it is\\napplicable on a minimal mobile hardware.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1807.06091</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1807.06091</id><submitter>Tetsuya Sato</submitter><version version=\"v1\"><date>Mon, 16 Jul 2018 20:20:20 GMT</date><size>861kb</size></version><version version=\"v2\"><date>Tue, 18 Sep 2018 14:06:47 GMT</date><size>122kb</size></version><title>Formal verification of higher-order probabilistic programs</title><authors>Tetsuya Sato, Alejandro Aguirre, Gilles Barthe, Marco Gaboardi, Deepak\\n  Garg, Justin Hsu</authors><categories>cs.LO</categories><doi>10.1145/3290351</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic programming provides a convenient lingua franca for writing\\nsuccinct and rigorous descriptions of probabilistic models and inference tasks.\\nSeveral probabilistic programming languages, including Anglican, Church or\\nHakaru, derive their expressiveness from a powerful combination of continuous\\ndistributions, conditioning, and higher-order functions. Although very\\nimportant for practical applications, these combined features raise fundamental\\nchallenges for program semantics and verification. Several recent works offer\\npromising answers to these challenges, but their primary focus is on semantical\\nissues.\\n  In this paper, we take a step further and we develop a set of program logics,\\nnamed PPV, for proving properties of programs written in an expressive\\nprobabilistic higher-order language with continuous distributions and operators\\nfor conditioning distributions by real-valued functions. Pleasingly, our\\nprogram logics retain the comfortable reasoning style of informal proofs thanks\\nto carefully selected axiomatizations of key results from probability theory.\\nThe versatility of our logics is illustrated through the formal verification of\\nseveral intricate examples from statistics, probabilistic inference, and\\nmachine learning. We further show the expressiveness of our logics by giving\\nsound embeddings of existing logics. In particular, we do this in a parametric\\nway by showing how the semantics idea of (unary and relational) TT-lifting can\\nbe internalized in our logics. The soundness of PPV follows by interpreting\\nprograms and assertions in quasi-Borel spaces (QBS), a recently proposed\\nvariant of Borel spaces with a good structure for interpreting higher order\\nprobabilistic programs.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1807.07372</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1807.07372</id><submitter>Mar\\\\\\'ia P\\\\\\'ia Mazzoleni</submitter><version version=\"v1\"><date>Thu, 19 Jul 2018 12:50:51 GMT</date><size>407kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 13:59:08 GMT</date><size>461kb</size><source_type>D</source_type></version><title>On some special classes of contact $B_0$-VPG graphs</title><authors>Flavia Bonomo-Braberman, Mar\\\\\\'ia P\\\\\\'ia Mazzoleni, Mariano Leonardo\\n  Rean, Bernard Ries</authors><categories>math.CO cs.DM</categories><comments>34 pages, 15 figures</comments><msc-class>05C99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph $G$ is a $B_0$-VPG graph if one can associate a path on a rectangular\\ngrid with each vertex such that two vertices are adjacent if and only if the\\ncorresponding paths intersect at at least one grid-point. A graph $G$ is a\\ncontact $B_0$-VPG graph if it is a $B_0$-VPG graph admitting a representation\\nwith no two paths crossing and no two paths sharing an edge of the grid. In\\nthis paper, we present a minimal forbidden induced subgraph characterisation of\\ncontact $B_0$-VPG graphs within four special graph classes: chordal graphs,\\ntree-cographs, $P_4$-tidy graphs and $P_5$-free graphs. Moreover, we present a\\npolynomial-time algorithm for recognising chordal contact $B_0$-VPG graphs.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1807.08305</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1807.08305</id><submitter>Nir Shlezinger</submitter><version version=\"v1\"><date>Sun, 22 Jul 2018 15:42:36 GMT</date><size>265kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 1 Aug 2019 09:58:45 GMT</date><size>271kb</size><source_type>D</source_type></version><title>Hardware-Limited Task-Based Quantization</title><authors>Nir Shlezinger, Yonina C. Eldar, and Miguel R. D. Rodrigues</authors><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2019.2935864</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantization plays a critical role in digital signal processing systems.\\nQuantizers are typically designed to obtain an accurate digital representation\\nof the input signal, operating independently of the system task, and are\\ncommonly implemented using serial scalar analog-to-digital converters (ADCs).\\nIn this work, we study hardware-limited task-based quantization, where a system\\nutilizing a serial scalar ADC is designed to provide a suitable representation\\nin order to allow the recovery of a parameter vector underlying the input\\nsignal. We propose hardware-limited task-based quantization systems for a fixed\\nand finite quantization resolution, and characterize their achievable\\ndistortion. We then apply the analysis to the practical setups of channel\\nestimation and eigen-spectrum recovery from quantized measurements. Our results\\nillustrate that properly designed hardware-limited systems can approach the\\noptimal performance achievable with vector quantizers, and that by taking the\\nunderlying task into account, the quantization error can be made negligible\\nwith a relatively small number of bits.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1807.08370</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1807.08370</id><submitter>Chia-Wen Lin</submitter><version version=\"v1\"><date>Sun, 22 Jul 2018 21:18:38 GMT</date><size>3233kb</size><source_type>D</source_type></version><title>SiGAN: Siamese Generative Adversarial Network for Identity-Preserving\\n  Face Hallucination</title><authors>Chih-Chung Hsu, Chia-Wen Lin, Weng-Tai Su, and Gene Cheung</authors><categories>cs.CV</categories><comments>13 pages</comments><doi>10.1109/TIP.2019.2924554</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite generative adversarial networks (GANs) can hallucinate\\nphoto-realistic high-resolution (HR) faces from low-resolution (LR) faces, they\\ncannot guarantee preserving the identities of hallucinated HR faces, making the\\nHR faces poorly recognizable. To address this problem, we propose a Siamese GAN\\n(SiGAN) to reconstruct HR faces that visually resemble their corresponding\\nidentities. On top of a Siamese network, the proposed SiGAN consists of a pair\\nof two identical generators and one discriminator. We incorporate\\nreconstruction error and identity label information in the loss function of\\nSiGAN in a pairwise manner. By iteratively optimizing the loss functions of the\\ngenerator pair and discriminator of SiGAN, we cannot only achieve\\nphoto-realistic face reconstruction, but also ensures the reconstructed\\ninformation is useful for identity recognition. Experimental results\\ndemonstrate that SiGAN significantly outperforms existing face hallucination\\nGANs in objective face verification performance, while achieving\\nphoto-realistic reconstruction. Moreover, for input LR faces from unknown\\nidentities who are not included in training, SiGAN can still do a good job.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1807.10881</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1807.10881</id><submitter>Lan Truong</submitter><version version=\"v1\"><date>Sat, 28 Jul 2018 03:22:57 GMT</date><size>238kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 04:30:14 GMT</date><size>239kb</size><source_type>D</source_type></version><title>On the Capacity of Symmetric $M$-user Gaussian Interference Channels\\n  with Feedback</title><authors>Lan V. Truong, Hirosuke Yamamoto</authors><categories>cs.IT math.IT</categories><comments>Accepted to IEEE Transactions on Information Theory in Sept. 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general time-varying feedback coding scheme is proposed for $M$-user fully\\nconnected symmetric Gaussian interference channels. Based on the analysis of\\nthe general coding scheme, we prove a theorem which gives a criterion for\\ndesigning good time-varying feedback codes for Gaussian interference channels.\\nThe proposed scheme improves the Suh-Tse and Kramer inner bounds of the channel\\ncapacity for the cases of weak and not very strong interference when $M=2$.\\nThis capacity improvement is more significant when the signal-to-noise ratio\\n(SNR) is not very high. In addition, our coding scheme can be proved\\nmathematically and numerically to outperform the Kramer code for $M\\\\geq 2$ when\\nSignal to Noise Ratio (SNR) is equal to Interference to Noise Ratio (INR).\\nBesides, the generalized degrees-of-freedom (GDoF) of our proposed coding\\nscheme can be proved to be optimal in the all network situations (very weak,\\nweak, strong, very strong) for any $M$. The numerical results show that our\\ncoding scheme can attain better performance than the Suh-Tse coding scheme for\\n$M=2$ or the Mohajer-Tandon-Poor lattice coding scheme for $M&gt;2$. Furthermore,\\nthe simplicity of the encoding/decoding algorithms is another strong point of\\nour proposed coding scheme compared with the Suh-Tse coding scheme when $M=2$\\nand the Mohajer-Tandon-Poor lattice coding scheme when $M&gt;2$. More importantly,\\nour results show that an optimal coding scheme for the symmetric Gaussian\\ninterference channels with feedback can be achieved by only using marginal\\nposterior distributions under a better cooperation strategy between\\ntransmitters.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1807.11087</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1807.11087</id><submitter>Bruno Bauwens</submitter><version version=\"v1\"><date>Sun, 29 Jul 2018 17:24:54 GMT</date><size>46kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 22:43:31 GMT</date><size>54kb</size><source_type>D</source_type></version><title>Information Distance Revisited</title><authors>Bruno Bauwens and Alexander Shen</authors><categories>cs.IT math.IT</categories><comments>Preliminary version, published for reference purposes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the notion of information distance between two objects x and y\\nintroduced by Bennett, G\\\\\\'acs, Li, Vitanyi, and Zurek [1] as the minimal length\\nof a program that computes x from y as well as computing y from x, and study\\ndifferent versions of this notion. It was claimed by Mahmud [11] that the\\nprefix version of information distance equals max(K(x|y), K(y|) + O(1) (this\\nequality with logarithmic precision was one of the main results of the paper by\\nBennett, G\\\\\\'acs, Li, Vitanyi, and Zurek). We show that this claim is false, but\\ndoes hold if the information distance is at least super logarithmic.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1807.11657</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1807.11657</id><submitter>Swaprava Nath</submitter><version version=\"v1\"><date>Tue, 31 Jul 2018 04:31:05 GMT</date><size>990kb</size></version><version version=\"v2\"><date>Wed, 2 Oct 2019 18:59:50 GMT</date><size>1312kb</size></version><version version=\"v3\"><date>Fri, 4 Oct 2019 10:46:11 GMT</date><size>1438kb</size></version><version version=\"v4\"><date>Mon, 7 Oct 2019 10:41:24 GMT</date><size>1438kb</size></version><title>Ensuring Honest Effort in Peer Grading</title><authors>Anujit Chakraborty and Jatin Jindal and Swaprava Nath</authors><categories>cs.GT</categories><comments>41 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive open online courses (MOOCs) pose a great challenge for grading the\\nhuge number of answer-scripts at high accuracy. Peer grading is a scalable\\nsolution to this challenge, but the current practices largely depend on the\\naltruism of the peer graders. Some peer-grading approaches treat it as a\\nbest-effort service of the graders, and statistically correct their\\ninaccuracies before awarding the final scores. Approaches that incentivize\\nnon-strategic behavior of the peer graders do not make use of certain possible\\nadditional information, e.g., that the true grade can eventually be observed at\\nthe additional cost of the teaching staff time if an affected student raises a\\nregrading request. In this paper, we use such additional information and\\nintroduce a mechanism, TRUPEQA, that (a) uses a constant number of\\ninstructor-graded answer-scripts to quantitatively measure the accuracies of\\nthe peer graders and corrects the scores accordingly, and (b) penalizes\\ndeliberate under-performing. We show that this mechanism is unique in its class\\nto satisfy certain properties. Our human subject experiments show that TRUPEQA\\nimproves the grading quality over the mechanisms currently used in standard\\nMOOCs.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1808.00337</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1808.00337</id><submitter>Miklas S. Kristoffersen</submitter><version version=\"v1\"><date>Mon, 30 Jul 2018 11:17:43 GMT</date><size>267kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 10:44:34 GMT</date><size>269kb</size><source_type>D</source_type></version><title>The Importance of Context When Recommending TV Content: Dataset and\\n  Algorithms</title><authors>Miklas S. Kristoffersen, Sven E. Shepstone, Zheng-Hua Tan</authors><categories>cs.IR cs.LG cs.MM stat.ML</categories><doi>10.1109/TMM.2019.2944214</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Home entertainment systems feature in a variety of usage scenarios with one\\nor more simultaneous users, for whom the complexity of choosing media to\\nconsume has increased rapidly over the last decade. Users\\' decision processes\\nare complex and highly influenced by contextual settings, but data supporting\\nthe development and evaluation of context-aware recommender systems are scarce.\\nIn this paper we present a dataset of self-reported TV consumption enriched\\nwith contextual information of viewing situations. We show how choice of genre\\nassociates with, among others, the number of present users and users\\' attention\\nlevels. Furthermore, we evaluate the performance of predicting chosen genres\\ngiven different configurations of contextual information, and compare the\\nresults to contextless predictions. The results suggest that including\\ncontextual features in the prediction cause notable improvements, and both\\ntemporal and social context show significant contributions.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1808.00441</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1808.00441</id><submitter>Pere Gim\\\\\\'enez-Febrer</submitter><version version=\"v1\"><date>Wed, 1 Aug 2018 17:41:23 GMT</date><size>373kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 28 Mar 2019 13:49:04 GMT</date><size>78kb</size></version><title>Matrix completion and extrapolation via kernel regression</title><authors>Pere Gim\\\\\\'enez-Febrer, Alba Pag\\\\`es-Zamora, Georgios B. Giannakis</authors><categories>stat.ML cs.LG</categories><doi>10.1109/TSP.2019.2932875</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrix completion and extrapolation (MCEX) are dealt with here over\\nreproducing kernel Hilbert spaces (RKHSs) in order to account for prior\\ninformation present in the available data. Aiming at a faster and\\nlow-complexity solver, the task is formulated as a kernel ridge regression. The\\nresultant MCEX algorithm can also afford online implementation, while the class\\nof kernel functions also encompasses several existing approaches to MC with\\nprior information. Numerical tests on synthetic and real datasets show that the\\nnovel approach performs faster than widespread methods such as alternating\\nleast squares (ALS) or stochastic gradient descent (SGD), and that the recovery\\nerror is reduced, especially when dealing with noisy data.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1808.00923</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1808.00923</id><submitter>Valeria Vignudelli</submitter><version version=\"v1\"><date>Thu, 2 Aug 2018 17:19:29 GMT</date><size>96kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 11 Aug 2018 14:01:53 GMT</date><size>96kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 15 Jan 2019 11:44:53 GMT</date><size>190kb</size></version><title>The Theory of Traces for Systems with Nondeterminism and Probability</title><authors>Filippo Bonchi, Ana Sokolova, Valeria Vignudelli</authors><categories>cs.LO</categories><journal-ref>2019 34th Annual ACM/IEEE Symposium on Logic in Computer Science\\n  (LICS), IEEE</journal-ref><doi>10.1109/LICS.2019.8785673</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies trace-based equivalences for systems combining\\nnondeterministic and probabilistic choices. We show how trace semantics for\\nsuch processes can be recovered by instantiating a coalgebraic construction\\nknown as the generalised powerset construction. We characterise and compare the\\nresulting semantics to known definitions of trace equivalences appearing in the\\nliterature. This inspires, in turn, a general theory of may and must testing\\nwhere tests are finite traces. Most of our results are based on the exciting\\ninterplay between monads and their presentations via algebraic theories.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1808.01546</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1808.01546</id><submitter>Jiliang Zhang</submitter><version version=\"v1\"><date>Sun, 5 Aug 2018 01:03:13 GMT</date><size>459kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 5 Oct 2019 09:04:07 GMT</date><size>564kb</size><source_type>D</source_type></version><title>ATMPA: Attacking Machine Learning-based Malware Visualization Detection\\n  Methods via Adversarial Examples</title><authors>Xinbo Liu, Jiliang Zhang, Yapin Lin and He Li</authors><categories>cs.CR</categories><comments>9 pages, 5 figures</comments><journal-ref>IEEE/ACM International Symposium on Quality of Service(2019)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the threat of malicious software (malware) has become increasingly\\nserious, automatic malware detection techniques have received increasing\\nattention, where machine learning (ML)-based visualization detection methods\\nbecome more and more popular. In this paper, we demonstrate that the\\nstate-of-the-art ML-based visualization detection methods are vulnerable to\\nAdversarial Example (AE) attacks. We develop a novel Adversarial Texture\\nMalware Perturbation Attack (ATMPA) method based on the gradient descent and\\nL-norm optimization method, where attackers can introduce some tiny\\nperturbations on the transformed dataset such that ML-based malware detection\\nmethods will completely fail. The experimental results on the MS BIG malware\\ndataset show that a small interference can reduce the accuracy rate down to 0%\\nfor several ML-based detection methods, and the rate of transferability is\\n74.1% on average.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1808.01961</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1808.01961</id><submitter>Juri Ranieri</submitter><version version=\"v1\"><date>Mon, 6 Aug 2018 15:22:15 GMT</date><size>2622kb</size><source_type>D</source_type></version><title>Super Resolution Phase Retrieval for Sparse Signals</title><authors>Gilles Baechler, Miranda Krekovi\\\\\\'c, Juri Ranieri, Amina Chebira, Yue\\n  M. Lu and Martin Vetterli</authors><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2019.2931169</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a variety of fields, in particular those involving imaging and optics, we\\noften measure signals whose phase is missing or has been irremediably\\ndistorted. Phase retrieval attempts to recover the phase information of a\\nsignal from the magnitude of its Fourier transform to enable the reconstruction\\nof the original signal. Solving the phase retrieval problem is equivalent to\\nrecovering a signal from its auto-correlation function. In this paper, we\\nassume the original signal to be sparse; this is a natural assumption in many\\napplications, such as X-ray crystallography, speckle imaging and blind channel\\nestimation. We propose an algorithm that resolves the phase retrieval problem\\nin three stages: i) we leverage the finite rate of innovation sampling theory\\nto super-resolve the auto-correlation function from a limited number of\\nsamples, ii) we design a greedy algorithm that identifies the locations of a\\nsparse solution given the super-resolved auto-correlation function, iii) we\\nrecover the amplitudes of the atoms given their locations and the measured\\nauto-correlation function. Unlike traditional approaches that recover a\\ndiscrete approximation of the underlying signal, our algorithm estimates the\\nsignal on a continuous domain, which makes it the first of its kind.\\n  Along with the algorithm, we derive its performance bound with a theoretical\\nanalysis and propose a set of enhancements to improve its computational\\ncomplexity and noise resilience. Finally, we demonstrate the benefits of the\\nproposed method via a comparison against Charge Flipping, a notable algorithm\\nin crystallography.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1808.02451</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1808.02451</id><submitter>Yu-Sung Tu</submitter><version version=\"v1\"><date>Tue, 7 Aug 2018 16:39:53 GMT</date><size>36kb</size></version><version version=\"v2\"><date>Tue, 1 Oct 2019 04:55:35 GMT</date><size>40kb</size></version><title>Evolution of Preferences in Multiple Populations</title><authors>Yu-Sung Tu and Wei-Torng Juang</authors><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the evolution of preferences in a multi-population setting. Each\\nindividual has subjective preferences over potential outcomes, and chooses a\\nbest response based on his preferences and the information about the opponents\\'\\npreferences. However, individuals\\' actual fitnesses are defined by material\\npayoff functions. Following Dekel et al. (2007), we assume that individuals can\\nobserve their opponents\\' preferences with some fixed probability $p$, and\\nderive necessary and sufficient conditions for stability for $p=1$ and $p=0$.\\nWe also check the robustness of our results against small perturbations on\\nobservability for the case of pure-strategy outcomes.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1808.03809</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1808.03809</id><submitter>Maria Filipkovska</submitter><version version=\"v1\"><date>Sat, 11 Aug 2018 14:19:09 GMT</date><size>2281kb</size></version><version version=\"v2\"><date>Wed, 12 Sep 2018 22:11:06 GMT</date><size>2315kb</size></version><version version=\"v3\"><date>Sat, 28 Sep 2019 10:56:35 GMT</date><size>1852kb</size></version><title>Two combined methods for the global solution of implicit semilinear\\n  differential equations with the use of spectral projectors and Taylor\\n  expansions</title><authors>M. S. Filipkovska</authors><categories>math.NA cs.NA</categories><msc-class>65L80, 65L20, 65L70, 34A09, 34A12, 34D23, 47N40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two combined numerical methods for solving semilinear differential-algebraic\\nequations (DAEs) are obtained and their convergence is proved. The comparative\\nanalysis of these methods is carried out and conclusions about the\\neffectiveness of their application in various situations are made. In\\ncomparison with other known methods, the obtained methods require weaker\\nrestrictions for the nonlinear part of the DAE. Also, the obtained methods\\nenable to compute approximate solutions of the DAEs on any given time interval\\nand, therefore, enable to carry out the numerical analysis of global dynamics\\nof mathematical models described by the DAEs. The examples demonstrating the\\ncapabilities of the developed methods are provided. To construct the methods we\\nuse the spectral projectors, Taylor expansions and finite differences. Since\\nthe used spectral projectors can be easily computed, to apply the methods it is\\nnot necessary to carry out additional analytical transformations.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1808.03906</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1808.03906</id><submitter>Raziyeh Dehbozorgi</submitter><version version=\"v1\"><date>Sun, 12 Aug 2018 08:19:41 GMT</date><size>85kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 15:04:57 GMT</date><size>85kb</size><source_type>D</source_type></version><title>Direct numerical scheme for all classes of nonlinear Volterra integral\\n  equations of the first kind</title><authors>R. Dehbozorgi, K. Maleknejad</authors><categories>math.NA cs.NA</categories><msc-class>65R20, 45D05, 45G10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a direct numerical scheme to approximate the solution of\\nall classes of nonlinear Volterra integral equations of the first kind. This\\ncomputational method is based on operational matrices and vectors. The\\noperational vector for hybrid block pulse functions and Chebyshev polynomials\\nis constructed. The scheme transforms the integral equation to a matrix\\nequation and solves it with a careful estimate of the error involved. The main\\ncharacteristic of the scheme is the low cost of setting up the equations\\nwithout using any projection method which is the consequence of using\\noperational vectors. Simple structure to implement, low computational cost and\\nperfect approximate solutions are the major points of the presented method.\\n  Error analysis and comparisons with other existing schemes demonstrate the\\nefficiency and the superiority of our scheme.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1808.04448</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1808.04448</id><submitter>Debajyoti Bera</submitter><version version=\"v1\"><date>Fri, 10 Aug 2018 17:15:41 GMT</date><size>227kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 12:11:25 GMT</date><size>373kb</size><source_type>D</source_type></version><title>Efficient Quantum Algorithms related to Autocorrelation Spectrum</title><authors>Debajyoti Bera and Subhamoy Maitra and SAPV Tharrmashastha</authors><categories>quant-ph cs.CC</categories><comments>Accepted in Indocrypt 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose efficient probabilistic algorithms for several\\nproblems regarding the autocorrelation spectrum. First, we present a quantum\\nalgorithm that samples from the Walsh spectrum of any derivative of $f()$.\\nInformally, the autocorrelation coefficient of a Boolean function $f()$ at some\\npoint $a$ measures the average correlation among the values $f(x)$ and $f(x\\n\\\\oplus a)$. The derivative of a Boolean function is an extension of\\nautocorrelation to correlation among multiple values of $f()$. The Walsh\\nspectrum is well-studied primarily due to its connection to the quantum circuit\\nfor the Deutsch-Jozsa problem. We extend the idea to &quot;Higher-order\\nDeutsch-Jozsa&quot; quantum algorithm to obtain points corresponding to large\\nabsolute values in the Walsh spectrum of a certain derivative of $f()$.\\nFurther, we design an algorithm to sample the input points according to squares\\nof the autocorrelation coefficients. Finally we provide a different set of\\nalgorithms for estimating the square of a particular coefficient or cumulative\\nsum of their squares.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1808.05415</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1808.05415</id><submitter>John Baez</submitter><version version=\"v1\"><date>Thu, 16 Aug 2018 11:04:45 GMT</date><size>30kb</size></version><version version=\"v2\"><date>Fri, 17 Aug 2018 06:01:18 GMT</date><size>31kb</size></version><version version=\"v3\"><date>Sat, 3 Nov 2018 17:52:35 GMT</date><size>31kb</size></version><version version=\"v4\"><date>Thu, 29 Nov 2018 00:43:01 GMT</date><size>29kb</size></version><version version=\"v5\"><date>Mon, 30 Sep 2019 00:06:18 GMT</date><size>31kb</size></version><title>Open Petri Nets</title><authors>John C. Baez and Jade Master</authors><categories>math.CT cs.LO</categories><comments>30 pages, TikZ figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reachability semantics for Petri nets can be studied using open Petri\\nnets. For us an &quot;open&quot; Petri net is one with certain places designated as\\ninputs and outputs via a cospan of sets. We can compose open Petri nets by\\ngluing the outputs of one to the inputs of another. Open Petri nets can be\\ntreated as morphisms of a category $\\\\mathsf{Open}(\\\\mathsf{Petri})$, which\\nbecomes symmetric monoidal under disjoint union. However, since the composite\\nof open Petri nets is defined only up to isomorphism, it is better to treat\\nthem as morphisms of a symmetric monoidal double category\\n$\\\\mathbb{O}\\\\mathbf{pen}(\\\\mathsf{Petri})$. We describe two forms of semantics\\nfor open Petri nets using symmetric monoidal double functors out of\\n$\\\\mathbb{O}\\\\mathbf{pen}(\\\\mathsf{Petri})$. The first, an operational semantics,\\ngives for each open Petri net a category whose morphisms are the processes that\\nthis net can carry out. This is done in a compositional way, so that these\\ncategories can be computed on smaller subnets and then glued together. The\\nsecond, a reachability semantics, simply says which markings of the outputs can\\nbe reached from a given marking of the inputs.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1808.06696</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1808.06696</id><submitter>Dmitry Ustalov</submitter><version version=\"v1\"><date>Mon, 20 Aug 2018 21:06:01 GMT</date><size>1088kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 22 Feb 2019 18:16:29 GMT</date><size>1228kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 8 Apr 2019 17:15:26 GMT</date><size>626kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Wed, 19 Jun 2019 08:04:32 GMT</date><size>626kb</size><source_type>D</source_type></version><title>Watset: Local-Global Graph Clustering with Applications in Sense and\\n  Frame Induction</title><authors>Dmitry Ustalov and Alexander Panchenko and Chris Biemann and Simone\\n  Paolo Ponzetto</authors><categories>cs.CL</categories><comments>58 pages, 17 figures, accepted at the Computational Linguistics\\n  journal</comments><msc-class>68T50</msc-class><acm-class>I.2.7</acm-class><journal-ref>Computational Linguistics 45:3 (2019) 423-479</journal-ref><doi>10.1162/COLI_a_00354</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a detailed theoretical and computational analysis of the Watset\\nmeta-algorithm for fuzzy graph clustering, which has been found to be widely\\napplicable in a variety of domains. This algorithm creates an intermediate\\nrepresentation of the input graph that reflects the &quot;ambiguity&quot; of its nodes.\\nThen, it uses hard clustering to discover clusters in this &quot;disambiguated&quot;\\nintermediate graph. After outlining the approach and analyzing its\\ncomputational complexity, we demonstrate that Watset shows competitive results\\nin three applications: unsupervised synset induction from a synonymy graph,\\nunsupervised semantic frame induction from dependency triples, and unsupervised\\nsemantic class induction from a distributional thesaurus. Our algorithm is\\ngeneric and can be also applied to other networks of linguistic data.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1808.07689</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1808.07689</id><submitter>Changick Song</submitter><version version=\"v1\"><date>Thu, 23 Aug 2018 10:17:24 GMT</date><size>791kb</size></version><title>Optimal Precoder Designs for Sum-utility Maximization in SWIPT-enabled\\n  Multi-user MIMO Cognitive Radio Networks</title><authors>Changick Song, Hoon Lee, Kyoung-Jae Lee</authors><categories>cs.IT math.IT</categories><comments>12pages, 9 figures, submitted to IEEE Systems Journal</comments><doi>10.1109/JSYST.2018.2875762</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a generalized framework that combines the cognitive\\nradio (CR) techniques for spectrum sharing and the simultaneous wireless\\ninformation and power transfer (SWIPT) for energy harvesting (EH) in the\\nconventional multi-user MIMO (MuMIMO) channels, which leads to an\\nMuMIMO-CR-SWIPT network. In this system, we have one secondary base-station\\n(S-BS) that supports multiple secondary information decoding (S-ID) and\\nsecondary EH (S-EH) users simultaneously under the condition that interference\\npower that affects the primary ID (P-ID) receivers should stay below a certain\\nthreshold. The goal of the paper is to develop a generalized precoder design\\nthat maximizes the sum-utility cost function under the transmit power\\nconstraint at the S-BS, and the EH constraint at each S-EH user, and the\\ninterference power constraint at each P-ID user. Therefore, the previous\\nstudies for the CR and SWIPT systems are casted as particular solutions of the\\nproposed framework. The problem is inherently non-convex and even the weighted\\nminimum mean squared error (WMMSE) transformation does not resolve the\\nnon-convexity of the original problem. To tackle the problem, we find a\\nsolution from the dual optimization via sub-gradient ellipsoid method based on\\nthe observation that the WMMSE transformation raises zero-duality gap between\\nthe primal and the dual problems. We also propose a simplified algorithm for\\nthe case of a single S-ID user, which is shown to achieve the global optimum.\\nFinally, we demonstrate the optimality and efficiency of the proposed\\nalgorithms through numerical simulation results.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1808.08639</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1808.08639</id><submitter>James P. Crutchfield</submitter><version version=\"v1\"><date>Sun, 26 Aug 2018 23:01:38 GMT</date><size>939kb</size><source_type>D</source_type></version><title>Strong and Weak Optimizations in Classical and Quantum Models of\\n  Stochastic Processes</title><authors>Samuel Loomis and James P. Crutchfield</authors><categories>quant-ph cond-mat.stat-mech cs.IT math.IT</categories><comments>14 pages, 14 figures;\\n  http://csc.ucdavis.edu/~cmg/compmech/pubs/uemum.htm</comments><doi>10.1007/s10955-019-02344-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Among the predictive hidden Markov models that describe a given stochastic\\nprocess, the {\\\\epsilon}-machine is strongly minimal in that it minimizes every\\nR\\\\\\'enyi-based memory measure. Quantum models can be smaller still. In contrast\\nwith the {\\\\epsilon}-machine\\'s unique role in the classical setting, however,\\namong the class of processes described by pure-state hidden quantum Markov\\nmodels, there are those for which there does not exist any strongly minimal\\nmodel. Quantum memory optimization then depends on which memory measure best\\nmatches a given problem circumstance.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1809.00325</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1809.00325</id><submitter>Long Teng</submitter><version version=\"v1\"><date>Sun, 2 Sep 2018 10:39:46 GMT</date><size>164kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 21 Mar 2019 00:04:32 GMT</date><size>282kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 25 Jun 2019 08:43:47 GMT</date><size>365kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Tue, 1 Oct 2019 09:49:33 GMT</date><size>365kb</size><source_type>D</source_type></version><title>A Review of Tree-based Approaches to solve Forward-Backward Stochastic\\n  Differential Equations</title><authors>Long Teng</authors><categories>math.NA cs.NA</categories><msc-class>60H10, 60H35, 62G08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study solving (decoupled) forward-backward stochastic\\ndifferential equations (FBSDEs) numerically using the regression trees. Based\\non the general theta-discretization for the time-integrands, we show how to\\nefficiently use regression tree-based methods to solve the resulting\\nconditional expectations. Several numerical experiments including\\nhigh-dimensional problems are provided to demonstrate the accuracy and\\nperformance of the tree-based approach. For the applicability of FBSDEs in\\nfinancial problems, we apply our tree-based approach to the Heston stochastic\\nvolatility model, the high-dimensional pricing problems of a Rainbow option and\\nan European financial derivative with different interest rates for borrowing\\nand lending.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1809.00751</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1809.00751</id><submitter>Chamsi Hssaine</submitter><version version=\"v1\"><date>Tue, 4 Sep 2018 00:24:27 GMT</date><size>31kb</size></version><version version=\"v2\"><date>Tue, 1 Jan 2019 19:06:50 GMT</date><size>31kb</size></version><version version=\"v3\"><date>Tue, 1 Oct 2019 23:24:47 GMT</date><size>74kb</size><source_type>D</source_type></version><title>Information Signal Design for Incentivizing Team Formation</title><authors>Chamsi Hssaine, Siddhartha Banerjee</authors><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the use of Bayesian persuasion (i.e., strategic use of information\\ndisclosure/signaling) in endogenous team formation. This is an important\\nconsideration in settings such as crowdsourcing competitions, open science\\nchallenges and group-based assignments, where a large number of agents organize\\nthemselves into small teams which then compete against each other. A central\\ntension here is between the strategic interests of agents who want to have the\\nhighest-performing team, and that of the principal who wants teams to be\\nbalanced. Moreover, although the principal cannot choose the teams or modify\\nrewards, she often has additional knowledge of agents\\' abilities, and can\\nleverage this information asymmetry to provide signals that influence team\\nformation. Our work uncovers the critical role of self-awareness (i.e.,\\nknowledge of one\\'s own abilities) for the design of such mechanisms. For\\nsettings with two-member teams and binary-valued agents partitioned into a\\nconstant number of prior classes, we provide signaling mechanisms which are\\nasymptotically optimal when agents are agnostic of their own abilities. On the\\nother hand, when agents are self-aware, then we show that there is no signaling\\nmechanism that can do better than not releasing information, while satisfying\\nagent participation constraints.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1809.00928</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1809.00928</id><submitter>Jirapat Likitlersuang</submitter><version version=\"v1\"><date>Thu, 30 Aug 2018 20:31:20 GMT</date><size>3944kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 20:02:31 GMT</date><size>3945kb</size><source_type>D</source_type></version><title>Egocentric Video: A New Tool for Capturing Hand Use of Individuals with\\n  Spinal Cord Injury at Home</title><authors>Jirapat Likitlersuang, Elizabeth R. Sumitro, Tianshi Cao, Ryan J.\\n  Visee, Sukhvinder Kalsi-Ryan, and Jose Zariffa</authors><categories>cs.HC</categories><comments>13 pages, 4 figures, and 2 tables</comments><acm-class>I.4.9; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current upper extremity outcome measures for persons with cervical spinal\\ncord injury (cSCI) lack the ability to directly collect quantitative\\ninformation in home and community environments. A wearable first-person\\n(egocentric) camera system is presented that can monitor functional hand use\\noutside of clinical settings. The system is based on computer vision algorithms\\nthat detect the hand, segment the hand outline, distinguish the user\\'s left or\\nright hand, and detect functional interactions of the hand with objects during\\nactivities of daily living. The algorithm was evaluated using egocentric video\\nrecordings from 9 participants with cSCI, obtained in a home simulation\\nlaboratory. The system produces a binary hand-object interaction decision for\\neach video frame, based on features reflecting motion cues of the hand, hand\\nshape and colour characteristics of the scene. This output was compared with a\\nmanual labelling of the video, yielding F1-scores of 0.74 $\\\\pm$ 0.15 for the\\nleft hand and 0.73 $\\\\pm$ 0.15 for the right hand. From the resulting\\nframe-by-frame binary data, functional hand use measures were extracted: the\\namount of total interaction as a percentage of testing time, the average\\nduration of interactions in seconds, and the number of interactions per hour.\\nModerate and significant correlations were found when comparing these output\\nmeasures to the results of the manual labelling, with $\\\\rho$ = 0.40, 0.54 and\\n0.55 respectively. These results demonstrate the potential of a wearable\\negocentric camera for capturing quantitative measures of hand use at home.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1809.02341</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1809.02341</id><submitter>Zhize Li</submitter><version version=\"v1\"><date>Fri, 7 Sep 2018 08:12:56 GMT</date><size>310kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 1 Apr 2019 13:30:50 GMT</date><size>333kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 7 Oct 2019 17:51:19 GMT</date><size>391kb</size><source_type>D</source_type></version><title>A Fast Anderson-Chebyshev Acceleration for Nonlinear Optimization</title><authors>Zhize Li, Jian Li</authors><categories>math.OC cs.LG cs.NA stat.ML</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anderson acceleration (or Anderson mixing) is an efficient acceleration\\nmethod for fixed point iterations $x_{t+1}=G(x_t)$, e.g., gradient descent can\\nbe viewed as iteratively applying the operation $G(x) \\\\triangleq x-\\\\alpha\\\\nabla\\nf(x)$. It is known that Anderson acceleration is quite efficient in practice\\nand can be viewed as an extension of Krylov subspace methods for nonlinear\\nproblems. In this paper, we show that Anderson acceleration with Chebyshev\\npolynomial can achieve the optimal convergence rate\\n$O(\\\\sqrt{\\\\kappa}\\\\ln\\\\frac{1}{\\\\epsilon})$, which improves the previous result\\n$O(\\\\kappa\\\\ln\\\\frac{1}{\\\\epsilon})$ provided by (Toth &amp; Kelley, 2015) for\\nquadratic functions. Moreover, we provide a convergence analysis for minimizing\\ngeneral nonlinear problems. Besides, if the hyperparameters (e.g., the\\nLipschitz smooth parameter $L$) are not available, we propose a guessing\\nalgorithm for guessing them dynamically and also prove a similar convergence\\nrate. Finally, the experimental results demonstrate that the proposed\\nAnderson-Chebyshev acceleration method converges significantly faster than\\nother algorithms, e.g., vanilla gradient descent (GD), Nesterov\\'s Accelerated\\nGD. Also, these algorithms combined with the proposed guessing algorithm\\n(guessing the hyperparameters dynamically) achieve much better performance.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1809.02479</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1809.02479</id><submitter>Zain Amin</submitter><version version=\"v1\"><date>Fri, 7 Sep 2018 13:56:06 GMT</date><size>161kb</size></version><version version=\"v2\"><date>Sun, 6 Oct 2019 15:13:07 GMT</date><size>847kb</size></version><title>Convolutional Neural Network: Text Classification Model for Open Domain\\n  Question Answering System</title><authors>Muhammad Zain Amin, Noman Nadeem</authors><categories>cs.IR</categories><comments>12 pages, typos corrected, tables added, references added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently machine learning is being applied to almost every data domain one of\\nwhich is Question Answering Systems (QAS). A typical Question Answering System\\nis fairly an information retrieval system, which matches documents or text and\\nretrieve the most accurate one. The idea of open domain question answering\\nsystem put forth, involves convolutional neural network text classifiers. The\\nClassification model presented in this paper is multi-class text classifier.\\nThe neural network classifier can be trained on large dataset. We report series\\nof experiments conducted on Convolution Neural Network (CNN) by training it on\\ntwo different datasets. Neural network model is trained on top of word\\nembedding. Softmax layer is applied to calculate loss and mapping of\\nsemantically related words. Gathered results can help justify the fact that\\nproposed hypothetical QAS is feasible. We further propose a method to integrate\\nConvolutional Neural Network Classifier to an open domain question answering\\nsystem. The idea of Open domain will be further explained, but the generality\\nof it indicates to the system of domain specific trainable models, thus making\\nit an open domain.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1809.02562</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1809.02562</id><submitter>Seong-Ho Kwon</submitter><version version=\"v1\"><date>Fri, 7 Sep 2018 16:11:21 GMT</date><size>1544kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 13:01:37 GMT</date><size>4873kb</size><source_type>D</source_type></version><title>[Extended version*] Generalized weak rigidity: Theory, and local and\\n  global convergence of formations</title><authors>Seong-Ho Kwon and Hyo-Sung Ahn</authors><categories>cs.SY</categories><comments>Submitted for journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses generalized weak rigidity theory, and aims to apply the\\ntheory to formation control problems with a gradient flow law. The generalized\\nweak rigidity theory is utilized in order that desired formations are\\ncharacterized by a general set of pure inter-agent distances and angles. As the\\nfirst result of its applications, the paper provides analysis of locally\\nexponential stability for formation systems with pure distance/angle\\nconstraints in the $2$- and $3$-dimensional spaces. Then, as the second result,\\nif there are three agents in the $2$-dimensional space, almost globally\\nexponential stability for formation systems is ensured. Through numerical\\nsimulations, the validity of analyses is illustrated.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1809.04258</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1809.04258</id><submitter>Zeheng Wang</submitter><version version=\"v1\"><date>Wed, 12 Sep 2018 05:04:58 GMT</date><size>553kb</size></version><version version=\"v2\"><date>Wed, 31 Jul 2019 07:02:37 GMT</date><size>820kb</size></version><version version=\"v3\"><date>Tue, 1 Oct 2019 05:02:36 GMT</date><size>1567kb</size></version><title>An Ontology-Based Artificial Intelligence Model for Medicine Side-Effect\\n  Prediction: Taking Traditional Chinese Medicine as An Example</title><authors>Yuanzhe Yao, Zeheng Wang, Liang Li, Kun Lu, Runyu Liu, Zhiyuan Liu,\\n  Jing Yan</authors><categories>cs.AI</categories><doi>10.1155/2019/8617503</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, an ontology-based model for AI-assisted medicine side-effect\\n(SE) prediction is developed, where three main components, including the drug\\nmodel, the treatment model, and the AI-assisted prediction model, of proposed\\nmodel are presented. To validate the proposed model, an ANN structure is\\nestablished and trained by two hundred and forty-two TCM prescriptions. These\\ndata are gathered and classified from the most famous ancient TCM book and more\\nthan one thousand SE reports, in which two ontology-based attributions, hot and\\ncold, are introduced to evaluate whether the prescription will cause SE or not.\\nThe results preliminarily reveal that it is a relationship between the\\nontology-based attributions and the corresponding predicted indicator that can\\nbe learnt by AI for predicting the SE, which suggests the proposed model has a\\npotential in AI-assisted SE prediction. However, it should be noted that, the\\nproposed model highly depends on the sufficient clinic data, and hereby, much\\ndeeper exploration is important for enhancing the accuracy of the prediction.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1809.04943</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1809.04943</id><submitter>Matus Medo</submitter><version version=\"v1\"><date>Thu, 13 Sep 2018 13:32:28 GMT</date><size>546kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 1 Aug 2019 21:57:59 GMT</date><size>527kb</size><source_type>D</source_type></version><title>Optimal timescale for community detection in growing networks</title><authors>Matus Medo, An Zeng, Yi-Cheng Zhang, Manuel S. Mariani</authors><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>14 pages, 8 figures + Supplementary Material</comments><journal-ref>New Journal of Physics 21, 093066 (2019)</journal-ref><doi>10.1088/1367-2630/ab413f</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time-stamped data are increasingly available for many social, economic, and\\ninformation systems that can be represented as networks growing with time. The\\nWorld Wide Web, social contact networks, and citation networks of scientific\\npapers and online news articles, for example, are of this kind. Static methods\\ncan be inadequate for the analysis of growing networks as they miss essential\\ninformation on the system\\'s dynamics. At the same time, time-aware methods\\nrequire the choice of an observation timescale, yet we lack principled ways to\\ndetermine it. We focus on the popular community detection problem which aims to\\npartition a network\\'s nodes into meaningful groups. We use a multi-layer\\nquality function to show, on both synthetic and real datasets, that the\\nobservation timescale that leads to optimal communities is tightly related to\\nthe system\\'s intrinsic aging timescale that can be inferred from the\\ntime-stamped network data. The use of temporal information leads to drastically\\ndifferent conclusions on the community structure of real information networks,\\nwhich challenges the current understanding of the large-scale organization of\\ngrowing networks. Our findings indicate that before attempting to assess\\nstructural patterns of evolving networks, it is vital to uncover the timescales\\nof the dynamical processes that generated them.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1809.05088</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1809.05088</id><submitter>Vibhaalakshmi Sivaraman</submitter><version version=\"v1\"><date>Thu, 13 Sep 2018 17:50:25 GMT</date><size>440kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 18 Sep 2018 16:02:55 GMT</date><size>424kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sun, 29 Sep 2019 14:07:35 GMT</date><size>3798kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Wed, 2 Oct 2019 15:23:14 GMT</date><size>3798kb</size><source_type>D</source_type></version><title>High Throughput Cryptocurrency Routing in Payment Channel Networks</title><authors>Vibhaalakshmi Sivaraman, Shaileshh Bojja Venkatakrishnan, Kathy Ruan,\\n  Parimarjan Negi, Lei Yang, Radhika Mittal, Mohammad Alizadeh, Giulia Fanti</authors><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite growing adoption of cryptocurrencies, making fast payments at scale\\nremains a challenge. Payment channel networks (PCNs) such as the Lightning\\nNetwork have emerged as a viable scaling solution. However, completing payments\\non PCNs is challenging: payments must be routed on paths with sufficient funds.\\nAs payments flow over a single channel (link) in the same direction, the\\nchannel eventually becomes depleted and cannot support further payments in that\\ndirection; hence, naive routing schemes like shortest-path routing can deplete\\nkey payment channels and paralyze the system. Today\\'s PCNs also route payments\\natomically, worsening the problem. In this paper, we present Spider, a routing\\nsolution that &quot;packetizes&quot; transactions and uses a multi-path transport\\nprotocol to achieve high-throughput routing in PCNs. Packetization allows\\nSpider to complete even large transactions on low-capacity payment channels\\nover time, while the multi-path congestion control protocol ensures balanced\\nutilization of channels and fairness across flows. Through extensive\\nsimulations, we show that Spider requires less than 25% of the funds needed by\\nstate-of-the-art approaches to successfully route over 95% of the transactions\\non balanced traffic demands, and requires only one on-chain transaction for\\nevery 10K transactions routed to achieve full throughput on imbalanced demands.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1809.05817</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1809.05817</id><submitter>Indranil Saha</submitter><version version=\"v1\"><date>Sun, 16 Sep 2018 04:58:47 GMT</date><size>3019kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 3 Mar 2019 15:54:01 GMT</date><size>3074kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 2 Oct 2019 14:18:51 GMT</date><size>412kb</size><source_type>D</source_type></version><title>T* : A Heuristic Search Based Algorithm for Motion Planning with\\n  Temporal Goals</title><authors>Danish Khalidi, Dhaval Gujarathi and Indranil Saha</authors><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motion planning is the core problem to solve for developing any application\\ninvolving an autonomous mobile robot. The fundamental motion planning problem\\ninvolves generating a trajectory for a robot for point-to-point navigation\\nwhile avoiding obstacles. Heuristic-based search algorithms like A* have been\\nshown to be extremely efficient in solving such planning problems. Recently,\\nthere has been an increased interest in specifying complex motion plans using\\ntemporal logic. In the state-of-the-art algorithm, the temporal logic motion\\nplanning problem is reduced to a graph search problem and Dijkstra\\'s shortest\\npath algorithm is used to compute the optimal trajectory satisfying the\\nspecification.\\n  The A* algorithm when used with a proper heuristic for the distance from the\\ndestination can generate an optimal path in a graph efficiently. The primary\\nchallenge for using A* algorithm in temporal logic path planning is that there\\nis no notion of a single destination state for the robot. In this thesis, we\\npresent a novel motion planning algorithm T* that uses the A* search procedure\\nin temporal logic path planning \\\\emph{opportunistically} to generate an optimal\\ntrajectory satisfying a temporal logic query. Our experimental results\\ndemonstrate that T* achieves an order of magnitude improvement over the\\nstate-of-the-art algorithm to solve many temporal logic motion planning\\nproblems.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1809.05978</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1809.05978</id><submitter>Dietmar Berwanger</submitter><version version=\"v1\"><date>Sun, 16 Sep 2018 23:23:32 GMT</date><size>25kb</size></version><version version=\"v2\"><date>Tue, 1 Oct 2019 13:53:36 GMT</date><size>75kb</size></version><title>Observation and Distinction. Representing Information in Infinite Games</title><authors>Dietmar Berwanger and Laurent Doyen</authors><categories>cs.GT cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare two approaches for modelling imperfect information in infinite\\ngames by using finite-state automata. The first, more standard approach views\\ninformation as the result of an observation process driven by a sequential\\nMealy machine. In contrast, the second approach features indistinguishability\\nrelations described by synchronous two-tape automata.\\n  The indistinguishability-relation model turns out to be strictly more\\nexpressive than the one based on observations. We present a characterisation of\\nthe indistinguishability relations that admit a representation as a\\nfinite-state observation function. We show that the characterisation is\\ndecidable, and give a procedure to construct a corresponding Mealy machine\\nwhenever one exists.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1809.09408</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1809.09408</id><submitter>Shengbin Jia</submitter><version version=\"v1\"><date>Tue, 25 Sep 2018 10:56:20 GMT</date><size>168kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 25 May 2019 08:53:05 GMT</date><size>93kb</size><source_type>D</source_type></version><title>Chinese User Service Intention Classification Based on Hybrid Neural\\n  Network</title><authors>Shengbin Jia and Yang Xiang</authors><categories>cs.AI cs.CL</categories><comments>CMVIT2019</comments><doi>10.1088/1742-6596/1229/1/012054</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to satisfy the consumers\\' increasing personalized service demand,\\nthe Intelligent service has arisen. User service intention recognition is an\\nimportant challenge for intelligent service system to provide precise service.\\nIt is difficult for the intelligent system to understand the semantics of user\\ndemand which leads to poor recognition effect, because of the noise in user\\nrequirement descriptions. Therefore, a hybrid neural network classification\\nmodel based on BiLSTM and CNN is proposed to recognize users service\\nintentions. The model can fuse the temporal semantics and spatial semantics of\\nthe user descriptions. The experimental results show that our model achieves a\\nbetter effect compared with other models, reaching 0.94 on the F1 score.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.00959</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.00959</id><submitter>Konstantinos Koufos</submitter><version version=\"v1\"><date>Mon, 1 Oct 2018 20:19:31 GMT</date><size>81kb</size></version><version version=\"v2\"><date>Sat, 5 Oct 2019 22:01:37 GMT</date><size>62kb</size></version><title>Performance of a Link in a Field of Vehicular Interferers with Hardcore\\n  Headway Distance</title><authors>Konstantinos Koufos and Carl P. Dettmann</authors><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of vehicular networks have been largely assessed using the\\nPoisson Point Process (PPP) to model the locations of vehicles along a road.\\nThe PPP is not always a realistic model, because it doesn\\'t account for the\\nsafety distance a driver maintains from the vehicle ahead. In this paper, we\\nmodel the inter-vehicle (or headway) distance equal to the sum of a constant\\nhardcore distance and a random distance following the exponential distribution.\\nUnfortunately, the probability generating functional of this point process is\\nunknown. In order to approximate the Laplace transform of interference at the\\norigin, we devise simple approximations for the variance and skewness of\\ninterference, and we select suitable probability functions to approximate the\\ninterference distribution. In some cases, the PPP (of equal intensity) gives a\\ngood approximation for the outage probability. When the\\ncoefficient-of-variation and the skewness of interference distribution are\\nhigh, the PPP approximation becomes loose in the upper tail. Relevant scenarios\\nare associated with urban microcells and highway macrocells with low intensity\\nof vehicles. The predictions of PPP deteriorate with multi-antenna maximum\\nratio combining receiver and temporal indicators related to the performance of\\nretransmission schemes. Our approximations generate good predictions in all\\nconsidered cases.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.01791</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.01791</id><submitter>Anil Koyuncu</submitter><version version=\"v1\"><date>Wed, 3 Oct 2018 15:21:20 GMT</date><size>926kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 11:44:54 GMT</date><size>6421kb</size><source_type>D</source_type></version><title>FixMiner: Mining Relevant Fix Patterns for Automated Program Repair</title><authors>Anil Koyuncu and Kui Liu and Tegawend\\\\\\'e F. Bissyand\\\\\\'e and Dongsun\\n  Kim and Jacques Klein and Martin Monperrus and Yves Le Traon</authors><categories>cs.SE</categories><comments>31 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Patching is a common activity in software development. It is generally\\nperformed on a source code base to address bugs or add new functionalities. In\\nthis context, given the recurrence of bugs across projects, the associated\\nsimilar patches can be leveraged to extract generic fix actions. While the\\nliterature includes various approaches leveraging similarity among patches to\\nguide program repair, these approaches often do not yield fix patterns that are\\ntractable and reusable as actionable input to APR systems. In this paper, we\\npropose a systematic and automated approach to mining relevant and actionable\\nfix patterns based on an iterative clustering strategy applied to atomic\\nchanges within patches. The goal of FixMiner is thus to infer separate and\\nreusable fix patterns that can be leveraged in other patch generation systems.\\nOur technique, FixMiner, leverages Rich Edit Script which is a specialized tree\\nstructure of the edit scripts that captures the AST-level context of the code\\nchanges. FixMiner uses different tree representations of Rich Edit Scripts for\\neach round of clustering to identify similar changes. These are abstract syntax\\ntrees, edit actions trees, and code context trees. We have evaluated FixMiner\\non thousands of software patches collected from open source projects.\\nPreliminary results show that we are able to mine accurate patterns,\\nefficiently exploiting change information in Rich Edit Scripts. We further\\nintegrated the mined patterns to an automated program repair prototype,\\nPARFixMiner, with which we are able to correctly fix 26 bugs of the Defects4J\\nbenchmark. Beyond this quantitative performance, we show that the mined fix\\npatterns are sufficiently relevant to produce patches with a high probability\\nof correctness: 81% of PARFixMiner\\'s generated plausible patches are correct.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.02695</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.02695</id><submitter>Xinjing Cheng</submitter><version version=\"v1\"><date>Thu, 4 Oct 2018 13:32:29 GMT</date><size>8433kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 13 Oct 2018 08:03:48 GMT</date><size>8480kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 4 Oct 2019 03:29:01 GMT</date><size>8283kb</size><source_type>D</source_type></version><title>Learning Depth with Convolutional Spatial Propagation Network</title><authors>Xinjing Cheng, Peng Wang and Ruigang Yang</authors><categories>cs.CV</categories><comments>v1.2: add some exps v1.1: fixed some mistakes, v1: 17 pages, 12\\n  figures. arXiv admin note: substantial text overlap with arXiv:1808.00150</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Depth prediction is one of the fundamental problems in computer vision. In\\nthis paper, we propose a simple yet effective convolutional spatial propagation\\nnetwork (CSPN) to learn the affinity matrix for various depth estimation tasks.\\nSpecifically, it is an efficient linear propagation model, in which the\\npropagation is performed with a manner of recurrent convolutional operation,\\nand the affinity among neighboring pixels is learned through a deep\\nconvolutional neural network (CNN). We can append this module to any output\\nfrom a state-of-the-art (SOTA) depth estimation networks to improve their\\nperformances. In practice, we further extend CSPN in two aspects: 1) take\\nsparse depth map as additional input, which is useful for the task of depth\\ncompletion; 2) similar to commonly used 3D convolution operation in CNNs, we\\npropose 3D CSPN to handle features with one additional dimension, which is\\neffective in the task of stereo matching using 3D cost volume. For the tasks of\\nsparse to dense, a.k.a depth completion. We experimented the proposed CPSN\\nconjunct algorithms over the popular NYU v2 and KITTI datasets, where we show\\nthat our proposed algorithms not only produce high quality (e.g., 30% more\\nreduction in depth error), but also run faster (e.g., 2 to 5x faster) than\\nprevious SOTA spatial propagation network. We also evaluated our stereo\\nmatching algorithm on the Scene Flow and KITTI Stereo datasets, and rank 1st on\\nboth the KITTI Stereo 2012 and 2015 benchmarks, which demonstrates the\\neffectiveness of the proposed module. The code of CSPN proposed in this work\\nwill be released at https://github.com/XinJCheng/CSPN.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.02854</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.02854</id><submitter>Andr\\\\\\'es Ortiz-Mu\\\\~noz</submitter><version version=\"v1\"><date>Sat, 6 Oct 2018 02:52:11 GMT</date><size>416kb</size><source_type>D</source_type></version><title>Stochastic Chemical Reaction Networks for Robustly Approximating\\n  Arbitrary Probability Distributions</title><authors>Daniele Cappelletti, Andr\\\\\\'es Ortiz-Mu\\\\~noz, David Anderson, Erik\\n  Winfree</authors><categories>cs.CC math.PR q-bio.MN</categories><msc-class>68Q19, 60J27, 60J28, 60G10, 37N25, 92C42</msc-class><journal-ref>Theoretical Computer Science, 2019</journal-ref><doi>10.1016/j.tcs.2019.08.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that discrete distributions on the $d$-dimensional non-negative\\ninteger lattice can be approximated arbitrarily well via the marginals of\\nstationary distributions for various classes of stochastic chemical reaction\\nnetworks. We begin by providing a class of detailed balanced networks and prove\\nthat they can approximate any discrete distribution to any desired accuracy.\\nHowever, these detailed balanced constructions rely on the ability to\\ninitialize a system precisely, and are therefore susceptible to perturbations\\nin the initial conditions. We therefore provide another construction based on\\nthe ability to approximate point mass distributions and prove that this\\nconstruction is capable of approximating arbitrary discrete distributions for\\nany choice of initial condition. In particular, the developed models are\\nergodic, so their limit distributions are robust to a finite number of\\nperturbations over time in the counts of molecules.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.02889</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.02889</id><submitter>Anirban Laha</submitter><version version=\"v1\"><date>Fri, 5 Oct 2018 21:07:11 GMT</date><size>282kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 27 Sep 2019 14:00:08 GMT</date><size>755kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 4 Oct 2019 21:34:15 GMT</date><size>769kb</size><source_type>D</source_type></version><title>Scalable Micro-planned Generation of Discourse from Structured Data</title><authors>Anirban Laha and Parag Jain and Abhijit Mishra and Karthik\\n  Sankaranarayanan</authors><categories>cs.CL</categories><comments>Accepted for Computational Linguistics journal on 17 Sep 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework for generating natural language description from\\nstructured data such as tables; the problem comes under the category of\\ndata-to-text natural language generation (NLG). Modern data-to-text NLG systems\\ntypically employ end-to-end statistical and neural architectures that learn\\nfrom a limited amount of task-specific labeled data, and therefore, exhibit\\nlimited scalability, domain-adaptability, and interpretability. Unlike these\\nsystems, ours is a modular, pipeline-based approach, and does not require\\ntask-specific parallel data. It rather relies on monolingual corpora and basic\\noff-the-shelf NLP tools. This makes our system more scalable and easily\\nadaptable to newer domains.\\n  Our system employs a 3-staged pipeline that: (i) converts entries in the\\nstructured data to canonical form, (ii) generates simple sentences for each\\natomic entry in the canonicalized representation, and (iii) combines the\\nsentences to produce a coherent, fluent and adequate paragraph description\\nthrough sentence compounding and co-reference replacement modules. Experiments\\non a benchmark mixed-domain dataset curated for paragraph description from\\ntables reveals the superiority of our system over existing data-to-text\\napproaches. We also demonstrate the robustness of our system in accepting other\\npopular datasets covering diverse data types such as Knowledge Graphs and\\nKey-Value maps.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.02963</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.02963</id><submitter>Rogers Mathew</submitter><version version=\"v1\"><date>Sat, 6 Oct 2018 09:02:12 GMT</date><size>5kb</size></version><version version=\"v2\"><date>Tue, 1 Oct 2019 07:22:23 GMT</date><size>15kb</size></version><title>Local Boxicity, Local Dimension, and Maximum Degree</title><authors>Atrayee Majumder, Rogers Mathew</authors><categories>math.CO cs.DM</categories><comments>11 pages</comments><msc-class>05C62</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we focus on two recently introduced parameters in the\\nliterature, namely `local boxicity\\' (a parameter on graphs) and `local\\ndimension\\' (a parameter on partially ordered sets). We give an `almost linear\\'\\nupper bound for both the parameters in terms of the maximum degree of a graph\\n(for local dimension we consider the comparability graph of a poset). Further,\\nwe give an $O(n\\\\Delta^2)$ time deterministic algorithm to compute a local box\\nrepresentation of dimension at most $3\\\\Delta$ for a claw-free graph, where $n$\\nand $\\\\Delta$ denote the number of vertices and the maximum degree,\\nrespectively, of the graph under consideration. We also prove two other upper\\nbounds for the local boxicity of a graph, one in terms of the number of\\nvertices and the other in terms of the number of edges. Finally, we show that\\nthe local boxicity of a graph is upper bounded by its `product dimension\\'.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.03657</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.03657</id><submitter>Alexander Hvatov</submitter><version version=\"v1\"><date>Mon, 8 Oct 2018 18:59:41 GMT</date><size>6638kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 5 Feb 2019 15:05:54 GMT</date><size>6886kb</size><source_type>D</source_type></version><title>Adaptation of NEMO-LIM3 model for multigrid high resolution Arctic\\n  simulation</title><authors>Alexander Hvatov and Nikolay O. Nikitin and Anna V. Kalyuzhnaya and\\n  Sergey S. Kosukhin</authors><categories>physics.ao-ph cs.SE</categories><doi>10.1016/j.ocemod.2019.101427</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-resolution regional hindcasting of ocean and sea ice plays an important\\nrole in the assessment of shipping and operational risks in the Arctic Ocean.\\nThe ice-ocean model NEMO-LIM3 was modified to improve its simulation quality\\nfor appropriate spatio-temporal resolutions. A multigrid model setup with\\nconnected coarse- (14 km) and fine-resolution (5 km) model configurations was\\ndevised. These two configurations were implemented and run separately. The\\nresulting computational cost was lower when compared to that of the built-in\\nAGRIF nesting system. Ice and tracer boundary-condition schemes were modified\\nto achieve the correct interaction between coarse- and fine grids through a\\nlong ice-covered open boundary. An ice-restoring scheme was implemented to\\nreduce spin-up time. The NEMO-LIM3 configuration described in this article\\nprovides more flexible and customisable tools for high-resolution regional\\nArctic simulations.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.04481</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.04481</id><submitter>Ireneusz Szcze\\\\\\'sniak</submitter><version version=\"v1\"><date>Wed, 10 Oct 2018 12:29:37 GMT</date><size>37kb</size></version><version version=\"v2\"><date>Thu, 21 Mar 2019 08:24:11 GMT</date><size>37kb</size></version><title>Generic Dijkstra for Optical Networks</title><authors>Ireneusz Szcze\\\\\\'sniak, Andrzej Jajszczyk, Bo\\\\.zena\\n  Wo\\\\\\'zna-Szcze\\\\\\'sniak</authors><categories>cs.NI</categories><journal-ref>Journal of Optical Communications and Networking, vol. 11, no. 11,\\n  pp. 568-577, November 2019</journal-ref><doi>10.1364/JOCN.11.000568</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the generic Dijkstra shortest path algorithm: an efficient\\nalgorithm for finding a shortest path in an optical network, both in a\\nwavelength-division multiplexed network, and an elastic optical network. Our\\nalgorithm is an enabler of the real-time softwarized control of large-scale\\nnetworks, and not only optical, we believe. The Dijkstra algorithm is a\\ngeneralization of the breadth-first search, and we generalize the Dijkstra\\nalgorithm further to resolve the continuity and contiguity constraints of the\\nfrequency slot units. Specifically, we generalize the notion of a label, change\\nwhat we iterate with, and reformulate the edge relaxation so that vertices are\\nrevisited, loops avoided, and worse labels discarded. We also used the typical\\nconstriction during edge relaxation to take care of the signal modulation\\nconstraints. The algorithm can be used with various spectrum allocation\\npolicies. We motivate and discuss the algorithm design, and provide our libre,\\nhigh-quality, and generic implementation using the Boost Graph Library. We\\ncarried out 85000 simulation runs for realistic and random networks (Gabriel\\ngraphs) of 75 vertices with about a billion shortest-path searches, and found\\nthat the proposed algorithm outperforms considerably other three competing\\noptimal algorithms, which are frequently used in research.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.04831</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.04831</id><submitter>Seyedakbar Mostafavi</submitter><version version=\"v1\"><date>Thu, 11 Oct 2018 03:26:15 GMT</date><size>1132kb</size></version><version version=\"v2\"><date>Fri, 9 Nov 2018 11:40:46 GMT</date><size>1135kb</size></version><version version=\"v3\"><date>Fri, 4 Oct 2019 08:19:39 GMT</date><size>888kb</size><source_type>D</source_type></version><title>A new rank-order clustering algorithm for prolonging the lifetime of\\n  wireless sensor networks</title><authors>Seyedakbar Mostafavi, Vesal Hakami</authors><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy efficient resource management is critical for prolonging the lifetime\\nof wireless sensor networks (WSN). Clustering of sensor nodes with the aim of\\ndistributing the traffic loads in the network is a proven approach for balanced\\nenergy consumption in WSN. The main body of literature in this topic can be\\nclassified as hierarchical and distance-based clustering techniques in which\\nmulti-hop, multi-level forwarding and distance-based criteria, respectively,\\nare utilized for categorization of sensor nodes. In this study, we propose the\\nApproximate Rank-Order Wireless Sensor Networks (ARO-WSN) clustering algorithm\\nas a combined hierarchical and distance-based clustering approach. ARO-WSN\\nalgorithm which has been extensively used in the field of image processing,\\nruns in the order of O(n) for a large data set, therefore it can be applied on\\nWSN. The results shows that ARO-WSN outperforms the classical LEACH, LEACH-C\\nand K-means clustering algorithms in the terms of energy consumption and\\nnetwork lifetime.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.05440</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.05440</id><submitter>Manolis Tsakiris</submitter><version version=\"v1\"><date>Fri, 12 Oct 2018 10:22:05 GMT</date><size>171kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 12:16:11 GMT</date><size>628kb</size></version><title>An algebraic-geometric approach for linear regression without\\n  correspondences</title><authors>Manolis C. Tsakiris, Liangzu Peng, Aldo Conca, Laurent Kneip, Yuanming\\n  Shi, Hayoung Choi</authors><categories>cs.LG stat.ML</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear regression without correspondences is the problem of performing a\\nlinear regression fit to a dataset for which the correspondences between the\\nindependent samples and the observations are unknown. Such a problem naturally\\narises in diverse domains such as computer vision, data mining, communications\\nand biology. In its simplest form, it is tantamount to solving a linear system\\nof equations, for which the entries of the right hand side vector have been\\npermuted. This type of data corruption renders the linear regression task\\nconsiderably harder, even in the absence of other corruptions, such as noise,\\noutliers or missing entries. Existing methods are either applicable only to\\nnoiseless data or they are very sensitive to initialization or they work only\\nfor partially shuffled data. In this paper we address these issues via an\\nalgebraic geometric approach, which uses symmetric polynomials to extract\\npermutation-invariant constraints that the parameters $\\\\xi^* \\\\in \\\\Re^n$ of the\\nlinear regression model must satisfy. This naturally leads to a polynomial\\nsystem of $n$ equations in $n$ unknowns, which contains $\\\\xi^*$ in its root\\nlocus. Using the machinery of algebraic geometry we prove that as long as the\\nindependent samples are generic, this polynomial system is always consistent\\nwith at most $n!$ complex roots, regardless of any type of corruption inflicted\\non the observations. The algorithmic implication of this fact is that one can\\nalways solve this polynomial system and use its most suitable root as\\ninitialization to the Expectation Maximization algorithm. To the best of our\\nknowledge, the resulting method is the first working solution for small values\\nof $n$ able to handle thousands of fully shuffled noisy observations in\\nmilliseconds.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.05858</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.05858</id><submitter>Sergey Ketkov</submitter><version version=\"v1\"><date>Sat, 13 Oct 2018 13:19:57 GMT</date><size>38kb</size></version><version version=\"v2\"><date>Thu, 31 Jan 2019 07:28:09 GMT</date><size>36kb</size></version><version version=\"v3\"><date>Wed, 2 Oct 2019 12:04:11 GMT</date><size>40kb</size></version><title>On Greedy and Strategic Evaders in Sequential Interdiction Settings with\\n  Incomplete Information</title><authors>Sergey S. Ketkov, Oleg A. Prokopyev</authors><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a class of sequential network interdiction problem settings where\\nthe interdictor has incomplete initial information about the network while the\\nevader has complete knowledge of the network including its structure and arc\\ncosts. In each decision epoch, the interdictor can block (for the duration of\\nthe epoch) at most $k$ arcs known to him/her. By observing the evader\\'s\\nactions, the interdictor learns about the network structure and costs and thus,\\ncan adjust his/her actions in subsequent decision epochs. It is known from the\\nliterature that if the evader is greedy (i.e., the shortest available path is\\nused in each decision epoch), then under some assumptions the greedy\\ninterdiction policies that block $k$-most vital arcs in each epoch are\\nefficient and have a finite regret. In this paper, we consider the evader\\'s\\nperspective and explore deterministic &quot;strategic&quot; evasion policies under the\\nassumption that the interdictor is greedy. We first study the theoretical\\ncomputational complexity of the evader\\'s problem. Then we derive basic\\nconstructive properties of optimal evasion policies for two decision epochs\\nwhen the interdictor has no initial information about the network structure.\\nThese properties are then exploited for the design of a heuristic algorithm for\\na strategic evader in a general setting with an arbitrary time horizon and any\\ninitial information available to the interdictor. Our computational experiments\\ndemonstrate that the proposed heuristic outperforms the greedy evasion policy\\non several classes of synthetic network instances under either perfect or noisy\\ninformation feedback. Finally, some interesting insights from our theoretical\\nand computational results conclude the paper.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.06089</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.06089</id><submitter>Edgar Dobriban</submitter><version version=\"v1\"><date>Sun, 14 Oct 2018 19:48:05 GMT</date><size>3629kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 6 Oct 2019 19:25:12 GMT</date><size>1515kb</size><source_type>D</source_type></version><title>Asymptotics for Sketching in Least Squares Regression</title><authors>Edgar Dobriban, Sifan Liu</authors><categories>math.ST cs.LG cs.NA math.NA stat.ME stat.ML stat.TH</categories><journal-ref>Updated manuscript to be consistent with version at NeurIPS 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a least squares regression problem where the data has been\\ngenerated from a linear model, and we are interested to learn the unknown\\nregression parameters. We consider &quot;sketch-and-solve&quot; methods that randomly\\nproject the data first, and do regression after. Previous works have analyzed\\nthe statistical and computational performance of such methods. However, the\\nexisting analysis is not fine-grained enough to show the fundamental\\ndifferences between various methods, such as the Subsampled Randomized Hadamard\\nTransform (SRHT) and Gaussian projections. In this paper, we make progress on\\nthis problem, working in an asymptotic framework where the number of datapoints\\nand dimension of features goes to infinity. We find the limits of the accuracy\\nloss (for estimation and test error) incurred by popular sketching methods. We\\nshow separation between different methods, so that SRHT is better than Gaussian\\nprojections. Our theoretical results are verified on both real and synthetic\\ndata. The analysis of SRHT relies on novel methods from random matrix theory\\nthat may be of independent interest.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.06498</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.06498</id><submitter>Yuankai Huo</submitter><version version=\"v1\"><date>Mon, 15 Oct 2018 16:23:08 GMT</date><size>3131kb</size></version><version version=\"v2\"><date>Fri, 27 Sep 2019 18:45:26 GMT</date><size>3131kb</size></version><title>SynSeg-Net: Synthetic Segmentation Without Target Modality Ground Truth</title><authors>Yuankai Huo, Zhoubing Xu, Hyeonsoo Moon, Shunxing Bao, Albert Assad,\\n  Tamara K. Moyo, Michael R. Savona, Richard G. Abramson, Bennett A. Landman</authors><categories>cs.CV</categories><comments>IEEE Transactions on Medical Imaging (TMI)</comments><journal-ref>&quot;Synseg-net: Synthetic segmentation without target modality ground\\n  truth.&quot; IEEE transactions on medical imaging 38, no. 4 (2018): 1016-1025</journal-ref><doi>10.1109/TMI.2018.2876633</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key limitation of deep convolutional neural networks (DCNN) based image\\nsegmentation methods is the lack of generalizability. Manually traced training\\nimages are typically required when segmenting organs in a new imaging modality\\nor from distinct disease cohort. The manual efforts can be alleviated if the\\nmanually traced images in one imaging modality (e.g., MRI) are able to train a\\nsegmentation network for another imaging modality (e.g., CT). In this paper, we\\npropose an end-to-end synthetic segmentation network (SynSeg-Net) to train a\\nsegmentation network for a target imaging modality without having manual\\nlabels. SynSeg-Net is trained by using (1) unpaired intensity images from\\nsource and target modalities, and (2) manual labels only from source modality.\\nSynSeg-Net is enabled by the recent advances of cycle generative adversarial\\nnetworks (CycleGAN) and DCNN. We evaluate the performance of the SynSeg-Net on\\ntwo experiments: (1) MRI to CT splenomegaly synthetic segmentation for\\nabdominal images, and (2) CT to MRI total intracranial volume synthetic\\nsegmentation (TICV) for brain images. The proposed end-to-end approach achieved\\nsuperior performance to two stage methods. Moreover, the SynSeg-Net achieved\\ncomparable performance to the traditional segmentation network using target\\nmodality labels in certain scenarios. The source code of SynSeg-Net is publicly\\navailable (https://github.com/MASILab/SynSeg-Net).\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.06544</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.06544</id><submitter>Nicholas Rhinehart</submitter><version version=\"v1\"><date>Mon, 15 Oct 2018 17:51:03 GMT</date><size>7195kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 31 Jan 2019 20:07:49 GMT</date><size>6589kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 4 Jun 2019 19:48:56 GMT</date><size>7944kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Tue, 1 Oct 2019 00:13:58 GMT</date><size>8847kb</size><source_type>D</source_type></version><title>Deep Imitative Models for Flexible Inference, Planning, and Control</title><authors>Nicholas Rhinehart, Rowan McAllister, Sergey Levine</authors><categories>cs.LG cs.AI cs.CV cs.RO stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Imitation Learning (IL) is an appealing approach to learn desirable\\nautonomous behavior. However, directing IL to achieve arbitrary goals is\\ndifficult. In contrast, planning-based algorithms use dynamics models and\\nreward functions to achieve goals. Yet, reward functions that evoke desirable\\nbehavior are often difficult to specify. In this paper, we propose Imitative\\nModels to combine the benefits of IL and goal-directed planning. Imitative\\nModels are probabilistic predictive models of desirable behavior able to plan\\ninterpretable expert-like trajectories to achieve specified goals. We derive\\nfamilies of flexible goal objectives, including constrained goal regions,\\nunconstrained goal sets, and energy-based goals. We show that our method can\\nuse these objectives to successfully direct behavior. Our method substantially\\noutperforms six IL approaches and a planning-based approach in a dynamic\\nsimulated autonomous driving task, and is efficiently learned from expert\\ndemonstrations without online data collection. We also show our approach is\\nrobust to poorly specified goals, such as goals on the wrong side of the road.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.07218</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.07218</id><submitter>Mengye Ren</submitter><version version=\"v1\"><date>Tue, 16 Oct 2018 18:25:17 GMT</date><size>85kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 5 May 2019 15:35:29 GMT</date><size>3044kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sun, 6 Oct 2019 21:08:47 GMT</date><size>1122kb</size><source_type>D</source_type></version><title>Incremental Few-Shot Learning with Attention Attractor Networks</title><authors>Mengye Ren, Renjie Liao, Ethan Fetaya, Richard S. Zemel</authors><categories>cs.LG cs.CV stat.ML</categories><comments>NeurIPS 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning classifiers are often trained to recognize a set of\\npre-defined classes. However, in many applications, it is often desirable to\\nhave the flexibility of learning additional concepts, with limited data and\\nwithout re-training on the full training set. This paper addresses this\\nproblem, incremental few-shot learning, where a regular classification network\\nhas already been trained to recognize a set of base classes, and several extra\\nnovel classes are being considered, each with only a few labeled examples.\\nAfter learning the novel classes, the model is then evaluated on the overall\\nclassification performance on both base and novel classes. To this end, we\\npropose a meta-learning model, the Attention Attractor Network, which\\nregularizes the learning of novel classes. In each episode, we train a set of\\nnew weights to recognize novel classes until they converge, and we show that\\nthe technique of recurrent back-propagation can back-propagate through the\\noptimization process and facilitate the learning of these parameters. We\\ndemonstrate that the learned attractor network can help recognize novel classes\\nwhile remembering old classes without the need to review the original training\\nset, outperforming various baselines.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.07348</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.07348</id><submitter>Andri Ashfahani</submitter><version version=\"v1\"><date>Wed, 17 Oct 2018 01:40:45 GMT</date><size>252kb</size></version><version version=\"v2\"><date>Wed, 29 May 2019 18:48:02 GMT</date><size>240kb</size></version><version version=\"v3\"><date>Tue, 8 Oct 2019 15:02:06 GMT</date><size>132kb</size><source_type>D</source_type></version><title>Autonomous Deep Learning: Continual Learning Approach for Dynamic\\n  Environments</title><authors>Andri Ashfahani and Mahardhika Pratama</authors><categories>cs.LG stat.ML</categories><journal-ref>This paper has been published in Proceedings of the 2019 SIAM\\n  International Conference on Data Mining</journal-ref><doi>10.1137/1.9781611975673.75</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The feasibility of deep neural networks (DNNs) to address data stream\\nproblems still requires intensive study because of the static and offline\\nnature of conventional deep learning approaches. A deep continual learning\\nalgorithm, namely autonomous deep learning (ADL), is proposed in this paper.\\nUnlike traditional deep learning methods, ADL features a flexible structure\\nwhere its network structure can be constructed from scratch with the absence of\\nan initial network structure via the self-constructing network structure. ADL\\nspecifically addresses catastrophic forgetting by having a different-depth\\nstructure which is capable of achieving a trade-off between plasticity and\\nstability. Network significance (NS) formula is proposed to drive the hidden\\nnodes growing and pruning mechanism. Drift detection scenario (DDS) is put\\nforward to signal distributional changes in data streams which induce the\\ncreation of a new hidden layer. The maximum information compression index\\n(MICI) method plays an important role as a complexity reduction module\\neliminating redundant layers. The efficacy of ADL is numerically validated\\nunder the prequential test-then-train procedure in lifelong environments using\\nnine popular data stream problems. The numerical results demonstrate that ADL\\nconsistently outperforms recent continual learning methods while characterizing\\nthe automatic construction of network structures.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.08004</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.08004</id><submitter>Hossein Vahidi</submitter><version version=\"v1\"><date>Thu, 18 Oct 2018 11:59:02 GMT</date><size>376kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 9 Mar 2019 12:47:48 GMT</date><size>387kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 4 Oct 2019 20:40:36 GMT</date><size>542kb</size><source_type>D</source_type></version><title>Complexity Landscape of Computing the Anti-Ramsey Numbers</title><authors>Saeed Akhoondian Amiri, Alexandru Popa, Mohammad Roghani, Golnoosh\\n  Shahkarami, Reza Soltani, Hossein Vahidi</authors><categories>cs.CC cs.DS</categories><acm-class>F.2.2; G.2.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The anti-Ramsey numbers are a fundamental notion in graph theory, introduced\\nin 1978, by Erd\\\\&quot; os, Simonovits and S\\\\\\' os. For given graphs $G$ and $H$ the\\nanti-Ramsey number $\\\\textrm{ar}(G,H)$ is defined to be the maximum number $k$\\nsuch that there exists an assignment of $k$ colors to the edges of $G$ in which\\nevery copy of $H$ in $G$ has at least two edges with the same color.\\n  Usually, combinatorists study extremal values of anti-Ramsey numbers for\\nvarious classes of graphs. There are works on computational complexity of the\\nproblem when $H$ is a star. Along this line of research, we study the\\ncomplexity of computing the anti-Ramsey number $\\\\textrm{ar}(G,P_k)$, where\\n$P_k$ is a path of length $k$. First we observe that when $k$ is close to $n$\\nthe problem is hard; hence, the challenging part is the computational\\ncomplexity of the problem when $k$ is a fixed constant.\\n  We provide a deep characterization of the problem for paths of constant\\nlength. Our first main contribution is to prove that computing\\n$\\\\textrm{ar}(G,P_k)$ for every integer $k&gt;2$ is NP-hard. We obtain this by\\nproviding several structural properties of such coloring in graphs. We\\ninvestigate further and show that even approximating $\\\\textrm{ar}(G,P_3)$ to a\\nfactor of $n^{-1/2 - \\\\epsilon}$ is hard already in $3$-partite graphs, unless\\n$NP{}={}ZPP$.\\n  Given the hardness of approximation and parametrization of the problem, it is\\nnatural to study the problem on restricted graph families. Along this line, we\\nfirst introduce the notion of color connected coloring, and, employing this\\nstructural property, we obtain a linear time algorithm to compute\\n$\\\\textrm{ar}(G,P_k)$, for every integer $k$, when the host graph, $G$, is a\\ntree. We have introduced several techniques in our algorithm that we believe\\nmight be helpful in providing approximation algorithms for other restricted\\nfamilies of graphs.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.08092</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.08092</id><submitter>Vivek Bagaria</submitter><version version=\"v1\"><date>Thu, 18 Oct 2018 14:55:40 GMT</date><size>7710kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 8 Nov 2018 17:40:56 GMT</date><size>3893kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 30 Sep 2019 19:40:17 GMT</date><size>8213kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Wed, 2 Oct 2019 00:47:25 GMT</date><size>4682kb</size><source_type>D</source_type></version><title>Deconstructing the Blockchain to Approach Physical Limits</title><authors>Vivek Bagaria, Sreeram Kannan, David Tse, Giulia Fanti, Pramod\\n  Viswanath</authors><categories>cs.CR cs.DC cs.IT math.IT</categories><comments>Computer and Communications Security, 2019</comments><journal-ref>Computer and Communications Security, 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transaction throughput, confirmation latency and confirmation reliability are\\nfundamental performance measures of any blockchain system in addition to its\\nsecurity. In a decentralized setting, these measures are limited by two\\nunderlying physical network attributes: communication capacity and\\nspeed-of-light propagation delay. Existing systems operate far away from these\\nphysical limits. In this work we introduce Prism, a new proof-of-work\\nblockchain protocol, which can achieve 1) security against up to 50%\\nadversarial hashing power; 2) optimal throughput up to the capacity C of the\\nnetwork; 3) confirmation latency for honest transactions proportional to the\\npropagation delay D, with confirmation error probability exponentially small in\\nCD ; 4) eventual total ordering of all transactions. Our approach to the design\\nof this protocol is based on deconstructing the blockchain into its basic\\nfunctionalities and systematically scaling up these functionalities to approach\\ntheir physical limits.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.08581</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.08581</id><submitter>Elvin Isufi</submitter><version version=\"v1\"><date>Fri, 19 Oct 2018 16:51:11 GMT</date><size>3986kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 10 Jul 2019 15:31:13 GMT</date><size>10665kb</size><source_type>D</source_type></version><title>Forecasting Time Series with VARMA Recursions on Graphs</title><authors>Elvin Isufi, Andreas Loukas, Nathanael Perraudin and Geert Leus</authors><categories>eess.SP cs.SY econ.EM</categories><comments>submitted to the IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2019.2929930</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph-based techniques emerged as a choice to deal with the dimensionality\\nissues in modeling multivariate time series. However, there is yet no complete\\nunderstanding of how the underlying structure could be exploited to ease this\\ntask. This work provides contributions in this direction by considering the\\nforecasting of a process evolving over a graph. We make use of the\\n(approximate) time-vertex stationarity assumption, i.e., timevarying graph\\nsignals whose first and second order statistical moments are invariant over\\ntime and correlated to a known graph topology. The latter is combined with VAR\\nand VARMA models to tackle the dimensionality issues present in predicting the\\ntemporal evolution of multivariate time series. We find out that by projecting\\nthe data to the graph spectral domain: (i) the multivariate model estimation\\nreduces to that of fitting a number of uncorrelated univariate ARMA models and\\n(ii) an optimal low-rank data representation can be exploited so as to further\\nreduce the estimation costs. In the case that the multivariate process can be\\nobserved at a subset of nodes, the proposed models extend naturally to Kalman\\nfiltering on graphs allowing for optimal tracking. Numerical experiments with\\nboth synthetic and real data validate the proposed approach and highlight its\\nbenefits over state-of-the-art alternatives.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.08869</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.08869</id><submitter>Ryan Kim</submitter><version version=\"v1\"><date>Sat, 20 Oct 2018 23:46:14 GMT</date><size>2051kb</size></version><version version=\"v2\"><date>Sat, 5 Oct 2019 15:06:54 GMT</date><size>2579kb</size></version><title>Learning-based Application-Agnostic 3D NoC Design for Heterogeneous\\n  Manycore Systems</title><authors>Biresh Kumar Joardar, Ryan Gary Kim, Janardhan Rao Doppa, Partha\\n  Pratim Pande, Diana Marculescu, and Radu Marculescu</authors><categories>cs.DC cs.LG stat.ML</categories><comments>Published in IEEE Transactions on Computers</comments><journal-ref>IEEE Transactions on Computers, vol. 68, no. 6, June 2019</journal-ref><doi>10.1109/TC.2018.2889053</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rising use of deep learning and other big-data algorithms has led to an\\nincreasing demand for hardware platforms that are computationally powerful, yet\\nenergy-efficient. Due to the amount of data parallelism in these algorithms,\\nhigh-performance 3D manycore platforms that incorporate both CPUs and GPUs\\npresent a promising direction. However, as systems use heterogeneity (e.g., a\\ncombination of CPUs, GPUs, and accelerators) to improve performance and\\nefficiency, it becomes more pertinent to address the distinct and likely\\nconflicting communication requirements (e.g., CPU memory access latency or GPU\\nnetwork throughput) that arise from such heterogeneity. Unfortunately, it is\\ndifficult to quickly explore the hardware design space and choose appropriate\\ntradeoffs between these heterogeneous requirements. To address these\\nchallenges, we propose the design of a 3D Network-on-Chip (NoC) for\\nheterogeneous manycore platforms that considers the appropriate design\\nobjectives for a 3D heterogeneous system and explores various tradeoffs using\\nan efficient ML-based multi-objective optimization technique. The proposed\\ndesign space exploration considers the various requirements of its\\nheterogeneous components and generates a set of 3D NoC architectures that\\nefficiently trades off these design objectives. Our findings show that by\\njointly considering these requirements (latency, throughput, temperature, and\\nenergy), we can achieve 9.6% better Energy-Delay Product on average at nearly\\niso-temperature conditions when compared to a thermally-optimized design for 3D\\nheterogeneous NoCs. More importantly, our results suggest that our 3D NoCs\\noptimized for a few applications can be generalized for unknown applications as\\nwell. Our results show that these generalized 3D NoCs only incur a 1.8%\\n(36-tile system) and 1.1% (64-tile system) average performance loss compared to\\napplication-specific NoCs.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.09425</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.09425</id><submitter>Jakub Mare\\\\v{c}ek</submitter><version version=\"v1\"><date>Mon, 22 Oct 2018 17:46:23 GMT</date><size>434kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 12:47:19 GMT</date><size>1558kb</size><source_type>D</source_type></version><title>Using Deep Learning to Extend the Range of Air-Pollution Monitoring and\\n  Forecasting</title><authors>Philipp Haehnel, Jakub Marecek, Julien Monteil, and Fearghal O\\'Donncha</authors><categories>cs.LG stat.ML</categories><comments>12 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Across numerous applications, forecasting relies on numerical solvers for\\npartial differential equations (PDEs). Although the use of deep-learning\\ntechniques has been proposed, actual applications have been restricted by the\\nfact the training data are obtained using traditional PDE solvers. Thereby, the\\nuses of deep-learning techniques were limited to domains, where the PDE solver\\nwas applicable.\\n  We demonstrate a deep-learning framework for air-pollution monitoring and\\nforecasting that provides the ability to train across different model domains,\\nas well as a reduction in the run-time by two orders of magnitude. It presents\\na first-of-a-kind implementation that combines deep-learning and\\ndomain-decomposition techniques to allow model deployments extend beyond the\\ndomain(s) on which the model has been trained.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.09506</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.09506</id><submitter>Ari Klein</submitter><version version=\"v1\"><date>Mon, 22 Oct 2018 19:00:46 GMT</date><size>588kb</size></version><title>Automatically Detecting Self-Reported Birth Defect Outcomes on Twitter\\n  for Large-scale Epidemiological Research</title><authors>Ari Z. Klein, Abeed Sarker, Davy Weissenbacher, Graciela\\n  Gonzalez-Hernandez</authors><categories>cs.CL</categories><journal-ref>npj Digital Medicine. 2019;2:96</journal-ref><doi>10.1038/s41746-019-0170-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent work, we identified and studied a small cohort of Twitter users\\nwhose pregnancies with birth defect outcomes could be observed via their\\npublicly available tweets. Exploiting social media\\'s large-scale potential to\\ncomplement the limited methods for studying birth defects, the leading cause of\\ninfant mortality, depends on the further development of automatic methods. The\\nprimary objective of this study was to take the first step towards scaling the\\nuse of social media for observing pregnancies with birth defect outcomes,\\nnamely, developing methods for automatically detecting tweets by users\\nreporting their birth defect outcomes. We annotated and pre-processed\\napproximately 23,000 tweets that mention birth defects in order to train and\\nevaluate supervised machine learning algorithms, including feature-engineered\\nand deep learning-based classifiers. We also experimented with various\\nunder-sampling and over-sampling approaches to address the class imbalance. A\\nSupport Vector Machine (SVM) classifier trained on the original, imbalanced\\ndata set, with n-grams, word clusters, and structural features, achieved the\\nbest baseline performance for the positive classes: an F1-score of 0.65 for the\\n&quot;defect&quot; class and 0.51 for the &quot;possible defect&quot; class. Our contributions\\ninclude (i) natural language processing (NLP) and supervised machine learning\\nmethods for automatically detecting tweets by users reporting their birth\\ndefect outcomes, (ii) a comparison of feature-engineered and deep\\nlearning-based classifiers trained on imbalanced, under-sampled, and\\nover-sampled data, and (iii) an error analysis that could inform classification\\nimprovements using our publicly available corpus. Future work will focus on\\nautomating user-level analyses for cohort inclusion.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.10257</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.10257</id><submitter>Tomer Libal</submitter><version version=\"v1\"><date>Wed, 24 Oct 2018 09:07:54 GMT</date><size>53kb</size></version><title>A general proof certification framework for modal logic</title><authors>Tomer Libal and Marco Volpe</authors><categories>cs.LO</categories><acm-class>F.4.1; K.7.3</acm-class><journal-ref>Math. Struct. Comp. Sci. 29 (2019) 1344-1378</journal-ref><doi>10.1017/S0960129518000440</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main issues in proof certification is that different theorem\\nprovers, even when designed for the same logic, tend to use different proof\\nformalisms and produce outputs in different formats. The project ProofCert\\npromotes the usage of a common specification language and of a small and\\ntrusted kernel in order to check proofs coming from different sources and for\\ndifferent logics. By relying on that idea and by using a classical focused\\nsequent calculus as a kernel, we propose here a general framework for checking\\nmodal proofs. We present the implementation of the framework in a Prolog-like\\nlanguage and show how it is possible to specialize it in a simple and modular\\nway in order to cover different proof formalisms, such as labeled systems,\\ntableaux, sequent calculi and nested sequent calculi. We illustrate the method\\nfor the logic K by providing several examples and discuss how to further extend\\nthe approach.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.10730</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.10730</id><submitter>Sai Mang Pun</submitter><version version=\"v1\"><date>Thu, 25 Oct 2018 06:17:05 GMT</date><size>26kb</size></version><version version=\"v2\"><date>Fri, 16 Nov 2018 15:55:00 GMT</date><size>209kb</size></version><version version=\"v3\"><date>Wed, 29 May 2019 02:34:27 GMT</date><size>1189kb</size></version><version version=\"v4\"><date>Fri, 27 Sep 2019 23:11:51 GMT</date><size>1973kb</size></version><title>Online adaptive basis enrichment for mixed CEM-GMsFEM</title><authors>Eric T. Chung and Sai-Mang Pun</authors><categories>math.NA cs.NA</categories><comments>22 pages, 10 figures. Accepted by SIAM Multiscale Modeling and\\n  Simulation</comments><msc-class>65N30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this research, an online basis enrichment strategy for the constraint\\nenergy minimizing generalized multiscale finite element method in mixed\\nformulation is proposed. The online approach is based on the technique of\\noversampling. One makes use of the information of residual and the data in the\\npartial differential equation such as the source function. The analysis\\npresented shows that the proposed online enrichment leads to a fast convergence\\nfrom multiscale approximation to the fine-scale solution. The error reduction\\ncan be made sufficiently large by suitably selecting oversampling regions and\\nthe number of oversampling layers. Also, the convergence rate of the enrichment\\ncan be tuned by a user-defined parameter. Numerical results are provided to\\nillustrate the efficiency of the proposed method.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.10777</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.10777</id><submitter>Vidyadhar Upadhya</submitter><version version=\"v1\"><date>Thu, 25 Oct 2018 08:51:19 GMT</date><size>817kb</size></version><version version=\"v2\"><date>Tue, 1 Oct 2019 04:58:01 GMT</date><size>641kb</size></version><title>Efficient Learning of Restricted Boltzmann Machines Using Covariance\\n  Estimates</title><authors>Vidyadhar Upadhya, P.S. Sastry</authors><categories>cs.LG stat.ML</categories><comments>Proceedings of Asian Conference on Machine Learning 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning RBMs using standard algorithms such as CD(k) involves gradient\\ndescent on the negative log-likelihood. One of the terms in the gradient, which\\ninvolves expectation w.r.t. the model distribution, is intractable and is\\nobtained through an MCMC estimate. In this work we show that the Hessian of the\\nlog-likelihood can be written in terms of covariances of hidden and visible\\nunits and hence, all elements of the Hessian can also be estimated using the\\nsame MCMC samples with small extra computational costs. Since inverting the\\nHessian may be computationally expensive, we propose an algorithm that uses\\ninverse of the diagonal approximation of the Hessian, instead. This essentially\\nresults in parameter-specific adaptive learning rates for the gradient descent\\nprocess and improves the efficiency of learning RBMs compared to the standard\\nmethods. Specifically we show that using the inverse of diagonal approximation\\nof Hessian in the stochastic DC (difference of convex functions) program\\napproach results in very efficient learning of RBMs.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.11282</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.11282</id><submitter>Binjie Qin</submitter><version version=\"v1\"><date>Fri, 26 Oct 2018 11:53:39 GMT</date><size>2997kb</size></version><title>Texture variation adaptive image denoising with nonlocal PCA</title><authors>Wenzhao Zhao, Qiegen Liu, Yisong Lv, and Binjie Qin</authors><categories>cs.CV</categories><doi>10.1109/TIP.2019.2916976</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image textures, as a kind of local variations, provide important information\\nfor human visual system. Many image textures, especially the small-scale or\\nstochastic textures are rich in high-frequency variations, and are difficult to\\nbe preserved. Current state-of-the-art denoising algorithms typically adopt a\\nnonlocal approach consisting of image patch grouping and group-wise denoising\\nfiltering. To achieve a better image denoising while preserving the variations\\nin texture, we first adaptively group high correlated image patches with the\\nsame kinds of texture elements (texels) via an adaptive clustering method. This\\nadaptive clustering method is applied in an\\nover-clustering-and-iterative-merging approach, where its noise robustness is\\nimproved with a custom merging threshold relating to the noise level and\\ncluster size. For texture-preserving denoising of each cluster, considering\\nthat the variations in texture are captured and wrapped in not only the\\nbetween-dimension energy variations but also the within-dimension variations of\\nPCA transform coefficients, we further propose a PCA-transform-domain variation\\nadaptive filtering method to preserve the local variations in textures.\\nExperiments on natural images show the superiority of the proposed\\ntransform-domain variation adaptive filtering to traditional PCA-based hard or\\nsoft threshold filtering. As a whole, the proposed denoising method achieves a\\nfavorable texture preserving performance both quantitatively and visually,\\nespecially for stochastic textures, which is further verified in camera raw\\nimage denoising.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.11334</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.11334</id><submitter>Yangjia Li</submitter><version version=\"v1\"><date>Fri, 26 Oct 2018 14:10:44 GMT</date><size>35kb</size></version><version version=\"v2\"><date>Tue, 1 Oct 2019 13:35:51 GMT</date><size>80kb</size><source_type>D</source_type></version><title>Reasoning about Parallel Quantum Programs</title><authors>Mingsheng Ying, Li Zhou and Yangjia Li</authors><categories>cs.LO cs.PL quant-ph</categories><comments>Added an application on formal verification of\\n  Bravyi-Gosset-K\\\\&quot;onig\\'s algorithm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate the study of parallel quantum programming by defining the\\noperational and denotational semantics of parallel quantum programs. The\\ntechnical contributions of this paper include: (1) find a series of useful\\nproof rules for reasoning about correctness of parallel quantum programs; (2)\\nprove a (relative) completeness of our proof rules for partial correctness of\\ndisjoint parallel quantum programs; and (3) prove a strong soundness theorem of\\nthe proof rules showing that partial correctness is well maintained at each\\nstep of transitions in the operational semantics of a general parallel quantum\\nprogram (with shared variables). This is achieved by partially overcoming the\\nfollowing conceptual challenges that are never present in classical parallel\\nprogramming: (i) the intertwining of nondeterminism caused by quantum\\nmeasurements and introduced by parallelism; (ii) entanglement between component\\nquantum programs; and (iii) combining quantum predicates in the overlap of\\nstate Hilbert spaces of component quantum programs with shared variables.\\nApplications of the techniques developed in this paper are illustrated by a\\nformal verification of Bravyi-Gosset-K\\\\&quot;onig\\'s parallel quantum algorithm\\nsolving a linear algebra problem, which gives for the first time an\\nunconditional proof of a computational quantum advantage.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.11702</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.11702</id><submitter>Christian Schroeder de Witt</submitter><version version=\"v1\"><date>Sat, 27 Oct 2018 20:45:19 GMT</date><size>697kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 5 Nov 2018 14:53:34 GMT</date><size>698kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 15 May 2019 13:05:30 GMT</date><size>1508kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Sun, 23 Jun 2019 16:45:17 GMT</date><size>330kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Tue, 1 Oct 2019 11:13:59 GMT</date><size>2297kb</size><source_type>D</source_type></version><title>Multi-Agent Common Knowledge Reinforcement Learning</title><authors>Christian A. Schroeder de Witt, Jakob N. Foerster, Gregory Farquhar,\\n  Philip H. S. Torr, Wendelin Boehmer, Shimon Whiteson</authors><categories>cs.MA cs.AI cs.GT cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative multi-agent reinforcement learning often requires decentralised\\npolicies, which severely limit the agents\\' ability to coordinate their\\nbehaviour. In this paper, we show that common knowledge between agents allows\\nfor complex decentralised coordination. Common knowledge arises naturally in a\\nlarge number of decentralised cooperative multi-agent tasks, for example, when\\nagents can reconstruct parts of each others\\' observations. Since agents an\\nindependently agree on their common knowledge, they can execute complex\\ncoordinated policies that condition on this knowledge in a fully decentralised\\nfashion. We propose multi-agent common knowledge reinforcement learning\\n(MACKRL), a novel stochastic actor-critic algorithm that learns a hierarchical\\npolicy tree. Higher levels in the hierarchy coordinate groups of agents by\\nconditioning on their common knowledge, or delegate to lower levels with\\nsmaller subgroups but potentially richer common knowledge. The entire policy\\ntree can be executed in a fully decentralised fashion. As the lowest policy\\ntree level consists of independent policies for each agent, MACKRL reduces to\\nindependently learnt decentralised policies as a special case. We demonstrate\\nthat our method can exploit common knowledge for superior performance on\\ncomplex decentralised coordination tasks, including a stochastic matrix game\\nand challenging problems in StarCraft II unit micromanagement.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.11878</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.11878</id><submitter>Richard Yuanzhe Pang</submitter><version version=\"v1\"><date>Sun, 28 Oct 2018 20:40:16 GMT</date><size>188kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 16:03:11 GMT</date><size>239kb</size><source_type>D</source_type></version><title>Unsupervised Evaluation Metrics and Learning Criteria for Non-Parallel\\n  Textual Transfer</title><authors>Richard Yuanzhe Pang, Kevin Gimpel</authors><categories>cs.CL cs.AI</categories><comments>EMNLP 2019 Workshop on Neural Generation and Translation (WNGT)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of automatically generating textual paraphrases with\\nmodified attributes or properties, focusing on the setting without parallel\\ndata (Hu et al., 2017; Shen et al., 2017). This setting poses challenges for\\nevaluation. We show that the metric of post-transfer classification accuracy is\\ninsufficient on its own, and propose additional metrics based on semantic\\npreservation and fluency as well as a way to combine them into a single overall\\nscore. We contribute new loss functions and training strategies to address the\\ndifferent metrics. Semantic preservation is addressed by adding a cyclic\\nconsistency loss and a loss based on paraphrase pairs, while fluency is\\nimproved by integrating losses based on style-specific language models. We\\nexperiment with a Yelp sentiment dataset and a new literature dataset that we\\npropose, using multiple models that extend prior work (Shen et al., 2017). We\\ndemonstrate that our metrics correlate well with human judgments, at both the\\nsentence-level and system-level. Automatic and manual evaluation also show\\nlarge improvements over the baseline method of Shen et al. (2017). We hope that\\nour proposed metrics can speed up system development for new textual transfer\\ntasks while also encouraging the community to address our three complementary\\naspects of transfer quality.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.12735</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.12735</id><submitter>Alice Coucke</submitter><version version=\"v1\"><date>Tue, 30 Oct 2018 13:49:37 GMT</date><size>19kb</size></version><version version=\"v2\"><date>Wed, 2 Oct 2019 15:03:22 GMT</date><size>15kb</size></version><title>Spoken Language Understanding on the Edge</title><authors>Alaa Saade, Alice Coucke, Alexandre Caulier, Joseph Dureau, Adrien\\n  Ball, Th\\\\\\'eodore Bluche, David Leroy, Cl\\\\\\'ement Doumouro, Thibault\\n  Gisselbrecht, Francesco Caltagirone, Thibaut Lavril, Ma\\\\&quot;el Primet</authors><categories>cs.CL cs.LG cs.SD eess.AS</categories><comments>arXiv admin note: text overlap with arXiv:1805.10190</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of performing Spoken Language Understanding (SLU) on\\nsmall devices typical of IoT applications. Our contributions are twofold.\\nFirst, we outline the design of an embedded, private-by-design SLU system and\\nshow that it has performance on par with cloud-based commercial solutions.\\nSecond, we release the datasets used in our experiments in the interest of\\nreproducibility and in the hope that they can prove useful to the SLU\\ncommunity.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.12780</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.12780</id><submitter>Di Jin</submitter><version version=\"v1\"><date>Tue, 30 Oct 2018 14:44:24 GMT</date><size>75kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 29 Nov 2018 02:38:02 GMT</date><size>76kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sat, 5 Oct 2019 17:26:41 GMT</date><size>95kb</size><source_type>D</source_type></version><title>Advancing PICO Element Detection in Biomedical Text via Deep Neural\\n  Networks</title><authors>Di Jin, Peter Szolovits</authors><categories>cs.CL cs.AI cs.LG</categories><comments>Accepted by the extended abstract track of Machine Learning for\\n  Health (ML4H) Workshop at NeurIPS 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In evidence-based medicine (EBM), defining a clinical question in terms of\\nthe specific patient problem aids the physicians to efficiently identify\\nappropriate resources and search for the best available evidence for medical\\ntreatment. In order to formulate a well-defined, focused clinical question, a\\nframework called PICO is widely used, which identifies the sentences in a given\\nmedical text that belong to the four components typically reported in clinical\\ntrials: Participants/Problem (P), Intervention (I), Comparison (C) and Outcome\\n(O). In this work, we propose a novel deep learning model for recognizing PICO\\nelements in biomedical abstracts. Based on the previous state-of-the-art\\nbidirectional long-short term memory (biLSTM) plus conditional random field\\n(CRF) architecture, we add another layer of biLSTM upon the sentence\\nrepresentation vectors so that the contextual information from surrounding\\nsentences can be gathered to help infer the interpretation of the current one.\\nIn addition, we propose two methods to further generalize and improve the\\nmodel: adversarial training and unsupervised pre-training over large corpora.\\nWe tested our proposed approach over two benchmark datasets. One is the\\nPubMed-PICO dataset, where our best results outperform the previous best by\\n5.5%, 7.9%, and 5.8% for P, I, and O elements in terms of F1 score,\\nrespectively. And for the other dataset named NICTA-PIBOSO, the improvements\\nfor P/I/O elements are 2.4%, 13.6%, and 1.0% in F1 score, respectively.\\nOverall, our proposed deep learning model can obtain unprecedented PICO element\\ndetection accuracy while avoiding the need for any manual feature selection.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1810.13108</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1810.13108</id><submitter>Andr\\\\\\'e Ricardo Belotto Da Silva</submitter><version version=\"v1\"><date>Wed, 31 Oct 2018 05:12:11 GMT</date><size>1419kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 14:21:22 GMT</date><size>915kb</size><source_type>D</source_type></version><title>A general system of differential equations to model first order adaptive\\n  algorithms</title><authors>Andr\\\\\\'e Belotto da Silva and Maxime Gazeau</authors><categories>cs.LG math.CA math.DS math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  First order optimization algorithms play a major role in large scale machine\\nlearning. A new class of methods, called adaptive algorithms, were recently\\nintroduced to adjust iteratively the learning rate for each coordinate. Despite\\ngreat practical success in deep learning, their behavior and performance on\\nmore general loss functions are not well understood. In this paper, we derive a\\nnon-autonomous system of differential equations, which is the continuous time\\nlimit of adaptive optimization methods. We prove global well-posedness of the\\nsystem and we investigate the numerical time convergence of its forward Euler\\napproximation. We study, furthermore, the convergence of its trajectories and\\ngive conditions under which the differential system, underlying all adaptive\\nalgorithms, is suitable for optimization. We discuss convergence to a critical\\npoint in the non-convex case and give conditions for the dynamics to avoid\\nsaddle points and local maxima. For convex and deterministic loss function, we\\nintroduce a suitable Lyapunov functional which allow us to study its rate of\\nconvergence. Several other properties of both the continuous and discrete\\nsystems are briefly discussed. The differential system studied in the paper is\\ngeneral enough to encompass many other classical algorithms (such as Heavy ball\\nand Nesterov\\'s accelerated method) and allow us to recover several known\\nresults for these algorithms.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.00431</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.00431</id><submitter>Mihai Nechita</submitter><version version=\"v1\"><date>Thu, 1 Nov 2018 15:26:46 GMT</date><size>619kb</size></version><version version=\"v2\"><date>Sat, 5 Oct 2019 20:09:49 GMT</date><size>746kb</size></version><title>A stabilized finite element method for inverse problems subject to the\\n  convection-diffusion equation. I: diffusion-dominated regime</title><authors>Erik Burman, Mihai Nechita, Lauri Oksanen</authors><categories>math.NA cs.NA math.AP</categories><comments>21 pages, 6 figures; in v2 we added two remarks and an appendix on\\n  psiDOs, and made some minor corrections</comments><msc-class>35J15, 65N20, 65N12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The numerical approximation of an inverse problem subject to the\\nconvection--diffusion equation when diffusion dominates is studied. We derive\\nCarleman estimates that are on a form suitable for use in numerical analysis\\nand with explicit dependence on the P\\\\\\'eclet number. A stabilized finite\\nelement method is then proposed and analysed. An upper bound on the condition\\nnumber is first derived. Combining the stability estimates on the continuous\\nproblem with the numerical stability of the method, we then obtain error\\nestimates in local $H^1$- or $L^2$-norms that are optimal with respect to the\\napproximation order, the problem\\'s stability and perturbations in data. The\\nconvergence order is the same for both norms, but the $H^1$-estimate requires\\nan additional divergence assumption for the convective field. The theory is\\nillustrated in some computational examples.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.00648</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.00648</id><submitter>Matthias Rottmann</submitter><version version=\"v1\"><date>Thu, 1 Nov 2018 22:00:00 GMT</date><size>8101kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 14:38:24 GMT</date><size>6516kb</size><source_type>D</source_type></version><title>Prediction Error Meta Classification in Semantic Segmentation: Detection\\n  via Aggregated Dispersion Measures of Softmax Probabilities</title><authors>Matthias Rottmann, Pascal Colling, Thomas-Paul Hack, Robin Chan,\\n  Fabian H\\\\&quot;uger, Peter Schlicht, Hanno Gottschalk</authors><categories>cs.CV cs.LG stat.ML</categories><msc-class>68T45, 62-07</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method that &quot;meta&quot; classifies whether seg-ments predicted by a\\nsemantic segmentation neural networkintersect with the ground truth. For this\\npurpose, we employ measures of dispersion for predicted pixel-wise class\\nprobability distributions, like classification entropy, that yield heat maps of\\nthe input scene\\'s size. We aggregate these dispersion measures segment-wise and\\nderive metrics that are well-correlated with the segment-wise IoU of prediction\\nand ground truth. This procedure yields an almost plug and play post-processing\\ntool to rate the prediction quality of semantic segmentation networks on\\nsegment level. This is especially relevant for monitoring neural networks in\\nonline applications like automated driving or medical imaging where reliability\\nis of utmost importance. In our tests, we use publicly available\\nstate-of-the-art networks trained on the Cityscapes dataset and the BraTS2017\\ndataset and analyze the predictive power of different metrics as well as\\ndifferent sets of metrics. To this end, we compute logistic LASSO regression\\nfits for the task of classifying IoU=0 vs. IoU&gt;0 per segment and obtain AUROC\\nvalues of up to 91.55%. We complement these tests with linear regression fits\\nto predict the segment-wise IoU and obtain prediction standard deviations of\\ndown to 0.130 as well as $R^2$ values of up to 84.15%. We show that these\\nresults clearly outperform standard approaches.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.01165</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.01165</id><submitter>Jiequn Han</submitter><version version=\"v1\"><date>Sat, 3 Nov 2018 06:08:23 GMT</date><size>411kb</size></version><version version=\"v2\"><date>Wed, 28 Nov 2018 04:55:30 GMT</date><size>379kb</size></version><version version=\"v3\"><date>Sat, 5 Oct 2019 04:34:43 GMT</date><size>380kb</size></version><title>Convergence of the Deep BSDE Method for Coupled FBSDEs</title><authors>Jiequn Han, Jihao Long</authors><categories>math.PR cs.LG cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently proposed numerical algorithm, deep BSDE method, has shown\\nremarkable performance in solving high-dimensional forward-backward stochastic\\ndifferential equations (FBSDEs) and parabolic partial differential equations\\n(PDEs). This article lays a theoretical foundation for the deep BSDE method in\\nthe general case of coupled FBSDEs. In particular, a posteriori error\\nestimation of the solution is provided and it is proved that the error\\nconverges to zero given the universal approximation capability of neural\\nnetworks. Numerical results are presented to demonstrate the accuracy of the\\nanalyzed algorithm in solving high-dimensional coupled FBSDEs.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.02227</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.02227</id><submitter>Alexander Veit</submitter><version version=\"v1\"><date>Tue, 6 Nov 2018 08:50:55 GMT</date><size>433kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 14:20:56 GMT</date><size>382kb</size><source_type>D</source_type></version><title>Numerical approximation of Poisson problems in long domains</title><authors>Michel Chipot, Wolfgang Hackbusch, Stefan Sauter, Alexander Veit</authors><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the Poisson equation on a &quot;long&quot; domain which is\\nthe Cartesian product of a one-dimensional long interval with a\\n(d-1)-dimensional domain. The right-hand side is assumed to have a rank-1\\ntensor structure. We will present and compare methods to construct\\napproximations of the solution which have tensor structure and the\\ncomputational effort is governed by only solving elliptic problems on\\nlower-dimensional domains. A zero-th order tensor approximation is derived by\\nusing tools from asymptotic analysis (method 1). The resulting approximation is\\nan elementary tensor and, hence has a fixed error which turns out to be very\\nclose to the best possible approximation of zero-th order. This approximation\\ncan be used as a starting guess for the derivation of higher-order tensor\\napproximations by an alternating-least-squares (ALS) type method (method 2).\\nNumerical experiments show that the ALS is converging towards the exact\\nsolution. Method 3 is based on the derivation of a tensor approximation via\\nexponential sums applied to discretised differential operators and their\\ninverses. It can be proved that this method converges exponentially with\\nrespect to the tensor rank. We present numerical experiments which compare the\\nperformance and sensitivity of these three methods.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.02605</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.02605</id><submitter>Ronald Caplan</submitter><version version=\"v1\"><date>Tue, 6 Nov 2018 19:30:50 GMT</date><size>2172kb</size><source_type>D</source_type></version><title>GPU Acceleration of an Established Solar MHD Code using OpenACC</title><authors>R. M. Caplan, J. A. Linker, Z. Miki\\\\\\'c, C. Downs, T. T\\\\&quot;or\\\\&quot;ok, and V.\\n  S. Titov</authors><categories>physics.comp-ph astro-ph.SR cs.DC cs.PL</categories><comments>13 pages, 9 figures</comments><doi>10.1088/1742-6596/1225/1/012012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  GPU accelerators have had a notable impact on high-performance computing\\nacross many disciplines. They provide high performance with low cost/power, and\\ntherefore have become a primary compute resource on many of the largest\\nsupercomputers. Here, we implement multi-GPU acceleration into our Solar MHD\\ncode (MAS) using OpenACC in a fully portable, single-source manner. Our\\npreliminary implementation is focused on MAS running in a reduced physics\\n&quot;zero-beta&quot; mode. While valuable on its own, our main goal is to pave the way\\nfor a full physics, thermodynamic MHD implementation. We describe the OpenACC\\nimplementation methodology and challenges. &quot;Time-to-solution&quot; performance\\nresults of a production-level flux rope eruption simulation on multi-CPU and\\nmulti-GPU systems are shown. We find that the GPU-accelerated MAS code has the\\nability to run &quot;zero-beta&quot; simulations on a single multi-GPU server at speeds\\npreviously requiring multiple CPU server-nodes of a supercomputer.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.02617</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.02617</id><submitter>Kieran Greer Dr</submitter><version version=\"v1\"><date>Tue, 6 Nov 2018 20:21:26 GMT</date><size>289kb</size></version><version version=\"v2\"><date>Wed, 16 Jan 2019 10:32:40 GMT</date><size>424kb</size></version><version version=\"v3\"><date>Thu, 3 Oct 2019 11:01:22 GMT</date><size>424kb</size></version><title>An Improved Batch Classifier with Bands and Dimensions</title><authors>Kieran Greer</authors><categories>cs.LG cs.AI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1711.07042</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper extends the earlier work on a one-pass categorical classifier.\\nSpecifically, it extends the design to include further corrections, by adding\\nnew layers to the classifier through a branching method. Each new layer\\nre-classifies a subset of the data, belonging to the parent classifier only.\\nThis technique is still consistent with earlier work and neural networks, or\\neven decision trees. With this extended design, the classifier can now achieve\\nthe high levels of accuracy reported previously. A second version then adds\\nfixed value ranges through bands, for each column or feature of the input\\ndataset. It is shown that some of the data can be correctly classified through\\nusing fixed value ranges only, while the rest can be classified by using the\\nclassifier technique. This can possibly present the classifier in terms of a\\nbiological model of neurons and neuron links.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.03376</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.03376</id><submitter>Xi Chen</submitter><version version=\"v1\"><date>Thu, 8 Nov 2018 12:26:42 GMT</date><size>860kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 10:35:03 GMT</date><size>752kb</size><source_type>D</source_type></version><title>Meta-Learning for Multi-objective Reinforcement Learning</title><authors>Xi Chen, Ali Ghadirzadeh, M{\\\\aa}rten Bj\\\\&quot;orkman and Patric Jensfelt</authors><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-objective reinforcement learning (MORL) is the generalization of\\nstandard reinforcement learning (RL) approaches to solve sequential decision\\nmaking problems that consist of several, possibly conflicting, objectives.\\nGenerally, in such formulations, there is no single optimal policy which\\noptimizes all the objectives simultaneously, and instead, a number of policies\\nhas to be found each optimizing a preference of the objectives. In other words,\\nthe MORL is framed as a meta-learning problem, with the task distribution given\\nby a distribution over the preferences. We demonstrate that such a formulation\\nresults in a better approximation of the Pareto optimal solutions in terms of\\nboth the optimality and the computational efficiency. We evaluated our method\\non obtaining Pareto optimal policies using a number of continuous control\\nproblems with high degrees of freedom.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.03438</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.03438</id><submitter>Xingkang He</submitter><version version=\"v1\"><date>Thu, 8 Nov 2018 14:28:49 GMT</date><size>77kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 1 Jun 2019 14:54:38 GMT</date><size>155kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 7 Oct 2019 09:39:17 GMT</date><size>116kb</size><source_type>D</source_type></version><title>Distributed Filtering for Uncertain Systems Under Switching Sensor\\n  Networks and Quantized Communications</title><authors>Xingkang He, Wenchao Xue, Xiaocheng Zhang, Haitao Fang</authors><categories>eess.SP cs.SY eess.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the distributed filtering problem for a class of\\nstochastic uncertain systems under quantized data flowing over switching sensor\\nnetworks. Employing the biased noisy observations of the local sensor and\\ninterval-quantized messages from neighboring sensors successively, an extended\\nstate based distributed Kalman filter (DKF) is proposed for simultaneously\\nestimating both system state and uncertain dynamics. To alleviate the effect of\\nobservation biases, an event-triggered update based DKF is presented with a\\ntighter mean square error bound than that of the time-driven one by designing a\\nproper threshold. Both the two DKFs are shown to provide the upper bounds of\\nmean square errors online for each sensor. Under mild conditions on systems and\\nnetworks, the mean square error boundedness and asymptotic unbiasedness for the\\nproposed two DKFs are proved. Finally, the numerical simulations demonstrate\\nthe effectiveness of the developed filters.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.03496</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.03496</id><submitter>Helge Spieker</submitter><version version=\"v1\"><date>Thu, 8 Nov 2018 15:39:35 GMT</date><size>34kb</size><source_type>D</source_type></version><title>Rotational Diversity in Multi-Cycle Assignment Problems</title><authors>Helge Spieker, Arnaud Gotlieb, Morten Mossige</authors><categories>cs.AI</categories><comments>AAAI-19</comments><journal-ref>Proceedings of the Thirty-Third AAAI Conference on Artificial\\n  Intelligence (2019)</journal-ref><doi>10.1609/aaai.v33i01.33017724</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multi-cycle assignment problems with rotational diversity, a set of tasks\\nhas to be repeatedly assigned to a set of agents. Over multiple cycles, the\\ngoal is to achieve a high diversity of assignments from tasks to agents. At the\\nsame time, the assignments\\' profit has to be maximized in each cycle. Due to\\nchanging availability of tasks and agents, planning ahead is infeasible and\\neach cycle is an independent assignment problem but influenced by previous\\nchoices. We approach the multi-cycle assignment problem as a two-part problem:\\nProfit maximization and rotation are combined into one objective value, and\\nthen solved as a General Assignment Problem. Rotational diversity is maintained\\nwith a single execution of the costly assignment model. Our simple, yet\\neffective method is applicable to different domains and applications.\\nExperiments show the applicability on a multi-cycle variant of the multiple\\nknapsack problem and a real-world case study on the test case selection and\\nassignment problem, an example from the software engineering domain, where test\\ncases have to be distributed over compatible test machines.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.03764</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.03764</id><submitter>Mahardhika Pratama Dr</submitter><version version=\"v1\"><date>Fri, 9 Nov 2018 03:58:27 GMT</date><size>4098kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 15:00:09 GMT</date><size>6736kb</size><source_type>D</source_type></version><title>PAC: A Novel Self-Adaptive Neuro-Fuzzy Controller for Micro Aerial\\n  Vehicles</title><authors>Md Meftahul Ferdaus, Mahardhika Pratama, Sreenatha G. Anavatti,\\n  Matthew A. Garratt and Edwin Lughofer</authors><categories>cs.RO</categories><comments>This paper has been accepted for publication in Information Science\\n  Journal 2019</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  There exists an increasing demand for a flexible and computationally\\nefficient controller for micro aerial vehicles (MAVs) due to a high degree of\\nenvironmental perturbations. In this work, an evolving neuro-fuzzy controller,\\nnamely Parsimonious Controller (PAC) is proposed. It features fewer network\\nparameters than conventional approaches due to the absence of rule premise\\nparameters. PAC is built upon a recently developed evolving neuro-fuzzy system\\nknown as parsimonious learning machine (PALM) and adopts new rule growing and\\npruning modules derived from the approximation of bias and variance. These rule\\nadaptation methods have no reliance on user-defined thresholds, thereby\\nincreasing the PAC\\'s autonomy for real-time deployment. PAC adapts the\\nconsequent parameters with the sliding mode control (SMC) theory in the\\nsingle-pass fashion. The boundedness and convergence of the closed-loop control\\nsystem\\'s tracking error and the controller\\'s consequent parameters are\\nconfirmed by utilizing the LaSalle-Yoshizawa theorem. Lastly, the controller\\'s\\nefficacy is evaluated by observing various trajectory tracking performance from\\na bio-inspired flapping-wing micro aerial vehicle (BI-FWMAV) and a rotary wing\\nmicro aerial vehicle called hexacopter. Furthermore, it is compared to three\\ndistinctive controllers. Our PAC outperforms the linear PID controller and\\nfeed-forward neural network (FFNN) based nonlinear adaptive controller.\\nCompared to its predecessor, G-controller, the tracking accuracy is comparable,\\nbut the PAC incurs significantly fewer parameters to attain similar or better\\nperformance than the G-controller.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.03836</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.03836</id><submitter>Joshua Lau</submitter><version version=\"v1\"><date>Fri, 9 Nov 2018 09:48:46 GMT</date><size>33kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 04:49:39 GMT</date><size>54kb</size><source_type>D</source_type></version><title>Minimizing and Computing the Inverse Geodesic Length on Trees</title><authors>Serge Gaspers and Joshua Lau</authors><categories>cs.DS</categories><comments>21 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For any fixed measure $H$ that maps graphs to real numbers, the MinH problem\\nis defined as follows: given a graph $G$, an integer $k$, and a target $\\\\tau$,\\nis there a set $S$ of $k$ vertices that can be deleted, so that $H(G - S)$ is\\nat most $\\\\tau$? In this paper, we consider the MinH problem on trees.\\n  We call $H$ &quot;balanced on trees&quot; if, whenever $G$ is a tree, there is an\\noptimal choice of $S$ such that the components of $G-S$ have sizes bounded by a\\npolynomial in $n/k$. We show that MinH on trees is FPT for parameter $n/k$, and\\nfurthermore, can be solved in subexponential time, and polynomial space, if $H$\\nis additive, balanced on trees, and computable in polynomial time.\\n  A measure of interest is the Inverse Geodesic Length (IGL), which is used to\\ngauge the connectedness of a graph. It is defined as the sum of inverse\\ndistances between every two vertices: $IGL(G)=\\\\sum_{\\\\{u,v\\\\} \\\\subseteq V}\\n\\\\frac{1}{d_G(u,v)}$. While MinIGL is W[1]-hard for parameter treewidth, and\\ncannot be solved in $2^{o(k+n+m)}$ time, even on bipartite graphs with $n$\\nvertices and $m$ edges, the complexity status of the problem remains open on\\ntrees. We show that IGL is balanced on trees, to give a $2^{O((n\\\\log\\nn)^{5/6})}$ time, polynomial space algorithm.\\n  The distance distribution of $G$ is the sequence $\\\\{a_i\\\\}$ describing the\\nnumber of vertex pairs distance $i$ apart in $G$: $a_i=|\\\\{\\\\{u, v\\\\}: d_G(u,\\nv)=i\\\\}|$. We show that the distance distribution of a tree can be computed in\\n$O(n\\\\log^2 n)$ time by reduction to polynomial multiplication. We extend our\\nresult to graphs with small treewidth by showing that the first $p$ values of\\nthe distance distribution can be computed in $2^{O(tw(G))} n^{1+\\\\varepsilon}\\n\\\\sqrt{p}$ time, and the entire distance distribution can be computed in\\n$2^{O(tw(G))} n^{1+\\\\varepsilon}$ time, when the diameter of $G$ is\\n$O(n^{\\\\varepsilon\\'})$ for every $\\\\varepsilon\\'&gt;0$.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.03906</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.03906</id><submitter>Helge Spieker</submitter><version version=\"v1\"><date>Fri, 9 Nov 2018 14:07:47 GMT</date><size>87kb</size><source_type>D</source_type></version><title>Stratified Constructive Disjunction and Negation in Constraint\\n  Programming</title><authors>Arnaud Gotlieb, Dusica Marijan, Helge Spieker</authors><categories>cs.AI</categories><comments>Published in the SAT/CSP Track of the International Conference on\\n  Tools with Artificial Intelligence (ICTAI 2018)</comments><journal-ref>2018 IEEE 30th International Conference on Tools with Artificial\\n  Intelligence (ICTAI)</journal-ref><doi>10.1109/ICTAI.2018.00026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constraint Programming (CP) is a powerful declarative programming paradigm\\ncombining inference and search in order to find solutions to various type of\\nconstraint systems. Dealing with highly disjunctive constraint systems is\\nnotoriously difficult in CP. Apart from trying to solve each disjunct\\nindependently from each other, there is little hope and effort to succeed in\\nconstructing intermediate results combining the knowledge originating from\\nseveral disjuncts. In this paper, we propose If Then Else (ITE), a lightweight\\napproach for implementing stratified constructive disjunction and negation on\\ntop of an existing CP solver, namely SICStus Prolog clp(FD). Although\\nconstructive disjunction is known for more than three decades, it does not have\\nstraightforward implementations in most CP solvers. ITE is a freely available\\nlibrary proposing stratified and constructive reasoning for various operators,\\nincluding disjunction and negation, implication and conditional. Our\\npreliminary experimental results show that ITE is competitive with existing\\napproaches that handle disjunctive constraint systems.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.04577</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.04577</id><submitter>Long Nguyen Msc</submitter><version version=\"v1\"><date>Mon, 12 Nov 2018 06:37:20 GMT</date><size>4659kb</size><source_type>D</source_type></version><title>Forecasting People\\'s Needs in Hurricane Events from Social Network</title><authors>Long Nguyen, Zhou Yang, Jia Li, Guofeng Cao, Fang Jin</authors><categories>cs.CL</categories><doi>10.1109/TBDATA.2019.2941887</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks can serve as a valuable communication channel for calls for\\nhelp, offering assistance, and coordinating rescue activities in disaster.\\nSocial networks such as Twitter allow users to continuously update relevant\\ninformation, which is especially useful during a crisis, where the rapidly\\nchanging conditions make it crucial to be able to access accurate information\\npromptly. Social media helps those directly affected to inform others of\\nconditions on the ground in real time and thus enables rescue workers to\\ncoordinate their efforts more effectively, better meeting the survivors\\' need.\\nThis paper presents a new sequence to sequence based framework for forecasting\\npeople\\'s needs during disasters using social media and weather data. It\\nconsists of two Long Short-Term Memory (LSTM) models, one of which encodes\\ninput sequences of weather information and the other plays as a conditional\\ndecoder that decodes the encoded vector and forecasts the survivors\\' needs.\\nCase studies utilizing data collected during Hurricane Sandy in 2012, Hurricane\\nHarvey and Hurricane Irma in 2017 were analyzed and the results compared with\\nthose obtained using a statistical language model n-gram and an LSTM generative\\nmodel. Our proposed sequence to sequence method forecast people\\'s needs more\\nsuccessfully than either of the other models. This new approach shows great\\npromise for enhancing disaster management activities such as evacuation\\nplanning and commodity flow management.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.05005</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.05005</id><submitter>Danielle Gonzalez</submitter><version version=\"v1\"><date>Mon, 12 Nov 2018 21:17:03 GMT</date><size>229kb</size><source_type>D</source_type></version><title>A Fine-Grained Approach for Automated Conversion of JUnit Assertions to\\n  English</title><authors>Danielle Gonzalez, Suzanne Prentice, Mehdi Mirakhorli</authors><categories>cs.SE</categories><comments>In Proceedings of the 4th ACM SIGSOFT International Workshop on NLP\\n  for Software Engineering (NL4SE 18), November 4, 2018, Lake Buena Vista, FL,\\n  USA. ACM, New York, NY, USA, 4 pages</comments><journal-ref>In Proceedings of the 4th ACM SIGSOFT International Workshop on\\n  NLP for Software Engineering (NL4SE 2018). ACM, New York, NY, USA, 14-17</journal-ref><doi>10.1145/3283812.3283819</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Converting source or unit test code to English has been shown to improve the\\nmaintainability, understandability, and analysis of software and tests. Code\\nsummarizers identify important statements in the source/tests and convert them\\nto easily understood English sentences using static analysis and NLP\\ntechniques. However, current test summarization approaches handle only a subset\\nof the variation and customization allowed in the JUnit assert API (a critical\\ncomponent of test cases) which may affect the accuracy of conversions. In this\\npaper, we present our work towards improving JUnit test summarization with a\\ndetailed process for converting a total of 45 unique JUnit assertions to\\nEnglish, including 37 previously-unhandled variations of the assertThat method.\\nThis process has also been implemented and released as the AssertConvert tool.\\nInitial evaluations have shown that this tool generates English conversions\\nthat accurately represent a wide variety of assertion statements which could be\\nused for code summarization or other NLP analyses.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.06308</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.06308</id><submitter>David Berga</submitter><version version=\"v1\"><date>Thu, 15 Nov 2018 12:11:24 GMT</date><size>6792kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 22 Nov 2018 11:34:37 GMT</date><size>6901kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 29 Nov 2018 17:57:33 GMT</date><size>6902kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Tue, 4 Dec 2018 16:21:19 GMT</date><size>7902kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Fri, 14 Dec 2018 18:04:06 GMT</date><size>7941kb</size><source_type>D</source_type></version><version version=\"v6\"><date>Fri, 1 Feb 2019 17:45:10 GMT</date><size>7938kb</size><source_type>D</source_type></version><version version=\"v7\"><date>Sun, 29 Sep 2019 22:19:01 GMT</date><size>7991kb</size><source_type>D</source_type></version><title>A Neurodynamic model of Saliency prediction in V1</title><authors>David Berga, Xavier Otazu</authors><categories>cs.CV</categories><comments>12 pages, 16 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lateral connections in the primary visual cortex (area V1 or striate cortex)\\nhave long been hypothesized to be responsible of several visual processing\\nmechanisms such as brightness induction, chromatic induction, visual discomfort\\nand bottom-up visual attention (also named saliency). Many computational models\\nhave been developed to independently predict these and other visual processes,\\nbut no computational model has been able to reproduce all of them\\nsimultaneously. In this work we show that a biologically plausible\\ncomputational model of lateral interactions of V1 is able to simultaneously\\npredict saliency and all the aforementioned visual processes. Our model\\'s\\n(named Neurodynamic Saliency WAvelet Model or NSWAM) architecture is based on\\nPennachio\\'s neurodynamic model of lateral connections of V1 (defined as a\\nnetwork of firing rate neurons, sensitive to visual features such as\\nbrightness, color, orientation and scale). We tested NSWAM saliency predictions\\nusing images from eye tracking datasets, showing that it is an improvement with\\nrespect to previous models as well as consistent with human psychophysics.\\nHence, we show that our biologically plausible model of lateral connections can\\nsimultaneously explain different visual proceses present in V1 (without\\napplying any type of training or optimization and keeping the same\\nparametrization for all the visual processes). This can be useful for the\\ndefinition of a unified architecture of the primary visual cortex.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.06582</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.06582</id><submitter>Neeti Narayan</submitter><version version=\"v1\"><date>Thu, 15 Nov 2018 20:23:46 GMT</date><size>962kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 16 Dec 2018 00:23:04 GMT</date><size>900kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 21 Jan 2019 23:20:00 GMT</date><size>900kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Thu, 3 Oct 2019 22:48:18 GMT</date><size>1715kb</size><source_type>D</source_type></version><title>CAN: Composite Appearance Network for Person Tracking and How to Model\\n  Errors in a Tracking System</title><authors>Neeti Narayan, Nishant Sankaran, Srirangaraj Setlur, Venu Govindaraju</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tracking multiple people across multiple cameras is an open problem. It is\\ntypically divided into two tasks: (i) single-camera tracking (SCT) - identify\\ntrajectories in the same scene, and (ii) inter-camera tracking (ICT) - identify\\ntrajectories across cameras for real surveillance scenes. Many methods cater to\\nSCT, while ICT still remains a challenge. In this paper, we propose a tracking\\nmethod which uses motion cues and a feature aggregation network for\\ntemplate-based person re-identification by incorporating metadata such as\\nperson bounding box and camera information. We present a feature aggregation\\narchitecture called Composite Appearance Network (CAN) to address the above\\nproblem. The key structure of this architecture is called EvalNet that pays\\nattention to each feature vector and learns to weight them based on gradients\\nit receives for the overall template for optimal re-identification performance.\\nWe demonstrate the efficiency of our approach with experiments on the\\nchallenging multi-camera tracking dataset, DukeMTMC. We also survey existing\\ntracking measures and present an online error metric called &quot;Inference Error&quot;\\n(IE) that provides a better estimate of tracking/re-identification error, by\\ntreating SCT and ICT errors uniformly.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.07546</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.07546</id><submitter>Takashi Goda</submitter><version version=\"v1\"><date>Mon, 19 Nov 2018 08:11:28 GMT</date><size>617kb</size></version><version version=\"v2\"><date>Thu, 24 Jan 2019 05:09:04 GMT</date><size>617kb</size></version><version version=\"v3\"><date>Thu, 23 May 2019 08:29:38 GMT</date><size>616kb</size></version><version version=\"v4\"><date>Mon, 7 Oct 2019 03:34:12 GMT</date><size>616kb</size></version><title>Multilevel Monte Carlo estimation of expected information gains</title><authors>Takashi Goda, Tomohiko Hironaka, Takeru Iwamoto</authors><categories>stat.CO cs.NA math.NA stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The expected information gain is an important quality criterion of Bayesian\\nexperimental designs, which measures how much the information entropy about\\nuncertain quantity of interest $\\\\theta$ is reduced on average by collecting\\nrelevant data $Y$. However, estimating the expected information gain has been\\nconsidered computationally challenging since it is defined as a nested\\nexpectation with an outer expectation with respect to $Y$ and an inner\\nexpectation with respect to $\\\\theta$. In fact, the standard, nested Monte Carlo\\nmethod requires a total computational cost of $O(\\\\varepsilon^{-3})$ to achieve\\na root-mean-square accuracy of $\\\\varepsilon$. In this paper we develop an\\nefficient algorithm to estimate the expected information gain by applying a\\nmultilevel Monte Carlo (MLMC) method. To be precise, we introduce an antithetic\\nMLMC estimator for the expected information gain and provide a sufficient\\ncondition on the data model under which the antithetic property of the MLMC\\nestimator is well exploited such that optimal complexity of\\n$O(\\\\varepsilon^{-2})$ is achieved. Furthermore, we discuss how to incorporate\\nimportance sampling techniques within the MLMC estimator to avoid arithmetic\\nunderflow. Numerical experiments show the considerable computational cost\\nsavings compared to the nested Monte Carlo method for a simple test case and a\\nmore realistic pharmacokinetic model.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.07659</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.07659</id><submitter>Yoshihiko Susuki</submitter><version version=\"v1\"><date>Mon, 19 Nov 2018 12:56:09 GMT</date><size>499kb</size><source_type>D</source_type></version><title>Synthesis of Spatial Charging/Discharging Patterns of In-Vehicle\\n  Batteries for Provision of Ancillary Service and Mitigation of Voltage Impact</title><authors>Naoto Mizuta, Yoshihiko Susuki, Yutaka Ota, Atsushi Ishigame</authors><categories>cs.SY math.OC</categories><comments>20 pages, 8 figures</comments><doi>10.1109/JSYST.2018.2883974</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop an algorithm for synthesizing a spatial pattern of\\ncharging/discharging operations of in-vehicle batteries for provision of\\nAncillary Service (AS) in power distribution grids. The algorithm is based on\\nthe ODE (Ordinary Differential Equation) model of distribution voltage that has\\nbeen recently introduced. In this paper, firstly, we derive analytical\\nsolutions of the ODE model for a single straight-line feeder through a partial\\nlinearization, thereby providing a physical insight to the impact of spatial EV\\ncharging/discharging to the distribution voltage. Second, based on the\\nanalytical solutions, we propose an algorithm for determining the values of\\ncharging/discharging power (active and reactive) by in-vehicle batteries in the\\nsingle feeder grid, so that the power demanded as AS (e.g. a regulation signal\\nto distribution system operator for primary frequency control reserve) is\\nprovided by EVs, and the deviation of distribution voltage from a nominal value\\nis reduced in the grid. Effectiveness of the algorithm is established with\\nnumerical simulations on the single feeder grid and on a realistic feeder grid\\nwith multiple bifurcations.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.07863</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.07863</id><submitter>Manuel Gomez Rodriguez</submitter><version version=\"v1\"><date>Mon, 19 Nov 2018 18:31:21 GMT</date><size>2707kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 16 Dec 2018 22:23:02 GMT</date><size>2707kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 20 May 2019 10:09:26 GMT</date><size>1325kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Tue, 21 May 2019 06:27:47 GMT</date><size>1325kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Tue, 8 Oct 2019 15:57:22 GMT</date><size>1331kb</size><source_type>D</source_type></version><title>Non-submodular Function Maximization subject to a Matroid Constraint,\\n  with Applications</title><authors>Khashayar Gatmiry and Manuel Gomez-Rodriguez</authors><categories>cs.SI cs.DM cs.DS cs.LG stat.ML</categories><comments>Added missing citations and changed strong submodularity ratio to\\n  generalized curvature</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The standard greedy algorithm has been recently shown to enjoy approximation\\nguarantees for constrained non-submodular nondecreasing set function\\nmaximization. While these recent results allow to better characterize the\\nempirical success of the greedy algorithm, they are only applicable to simple\\ncardinality constraints. In this paper, we study the problem of maximizing a\\nnon-submodular nondecreasing set function subject to a general matroid\\nconstraint. We first show that the standard greedy algorithm offers an\\napproximation factor of $\\\\frac{0.4 {\\\\gamma}^{2}}{\\\\sqrt{\\\\gamma r} + 1}$, where\\n$\\\\gamma$ is the submodularity ratio of the function and $r$ is the rank of the\\nmatroid. Then, we show that the same greedy algorithm offers a constant\\napproximation factor of $(1 + 1/(1-\\\\alpha))^{-1}$, where $\\\\alpha$ is the\\ngeneralized curvature of the function. In addition, we demonstrate that these\\napproximation guarantees are applicable to several real-world applications in\\nwhich the submodularity ratio and the generalized curvature can be bounded.\\nFinally, we show that our greedy algorithm does achieve a competitive\\nperformance in practice using a variety of experiments on synthetic and\\nreal-world data.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.08237</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.08237</id><submitter>Pierre-Jean Spaenlehauer</submitter><version version=\"v1\"><date>Tue, 20 Nov 2018 13:28:38 GMT</date><size>36kb</size></version><version version=\"v2\"><date>Fri, 30 Nov 2018 09:57:16 GMT</date><size>36kb</size></version><version version=\"v3\"><date>Mon, 13 May 2019 12:40:15 GMT</date><size>40kb</size></version><version version=\"v4\"><date>Thu, 16 May 2019 08:59:27 GMT</date><size>40kb</size></version><version version=\"v5\"><date>Tue, 8 Oct 2019 09:44:54 GMT</date><size>42kb</size></version><title>A Fast Randomized Geometric Algorithm for Computing Riemann-Roch Spaces</title><authors>Aude Le Gluher and Pierre-Jean Spaenlehauer</authors><categories>cs.SC cs.CC math.AG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a probabilistic variant of Brill-Noether\\'s algorithm for computing\\na basis of the Riemann-Roch space $L(D)$ associated to a divisor $D$ on a\\nprojective nodal plane curve $\\\\mathcal C$ over a sufficiently large perfect\\nfield $k$. Our main result shows that this algorithm requires at most\\n$O(\\\\max(\\\\mathrm{deg}(\\\\mathcal C)^{2\\\\omega}, \\\\mathrm{deg}(D_+)^\\\\omega))$\\narithmetic operations in $k$, where $\\\\omega$ is a feasible exponent for matrix\\nmultiplication and $D_+$ is the smallest effective divisor such that $D_+\\\\geq\\nD$. This improves the best known upper bounds on the complexity of computing\\nRiemann-Roch spaces. Our algorithm may fail, but we show that provided that a\\nfew mild assumptions are satisfied, the failure probability is bounded by\\n$O(\\\\max(\\\\mathrm{deg}(\\\\mathcal C)^4, \\\\mathrm{deg}(D_+)^2)/\\\\lvert \\\\mathcal\\nE\\\\rvert)$, where $\\\\mathcal E$ is a finite subset of $k$ in which we pick\\nelements uniformly at random. We provide a freely available C++/NTL\\nimplementation of the proposed algorithm and we present experimental data. In\\nparticular, our implementation enjoys a speedup larger than 6 on many examples\\n(and larger than 200 on some instances over large finite fields) compared to\\nthe reference implementation in the Magma computer algebra system. As a\\nby-product, our algorithm also yields a method for computing the group law on\\nthe Jacobian of a smooth plane curve of genus $g$ within $O(g^\\\\omega)$\\noperations in $k$, which equals the best known complexity for this problem.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.08656</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.08656</id><submitter>Andrea Pozzi</submitter><version version=\"v1\"><date>Wed, 21 Nov 2018 10:00:24 GMT</date><size>823kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 17:03:16 GMT</date><size>823kb</size><source_type>D</source_type></version><title>Optimal design of experiments for a lithium-ion cell: parameters\\n  identification of an isothermal single particle model with electrolyte\\n  dynamics</title><authors>Andrea Pozzi, Gabriele Ciaramella, Stefan Volkwein, Davide M. Raimondo</authors><categories>cs.SY math.OC</categories><comments>Published in Ind. Eng. Chem. Res. 2019, 58, 3, 1286-1299</comments><journal-ref>Ind. Eng. Chem. Res. 2019, 58, 3, 1286-1299</journal-ref><doi>10.1021/acs.iecr.8b04580</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advanced battery management systems rely on mathematical models to guarantee\\noptimal functioning of Lithium-ion batteries. The Pseudo-Two Dimensional (P2D)\\nmodel is a very detailed electrochemical model suitable for simulations. On the\\nother side, its complexity prevents its usage in control and state estimation.\\nTherefore, it is more appropriate the use of simplified electrochemical models\\nsuch as the Single Particle Model with electrolyte dynamics (SPMe), which\\nexhibits good adherence to real data when suitably calibrated. This work\\nfocuses on a Fisher-based optimal experimental design for identifying the SPMe\\nparameters. The proposed approach relies on a nonlinear optimization to\\nminimize the covariance parameters matrix. At first, the parameters are\\nestimated by considering the SPMe as the real plant. Subsequently, a more\\nrealistic scenario is considered where the P2D model is used to reproduce a\\nreal battery behavior. Results show the effectiveness of the optimal\\nexperimental design when compared to standard strategies.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.09188</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.09188</id><submitter>Corentin Briat Dr</submitter><version version=\"v1\"><date>Thu, 22 Nov 2018 14:23:19 GMT</date><size>1094kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 8 Sep 2019 22:33:13 GMT</date><size>1163kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 1 Oct 2019 06:48:13 GMT</date><size>1165kb</size><source_type>D</source_type></version><title>Ergodicity analysis and antithetic integral control of a class of\\n  stochastic reaction networks with delays</title><authors>Corentin Briat and Mustafa Khammash</authors><categories>math.OC cs.SY q-bio.MN</categories><comments>42 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Delays are an important phenomenon arising in a wide variety of real world\\nsystems. They occur in biological models because of diffusion effects or as\\nsimplifying modeling elements. We propose here to consider delayed stochastic\\nreaction networks. The difficulty here lies in the fact that the state-space of\\na delayed reaction network is infinite-dimensional, which makes their analysis\\nmore involved. We demonstrate here that a particular class of stochastic\\ntime-varying delays, namely those that follow a phase-type distribution, can be\\nexactly implemented in terms of a chemical reaction network. Hence, any\\ndelay-free network can be augmented to incorporate those delays through the\\naddition of delay-species and delay-reactions. Hence, for this class of\\nstochastic delays, which can be used to approximate any delay distribution\\narbitrarily accurately, the state-space remains finite-dimensional and,\\ntherefore, standard tools developed for standard reaction network still apply.\\nIn particular, we demonstrate that for unimolecular mass-action reaction\\nnetworks that the delayed stochastic reaction network is ergodic if and only if\\nthe non-delayed network is ergodic as well. Bimolecular reactions are more\\ndifficult to consider but an analogous result is also obtained. These results\\ntell us that delays that are phase-type distributed, regardless of their\\ndistribution, are not harmful to the ergodicity property of reaction networks.\\nWe also prove that the presence of those delays adds convolution terms in the\\nmoment equation but does not change the value of the stationary means compared\\nto the delay-free case. Finally, the control of a certain class of delayed\\nstochastic reaction network using a delayed antithetic integral controller is\\nconsidered. It is proven that this controller achieves its goal provided that\\nthe delay-free network satisfy the conditions of ergodicity and\\noutput-controllability.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.09751</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.09751</id><submitter>Zirui Wang</submitter><version version=\"v1\"><date>Sat, 24 Nov 2018 03:26:10 GMT</date><size>1926kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 31 Mar 2019 21:31:51 GMT</date><size>1930kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 23 Sep 2019 02:05:54 GMT</date><size>1934kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Sat, 5 Oct 2019 03:43:52 GMT</date><size>1934kb</size><source_type>D</source_type></version><title>Characterizing and Avoiding Negative Transfer</title><authors>Zirui Wang, Zihang Dai, Barnab\\\\\\'as P\\\\\\'oczos, Jaime Carbonell</authors><categories>cs.LG stat.ML</categories><comments>Published at CVPR 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When labeled data is scarce for a specific target task, transfer learning\\noften offers an effective solution by utilizing data from a related source\\ntask. However, when transferring knowledge from a less related source, it may\\ninversely hurt the target performance, a phenomenon known as negative transfer.\\nDespite its pervasiveness, negative transfer is usually described in an\\ninformal manner, lacking rigorous definition, careful analysis, or systematic\\ntreatment. This paper proposes a formal definition of negative transfer and\\nanalyzes three important aspects thereof. Stemming from this analysis, a novel\\ntechnique is proposed to circumvent negative transfer by filtering out\\nunrelated source data. Based on adversarial networks, the technique is highly\\ngeneric and can be applied to a wide range of transfer learning algorithms. The\\nproposed approach is evaluated on six state-of-the-art deep transfer methods\\nvia experiments on four benchmark datasets with varying levels of difficulty.\\nEmpirically, the proposed method consistently improves the performance of all\\nbaseline methods and largely avoids negative transfer, even when the source\\ndata is degenerate.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.10050</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.10050</id><submitter>Daegeon Kim</submitter><version version=\"v1\"><date>Sun, 25 Nov 2018 16:36:30 GMT</date><size>1495kb</size></version><version version=\"v2\"><date>Sat, 7 Sep 2019 13:29:26 GMT</date><size>2293kb</size></version><version version=\"v3\"><date>Sun, 6 Oct 2019 08:15:17 GMT</date><size>2293kb</size></version><title>Automated Dataset Generation System for Collaborative Research of Cyber\\n  Threat Analysis</title><authors>Daegeon Kim and Huy Kang Kim</authors><categories>cs.CR</categories><comments>preprint version of paper published in Security and Communication\\n  Networks special issue on Data-Driven Cybersecurity</comments><doi>10.1155/2019/6268476</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objectives of cyberattacks are becoming sophisticated, and attackers are\\nconcealing their identity by masquerading as other attackers. Cyber threat\\nintelligence (CTI) is gaining attention as a way to collect meaningful\\nknowledge to better understand the intention of an attacker and eventually\\npredict future attacks. A systemic threat analysis based on data acquired from\\nactual cyber incidents is a useful approach to generating intelligence for such\\nan objective. Developing an analysis technique requires a high volume and fine\\nquality data. However, researchers can become discouraged by an inaccessibility\\nto data because organizations rarely release their data to the research\\ncommunity. Owing to a data inaccessibility issue, academic research tends to be\\nbiased toward techniques that develope steps of the CTI process other than\\nanalysis and production. In this paper, we propose an automated dataset\\ngeneration system called CTIMiner. The system collects threat data from\\npublicly available security reports and malware repositories. The data are\\nstored in a structured format. We released the source codes and dataset to the\\npublic, including approximately 640,000 records from 612 security reports\\npublished from January 2008 to June 2019. In addition, we present a statistical\\nfeature of the dataset and techniques that can be developed using it. Moreover,\\nwe demonstrate an application example of the dataset that analyzes the\\ncorrelation and characteristics of an incident. We believe our dataset will\\npromote collaborative research on threat analysis for the generation of CTI.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.10372</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.10372</id><submitter>Matija Pi\\\\v{s}korec</submitter><version version=\"v1\"><date>Mon, 26 Nov 2018 14:06:02 GMT</date><size>6457kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 27 Nov 2018 11:26:00 GMT</date><size>6457kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 27 Mar 2019 16:35:42 GMT</date><size>4618kb</size><source_type>D</source_type></version><title>Disentangling sources of influence in online social networks</title><authors>Matija Pi\\\\v{s}korec, Tomislav \\\\v{S}muc, Mile \\\\v{S}iki\\\\\\'c</authors><categories>cs.SI physics.soc-ph</categories><journal-ref>IEEE Access, vol. 7, pp. 131692-131704 (2019)</journal-ref><doi>10.1109/ACCESS.2019.2940762</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information propagation in online social networks is facilitated by two types\\nof influence - endogenous (peer) influence that acts between users of the\\nsocial network and exogenous (external) that corresponds to various external\\nmediators such as online news media. However, inference of these influences\\nfrom data remains a challenge, especially when data on the activation of users\\nis scarce. In this paper we propose a methodology that yields estimates of both\\nendogenous and exogenous influence using only a social network structure and a\\nsingle activation cascade. Our method exploits the statistical differences\\nbetween the two types of influence - endogenous is dependent on the social\\nnetwork structure and current state of each user while exogenous is independent\\nof these. We evaluate our methodology on simulated activation cascades as well\\nas on cascades obtained from several large Facebook political survey\\napplications. We show that our methodology is able to provide estimates of\\nendogenous and exogenous influence in online social networks, characterize\\nactivation of each individual user as being endogenously or exogenously driven,\\nand identify most influential groups of users.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.10433</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.10433</id><submitter>Buser Say</submitter><version version=\"v1\"><date>Mon, 26 Nov 2018 14:59:29 GMT</date><size>1208kb</size><source_type>D</source_type></version><version version=\"v10\"><date>Tue, 9 Apr 2019 00:23:16 GMT</date><size>1210kb</size><source_type>D</source_type></version><version version=\"v11\"><date>Thu, 3 Oct 2019 13:08:37 GMT</date><size>1518kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 27 Nov 2018 18:48:51 GMT</date><size>1208kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 28 Nov 2018 02:08:22 GMT</date><size>1208kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Thu, 29 Nov 2018 15:31:00 GMT</date><size>1209kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Fri, 30 Nov 2018 17:15:31 GMT</date><size>1209kb</size><source_type>D</source_type></version><version version=\"v6\"><date>Thu, 6 Dec 2018 18:13:01 GMT</date><size>1210kb</size><source_type>D</source_type></version><version version=\"v7\"><date>Fri, 7 Dec 2018 14:38:28 GMT</date><size>1210kb</size><source_type>D</source_type></version><version version=\"v8\"><date>Mon, 10 Dec 2018 02:21:17 GMT</date><size>1210kb</size><source_type>D</source_type></version><version version=\"v9\"><date>Thu, 10 Jan 2019 11:25:20 GMT</date><size>1210kb</size><source_type>D</source_type></version><title>Compact and Efficient Encodings for Planning in Factored State and\\n  Action Spaces with Learned Binarized Neural Network Transition Models</title><authors>Buser Say, Scott Sanner</authors><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we leverage the efficiency of Binarized Neural Networks (BNNs)\\nto learn complex state transition models of planning domains with discretized\\nfactored state and action spaces. In order to directly exploit this transition\\nstructure for planning, we present two novel compilations of the learned\\nfactored planning problem with BNNs based on reductions to Weighted Partial\\nMaximum Boolean Satisfiability (FD-SAT-Plan+) as well as Binary Linear\\nProgramming (FD-BLP-Plan+). Theoretically, we show that our SAT-based\\nBi-Directional Neuron Activation Encoding is asymptotically the most compact\\nencoding in the literature and maintains the generalized arc-consistency\\nproperty through unit propagation -- an important property that facilitates\\nefficiency in SAT solvers. Experimentally, we validate the computational\\nefficiency of our Bi-Directional Neuron Activation Encoding in comparison to an\\nexisting neuron activation encoding and demonstrate the effectiveness of\\nlearning complex transition models with BNNs. We test the runtime efficiency of\\nboth FD-SAT-Plan+ and FD-BLP-Plan+ on the learned factored planning problem\\nshowing that FD-SAT-Plan+ scales better with increasing BNN size and\\ncomplexity. Finally, we present a finite-time incremental constraint generation\\nalgorithm based on generalized landmark constraints to improve the planning\\naccuracy of our encodings through simulated or real-world interaction.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.10469</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.10469</id><submitter>Jinwei Zhao</submitter><version version=\"v1\"><date>Wed, 21 Nov 2018 15:36:35 GMT</date><size>1418kb</size></version><version version=\"v2\"><date>Sat, 5 Oct 2019 04:24:27 GMT</date><size>1508kb</size></version><title>How to improve the interpretability of kernel learning</title><authors>Jinwei Zhao, Qizhou Wang, Yufei Wang, Yu Liu, Zhenghao Shi, Xinhong\\n  Hei</authors><categories>cs.LG stat.ML</categories><comments>arXiv admin note: text overlap with arXiv:1811.07747</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, machine learning researchers have focused on methods to\\nconstruct flexible and interpretable prediction models. However, an\\ninterpretability evaluation, a relationship between generalization performance\\nand an interpretability of the model and a method for improving the\\ninterpretability have to be considered. In this paper, a quantitative index of\\nthe interpretability is proposed and its rationality is proved, and equilibrium\\nproblem between the interpretability and the generalization performance is\\nanalyzed. Probability upper bound of the sum of the two performances is\\nanalyzed. For traditional supervised kernel machine learning problem, a\\nuniversal learning framework is put forward to solve the equilibrium problem\\nbetween the two performances. The condition for global optimal solution based\\non the framework is deduced. The learning framework is applied to the\\nleast-squares support vector machine and is evaluated by some experiments.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.11872</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.11872</id><submitter>Giovanni Poggi</submitter><version version=\"v1\"><date>Wed, 28 Nov 2018 23:05:35 GMT</date><size>9488kb</size><source_type>D</source_type></version><title>Guided patch-wise nonlocal SAR despeckling</title><authors>Sergio Vitale, Davide Cozzolino, Giuseppe Scarpa, Luisa Verdoliva,\\n  Giovanni Poggi</authors><categories>cs.CV</categories><doi>10.1109/TGRS.2019.2906412</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method for SAR image despeckling which leverages information\\ndrawn from co-registered optical imagery. Filtering is performed by plain\\npatch-wise nonlocal means, operating exclusively on SAR data. However, the\\nfiltering weights are computed by taking into account also the optical guide,\\nwhich is much cleaner than the SAR data, and hence more discriminative. To\\navoid injecting optical-domain information into the filtered image, a\\nSAR-domain statistical test is preliminarily performed to reject right away any\\nrisky predictor. Experiments on two SAR-optical datasets prove the proposed\\nmethod to suppress very effectively the speckle, preserving structural details,\\nand without introducing visible filtering artifacts. Overall, the proposed\\nmethod compares favourably with all state-of-the-art despeckling filters, and\\nalso with our own previous optical-guided filter.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.11922</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.11922</id><submitter>Zhuoyi Yang</submitter><version version=\"v1\"><date>Thu, 29 Nov 2018 02:05:09 GMT</date><size>631kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 20 Sep 2019 20:23:16 GMT</date><size>631kb</size><source_type>D</source_type></version><title>Distributed Inference for Linear Support Vector Machine</title><authors>Xiaozhou Wang, Zhuoyi Yang, Xi Chen, Weidong Liu</authors><categories>stat.ML cs.LG stat.ME</categories><comments>50 pages, 11 figures</comments><journal-ref>Journal of Machine Learning Research (JMLR), v20(113):1-41, 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing size of modern data brings many new challenges to existing\\nstatistical inference methodologies and theories, and calls for the development\\nof distributed inferential approaches. This paper studies distributed inference\\nfor linear support vector machine (SVM) for the binary classification task.\\nDespite a vast literature on SVM, much less is known about the inferential\\nproperties of SVM, especially in a distributed setting. In this paper, we\\npropose a multi-round distributed linear-type (MDL) estimator for conducting\\ninference for linear SVM. The proposed estimator is computationally efficient.\\nIn particular, it only requires an initial SVM estimator and then successively\\nrefines the estimator by solving simple weighted least squares problem.\\nTheoretically, we establish the Bahadur representation of the estimator. Based\\non the representation, the asymptotic normality is further derived, which shows\\nthat the MDL estimator achieves the optimal statistical efficiency, i.e., the\\nsame efficiency as the classical linear SVM applying to the entire data set in\\na single machine setup. Moreover, our asymptotic result avoids the condition on\\nthe number of machines or data batches, which is commonly assumed in\\ndistributed estimation literature, and allows the case of diverging dimension.\\nWe provide simulation studies to demonstrate the performance of the proposed\\nMDL estimator.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.12212</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.12212</id><submitter>Philipp Otte</submitter><version version=\"v1\"><date>Thu, 29 Nov 2018 14:49:32 GMT</date><size>191kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 07:31:23 GMT</date><size>359kb</size><source_type>D</source_type></version><title>A Structured Approach to the Construction of Stable Linear\\n  Lattice-Boltzmann Collision Operators</title><authors>Philipp Otte and Martin Frank</authors><categories>math.NA cs.NA</categories><msc-class>65M12</msc-class><journal-ref>Computers &amp; Mathematics with Applications, 2019</journal-ref><doi>10.1016/j.camwa.2019.09.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a structured approach to the construction of linear BGK-type\\ncollision operators ensuring that the resulting Lattice-Boltzmann methods are\\nstable with respect to a weighted $L^2$-norm. The results hold for particular\\nboundary conditions including periodic, bounce-back, and bounce-back with\\nflipping of sign. This construction uses the equivalent moment-space definition\\nof BGK-type collision operators and the notion of stability structures as\\nguiding principle for the choice of the equilibrium moments for those moments\\ninfluencing the error term only but not the order of consistency. The presented\\nstructured approach is then applied to the 3D isothermal linearized Euler\\nequations with non-vanishing background velocity. Finally, convergence results\\nin the strong discrete $L^\\\\infty$-norm highlight the suitability of the\\nstructured approach introduced in this manuscript.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1811.12819</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1811.12819</id><submitter>Klaus Albert</submitter><version version=\"v1\"><date>Thu, 29 Nov 2018 15:24:51 GMT</date><size>975kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 13:45:02 GMT</date><size>1268kb</size><source_type>D</source_type></version><title>Structure-Preserving Constrained Optimal Trajectory Planning of a\\n  Wheeled Inverted Pendulum</title><authors>Klaus Albert, Karmvir Singh Phogat, Felix Anhalt, Ravi N Banavar,\\n  Debasish Chatterjee, Boris Lohmann</authors><categories>cs.SY cs.RO</categories><comments>12 pages, 8 figures, 1 table. arXiv admin note: text overlap with\\n  arXiv:1710.10932</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Wheeled Inverted Pendulum (WIP) is an underactuated, nonholonomic\\nmechatronic system, and has been popularized commercially as the Segway.\\nDesigning a control law for motion planning, that incorporates the state and\\ncontrol constraints, while respecting the configuration manifold, is a\\nchallenging problem. In this article we derive a discrete-time model of the WIP\\nsystem using discrete mechanics and generate optimal trajectories for the WIP\\nsystem by solving a discrete-time constrained optimal control problem. Further,\\nwe describe a nonlinear continuous-time model with parameters for designing a\\nclosed loop LQ-controller. A dual control architecture is implemented in which\\nthe designed optimal trajectory is then provided as a reference to the robot\\nwith the optimal control trajectory as a feedforward control action, and an\\nLQ-controller in the feedback mode is employed to mitigate noise and\\ndisturbances for ensuing stable motion of the WIP system. While performing\\nexperiments on the WIP system involving aggressive maneuvers with fairly sharp\\nturns, we found a high degree of congruence in the designed optimal\\ntrajectories and the path traced by the robot while tracking these\\ntrajectories. This corroborates the validity of the nonlinear model and the\\ncontrol scheme. Finally, these experiments demonstrate the highly nonlinear\\nnature of the WIP system and robustness of the control scheme.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.00965</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.00965</id><submitter>Markus Haltmeier</submitter><version version=\"v1\"><date>Mon, 3 Dec 2018 18:39:26 GMT</date><size>13kb</size></version><version version=\"v2\"><date>Fri, 26 Jul 2019 10:43:08 GMT</date><size>600kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sat, 14 Sep 2019 15:19:49 GMT</date><size>904kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Sat, 28 Sep 2019 06:18:26 GMT</date><size>904kb</size><source_type>D</source_type></version><title>Big in Japan: Regularizing networks for solving inverse problems</title><authors>Johannes Schwab, Stephan Antholzer, Markus Haltmeier</authors><categories>math.NA cs.NA</categories><doi>10.1007/s10851-019-00911-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning and (deep) neural networks are emerging tools to address\\ninverse problems and image reconstruction tasks. Despite outstanding\\nperformance, the mathematical analysis for solving inverse problems by neural\\nnetworks is mostly missing. In this paper, we introduce and rigorously analyze\\nfamilies of deep regularizing neural networks (RegNets) of the form $B_\\\\alpha +\\nN_{\\\\theta(\\\\alpha)} B_\\\\alpha $, where $B_\\\\alpha$ is a classical regularization\\nand the network $N_{\\\\theta(\\\\alpha)} B_\\\\alpha $ is trained to recover the\\nmissing part $\\\\operatorname{Id}_X - B_\\\\alpha$ not found by the classical\\nregularization. We show that these regularizing networks yield a convergent\\nregularization method for solving inverse problems. Additionally, we derive\\nconvergence rates (quantitative error estimates) assuming a sufficient decay of\\nthe associated distance function. We demonstrate that our results recover\\nexisting convergence and convergence rates results for filter-based\\nregularization methods as well as the recently introduced null space network as\\nspecial cases. Numerical results are presented for a tomographic sparse data\\nproblem, which clearly demonstrate that the proposed RegNets improve the\\nclassical regularization as well as the null space network.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.00992</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.00992</id><submitter>Irene C\\\\\\'ordoba</submitter><version version=\"v1\"><date>Sun, 2 Dec 2018 15:53:24 GMT</date><size>1123kb</size><source_type>D</source_type></version><title>Ann: A domain-specific language for the effective design and validation\\n  of Java annotations</title><authors>Irene C\\\\\\'ordoba, Juan de Lara</authors><categories>cs.PL cs.SE</categories><comments>45 pages, 14 figures, 2016 journal publication. arXiv admin note:\\n  text overlap with arXiv:1807.03566</comments><journal-ref>Computer Languages, Systems and Structures, 45:164-190, 2016</journal-ref><doi>10.1016/j.cl.2016.02.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a new modelling language for the effective design and\\nvalidation of Java annotations. Since their inclusion in the 5th edition of\\nJava, annotations have grown from a useful tool for the addition of meta-data\\nto play a central role in many popular software projects. Usually they are not\\nconceived in isolation, but in groups, with dependency and integrity\\nconstraints between them. However, the native support provided by Java for\\nexpressing this design is very limited.\\n  To overcome its deficiencies and make explicit the rich conceptual model\\nwhich lies behind a set of annotations, we propose a domain-specific modelling\\nlanguage. The proposal has been implemented as an Eclipse plug-in, including an\\neditor and an integrated code generator that synthesises annotation processors.\\nThe environment also integrates a model finder, able to detect unsatisfiable\\nconstraints between different annotations, and to provide examples of correct\\nannotation usages for validation. The language has been tested using a real set\\nof annotations from the Java Persistence API (JPA). Within this subset we have\\nfound enough rich semantics expressible with Ann and omitted nowadays by the\\nJava language, which shows the benefits of Ann in a relevant field of\\napplication.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.01082</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.01082</id><submitter>Zhiyu Sun</submitter><version version=\"v1\"><date>Mon, 3 Dec 2018 21:11:48 GMT</date><size>8665kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 13 Apr 2019 03:44:26 GMT</date><size>34067kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 4 Oct 2019 07:40:15 GMT</date><size>30935kb</size><source_type>D</source_type></version><title>ZerNet: Convolutional Neural Networks on Arbitrary Surfaces via Zernike\\n  Local Tangent Space Estimation</title><authors>Zhiyu Sun, Ethan Rooke, Jerome Charton, Yusen He, Jia Lu and Stephen\\n  Baek</authors><categories>cs.CV cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel formulation to extend CNNs to\\ntwo-dimensional (2D) manifolds using orthogonal basis functions, called Zernike\\npolynomials. In many areas, geometric features play a key role in understanding\\nscientific phenomena. Thus, an ability to codify geometric features into a\\nmathematical quantity can be critical. Recently, convolutional neural networks\\n(CNNs) have demonstrated the promising capability of extracting and codifying\\nfeatures from visual information. However, the progress has been concentrated\\nin computer vision applications where there exists an inherent grid-like\\nstructure. In contrast, many geometry processing problems are defined on curved\\nsurfaces, and the generalization of CNNs is not quite trivial. The difficulties\\nare rooted in the lack of key ingredients such as the canonical grid-like\\nrepresentation, the notion of consistent orientation, and a compatible local\\ntopology across the domain. In this paper, we prove that the convolution of two\\nfunctions can be represented as a simple dot product between Zernike polynomial\\ncoefficients; and the rotation of a convolution kernel is essentially a set of\\n2-by-2 rotation matrices applied to the coefficients. As such, the key\\ncontribution of this work resides in a concise but rigorous mathematical\\ngeneralization of the CNN building blocks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.01233</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.01233</id><submitter>Roei Herzig</submitter><version version=\"v1\"><date>Tue, 4 Dec 2018 05:58:20 GMT</date><size>8798kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 16:57:16 GMT</date><size>7868kb</size><source_type>D</source_type></version><title>Spatio-Temporal Action Graph Networks</title><authors>Roei Herzig, Elad Levi, Huijuan Xu, Hang Gao, Eli Brosh, Xiaolong\\n  Wang, Amir Globerson, Trevor Darrell</authors><categories>cs.CV</categories><comments>IEEE/CVF International Conference on Computer Vision Workshop\\n  (ICCVW), 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Events defined by the interaction of objects in a scene are often of critical\\nimportance; yet important events may have insufficient labeled examples to\\ntrain a conventional deep model to generalize to future object appearance.\\nActivity recognition models that represent object interactions explicitly have\\nthe potential to learn in a more efficient manner than those that represent\\nscenes with global descriptors. We propose a novel inter-object graph\\nrepresentation for activity recognition based on a disentangled graph embedding\\nwith direct observation of edge appearance. We employ a novel factored\\nembedding of the graph structure, disentangling a representation hierarchy\\nformed over spatial dimensions from that found over temporal variation. We\\ndemonstrate the effectiveness of our model on the Charades activity recognition\\nbenchmark, as well as a new dataset of driving activities focusing on\\nmulti-object interactions with near-collision events. Our model offers\\nsignificantly improved performance compared to baseline approaches without\\nobject-graph representations, or with previous graph-based models.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.01319</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.01319</id><submitter>Fedor Ratnikov</submitter><version version=\"v1\"><date>Tue, 4 Dec 2018 10:36:17 GMT</date><size>456kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 6 Apr 2019 12:32:13 GMT</date><size>456kb</size><source_type>D</source_type></version><title>Generative Models for Fast Calorimeter Simulation.LHCb case</title><authors>Viktoria Chekalina, Elena Orlova, Fedor Ratnikov, Dmitry Ulyanov,\\n  Andrey Ustyuzhanin, and Egor Zakharov</authors><categories>physics.data-an cs.LG</categories><comments>Proceedings of the presentation at CHEP 2018 Conference</comments><doi>10.1051/epjconf/201921402034</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulation is one of the key components in high energy physics. Historically\\nit relies on the Monte Carlo methods which require a tremendous amount of\\ncomputation resources. These methods may have difficulties with the expected\\nHigh Luminosity Large Hadron Collider (HL LHC) need, so the experiment is in\\nurgent need of new fast simulation techniques. We introduce a new Deep Learning\\nframework based on Generative Adversarial Networks which can be faster than\\ntraditional simulation methods by 5 order of magnitude with reasonable\\nsimulation accuracy. This approach will allow physicists to produce a big\\nenough amount of simulated data needed by the next HL LHC experiments using\\nlimited computing resources.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.01393</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.01393</id><submitter>Yongchao Xu</submitter><version version=\"v1\"><date>Tue, 4 Dec 2018 13:12:58 GMT</date><size>3743kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 29 Jul 2019 06:18:33 GMT</date><size>5183kb</size><source_type>D</source_type></version><title>TextField: Learning A Deep Direction Field for Irregular Scene Text\\n  Detection</title><authors>Yongchao Xu, Yukang Wang, Wei Zhou, Yongpan Wang, Zhibo Yang, Xiang\\n  Bai</authors><categories>cs.CV</categories><comments>To appear in IEEE TIP</comments><doi>10.1109/TIP.2019.2900589</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scene text detection is an important step of scene text reading system. The\\nmain challenges lie on significantly varied sizes and aspect ratios, arbitrary\\norientations and shapes. Driven by recent progress in deep learning, impressive\\nperformances have been achieved for multi-oriented text detection. Yet, the\\nperformance drops dramatically in detecting curved texts due to the limited\\ntext representation (e.g., horizontal bounding boxes, rotated rectangles, or\\nquadrilaterals). It is of great interest to detect curved texts, which are\\nactually very common in natural scenes. In this paper, we present a novel text\\ndetector named TextField for detecting irregular scene texts. Specifically, we\\nlearn a direction field pointing away from the nearest text boundary to each\\ntext point. This direction field is represented by an image of two-dimensional\\nvectors and learned via a fully convolutional neural network. It encodes both\\nbinary text mask and direction information used to separate adjacent text\\ninstances, which is challenging for classical segmentation-based approaches.\\nBased on the learned direction field, we apply a simple yet effective\\nmorphological-based post-processing to achieve the final detection.\\nExperimental results show that the proposed TextField outperforms the\\nstate-of-the-art methods by a large margin (28% and 8%) on two curved text\\ndatasets: Total-Text and CTW1500, respectively, and also achieves very\\ncompetitive performance on multi-oriented datasets: ICDAR 2015 and MSRA-TD500.\\nFurthermore, TextField is robust in generalizing to unseen datasets. The code\\nis available at https://github.com/YukangWang/TextField.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.01467</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.01467</id><submitter>Madelon de Kemp</submitter><version version=\"v1\"><date>Tue, 4 Dec 2018 14:53:43 GMT</date><size>65kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 08:34:15 GMT</date><size>78kb</size><source_type>D</source_type></version><title>Performance of the smallest-variance-first rule in appointment\\n  sequencing</title><authors>Madelon A. de Kemp, Michel Mandjes, Neil Olver</authors><categories>math.PR cs.DS math.OC</categories><comments>54 pages, 2 figures</comments><msc-class>90B36 (Primary), 68M20, 60K30, 68W25 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A classical problem in appointment scheduling, with applications in health\\ncare, concerns the determination of the patients\\' arrival times that minimize a\\ncost function that is a weighted sum of mean waiting times and mean idle times.\\nOne aspect of this problem is the sequencing problem, which focuses on ordering\\nthe patients. We assess the performance of the smallest-variance-first (SVF)\\nrule, which sequences patients in order of increasing variance of their service\\ndurations. While it was known that SVF is not always optimal, it has been\\nwidely observed that it performs well in practice and simulation. We provide a\\ntheoretical justification for this observation by proving, in various settings,\\nquantitative worst-case bounds on the ratio between the cost incurred by the\\nSVF rule and the minimum attainable cost. We also show that, in great\\ngenerality, SVF is asymptotically optimal, i.e., the ratio approaches 1 as the\\nnumber of patients grows large. While evaluating policies by considering an\\napproximation ratio is a standard approach in many algorithmic settings, our\\nresults appear to be the first of this type in the appointment scheduling\\nliterature.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.02307</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.02307</id><submitter>Mario Graff</submitter><version version=\"v1\"><date>Thu, 29 Nov 2018 23:33:59 GMT</date><size>1306kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 23 Mar 2019 03:12:40 GMT</date><size>1127kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 14 Jun 2019 18:10:24 GMT</date><size>891kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Mon, 30 Sep 2019 16:52:27 GMT</date><size>896kb</size><source_type>D</source_type></version><title>EvoMSA: A Multilingual Evolutionary Approach for Sentiment Analysis</title><authors>Mario Graff and Sabino Miranda-Jim\\\\\\'enez and Eric S. Tellez and\\n  Daniela Moctezuma</authors><categories>cs.CL cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sentiment analysis (SA) is a task related to understanding people\\'s feelings\\nin written text; the starting point would be to identify the polarity level\\n(positive, neutral or negative) of a given text, moving on to identify emotions\\nor whether a text is humorous or not. This task has been the subject of several\\nresearch competitions in a number of languages, e.g., English, Spanish, and\\nArabic, among others. In this contribution, we propose an SA system, namely\\nEvoMSA, that unifies our participating systems in various SA competitions,\\nmaking it domain independent and multilingual by processing text using only\\nlanguage-independent techniques. EvoMSA is a classifier, based on Genetic\\nProgramming, that works by combining the output of different text classifiers\\nand text models to produce the final prediction. We analyze EvoMSA on different\\nSA competitions to provide a global overview of its performance, and as the\\nresults show, EvoMSA is competitive obtaining top rankings in several SA\\ncompetitions. Furthermore, we performed an analysis of EvoMSA\\'s components to\\nmeasure their contribution to the performance; the idea is to facilitate a\\npractitioner or newcomer to implement a competitive SA classifier. Finally, it\\nis worth to mention that EvoMSA is available as open-source software.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.02464</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.02464</id><submitter>Craig Atkinson</submitter><version version=\"v1\"><date>Thu, 6 Dec 2018 11:20:18 GMT</date><size>227kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 24 Feb 2019 22:31:47 GMT</date><size>170kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 26 Aug 2019 02:16:29 GMT</date><size>932kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Mon, 7 Oct 2019 01:42:20 GMT</date><size>932kb</size><source_type>D</source_type></version><title>Pseudo-Rehearsal: Achieving Deep Reinforcement Learning without\\n  Catastrophic Forgetting</title><authors>Craig Atkinson and Brendan McCane and Lech Szymanski and Anthony\\n  Robins</authors><categories>cs.LG cs.AI cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural networks can achieve extraordinary results on a wide variety of tasks.\\nHowever, when they attempt to sequentially learn a number of tasks, they tend\\nto learn the new task while destructively forgetting previous tasks. One\\nsolution to this problem is pseudo-rehearsal, which involves learning the new\\ntask while rehearsing generated items representative of previous tasks. Our\\nmodel combines pseudo-rehearsal with a deep generative model and a dual memory\\nsystem, resulting in a method that does not demand additional storage\\nrequirements as the number of tasks increase. Our model iteratively learns\\nthree Atari 2600 games while retaining above human level performance on all\\nthree games and performing as well as a set of networks individually trained on\\nthe tasks. This result is achieved without revisiting or storing raw data from\\npast tasks. Furthermore, previous state-of-the-art solutions demonstrate\\nsubstantial forgetting compared to our model on these complex deep\\nreinforcement learning tasks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.02621</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.02621</id><submitter>Mihir Chauhan</submitter><version version=\"v1\"><date>Mon, 19 Nov 2018 02:02:28 GMT</date><size>1266kb</size><source_type>D</source_type></version><title>Hybrid Feature Learning for Handwriting Verification</title><authors>Mohammad Abuzar Shaikh, Mihir Chauhan, Jun Chu and Sargur Srihari</authors><categories>cs.CV</categories><comments>Accepted and presented in International Conference on Frontiers in\\n  Handwriting Recognition (ICFHR) 2018</comments><doi>10.1109/ICFHR-2018.2018.00041</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an effective Hybrid Deep Learning (HDL) architecture for the task\\nof determining the probability that a questioned handwritten word has been\\nwritten by a known writer. HDL is an amalgamation of Auto-Learned Features\\n(ALF) and Human-Engineered Features (HEF). To extract auto-learned features we\\nuse two methods: First, Two Channel Convolutional Neural Network (TC-CNN);\\nSecond, Two Channel Autoencoder (TC-AE). Furthermore, human-engineered features\\nare extracted by using two methods: First, Gradient Structural Concavity (GSC);\\nSecond, Scale Invariant Feature Transform (SIFT). Experiments are performed by\\ncomplementing one of the HEF methods with one ALF method on 150000 pairs of\\nsamples of the word &quot;AND&quot; cropped from handwritten notes written by 1500\\nwriters. Our results indicate that HDL architecture with AE-GSC achieves 99.7%\\naccuracy on seen writer dataset and 92.16% accuracy on shuffled writer dataset\\nwhich out performs CEDAR-FOX, as for unseen writer dataset, AE-SIFT performs\\ncomparable to this sophisticated handwriting comparison tool.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.02975</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.02975</id><submitter>Kevin Alexander Laube</submitter><version version=\"v1\"><date>Fri, 7 Dec 2018 10:53:31 GMT</date><size>153kb</size><source_type>D</source_type></version><title>ShuffleNASNets: Efficient CNN models through modified Efficient Neural\\n  Architecture Search</title><authors>Kevin Alexander Laube and Andreas Zell</authors><categories>cs.LG stat.ML</categories><comments>6 pages, 5 figures, 1 table</comments><doi>10.1109/IJCNN.2019.8852294</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural network architectures found by sophistic search algorithms achieve\\nstrikingly good test performance, surpassing most human-crafted network models\\nby significant margins. Although computationally efficient, their design is\\noften very complex, impairing execution speed. Additionally, finding models\\noutside of the search space is not possible by design. While our space is still\\nlimited, we implement undiscoverable expert knowledge into the economic search\\nalgorithm Efficient Neural Architecture Search (ENAS), guided by the design\\nprinciples and architecture of ShuffleNet V2. While maintaining baseline-like\\n2.85% test error on CIFAR-10, our ShuffleNASNets are significantly less\\ncomplex, require fewer parameters, and are two times faster than the ENAS\\nbaseline in a classification task. These models also scale well to a low\\nparameter space, achieving less than 5% test error with little regularization\\nand only 236K parameters.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.03145</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.03145</id><submitter>Oksana Shadura</submitter><version version=\"v1\"><date>Fri, 7 Dec 2018 18:09:25 GMT</date><size>1524kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 11 Dec 2018 14:55:19 GMT</date><size>632kb</size></version><title>Extending ROOT through Modules</title><authors>Oksana Shadura (1), Brian Paul Bockelman (1) and Vassil Vassilev (2)\\n  ((1) University of Nebraska Lincoln, (2) Princeton University)</authors><categories>cs.SE</categories><comments>8 pages, 2 figures, 1 listing, CHEP 2018 - 23rd International\\n  Conference on Computing in High Energy and Nuclear Physics</comments><doi>10.1051/epjconf/201921405011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ROOT software framework is foundational for the HEP ecosystem, providing\\ncapabilities such as IO, a C++ interpreter, GUI, and math libraries. It uses\\nobject-oriented concepts and build-time components to layer between them. We\\nbelieve additional layering formalisms will benefit ROOT and its users. We\\npresent the modularization strategy for ROOT which aims to formalize the\\ndescription of existing source components, making available the dependencies\\nand other metadata externally from the build system, and allow post-install\\nadditions of functionality in the runtime environment. components can then be\\ngrouped into packages, installable from external repositories to deliver\\npost-install step of missing packages. This provides a mechanism for the wider\\nsoftware ecosystem to interact with a minimalistic install. Reducing\\nintra-component dependencies improves maintainability and code hygiene. We\\nbelieve helping maintain the smallest &quot;base install&quot; possible will help\\nembedding use cases. The modularization effort draws inspiration from the Java,\\nPython, and Swift ecosystems. Keeping aligned with the modern C++, this\\nstrategy relies on forthcoming features such as C++ modules. We hope\\nformalizing the component layer will provide simpler ROOT installs, improve\\nextensibility, and decrease the complexity of embedding in other ecosystems\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.03149</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.03149</id><submitter>Oksana Shadura</submitter><version version=\"v1\"><date>Fri, 7 Dec 2018 18:17:03 GMT</date><size>632kb</size></version><version version=\"v2\"><date>Tue, 11 Dec 2018 15:01:59 GMT</date><size>1524kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 21 Feb 2019 10:31:39 GMT</date><size>1427kb</size><source_type>D</source_type></version><title>Continuous Performance Benchmarking Framework for ROOT</title><authors>Oksana Shadura (1), Vassil Vassilev (2), Brian Paul Bockelman (1) ((1)\\n  University of Nebraska-Lincoln, (2) Princeton University)</authors><categories>cs.SE</categories><comments>8 pages, 5 figures, CHEP 2018 - 23rd International Conference on\\n  Computing in High Energy and Nuclear Physics</comments><doi>10.1051/epjconf/201921405003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Foundational software libraries such as ROOT are under intense pressure to\\navoid software regression, including performance regressions. Continuous\\nperformance benchmarking, as a part of continuous integration and other code\\nquality testing, is an industry best-practice to understand how the performance\\nof a software product evolves over time. We present a framework, built from\\nindustry best practices and tools, to help to understand ROOT code performance\\nand monitor the efficiency of the code for a several processor architectures.\\nIt additionally allows historical performance measurements for ROOT I/O,\\nvectorization and parallelization sub-systems.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.03859</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.03859</id><submitter>Gennady Ososkov Alexeevich</submitter><version version=\"v1\"><date>Fri, 7 Dec 2018 08:18:35 GMT</date><size>492kb</size></version><title>The particle track reconstruction based on deep learning neural networks</title><authors>Dmitriy Baranov, Sergey Mitsyn, Pavel Goncharov, Gennady Ososkov</authors><categories>cs.LG stat.ML</categories><comments>8 pages, 3 figures, CHEP 2018, the 23rd International Conference on\\n  Computing in High Energy and Nuclear Physics, Sofia, Bulgaria on July 9-13,\\n  2018. arXiv admin note: text overlap with arXiv:1811.06002</comments><msc-class>I.2.6</msc-class><doi>10.1051/epjconf/201921406018</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  One of the most important problems of data processing in high energy and\\nnuclear physics is the event reconstruction. Its main part is the track\\nreconstruction procedure which consists in looking for all tracks that\\nelementary particles leave when they pass through a detector among a huge\\nnumber of points, so-called hits, produced when flying particles fire detector\\ncoordinate planes. Unfortunately, the tracking is seriously impeded by the\\nfamous shortcoming of multiwired, strip in GEM detectors due to the appearance\\nin them a lot of fake hits caused by extra spurious crossings of fired strips.\\nSince the number of those fakes is several orders of magnitude greater than for\\ntrue hits, one faces with the quite serious difficulty to unravel possible\\ntrack-candidates via true hits ignoring fakes. On the basis of our previous\\ntwo-stage approach based on hits preprocessing using directed K-d tree search\\nfollowed by a deep neural classifier we introduce here two new tracking\\nalgorithms. Both algorithms combine those two stages in one while using\\ndifferent types of deep neural nets. We show that both proposed deep networks\\ndo not require any special preprocessing stage, are more accurate, faster and\\ncan be easier parallelized. Preliminary results of our new approaches for\\nsimulated events are presented.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.03960</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.03960</id><submitter>Sandor Kisfaludi-Bak</submitter><version version=\"v1\"><date>Mon, 10 Dec 2018 18:15:49 GMT</date><size>1233kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 13:07:40 GMT</date><size>2370kb</size><source_type>D</source_type></version><title>Hyperbolic intersection graphs and (quasi)-polynomial time</title><authors>S\\\\\\'andor Kisfaludi-Bak</authors><categories>cs.CG</categories><comments>Short version appears in SODA 2020</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study unit ball graphs (and, more generally, so-called noisy uniform ball\\ngraphs) in $d$-dimensional hyperbolic space, which we denote by $\\\\mathbb{H}^d$.\\nUsing a new separator theorem, we show that unit ball graphs in $\\\\mathbb{H}^d$\\nenjoy similar properties as their Euclidean counterparts, but in one dimension\\nlower: many standard graph problems, such as Independent Set, Dominating Set,\\nSteiner Tree, and Hamiltonian Cycle can be solved in $2^{O(n^{1-1/(d-1)})}$\\ntime for any fixed $d\\\\geq 3$, while the same problems need $2^{O(n^{1-1/d})}$\\ntime in $\\\\mathbb{R}^d$. We also show that these algorithms in $\\\\mathbb{H}^d$\\nare optimal up to constant factors in the exponent under ETH.\\n  This drop in dimension has the largest impact in $\\\\mathbb{H}^2$, where we\\nintroduce a new technique to bound the treewidth of noisy uniform disk graphs.\\nThe bounds yield quasi-polynomial ($n^{O(\\\\log n)}$) algorithms for all of the\\nstudied problems, while in the case of Hamiltonian Cycle and $3$-Coloring we\\neven get polynomial time algorithms. Furthermore, if the underlying noisy disks\\nin $\\\\mathbb{H}^2$ have constant maximum degree, then all studied problems can\\nbe solved in polynomial time. This contrasts with the fact that these problems\\nrequire $2^{\\\\Omega(\\\\sqrt{n})}$ time under ETH in constant maximum degree\\nEuclidean unit disk graphs.\\n  Finally, we complement our quasi-polynomial algorithm for Independent Set in\\nnoisy uniform disk graphs with a matching $n^{\\\\Omega(\\\\log n)}$ lower bound\\nunder ETH. This shows that the hyperbolic plane is a potential source of\\nNP-intermediate problems.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.03992</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.03992</id><submitter>Yuka Takahashi</submitter><version version=\"v1\"><date>Mon, 10 Dec 2018 12:52:09 GMT</date><size>673kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 17 May 2019 06:13:02 GMT</date><size>674kb</size><source_type>D</source_type></version><title>Optimizing Frameworks Performance Using C++ Modules Aware ROOT</title><authors>Yuka Takahashi (1 and 2), Vassil Vassilev (1), Oksana Shadura (3),\\n  Raphael Isemann (2 and 4) ((1) Princeton University (2) CERN (3) University\\n  of Nebraska Lincoln (4) Chalmers University of Technology)</authors><categories>cs.PL</categories><comments>8 pages, 3 figures, 6 listing, CHEP 2018 - 23rd International\\n  Conference on Computing in High Energy and Nuclear Physics</comments><doi>10.1051/epjconf/201921402011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ROOT is a data analysis framework broadly used in and outside of High Energy\\nPhysics (HEP). Since HEP software frameworks always strive for performance\\nimprovements, ROOT was extended with experimental support of runtime C++\\nModules. C++ Modules are designed to improve the performance of C++ code\\nparsing. C++ Modules offers a promising way to improve ROOT\\'s runtime\\nperformance by saving the C++ header parsing time which happens during ROOT\\nruntime. This paper presents the results and challenges of integrating C++\\nModules into ROOT.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.04157</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.04157</id><submitter>Reza Khodayi-mehr</submitter><version version=\"v1\"><date>Tue, 11 Dec 2018 00:07:26 GMT</date><size>2150kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 5 Oct 2019 00:43:32 GMT</date><size>1036kb</size><source_type>D</source_type></version><title>Deep Learning for Robotic Mass Transport Cloaking</title><authors>Reza Khodayi-mehr, Michael M. Zavlanos</authors><categories>cs.RO cs.LG</categories><comments>8 pages, 11 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of Mass Transport Cloaking using mobile robots. The\\nrobots move along a predefined curve that encloses the safe zone and carry\\nsources that collectively counteract a chemical agent released in the\\nenvironment. The goal is to steer the mass flux around a desired region so that\\nit remains unaffected by the external concentration. We formulate the problem\\nof controlling the robot positions and release rates as a PDE-constrained\\noptimization, where the propagation of the chemical is modeled by the\\nAdvection-Diffusion (AD) PDE. We use a Deep Neural Network (NN) to approximate\\nthe solution of the PDE. Particularly, we propose a novel loss function for the\\nNN that utilizes the variational form of the AD-PDE and allows us to\\nreformulate the planning problem as an unsupervised model-based learning\\nproblem. Our loss function is discretization-free and highly parallelizable.\\nUnlike passive cloaking methods that use metamaterials to steer the mass flux,\\nour method is the first to use mobile robots to actively control the\\nconcentration levels and create safe zones independent of environmental\\nconditions. We demonstrate the performance of our method in simulations.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.04246</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.04246</id><submitter>Ryota Yoshihashi</submitter><version version=\"v1\"><date>Tue, 11 Dec 2018 07:34:28 GMT</date><size>1332kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 17 Dec 2018 06:24:38 GMT</date><size>1332kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sun, 6 Oct 2019 07:55:48 GMT</date><size>1291kb</size><source_type>D</source_type></version><title>Classification-Reconstruction Learning for Open-Set Recognition</title><authors>Ryota Yoshihashi, Wen Shao, Rei Kawakami, Shaodi You, Makoto Iida,\\n  Takeshi Naemura</authors><categories>cs.CV</categories><comments>11 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Open-set classification is a problem of handling `unknown\\' classes that are\\nnot contained in the training dataset, whereas traditional classifiers assume\\nthat only known classes appear in the test environment. Existing open-set\\nclassifiers rely on deep networks trained in a supervised manner on known\\nclasses in the training set; this causes specialization of learned\\nrepresentations to known classes and makes it hard to distinguish unknowns from\\nknowns. In contrast, we train networks for joint classification and\\nreconstruction of input data. This enhances the learned representation so as to\\npreserve information useful for separating unknowns from knowns, as well as to\\ndiscriminate classes of knowns. Our novel Classification-Reconstruction\\nlearning for Open-Set Recognition (CROSR) utilizes latent representations for\\nreconstruction and enables robust unknown detection without harming the\\nknown-class classification accuracy. Extensive experiments reveal that the\\nproposed method outperforms existing deep open-set classifiers in multiple\\nstandard datasets and is robust to diverse outliers. The code is available in\\nhttps://nae-lab.org/~rei/research/crosr/.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.04419</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.04419</id><submitter>Daewon Seo</submitter><version version=\"v1\"><date>Fri, 23 Nov 2018 19:02:57 GMT</date><size>395kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 5 Aug 2019 19:32:21 GMT</date><size>378kb</size><source_type>D</source_type></version><title>Beliefs in Decision-Making Cascades</title><authors>Daewon Seo, Ravi Kiran Raman, Joong Bum Rhim, Vivek K Goyal, Lav R\\n  Varshney</authors><categories>cs.GT stat.ML</categories><comments>final version, to appear in IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2019.2935865</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work explores a social learning problem with agents having nonidentical\\nnoise variances and mismatched beliefs. We consider an $N$-agent binary\\nhypothesis test in which each agent sequentially makes a decision based not\\nonly on a private observation, but also on preceding agents\\' decisions. In\\naddition, the agents have their own beliefs instead of the true prior, and have\\nnonidentical noise variances in the private signal. We focus on the Bayes risk\\nof the last agent, where preceding agents are selfish.\\n  We first derive the optimal decision rule by recursive belief update and\\nconclude, counterintuitively, that beliefs deviating from the true prior could\\nbe optimal in this setting. The effect of nonidentical noise levels in the\\ntwo-agent case is also considered and analytical properties of the optimal\\nbelief curves are given. Next, we consider a predecessor selection problem\\nwherein the subsequent agent of a certain belief chooses a predecessor from a\\nset of candidates with varying beliefs. We characterize the decision region for\\nchoosing such a predecessor and argue that a subsequent agent with beliefs\\nvarying from the true prior often ends up selecting a suboptimal predecessor,\\nindicating the need for a social planner. Lastly, we discuss an augmented\\nintelligence design problem that uses a model of human behavior from cumulative\\nprospect theory and investigate its near-optimality and suboptimality.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.06574</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.06574</id><submitter>Yunzhe Hao</submitter><version version=\"v1\"><date>Mon, 17 Dec 2018 01:38:14 GMT</date><size>4286kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 1 Jul 2019 03:33:43 GMT</date><size>4285kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sun, 6 Oct 2019 09:27:27 GMT</date><size>3822kb</size><source_type>D</source_type></version><title>A Biologically Plausible Supervised Learning Method for Spiking Neural\\n  Networks Using the Symmetric STDP Rule</title><authors>Yunzhe Hao, Xuhui Huang, Meng Dong, Bo Xu</authors><categories>cs.NE cs.AI cs.LG q-bio.NC</categories><comments>29 pages, 6 figures</comments><journal-ref>Neural Networks 121C (2020) pp. 387-395</journal-ref><doi>10.1016/j.neunet.2019.09.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spiking neural networks (SNNs) possess energy-efficient potential due to\\nevent-based computation. However, supervised training of SNNs remains a\\nchallenge as spike activities are non-differentiable. Previous SNNs training\\nmethods can be generally categorized into two basic classes, i.e.,\\nbackpropagation-like training methods and plasticity-based learning methods.\\nThe former methods are dependent on energy-inefficient real-valued computation\\nand non-local transmission, as also required in artificial neural networks\\n(ANNs), whereas the latter are either considered to be biologically implausible\\nor exhibit poor performance. Hence, biologically plausible (bio-plausible)\\nhigh-performance supervised learning (SL) methods for SNNs remain deficient. In\\nthis paper, we proposed a novel bio-plausible SNN model for SL based on the\\nsymmetric spike-timing dependent plasticity (sym-STDP) rule found in\\nneuroscience. By combining the sym-STDP rule with bio-plausible synaptic\\nscaling and intrinsic plasticity of the dynamic threshold, our SNN model\\nimplemented SL well and achieved good performance in the benchmark recognition\\ntask (MNIST dataset). To reveal the underlying mechanism of our SL model, we\\nvisualized both layer-based activities and synaptic weights using the\\nt-distributed stochastic neighbor embedding (t-SNE) method after training and\\nfound that they were well clustered, thereby demonstrating excellent\\nclassification ability. Furthermore, to verify the robustness of our model, we\\ntrained it on another more realistic dataset (Fashion-MNIST), which also showed\\ngood performance. As the learning rules were bio-plausible and based purely on\\nlocal spike events, our model could be easily applied to neuromorphic hardware\\nfor online training and may be helpful for understanding SL information\\nprocessing at the synaptic level in biological neural systems.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.07372</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.07372</id><submitter>Ahmed Elkordy</submitter><version version=\"v1\"><date>Sat, 15 Dec 2018 18:00:24 GMT</date><size>351kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 08:20:50 GMT</date><size>2441kb</size><source_type>D</source_type></version><title>Cache-Aided Combination Networks with Interference</title><authors>Ahmed Roushdy, Abolfazl Seyed Motahari, Mohammed Nafie and Deniz\\n  Gunduz</authors><categories>cs.IT math.IT</categories><comments>Submitted for Publication. arXiv admin note: substantial text overlap\\n  with arXiv:1802.09087</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Centralized coded caching and delivery is studied for a radio access\\ncombination network (RACN), whereby a set of $H$ edge nodes (ENs), connected to\\na cloud server via orthogonal fronthaul links with limited capacity, serve a\\ntotal of $K$ user equipments (UEs) over wireless links. Each user, equipped\\nwith a cache of size $\\\\mu_R N F$ bits, is connected to a distinct set of $r$\\nENs each of which equipped with a cache of size $\\\\mu_T N F$ bits, where\\n$\\\\mu_T$, $\\\\mu_R \\\\in [0,1]$ are the fractional cache capacities of the UEs and\\nthe ENs, respectively. The objective is to minimize the normalized delivery\\ntime (NDT. Three coded caching and transmission schemes are considered, namely\\nthe\\\\textit{ MDS-IA}, \\\\textit{soft-transfer} and \\\\textit{ zero-forcing (ZF)}\\nschemes. The achievable NDT for MDS-IA scheme is presented for $r=2$ and\\narbitrary fractional cache sizes $\\\\mu_T$ and $\\\\mu_R$, and also for arbitrary\\nvalue of $r$ and fractional cache size $\\\\mu_T$ when the cache capacity of the\\nUE is above a certain threshold. The achievable NDT for the soft-transfer\\nscheme is presented for arbitrary $r$ and arbitrary fractional cache sizes\\n$\\\\mu_T$ and $\\\\mu_R$. The last scheme utilizes ZF between the ENs and the UEs\\nwithout the participation of the cloud server in the delivery phase. The\\nachievable NDT for this scheme is presented for an arbitrary value of $r$ when\\nthe total cache size at a pair of UE and EN is sufficient to store the whole\\nlibrary, i.e., $\\\\mu_T+\\\\mu_R \\\\geq 1$. The results indicate that the fronthaul\\ncapacity determines which scheme achieves a better performance in terms of the\\nNDT, and the soft-transfer scheme becomes favorable as the fronthaul capacity\\nincreases.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.07431</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.07431</id><submitter>Mor Joseph-Rivlin</submitter><version version=\"v1\"><date>Tue, 18 Dec 2018 15:25:04 GMT</date><size>7391kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 05:25:51 GMT</date><size>6198kb</size><source_type>D</source_type></version><title>Momen(e)t: Flavor the Moments in Learning to Classify Shapes</title><authors>Mor Joseph-Rivlin, Alon Zvirin and Ron Kimmel</authors><categories>cs.CG cs.LG</categories><journal-ref>GMDL Workshop, ICCV 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental question in learning to classify 3D shapes is how to treat the\\ndata in a way that would allow us to construct efficient and accurate geometric\\nprocessing and analysis procedures. Here, we restrict ourselves to networks\\nthat operate on point clouds. There were several attempts to treat point clouds\\nas non-structured data sets by which a neural network is trained to extract\\ndiscriminative properties. The idea of using 3D coordinates as class\\nidentifiers motivated us to extend this line of thought to that of shape\\nclassification by comparing attributes that could easily account for the shape\\nmoments. Here, we propose to add polynomial functions of the coordinates\\nallowing the network to account for higher order moments of a given shape.\\nExperiments on two benchmarks show that the suggested network is able to\\nprovide state of the art results and at the same token learn more efficiently\\nin terms of memory and computational complexity.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.07446</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.07446</id><submitter>Fanyi Yang</submitter><version version=\"v1\"><date>Tue, 18 Dec 2018 15:52:49 GMT</date><size>4608kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 13:03:57 GMT</date><size>3201kb</size><source_type>D</source_type></version><title>A Discontinuous Galerkin Method by Patch Reconstruction for Elliptic\\n  Interface Problem on Unfitted Mesh</title><authors>Ruo Li and Fanyi Yang</authors><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a discontinuous Galerkin(DG) method to approximate the elliptic\\ninterface problem on unfitted mesh using a new approximation space. The\\napproximation space is constructed by patch reconstruction with one degree of\\nfreedom per element. The optimal error estimates in both L2 norm and DG energy\\nnorm are obtained, without the typical constraints for DG method on how the\\ninterface intersects to the elements in the mesh. Other than enjoying the\\nadvantages of DG method, our method may achieve even better efficiency than the\\nconforming finite element method, as illustrated by numerical examples.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.07683</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.07683</id><submitter>Nelly Elsayed</submitter><version version=\"v1\"><date>Tue, 18 Dec 2018 22:57:46 GMT</date><size>763kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 27 Dec 2018 04:25:11 GMT</date><size>885kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 20 Feb 2019 02:41:10 GMT</date><size>1302kb</size><source_type>D</source_type></version><title>Deep Gated Recurrent and Convolutional Network Hybrid Model for\\n  Univariate Time Series Classification</title><authors>Nelly Elsayed, Anthony S. Maida, Magdy Bayoumi</authors><categories>cs.LG stat.ML</categories><comments>The paper modified and has several new results</comments><journal-ref>International Journal of Advanced Computer Science and\\n  Applications (IJACSA), 10(5), 2019</journal-ref><doi>10.14569/IJACSA.2019.0100582</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid LSTM-fully convolutional networks (LSTM-FCN) for time series\\nclassification have produced state-of-the-art classification results on\\nunivariate time series. We show that replacing the LSTM with a gated recurrent\\nunit (GRU) to create a GRU-fully convolutional network hybrid model (GRU-FCN)\\ncan offer even better performance on many time series datasets. The proposed\\nGRU-FCN model outperforms state-of-the-art classification performance in many\\nunivariate and multivariate time series datasets. In addition, since the GRU\\nuses a simpler architecture than the LSTM, it has fewer training parameters,\\nless training time, and a simpler hardware implementation, compared to the\\nLSTM-based models.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.08658</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.08658</id><submitter>Harsh Agrawal</submitter><version version=\"v1\"><date>Thu, 20 Dec 2018 16:04:05 GMT</date><size>6934kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 22 Apr 2019 23:13:30 GMT</date><size>8646kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 30 Sep 2019 20:10:33 GMT</date><size>9274kb</size><source_type>D</source_type></version><title>nocaps: novel object captioning at scale</title><authors>Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain,\\n  Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, Peter Anderson</authors><categories>cs.CV cs.AI cs.CL cs.LG</categories><journal-ref>IEEE International Conference on Computer Vision (ICCV) 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image captioning models have achieved impressive results on datasets\\ncontaining limited visual concepts and large amounts of paired image-caption\\ntraining data. However, if these models are to ever function in the wild, a\\nmuch larger variety of visual concepts must be learned, ideally from less\\nsupervision. To encourage the development of image captioning models that can\\nlearn visual concepts from alternative data sources, such as object detection\\ndatasets, we present the first large-scale benchmark for this task. Dubbed\\n\\'nocaps\\', for novel object captioning at scale, our benchmark consists of\\n166,100 human-generated captions describing 15,100 images from the OpenImages\\nvalidation and test sets. The associated training data consists of COCO\\nimage-caption pairs, plus OpenImages image-level labels and object bounding\\nboxes. Since OpenImages contains many more classes than COCO, nearly 400 object\\nclasses seen in test images have no or very few associated training captions\\n(hence, nocaps). We extend existing novel object captioning models to establish\\nstrong baselines for this benchmark and provide analysis to guide future work\\non this task.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.09903</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.09903</id><submitter>Yuval Atzmon</submitter><version version=\"v1\"><date>Mon, 24 Dec 2018 11:54:41 GMT</date><size>774kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 13 May 2019 10:25:53 GMT</date><size>1273kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 7 Oct 2019 16:01:33 GMT</date><size>1274kb</size><source_type>D</source_type></version><title>Adaptive Confidence Smoothing for Generalized Zero-Shot Learning</title><authors>Yuval Atzmon, Gal Chechik</authors><categories>cs.CV</categories><comments>(1) Accepted to CVPR 2019. (2) Previous title was &quot;Domain-Aware\\n  Generalized Zero-Shot Learning&quot;. (3) This arxiv version is as the CVPR final\\n  version with the following modifications: (a) corrected typos found in Table\\n  3 (b) updated &quot;Related Work&quot; with [52, 10, 20] (c) add a paragraph to the\\n  abstract (d) add a probabilistic explanation for the smoothing term</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generalized zero-shot learning (GZSL) is the problem of learning a classifier\\nwhere some classes have samples and others are learned from side information,\\nlike semantic attributes or text description, in a zero-shot learning fashion\\n(ZSL). Training a single model that operates in these two regimes\\nsimultaneously is challenging. Here we describe a probabilistic approach that\\nbreaks the model into three modular components, and then combines them in a\\nconsistent way. Specifically, our model consists of three classifiers: A\\n&quot;gating&quot; model that makes soft decisions if a sample is from a &quot;seen&quot; class,\\nand two experts: a ZSL expert, and an expert model for seen classes.\\n  We address two main difficulties in this approach: How to provide an accurate\\nestimate of the gating probability without any training samples for unseen\\nclasses; and how to use expert predictions when it observes samples outside of\\nits domain. The key insight to our approach is to pass information between the\\nthree models to improve each one\\'s accuracy, while maintaining the modular\\nstructure. We test our approach, adaptive confidence smoothing (COSMO), on four\\nstandard GZSL benchmark datasets and find that it largely outperforms\\nstate-of-the-art GZSL models. COSMO is also the first model that closes the gap\\nand surpasses the performance of generative models for GZSL, even-though it is\\na light-weight model that is much easier to train and tune.\\n  Notably, COSMO offers a new view for developing zero-shot models. Thanks to\\nCOSMO\\'s modular structure, instead of trying to perform well both on seen and\\non unseen classes, models can focus on accurate classification of unseen\\nclasses, and later consider seen class models.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.10519</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.10519</id><submitter>Jesus Daniel Arroyo Reli\\\\\\'on</submitter><version version=\"v1\"><date>Wed, 26 Dec 2018 20:01:30 GMT</date><size>294kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 1 Feb 2019 20:23:08 GMT</date><size>237kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 30 Sep 2019 18:45:41 GMT</date><size>1091kb</size><source_type>D</source_type></version><title>Maximum Likelihood Estimation and Graph Matching in Errorfully Observed\\n  Networks</title><authors>Jes\\\\\\'us Arroyo, Daniel L. Sussman, Carey E. Priebe, Vince Lyzinski</authors><categories>stat.ML cs.LG math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a pair of graphs with the same number of vertices, the inexact graph\\nmatching problem consists in finding a correspondence between the vertices of\\nthese graphs that minimizes the total number of induced edge disagreements. We\\nstudy this problem from a statistical framework in which one of the graphs is\\nan errorfully observed copy of the other. We introduce a corrupting channel\\nmodel, and show that in this model framework, the solution to the graph\\nmatching problem is a maximum likelihood estimator. Necessary and sufficient\\nconditions for consistency of this MLE are presented, as well as a relaxed\\nnotion of consistency in which a negligible fraction of the vertices need not\\nbe matched correctly. The results are used to study matchability in several\\nfamilies of random graphs, including edge independent models, random regular\\ngraphs and small-world networks. We also use these results to introduce\\nmeasures of matching feasibility, and experimentally validate the results on\\nsimulated and real-world networks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.10579</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.10579</id><submitter>Truong Nghiem</submitter><version version=\"v1\"><date>Thu, 27 Dec 2018 00:19:24 GMT</date><size>104kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 15 Mar 2019 22:22:50 GMT</date><size>93kb</size></version><version version=\"v3\"><date>Tue, 1 Oct 2019 18:18:03 GMT</date><size>93kb</size></version><title>Linearized Gaussian Processes for Fast Data-driven Model Predictive\\n  Control</title><authors>Truong X. Nghiem</authors><categories>cs.SY</categories><comments>9 pages, 4 figures, 1 table, published at 2019 American Control\\n  Conference: https://ieeexplore.ieee.org/abstract/document/8814476</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data-driven Model Predictive Control (MPC), where the system model is learned\\nfrom data with machine learning, has recently gained increasing interests in\\nthe control community. Gaussian Processes (GP), as a type of statistical\\nmodels, are particularly attractive due to their modeling flexibility and their\\nability to provide probabilistic estimates of prediction uncertainty. GP-based\\nMPC has been developed and applied, however the optimization problem is\\ntypically non-convex and highly demanding, and scales poorly with model size.\\nThis causes unsatisfactory solving performance, even with state-of-the-art\\nsolvers, and makes the approach less suitable for real-time control. We develop\\na method based on a new concept, called linearized Gaussian Process, and\\nSequential Convex Programming, that can significantly improve the solving\\nperformance of GP-based MPC. Our method is not only faster but also much more\\nscalable and predictable than other commonly used methods, as it is much less\\ninfluenced by the model size. The efficiency and advantages of the algorithm\\nare demonstrated clearly in a numerical example.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.10706</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.10706</id><submitter>Long Zhang</submitter><version version=\"v1\"><date>Thu, 27 Dec 2018 12:23:37 GMT</date><size>144kb</size></version><version version=\"v2\"><date>Fri, 24 May 2019 09:40:40 GMT</date><size>327kb</size></version><version version=\"v3\"><date>Tue, 27 Aug 2019 19:35:36 GMT</date><size>344kb</size></version><title>TripleAgent: Monitoring, Perturbation and Failure-obliviousness for\\n  Automated Resilience Improvement in Java Applications</title><authors>Long Zhang and Martin Monperrus</authors><categories>cs.SE</categories><journal-ref>Proceedings of the IEEE International Symposium on Software\\n  Reliability Engineering, 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel resilience improvement system for Java\\napplications. The unique feature of this system is to combine automated\\nmonitoring, automated perturbation injection, and automated resilience\\nimprovement. The latter is achieved thanks to the failure-oblivious computing,\\na concept introduced in 2004 by Rinard and colleagues. We design and implement\\nthe system as agents for the Java virtual machine. We evaluate the system on\\ntwo real-world applications: a file transfer client and an email server. Our\\nresults show that it is possible to automatically improve the resilience of\\nJava applications with respect to uncaught or mishandled exceptions.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.10761</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.10761</id><submitter>Shen-Huan Lyu</submitter><version version=\"v1\"><date>Thu, 27 Dec 2018 16:34:54 GMT</date><size>612kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 15:02:14 GMT</date><size>1580kb</size><source_type>D</source_type></version><title>Improving Generalization of Deep Neural Networks by Leveraging Margin\\n  Distribution</title><authors>Shen-Huan Lyu, Lu Wang, Zhi-Hua Zhou</authors><categories>cs.LG stat.ML</categories><comments>16 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent researches use margin theory to analyze the generalization performance\\nfor deep neural networks. The main results are based on the\\nspectrally-normalized minimum margin. However, optimizing the minimum margin\\nignores a mass of information about margin distribution which is crucial to\\ngeneralization performance. In this paper, we prove a generalization bound\\ndominated by a ratio of the margin standard deviation to the margin mean, where\\nthe huge magnitude of spectral norms is reduced. Compared with the spectral\\nnorm terms in the existing results, the margin ratio term in our bound is\\norders of magnitude better in practice. On the other hand, our bound inspires\\nus to optimize the margin ratio. We utilize a convex margin distribution loss\\nfunction on the deep neural networks to validate our theoretical results.\\nExperiments and visualizations confirm the effectiveness of our approach in\\nterms of performance and representation learning ability.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.11725</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.11725</id><submitter>Xingguo Liu</submitter><version version=\"v1\"><date>Mon, 31 Dec 2018 08:54:33 GMT</date><size>5121kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Jan 2019 12:36:36 GMT</date><size>5211kb</size><source_type>D</source_type></version><title>Total Variation with Overlapping Group Sparsity and Lp Quasinorm for\\n  Infrared Image Deblurring under Salt-and-Pepper Noise</title><authors>Xingguo Liu, Yinping Chen, Zhenming Peng, Juan Wu</authors><categories>cs.CV</categories><doi>10.1117/1.JEI.28.4.043031</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Because of the limitations of the infrared imaging principle and the\\nproperties of infrared imaging systems, infrared images have some drawbacks,\\nincluding a lack of details, indistinct edges, and a large amount of\\nsalt-andpepper noise. To improve the sparse characteristics of the image while\\nmaintaining the image edges and weakening staircase artifacts, this paper\\nproposes a method that uses the Lp quasinorm instead of the L1 norm and for\\ninfrared image deblurring with an overlapping group sparse total variation\\nmethod. The Lp quasinorm introduces another degree of freedom, better describes\\nimage sparsity characteristics, and improves image restoration. Furthermore, we\\nadopt the accelerated alternating direction method of multipliers and fast\\nFourier transform theory in the proposed method to improve the efficiency and\\nrobustness of our algorithm. Experiments show that under different conditions\\nfor blur and salt-and-pepper noise, the proposed method leads to excellent\\nperformance in terms of objective evaluation and subjective visual results.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1812.11789</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1812.11789</id><submitter>Alin Bostan</submitter><version version=\"v1\"><date>Mon, 31 Dec 2018 13:27:17 GMT</date><size>19kb</size></version><version version=\"v2\"><date>Sat, 5 Oct 2019 08:49:59 GMT</date><size>24kb</size></version><title>Subresultants of $(x-\\\\alpha)^m$ and $(x-\\\\beta)^n$, Jacobi polynomials\\n  and complexity</title><authors>A. Bostan, T. Krick, A. Szanto, M. Valdettaro</authors><categories>cs.SC math.CA</categories><comments>34 pages, accepted for publication in Journal of Symbolic Computation</comments><msc-class>13P15, 15B05, 33C05, 33C45, 33F10, 68W30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an earlier article together with Carlos D\\'Andrea [BDKSV2017], we described\\nexplicit expressions for the coefficients of the order-$d$ polynomial\\nsubresultant of $(x-\\\\alpha)^m$ and $(x-\\\\beta)^n $ with respect to Bernstein\\'s\\nset of polynomials $\\\\{(x-\\\\alpha)^j(x-\\\\beta)^{d-j}, \\\\, 0\\\\le j\\\\le d\\\\}$, for $0\\\\le\\nd&lt;\\\\min\\\\{m, n\\\\}$. The current paper further develops the study of these\\nstructured polynomials and shows that the coefficients of the subresultants of\\n$(x-\\\\alpha)^m$ and $(x-\\\\beta)^n$ with respect to the monomial basis can be\\ncomputed in linear arithmetic complexity, which is faster than for arbitrary\\npolynomials. The result is obtained as a consequence of the amazing though\\nseemingly unnoticed fact that these subresultants are scalar multiples of\\nJacobi polynomials up to an affine change of variables.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.00316</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.00316</id><submitter>Alexandr Kazda</submitter><version version=\"v1\"><date>Wed, 2 Jan 2019 10:46:37 GMT</date><size>17kb</size></version><version version=\"v2\"><date>Tue, 8 Oct 2019 08:58:23 GMT</date><size>21kb</size></version><title>Deciding the existence of minority terms</title><authors>Alexandr Kazda, Jakub Opr\\\\v{s}al, Matt Valeriote, and Dmitriy Zhuk</authors><categories>math.LO cs.CC math.RA</categories><msc-class>68Q25 (Primary), 03B05, 08A40 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the computational complexity of deciding if a given\\nfinite idempotent algebra has a ternary term operation $m$ that satisfies the\\nminority equations $m(y,x,x) \\\\approx m(x,y,x) \\\\approx m(x,x,y) \\\\approx y$. We\\nshow that a common polynomial-time approach to testing for this type of\\ncondition will not work in this case and that this decision problem lies in the\\nclass NP.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.00395</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.00395</id><submitter>Indrava Roy</submitter><version version=\"v1\"><date>Wed, 2 Jan 2019 14:35:53 GMT</date><size>1086kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 6 Sep 2019 21:21:37 GMT</date><size>6670kb</size><source_type>D</source_type></version><title>Persistent homology of unweighted complex networks via discrete Morse\\n  theory</title><authors>Harish Kannan, Emil Saucan, Indrava Roy, Areejit Samal</authors><categories>cs.DM math.CO</categories><comments>36 pages, 6 main figures, SI Appendix and SI Figures; SI Tables\\n  available upon request from authors</comments><journal-ref>Scientific Reports, 9-13817 (2019)</journal-ref><doi>10.1038/s41598-019-50202-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topological data analysis can reveal higher-order structure beyond pairwise\\nconnections between vertices in complex networks. We present a new method based\\non discrete Morse theory to study topological properties of unweighted and\\nundirected networks using persistent homology. Leveraging on the features of\\ndiscrete Morse theory, our method not only captures the topology of the clique\\ncomplex of such graphs via the concept of critical simplices, but also achieves\\nclose to the theoretical minimum number of critical simplices in several\\nanalyzed model and real networks. This leads to a reduced filtration scheme\\nbased on the subsequence of the corresponding critical weights, thereby leading\\nto a significant increase in computational efficiency. We have employed our\\nfiltration scheme to explore the persistent homology of several model and\\nreal-world networks. In particular, we show that our method can detect\\ndifferences in the higher-order structure of networks, and the corresponding\\npersistence diagrams can be used to distinguish between different model\\nnetworks. In summary, our method based on discrete Morse theory further\\nincreases the applicability of persistent homology to investigate the global\\ntopology of complex networks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.01608</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.01608</id><submitter>Mario Geiger</submitter><version version=\"v1\"><date>Sun, 6 Jan 2019 21:11:25 GMT</date><size>221kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 18 Jan 2019 20:04:44 GMT</date><size>167kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 24 Jan 2019 01:04:08 GMT</date><size>168kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Tue, 18 Jun 2019 13:23:00 GMT</date><size>227kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Tue, 8 Oct 2019 08:43:29 GMT</date><size>248kb</size><source_type>D</source_type></version><title>Scaling description of generalization with number of parameters in deep\\n  learning</title><authors>Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent\\n  Sagun, St\\\\\\'ephane d\\'Ascoli, Giulio Biroli, Cl\\\\\\'ement Hongler and Matthieu\\n  Wyart</authors><categories>cond-mat.dis-nn cs.LG</categories><comments>The clarity of the text has been improved: the section &quot;Related\\n  works&quot; has been updated and the section &quot;3.1 Regression task&quot; has been added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supervised deep learning involves the training of neural networks with a\\nlarge number $N$ of parameters. For large enough $N$, in the so-called\\nover-parametrized regime, one can essentially fit the training data points.\\nSparsity-based arguments would suggest that the generalization error increases\\nas $N$ grows past a certain threshold $N^{*}$. Instead, empirical studies have\\nshown that in the over-parametrized regime, generalization error keeps\\ndecreasing with $N$. We resolve this paradox through a new framework. We rely\\non the so-called Neural Tangent Kernel, which connects large neural nets to\\nkernel methods, to show that the initialization causes finite-size random\\nfluctuations $\\\\|f_{N}-\\\\bar{f}_{N}\\\\|\\\\sim N^{-1/4}$ of the neural net output\\nfunction $f_{N}$ around its expectation $\\\\bar{f}_{N}$. These affect the\\ngeneralization error $\\\\epsilon_{N}$ for classification: under natural\\nassumptions, it decays to a plateau value $\\\\epsilon_{\\\\infty}$ in a power-law\\nfashion $\\\\sim N^{-1/2}$. This description breaks down at a so-called jamming\\ntransition $N=N^{*}$. At this threshold, we argue that $\\\\|f_{N}\\\\|$ diverges.\\nThis result leads to a plausible explanation for the cusp in test error known\\nto occur at $N^{*}$. Our results are confirmed by extensive empirical\\nobservations on the MNIST and CIFAR image datasets. Our analysis finally\\nsuggests that, given a computational envelope, the smallest generalization\\nerror is obtained using several networks of intermediate sizes, just beyond\\n$N^{*}$, and averaging their outputs.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.01704</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.01704</id><submitter>Roberto Doriguzzi Corin</submitter><version version=\"v1\"><date>Mon, 7 Jan 2019 08:45:37 GMT</date><size>324kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 6 Jun 2019 09:21:45 GMT</date><size>446kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 24 Sep 2019 15:38:23 GMT</date><size>446kb</size><source_type>D</source_type></version><title>Dynamic and Application-Aware Provisioning of Chained Virtual Security\\n  Network Functions</title><authors>Roberto Doriguzzi-Corin, Sandra Scott-Hayward, Domenico Siracusa,\\n  Marco Savi, Elio Salvadori</authors><categories>cs.NI</categories><doi>10.1109/TNSM.2019.2941128</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A promising area of application for Network Function Virtualization is in\\nnetwork security, where chains of Virtual Security Network Functions (VSNFs),\\ni.e., security-specific virtual functions such as firewalls or Intrusion\\nPrevention Systems, can be dynamically created and configured to inspect,\\nfilter or monitor the network traffic. However, the traffic handled by VSNFs\\ncould be sensitive to specific network requirements, such as minimum bandwidth\\nor maximum end-to-end latency. Therefore, the decision on which VSNFs should\\napply for a given application, where to place them and how to connect them,\\nshould take such requirements into consideration. Otherwise, security services\\ncould affect the quality of service experienced by customers. In this paper we\\npropose PESS (Progressive Embedding of Security Services), a solution to\\nefficiently deploy chains of virtualised security functions based on the\\nsecurity requirements of individual applications and operators\\' policies, while\\noptimizing resource utilization. We provide the PESS mathematical model and\\nheuristic solution. Simulation results show that, compared to state-of-the-art\\napplication-agnostic VSNF provisioning models, PESS reduces computational\\nresource utilization by up to 50%, in different network scenarios. This result\\nultimately leads to a higher number of provisioned security services and to up\\nto a 40% reduction in end-to-end latency of application traffic.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.01708</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.01708</id><submitter>Mateusz Trokielewicz</submitter><version version=\"v1\"><date>Mon, 7 Jan 2019 08:57:35 GMT</date><size>2733kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 09:42:08 GMT</date><size>2751kb</size><source_type>D</source_type></version><title>Post-mortem Iris Recognition with Deep-Learning-based Image Segmentation</title><authors>Mateusz Trokielewicz, Adam Czajka, Piotr Maciejewicz</authors><categories>cs.CV</categories><comments>Paper submitted for the Elsevier Image and Vision Computing Journal\\n  on Jan 5th, 2019, revised version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes the first known to us iris recognition methodology\\ndesigned specifically for post-mortem samples. We propose to use deep\\nlearning-based iris segmentation models to extract highly irregular iris\\ntexture areas in post-mortem iris images. We show how to use segmentation masks\\npredicted by neural networks in conventional, Gabor-based iris recognition\\nmethod, which employs circular approximations of the pupillary and limbic iris\\nboundaries. As a whole, this method allows for a significant improvement in\\npost-mortem iris recognition accuracy over the methods designed only for\\nante-mortem irises, including the academic OSIRIS and commercial IriCore\\nimplementations. The proposed method reaches the EER less than 1% for samples\\ncollected up to 10 hours after death, when compared to 16.89% and 5.37% of EER\\nobserved for OSIRIS and IriCore, respectively. For samples collected up to 369\\nhours post-mortem, the proposed method achieves the EER 21.45%, while 33.59%\\nand 25.38% are observed for OSIRIS and IriCore, respectively. Additionally, the\\nmethod is tested on a database of iris images collected from ophthalmology\\nclinic patients, for which it also offers an advantage over the two other\\nalgorithms. This work is the first step towards post-mortem-specific iris\\nrecognition, which increases the chances of identification of deceased subjects\\nin forensic investigations. The new database of post-mortem iris images\\nacquired from 42 subjects, as well as the deep learning-based segmentation\\nmodels are made available along with the paper, to ensure all the results\\npresented in this manuscript are reproducible.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.02125</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.02125</id><submitter>Wei Wang</submitter><version version=\"v1\"><date>Tue, 8 Jan 2019 02:02:48 GMT</date><size>4888kb</size></version><version version=\"v2\"><date>Wed, 9 Jan 2019 10:07:26 GMT</date><size>4884kb</size></version><version version=\"v3\"><date>Thu, 18 Jul 2019 14:08:37 GMT</date><size>2798kb</size></version><title>Coevolution spreading in complex networks</title><authors>Wei Wang, Quan-Hui Liu, Junhao Liang, Yanqing Hu, Tao Zhou</authors><categories>physics.soc-ph cs.SI</categories><comments>115 pages, 28 figures</comments><doi>10.1016/j.physrep.2019.07.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The propagations of diseases, behaviors and information in real systems are\\nrarely independent of each other, but they are coevolving with strong\\ninteractions. To uncover the dynamical mechanisms, the evolving spatiotemporal\\npatterns and critical phenomena of networked coevolution spreading are\\nextremely important, which provide theoretical foundations for us to control\\nepidemic spreading, predict collective behaviors in social systems, and so on.\\nThe coevolution spreading dynamics in complex networks has thus attracted much\\nattention in many disciplines. In this review, we introduce recent progress in\\nthe study of coevolution spreading dynamics, emphasizing the contributions from\\nthe perspectives of statistical mechanics and network science. The theoretical\\nmethods, critical phenomena, phase transitions, interacting mechanisms, and\\neffects of network topology for four representative types of coevolution\\nspreading mechanisms, including the coevolution of biological contagions,\\nsocial contagions, epidemic-awareness, and epidemic-resources, are presented in\\ndetail, and the challenges in this field as well as open issues for future\\nstudies are also discussed.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.02172</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.02172</id><submitter>Yu Song</submitter><version version=\"v1\"><date>Tue, 8 Jan 2019 06:40:13 GMT</date><size>2818kb</size></version><version version=\"v2\"><date>Wed, 16 Jan 2019 14:51:26 GMT</date><size>2747kb</size></version><version version=\"v3\"><date>Mon, 18 Mar 2019 07:24:16 GMT</date><size>2734kb</size></version><title>Solar-Sail Trajectory Design for Multiple Near Earth Asteroid\\n  Exploration Based on Deep Neural Networks</title><authors>Yu Song, Shengping Gong</authors><categories>cs.CE astro-ph.IM cs.NE</categories><comments>34 pages, 19 figures</comments><journal-ref>Aerospace Scienceand Technology 91 (2019) 28-40</journal-ref><doi>10.1016/j.ast.2019.04.056</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the preliminary trajectory design of the multi-target rendezvous problem,\\na model that can quickly estimate the cost of the orbital transfer is\\nessential. The estimation of the transfer time using solar sail between two\\narbitrary orbits is difficult and usually requires to solve an optimal control\\nproblem. Inspired by the successful applications of the deep neural network in\\nnonlinear regression, this work explores the possibility and effectiveness of\\nmapping the transfer time for solar sail from the orbital characteristics using\\nthe deep neural network. Furthermore, the Monte Carlo Tree Search method is\\ninvestigated and used to search the optimal sequence considering a\\nmulti-asteroid exploration problem. The obtained sequences from preliminary\\ndesign will be solved and verified by sequentially solving the optimal control\\nproblem. Two examples of different application backgrounds validate the\\neffectiveness of the proposed approach.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.02405</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.02405</id><submitter>Julian Marcon</submitter><version version=\"v1\"><date>Tue, 8 Jan 2019 17:03:21 GMT</date><size>1957kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 30 Aug 2019 13:44:37 GMT</date><size>3984kb</size><source_type>D</source_type></version><title>A High Resolution PDE Approach to Quadrilateral Mesh Generation</title><authors>Julian Marcon, David A. Kopriva, Spencer J. Sherwin, Joaquim Peir\\\\\\'o</authors><categories>math.NA cs.CG cs.NA</categories><comments>31 pages, 21 figures, accepted for publication in Journal of\\n  Computational Physics</comments><msc-class>65N50</msc-class><journal-ref>Journal of Computational Physics 399C (2019) 108918</journal-ref><doi>10.1016/j.jcp.2019.108918</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a high order technique to generate quadrilateral decompositions\\nand meshes for complex two dimensional domains using spectral elements in a\\nfield guided procedure. Inspired by cross field methods, we never actually\\ncompute crosses. Instead, we compute a high order accurate guiding field using\\na continuous Galerkin (CG) or discontinuous Galerkin (DG) spectral element\\nmethod to solve a Laplace equation for each of the field variables using the\\nopen source code Nektar++. The spectral method provides spectral convergence\\nand sub-element resolution of the fields. The DG approximation allows meshing\\nof corners that are not multiples of $\\\\pi/2$ in a discretization consistent\\nmanner, when needed. The high order field can then be exploited to accurately\\nfind irregular nodes, and can be accurately integrated using a high order\\nseparatrix integration method to avoid features like limit cycles. The result\\nis a mesh with naturally curved quadrilateral elements that do not need to be\\ncurved a posteriori to eliminate invalid elements. The mesh generation\\nprocedure is implemented in the open source mesh generation program NekMesh.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.03283</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.03283</id><submitter>Mario Teixeira Parente</submitter><version version=\"v1\"><date>Thu, 10 Jan 2019 17:24:04 GMT</date><size>810kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 8 May 2019 11:12:49 GMT</date><size>2262kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 16 Jul 2019 11:18:13 GMT</date><size>2262kb</size><source_type>D</source_type></version><title>Bayesian calibration and sensitivity analysis for a karst aquifer model\\n  using active subspaces</title><authors>Mario Teixeira Parente, Daniel Bittner, Steven Mattis, Gabriele\\n  Chiogna, Barbara Wohlmuth</authors><categories>stat.CO cs.NA math.NA</categories><comments>27 pages, 5 figures, 2 tables; 5 pages supplementary information</comments><journal-ref>Water Resources Research 55 (8), 7086-7107, 2019</journal-ref><doi>10.1029/2019WR024739</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we perform a parameter study for a recently developed karst\\nhydrological model. The study consists of a high-dimensional Bayesian inverse\\nproblem and a global sensitivity analysis. For the first time in karst\\nhydrology, we use the active subspace method to find directions in the\\nparameter space that dominate the Bayesian update from the prior to the\\nposterior distribution in order to effectively reduce the dimension of the\\nproblem and for computational efficiency. Additionally, the calculated active\\nsubspace can be exploited to construct sensitivity metrics on each of the\\nindividual parameters and be used to construct a natural model surrogate. The\\nmodel consists of 21 parameters to reproduce the hydrological behavior of\\nspring discharge in a karst aquifer located in the Kerschbaum spring recharge\\narea at Waidhofen a.d. Ybbs in Austria. The experimental spatial and time\\nseries data for the inference process were collected by the water works in\\nWaidhofen. We show that this case study has implicit low-dimensionality, and we\\nrun an adjusted Markov chain Monte Carlo algorithm in a low-dimensional\\nsubspace to construct samples of the posterior distribution. The results are\\nvisualized and verified by plots of the posterior\\'s push-forward distribution\\ndisplaying the uncertainty in predicting discharge values due to the\\nexperimental noise in the data. Finally, a discussion provides hydrological\\ninterpretation of these results for the Kerschbaum area.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.03301</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.03301</id><submitter>Yulong Zou Dr.</submitter><version version=\"v1\"><date>Thu, 10 Jan 2019 18:08:21 GMT</date><size>218kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 15 May 2019 08:53:10 GMT</date><size>218kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 8 Oct 2019 04:10:39 GMT</date><size>613kb</size></version><title>Joint Power Splitting and Relay Selection in Energy-Harvesting\\n  Communications for IoT Networks</title><authors>Yulong Zou, Jia Zhu, and Xiao Jiang</authors><categories>cs.IT math.IT</categories><comments>14 pages</comments><journal-ref>IEEE Internet of Things Journal, 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider an energy-harvesting (EH) relay system consisting\\nof a source, a destination, and multiple EH decode-and-forward (DF) relays that\\ncan harvest the energy from their received radio signals. A power-splitting\\nratio is employed at an EH DF relay to characterize a trade-off between an\\nenergy used for decoding the source signal received at the relay and the\\nremaining energy harvested for retransmitting the decode outcome. We propose an\\noptimal power splitting and relay selection (OPS-RS) framework and also\\nconsider the conventional equal power splitting and relay selection (EPS-RS)\\nfor comparison purposes. We derive closed-form expressions of outage\\nprobability for both the OPS-RS and EPS-RS schemes and characterize their\\ndiversity gains through an asymptotic outage analysis in the high\\nsignal-to-noise ratio region. We further examine an extension of our OPS-RS\\nframework to an energy-harvesting battery (EHB) enabled cooperative relay\\nscenario, where the EH relays are equipped with batteries used to store their\\nharvested energies. We propose an amplify-and-forward (AF) based EHB-OPS-RS and\\na DF based EHB-OPS-RS schemes for AF and DF relay networks, respectively.\\nNumerical results show that the proposed OPS-RS always outperforms the EPS-RS\\nin terms of outage probability. Moreover, the outage probabilities of AF and DF\\nbased EHB-OPS-RS schemes are much smaller than that of OPS-RS and EPS-RS\\nmethods, demonstrating the benefit of exploiting the batteries in EH relays.\\nAdditionally, the DF based EHB-OPS-RS substantially outperforms the AF based\\nEHB-OPS-RS and such an outage advantage becomes more significant, as the number\\nof EH relays increases.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.03393</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.03393</id><submitter>Steven Weber</submitter><version version=\"v1\"><date>Fri, 4 Jan 2019 21:16:56 GMT</date><size>330kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 22:07:21 GMT</date><size>0kb</size><source_type>I</source_type></version><title>Star sampling with and without replacement</title><authors>Jonathan Stokes and Steven Weber</authors><categories>cs.PF math.PR</categories><comments>Superseded by arXiv:1910.00431</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Star sampling (SS) is a random sampling procedure on a graph wherein each\\nsample consists of a randomly selected vertex (the star center) and its one-hop\\nneighbors (the star endpoints). We consider the use of star sampling to find\\nany member of an arbitrary target set of vertices in a graph, where the figure\\nof merit (cost) is either the expected number of samples (unit cost) or the\\nexpected number of star centers plus star endpoints (linear cost) until a\\nvertex in the target set is encountered, either as a star center or as a star\\npoint. We analyze this performance measure on three related star sampling\\nparadigms: SS with replacement (SSR), SS without center replacement (SSC), and\\nSS without star replacement (SSS). We derive exact and approximate expressions\\nfor the expected unit and linear costs of SSR, SSC, and SSS on Erdos-Renyi (ER)\\ngraphs. Our results show there is i) little difference in unit cost, but ii)\\nsignificant difference in linear cost, across the three paradigms. Although our\\nresults are derived for ER graphs, experiments on &quot;real-world&quot; graphs suggest\\nour performance expressions are reasonably accurate for non-ER graphs.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.03611</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.03611</id><submitter>Devansh Arpit</submitter><version version=\"v1\"><date>Fri, 11 Jan 2019 15:16:31 GMT</date><size>625kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 12 Mar 2019 23:52:45 GMT</date><size>988kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 2 Oct 2019 17:34:31 GMT</date><size>326kb</size><source_type>D</source_type></version><title>The Benefits of Over-parameterization at Initialization in Deep ReLU\\n  Networks</title><authors>Devansh Arpit, Yoshua Bengio</authors><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been noted in existing literature that over-parameterization in ReLU\\nnetworks generally improves performance. While there could be several factors\\ninvolved behind this, we prove some desirable theoretical properties at\\ninitialization which may be enjoyed by ReLU networks. Specifically, it is known\\nthat He initialization in deep ReLU networks asymptotically preserves variance\\nof activations in the forward pass and variance of gradients in the backward\\npass for infinitely wide networks, thus preserving the flow of information in\\nboth directions. Our paper goes beyond these results and shows novel properties\\nthat hold under He initialization: i) the norm of hidden activation of each\\nlayer is equal to the norm of the input, and, ii) the norm of weight gradient\\nof each layer is equal to the product of norm of the input vector and the error\\nat output layer. These results are derived using the PAC analysis framework,\\nand hold true for finitely sized datasets such that the width of the ReLU\\nnetwork only needs to be larger than a certain finite lower bound. As we show,\\nthis lower bound depends on the depth of the network and the number of samples,\\nand by the virtue of being a lower bound, over-parameterized ReLU networks are\\nendowed with these desirable properties. For the aforementioned hidden\\nactivation norm property under He initialization, we further extend our theory\\nand show that this property holds for a finite width network even when the\\nnumber of data samples is infinite. Thus we overcome several limitations of\\nexisting papers, and show new properties of deep ReLU networks at\\ninitialization.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.04407</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.04407</id><submitter>Alberto De Souza</submitter><version version=\"v1\"><date>Mon, 14 Jan 2019 17:10:40 GMT</date><size>1270kb</size></version><version version=\"v2\"><date>Wed, 2 Oct 2019 12:32:44 GMT</date><size>2809kb</size><source_type>D</source_type></version><title>Self-Driving Cars: A Survey</title><authors>Claudine Badue, R\\\\^anik Guidolini, Raphael Vivacqua Carneiro, Pedro\\n  Azevedo, Vinicius Brito Cardoso, Avelino Forechi, Luan Jesus, Rodrigo\\n  Berriel, Thiago Paix\\\\~ao, Filipe Mutz, Lucas Veronese, Thiago\\n  Oliveira-Santos, Alberto Ferreira De Souza</authors><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey research on self-driving cars published in the literature focusing\\non autonomous cars developed since the DARPA challenges, which are equipped\\nwith an autonomy system that can be categorized as SAE level 3 or higher. The\\narchitecture of the autonomy system of self-driving cars is typically organized\\ninto the perception system and the decision-making system. The perception\\nsystem is generally divided into many subsystems responsible for tasks such as\\nself-driving-car localization, static obstacles mapping, moving obstacles\\ndetection and tracking, road mapping, traffic signalization detection and\\nrecognition, among others. The decision-making system is commonly partitioned\\nas well into many subsystems responsible for tasks such as route planning, path\\nplanning, behavior selection, motion planning, and control. In this survey, we\\npresent the typical architecture of the autonomy system of self-driving cars.\\nWe also review research on relevant methods for perception and decision making.\\nFurthermore, we present a detailed description of the architecture of the\\nautonomy system of the self-driving car developed at the Universidade Federal\\ndo Esp\\\\\\'irito Santo (UFES), named Intelligent Autonomous Robotics Automobile\\n(IARA). Finally, we list prominent self-driving car research platforms\\ndeveloped by academia and technology companies, and reported in the media.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.05633</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.05633</id><submitter>Xiaoguang Tu</submitter><version version=\"v1\"><date>Thu, 17 Jan 2019 05:49:07 GMT</date><size>7477kb</size><source_type>D</source_type></version><title>Deep Transfer Across Domains for Face Anti-spoofing</title><authors>Xiaoguang Tu, Hengsheng Zhang, Mei Xie, Yao Luo, Yuefei Zhang, Zheng\\n  Ma</authors><categories>cs.CV</categories><comments>8 pages; 3 figures; 2 tables</comments><doi>10.1117/1.JEI.28.4.043001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A practical face recognition system demands not only high recognition\\nperformance, but also the capability of detecting spoofing attacks. While\\nemerging approaches of face anti-spoofing have been proposed in recent years,\\nmost of them do not generalize well to new database. The generalization ability\\nof face anti-spoofing needs to be significantly improved before they can be\\nadopted by practical application systems. The main reason for the poor\\ngeneralization of current approaches is the variety of materials among the\\nspoofing devices. As the attacks are produced by putting a spoofing display\\n(e.t., paper, electronic screen, forged mask) in front of a camera, the variety\\nof spoofing materials can make the spoofing attacks quite different.\\nFurthermore, the background/lighting condition of a new environment can make\\nboth the real accesses and spoofing attacks different. Another reason for the\\npoor generalization is that limited labeled data is available for training in\\nface anti-spoofing. In this paper, we focus on improving the generalization\\nability across different kinds of datasets. We propose a CNN framework using\\nsparsely labeled data from the target domain to learn features that are\\ninvariant across domains for face anti-spoofing. Experiments on public-domain\\nface spoofing databases show that the proposed method significantly improve the\\ncross-dataset testing performance only with a small number of labeled samples\\nfrom the target domain.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.05773</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.05773</id><submitter>Shizuo Kaji</submitter><version version=\"v1\"><date>Thu, 17 Jan 2019 13:02:59 GMT</date><size>1789kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 03:43:02 GMT</date><size>1342kb</size><source_type>D</source_type></version><title>Visual enhancement of Cone-beam CT by use of CycleGAN</title><authors>S. Kida, S. Kaji, K. Nawa, T. Imae, T. Nakamoto, S. Ozaki, T. Ohta, Y.\\n  Nozawa, K. Nakagawa</authors><categories>cs.CV physics.med-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cone-beam computed tomography (CBCT) offers advantages over conventional\\nfan-beam CT in that it requires a shorter time and less exposure to obtain\\nimages. CBCT has found a wide variety of applications in patient positioning\\nfor image-guided radiation therapy, extracting radiomic information for\\ndesigning patient-specific treatment, and computing fractional dose\\ndistributions for adaptive radiation therapy. However, CBCT images suffer from\\nlow soft-tissue contrast, noise, and artifacts compared to conventional\\nfan-beam CT images. Therefore, it is essential to improve the image quality of\\nCBCT. In this paper, we propose a synthetic approach to translate CBCT images\\nwith deep neural networks. Our method requires only unpaired and unaligned CBCT\\nimages and planning fan-beam CT (PlanCT) images for training. Once trained, 3D\\nreconstructed CBCT images can be directly translated to high-quality\\nPlanCT-like images. We demonstrate the effectiveness of our method with images\\nobtained from 24 prostate patients, and we provide a statistical and visual\\ncomparison. The image quality of the translated images shows substantial\\nimprovement in voxel values, spatial uniformity, and artifact suppression\\ncompared to those of the original CBCT. The anatomical structures of the\\noriginal CBCT images were also well preserved in the translated images. Our\\nmethod enables more accurate adaptive radiation therapy, and opens up new\\napplications for CBCT that hinge on high-quality images.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.06032</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.06032</id><submitter>Asifullah Khan</submitter><version version=\"v1\"><date>Thu, 17 Jan 2019 23:20:23 GMT</date><size>873kb</size></version><version version=\"v2\"><date>Wed, 3 Apr 2019 03:10:29 GMT</date><size>944kb</size></version><version version=\"v3\"><date>Thu, 3 Oct 2019 04:49:30 GMT</date><size>1046kb</size></version><title>A Survey of the Recent Architectures of Deep Convolutional Neural\\n  Networks</title><authors>Asifullah Khan, Anabia Sohail, Umme Zahoora, and Aqsa Saeed Qureshi</authors><categories>cs.CV</categories><comments>Number of Pages: 67, Number of Figures: 12, Number of Tables:1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Convolutional Neural Networks (CNNs) are a special type of Neural\\nNetworks, which have shown state of the art performance on various competitive\\nbenchmarks. The powerful learning ability of deep CNN is largely due to the use\\nof multiple feature extraction stages (hidden layers) that can automatically\\nlearn representations from the data. Availability of a large amount of data and\\nimprovements in the hardware processing units have accelerated the research in\\nCNNs, and recently very interesting deep CNN architectures are reported. The\\nrecent race in developing deep CNNs shows that the innovative architectural\\nideas, as well as parameter optimization, can improve CNN performance. In this\\nregard, different ideas in the CNN design have been explored such as the use of\\ndifferent activation and loss functions, parameter optimization,\\nregularization, and restructuring of the processing units. However, the major\\nimprovement in representational capacity of the deep CNN is achieved by the\\nrestructuring of the processing units. Especially, the idea of using a block as\\na structural unit instead of a layer is receiving substantial attention. This\\nsurvey thus focuses on the intrinsic taxonomy present in the recently reported\\ndeep CNN architectures and consequently, classifies the recent innovations in\\nCNN architectures into seven different categories. These seven categories are\\nbased on spatial exploitation, depth, multi-path, width, feature map\\nexploitation, channel boosting, and attention. Additionally, this survey also\\ncovers the elementary understanding of CNN components and sheds light on its\\ncurrent challenges and applications.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.06748</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.06748</id><submitter>Olena Burkovska</submitter><version version=\"v1\"><date>Sun, 20 Jan 2019 23:24:03 GMT</date><size>599kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 20:08:10 GMT</date><size>533kb</size><source_type>D</source_type></version><title>Affine approximation of parametrized kernels and model order reduction\\n  for nonlocal and fractional Laplace models</title><authors>Olena Burkovska, Max Gunzburger</authors><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider parametrized problems driven by spatially nonlocal integral\\noperators with parameter-dependent kernels. In particular, kernels with varying\\nnonlocal interaction radius $\\\\delta &gt; 0$ and fractional Laplace kernels,\\nparametrized by the fractional power $s\\\\in(0,1)$, are studied. In order to\\nprovide an efficient and reliable approximation of the solution for different\\nvalues of the parameters, we develop the reduced basis method as a parametric\\nmodel order reduction approach. Major difficulties arise since the kernels are\\nnot affine in the parameters, singular, and discontinuous. Moreover, the\\nspatial regularity of the solutions depends on the varying fractional power\\n$s$. To address this, we derive regularity and differentiability results with\\nrespect to $\\\\delta$ and $s$, which are of independent interest for other\\napplications such as optimization and parameter identification. We then use\\nthese results to construct affine approximations of the kernels by local\\npolynomials. Finally, we certify the method by providing reliable a posteriori\\nerror estimators, which account for all approximation errors, and support the\\ntheoretical findings by numerical experiments.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.06778</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.06778</id><submitter>Haofan Wang</submitter><version version=\"v1\"><date>Mon, 21 Jan 2019 03:07:05 GMT</date><size>848kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 23:25:54 GMT</date><size>852kb</size><source_type>D</source_type></version><title>Hybrid coarse-fine classification for head pose estimation</title><authors>Haofan Wang, Zhenghua Chen, Yi Zhou</authors><categories>cs.CV</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Head pose estimation, which computes the intrinsic Euler angles (yaw, pitch,\\nroll) from the human, is crucial for gaze estimation, face alignment, and 3D\\nreconstruction. Traditional approaches heavily relies on the accuracy of facial\\nlandmarks. It limits their performances, especially when the visibility of the\\nface is not in good condition. In this paper, to do the estimation without\\nfacial landmarks, we combine the coarse and fine regression output together for\\na deep network. Utilizing more quantization units for the angles, a fine\\nclassifier is trained with the help of other auxiliary coarse units.\\nIntegrating regression is adopted to get the final prediction. The proposed\\napproach is evaluated on three challenging benchmarks. It achieves the\\nstate-of-the-art on AFLW2000, BIWI and performs favorably on AFLW. The code has\\nbeen released on Github.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.06789</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.06789</id><submitter>Jing Hao</submitter><version version=\"v1\"><date>Mon, 21 Jan 2019 04:47:11 GMT</date><size>180kb</size><source_type>D</source_type></version><title>Dual Loomis-Whitney inequalities via information theory</title><authors>Jing Hao and Varun Jog</authors><categories>cs.IT math.IT math.PR</categories><doi>10.3390/e21080809</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish lower bounds on the volume and the surface area of a geometric\\nbody using the size of its slices along different directions. In the first part\\nof the paper, we derive volume bounds for convex bodies using generalized\\nsubadditivity properties of entropy combined with entropy bounds for\\nlog-concave random variables. In the second part, we investigate a new notion\\nof Fisher information which we call the $L_1$-Fisher information, and show that\\ncertain superadditivity properties of the $L_1$-Fisher information lead to\\nlower bounds for the surface areas of polyconvex sets in terms of its slices.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.06885</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.06885</id><submitter>Georg Muntingh PhD</submitter><version version=\"v1\"><date>Mon, 21 Jan 2019 11:33:25 GMT</date><size>1699kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 16:53:02 GMT</date><size>1724kb</size><source_type>D</source_type></version><title>B-spline-like bases for $C^2$ cubics on the Powell-Sabin 12-split</title><authors>Tom Lyche and Georg Muntingh</authors><categories>math.NA cs.CG cs.NA</categories><msc-class>41A15, 65D07, 65D17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For spaces of constant, linear, and quadratic splines of maximal smoothness\\non the Powell-Sabin 12-split of a triangle, the so-called S-bases were recently\\nintroduced. These are simplex spline bases with B-spline-like properties on the\\n12-split of a single triangle, which are tied together across triangles in a\\nB\\\\\\'ezier-like manner.\\n  In this paper we give a formal definition of an S-basis in terms of certain\\nbasic properties. We proceed to investigate the existence of S-bases for the\\naforementioned spaces and additionally the cubic case, resulting in an\\nexhaustive list. From their nature as simplex splines, we derive simple\\ndifferentiation and recurrence formulas to other S-bases. We establish a\\nMarsden identity that gives rise to various quasi-interpolants and domain\\npoints forming an intuitive control net, in terms of which conditions for\\n$C^0$-, $C^1$-, and $C^2$-smoothness are derived.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.06919</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.06919</id><submitter>Mikael Le Pendu</submitter><version version=\"v1\"><date>Mon, 21 Jan 2019 13:11:11 GMT</date><size>9136kb</size><source_type>D</source_type></version><title>A Fourier Disparity Layer representation for Light Fields</title><authors>Mikael Le Pendu and Christine Guillemot and Aljosa Smolic</authors><categories>cs.CV cs.MM</categories><comments>12 pages, 11 figures</comments><doi>10.1109/TIP.2019.2922099</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we present a new Light Field representation for efficient\\nLight Field processing and rendering called Fourier Disparity Layers (FDL). The\\nproposed FDL representation samples the Light Field in the depth (or\\nequivalently the disparity) dimension by decomposing the scene as a discrete\\nsum of layers. The layers can be constructed from various types of Light Field\\ninputs including a set of sub-aperture images, a focal stack, or even a\\ncombination of both. From our derivations in the Fourier domain, the layers are\\nsimply obtained by a regularized least square regression performed\\nindependently at each spatial frequency, which is efficiently parallelized in a\\nGPU implementation. Our model is also used to derive a gradient descent based\\ncalibration step that estimates the input view positions and an optimal set of\\ndisparity values required for the layer construction. Once the layers are\\nknown, they can be simply shifted and filtered to produce different viewpoints\\nof the scene while controlling the focus and simulating a camera aperture of\\narbitrary shape and size. Our implementation in the Fourier domain allows real\\ntime Light Field rendering. Finally, direct applications such as view\\ninterpolation or extrapolation and denoising are presented and evaluated.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.07143</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.07143</id><submitter>Matteo Cremonesi</submitter><version version=\"v1\"><date>Tue, 22 Jan 2019 01:35:59 GMT</date><size>3132kb</size><source_type>D</source_type></version><title>Using Big Data Technologies for HEP Analysis</title><authors>Matteo Cremonesi, Claudio Bellini, Bianny Bian, Luca Canali, Vasileios\\n  Dimakopoulos, Peter Elmer, Ian Fisk, Maria Girone, Oliver Gutsche, Siew-Yan\\n  Hoh, Bo Jayatilaka, Viktor Khristenko, Andrea Luiselli, Andrew Melo,\\n  Evangelos Evangelos, Dominick Olivito, Jacopo Pazzini, Jim Pivarski, Alexey\\n  Svyatkovskiy, Marco Zanetti</authors><categories>cs.DC</categories><doi>10.1051/epjconf/201921406030</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The HEP community is approaching an era were the excellent performances of\\nthe particle accelerators in delivering collision at high rate will force the\\nexperiments to record a large amount of information. The growing size of the\\ndatasets could potentially become a limiting factor in the capability to\\nproduce scientific results timely and efficiently. Recently, new technologies\\nand new approaches have been developed in industry to answer to the necessity\\nto retrieve information as quickly as possible to analyze PB and EB datasets.\\nProviding the scientists with these modern computing tools will lead to\\nrethinking the principles of data analysis in HEP, making the overall\\nscientific process faster and smoother.\\n  In this paper, we are presenting the latest developments and the most recent\\nresults on the usage of Apache Spark for HEP analysis. The study aims at\\nevaluating the efficiency of the application of the new tools both\\nquantitatively, by measuring the performances, and qualitatively, focusing on\\nthe user experience. The first goal is achieved by developing a data reduction\\nfacility: working together with CERN Openlab and Intel, CMS replicates a real\\nphysics search using Spark-based technologies, with the ambition of reducing 1\\nPB of public data in 5 hours, collected by the CMS experiment, to 1 TB of data\\nin a format suitable for physics analysis.\\n  The second goal is achieved by implementing multiple physics use-cases in\\nApache Spark using as input preprocessed datasets derived from official CMS\\ndata and simulation. By performing different end-analyses up to the publication\\nplots on different hardware, feasibility, usability and portability are\\ncompared to the ones of a traditional ROOT-based workflow.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.07503</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.07503</id><submitter>Oscar Defrain</submitter><version version=\"v1\"><date>Tue, 22 Jan 2019 18:26:39 GMT</date><size>79kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 25 Mar 2019 12:52:59 GMT</date><size>97kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 5 Apr 2019 10:34:36 GMT</date><size>118kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Sun, 29 Sep 2019 17:01:45 GMT</date><size>70kb</size><source_type>D</source_type></version><title>Dualization in lattices given by implicational bases</title><authors>Oscar Defrain and Lhouari Nourine</authors><categories>cs.DM cs.DS</categories><comments>11 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It was recently proved that the dualization in lattices given by\\nimplicational bases is impossible in output-polynomial time unless P=NP. In\\nthis paper, we~show that this result holds even when the premises in the\\nimplicational base are of size at most two. Then we show using hypergraph\\ndualization that the problem can be solved in output quasi-polynomial time\\nwhenever the implicational base has bounded independent-width, defined as the\\nsize of a maximum set of implications having independent conclusions. Lattices\\nthat share this property include distributive lattices coded by the ideals of\\nan interval order, when both the independent-width and the size of the premises\\nequal one.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.07621</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.07621</id><submitter>Eric Steinberger</submitter><version version=\"v1\"><date>Tue, 22 Jan 2019 21:52:29 GMT</date><size>38kb</size></version><version version=\"v2\"><date>Sun, 10 Feb 2019 15:04:02 GMT</date><size>39kb</size></version><version version=\"v3\"><date>Wed, 13 Mar 2019 17:32:41 GMT</date><size>45kb</size></version><version version=\"v4\"><date>Fri, 4 Oct 2019 14:55:20 GMT</date><size>32kb</size></version><title>Single Deep Counterfactual Regret Minimization</title><authors>Eric Steinberger</authors><categories>cs.GT cs.AI cs.LG cs.MA</categories><comments>4th version changes: fix minor notational errors; improve format;\\n  incorporate structural feedback from NeurIPS review; *RESULTS ARE UNCHANGED*</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Counterfactual Regret Minimization (CFR) is the most successful algorithm for\\nfinding approximate Nash equilibria in imperfect information games. However,\\nCFR\\'s reliance on full game-tree traversals limits its scalability. For this\\nreason, the game\\'s state- and action-space is often abstracted (i.e.\\nsimplified) for CFR, and the resulting strategy is then translated back to the\\nfull game, which requires extensive expert-knowledge and often converges to\\nhighly exploitable policies. A recently proposed method, Deep CFR, applies deep\\nlearning directly to CFR, allowing the agent to intrinsically abstract and\\ngeneralize over the state-space from samples, without requiring expert\\nknowledge. In this paper, we introduce Single Deep CFR (SD-CFR), a simplified\\nvariant of Deep CFR that has a lower overall approximation error by avoiding\\nthe training of an average strategy network. We show that SD-CFR is more\\nattractive from a theoretical perspective and empirically outperforms Deep CFR\\nwith respect to exploitability and one-on-one play in poker.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.07651</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.07651</id><submitter>Hwiyeol Jo</submitter><version version=\"v1\"><date>Tue, 22 Jan 2019 23:55:49 GMT</date><size>371kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 6 Mar 2019 03:59:39 GMT</date><size>0kb</size><source_type>I</source_type></version><version version=\"v3\"><date>Sat, 28 Sep 2019 06:55:37 GMT</date><size>442kb</size><source_type>D</source_type></version><title>Delta-training: Simple Semi-Supervised Text Classification using\\n  Pretrained Word Embeddings</title><authors>Hwiyeol Jo, Ceyda Cinarel</authors><categories>cs.CL</categories><comments>Accepted at EMNLP-IJCNLP2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel and simple method for semi-supervised text classification.\\nThe method stems from the hypothesis that a classifier with pretrained word\\nembeddings always outperforms the same classifier with randomly initialized\\nword embeddings, as empirically observed in NLP tasks. Our method first builds\\ntwo sets of classifiers as a form of model ensemble, and then initializes their\\nword embeddings differently: one using random, the other using pretrained word\\nembeddings. We focus on different predictions between the two classifiers on\\nunlabeled data while following the self-training framework. We also use\\nearly-stopping in meta-epoch to improve the performance of our method. Our\\nmethod, Delta-training, outperforms the self-training and the co-training\\nframework in 4 different text classification datasets, showing robustness\\nagainst error accumulation.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.08393</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.08393</id><submitter>Alexander Haberl</submitter><version version=\"v1\"><date>Thu, 24 Jan 2019 13:19:52 GMT</date><size>1871kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 3 Jul 2019 16:46:43 GMT</date><size>4677kb</size><source_type>D</source_type></version><title>Adaptive boundary element methods for the computation of the\\n  electrostatic capacity on complex polyhedra</title><authors>Timo Betcke, Alexander Haberl, and Dirk Praetorius</authors><categories>math.NA cs.NA physics.comp-ph</categories><journal-ref>Journal of Computational Physics, 397 (2019), 108837</journal-ref><doi>10.1016/j.jcp.2019.07.036</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The accurate computation of the electrostatic capacity of three dimensional\\nobjects is a fascinating benchmark problem with a long and rich history. In\\nparticular, the capacity of the unit cube has widely been studied, and recent\\nadvances allow to compute its capacity to more than ten digits of accuracy.\\nHowever, the accurate computation of the capacity for general three dimensional\\npolyhedra is still an open problem. In this paper, we propose a new algorithm\\nbased on a combination of ZZ-type a posteriori error estimation and effective\\noperator preconditioned boundary integral formulations to easily compute the\\ncapacity of complex three dimensional polyhedra to 5 digits and more. While\\nthis paper focuses on the capacity as a benchmark problem, it also discusses\\nimplementational issues of adaptive boundary element solvers, and we provide\\ncodes based on the boundary element package Bempp to make the underlying\\ntechniques accessible to a wide range of practical problems.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.09006</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.09006</id><submitter>Martin Engelcke</submitter><version version=\"v1\"><date>Fri, 25 Jan 2019 18:11:52 GMT</date><size>420kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 10:12:47 GMT</date><size>387kb</size><source_type>D</source_type></version><title>On the Limitations of Representing Functions on Sets</title><authors>Edward Wagstaff, Fabian B. Fuchs, Martin Engelcke, Ingmar Posner,\\n  Michael Osborne</authors><categories>cs.LG cs.AI cs.NE cs.RO stat.ML</categories><comments>Published at the International Conference on Machine Learning (2019)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work on the representation of functions on sets has considered the use\\nof summation in a latent space to enforce permutation invariance. In\\nparticular, it has been conjectured that the dimension of this latent space may\\nremain fixed as the cardinality of the sets under consideration increases.\\nHowever, we demonstrate that the analysis leading to this conjecture requires\\nmappings which are highly discontinuous and argue that this is only of limited\\npractical use. Motivated by this observation, we prove that an implementation\\nof this model via continuous mappings (as provided by e.g. neural networks or\\nGaussian processes) actually imposes a constraint on the dimensionality of the\\nlatent space. Practical universal function representation for set inputs can\\nonly be achieved with a latent dimension at least the size of the maximum\\nnumber of input elements.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.09007</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.09007</id><submitter>Thomas Trogdon</submitter><version version=\"v1\"><date>Fri, 25 Jan 2019 18:14:49 GMT</date><size>122kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 1 Feb 2019 04:05:44 GMT</date><size>122kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 2 Oct 2019 18:47:09 GMT</date><size>128kb</size><source_type>D</source_type></version><title>The conjugate gradient algorithm on well-conditioned Wishart matrices is\\n  almost deterministic</title><authors>Percy Deift and Thomas Trogdon</authors><categories>math.NA cs.CC cs.NA math.PR</categories><msc-class>65F10, 60B20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the number of iterations required to solve a random positive\\ndefinite linear system with the conjugate gradient algorithm is almost\\ndeterministic for large matrices. We treat the case of Wishart matrices $W =\\nXX^*$ where $X$ is $n \\\\times m$ and $n/m \\\\sim d$ for $0 &lt; d &lt; 1$. Precisely, we\\nprove that for most choices of error tolerance, as the matrix increases in\\nsize, the probability that the iteration count deviates from an explicit\\ndeterministic value tends to zero. In addition, for a fixed iteration count, we\\nshow that the norm of the error vector and the norm of the residual converge\\nexponentially fast in probability, converge in mean and converge almost surely.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.09197</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.09197</id><submitter>Ahmed Shahin</submitter><version version=\"v1\"><date>Sat, 26 Jan 2019 10:55:27 GMT</date><size>1501kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 4 Apr 2019 13:27:37 GMT</date><size>1501kb</size><source_type>D</source_type></version><title>Deep Convolutional Encoder-Decoders with Aggregated Multi-Resolution\\n  Skip Connections for Skin Lesion Segmentation</title><authors>Ahmed H. Shahin, Karim Amer, Mustafa A. Elattar</authors><categories>cs.CV</categories><comments>Accepted for publication at the IEEE International Symposium on\\n  Biomedical Imaging (ISBI 2019)</comments><doi>10.1109/ISBI.2019.8759172</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The prevalence of skin melanoma is rapidly increasing as well as the recorded\\ndeath cases of its patients. Automatic image segmentation tools play an\\nimportant role in providing standardized computer-assisted analysis for skin\\nmelanoma patients. Current state-of-the-art segmentation methods are based on\\nfully convolutional neural networks, which utilize an encoder-decoder approach.\\nHowever, these methods produce coarse segmentation masks due to the loss of\\nlocation information during the encoding layers. Inspired by Pyramid Scene\\nParsing Network (PSP-Net), we propose an encoder-decoder model that utilizes\\npyramid pooling modules in the deep skip connections which aggregate the global\\ncontext and compensate for the lost spatial information. We trained and\\nvalidated our approach using ISIC 2018: Skin Lesion Analysis Towards Melanoma\\nDetection grand challenge dataset. Our approach showed a validation accuracy\\nwith a Jaccard index of 0.837, which outperforms U-Net. We believe that with\\nthis reported reliable accuracy, this method can be introduced for clinical\\npractice.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.09326</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.09326</id><submitter>Chao Qu</submitter><version version=\"v1\"><date>Sun, 27 Jan 2019 05:56:42 GMT</date><size>666kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 19 Mar 2019 02:21:34 GMT</date><size>712kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 9 Jul 2019 05:09:46 GMT</date><size>707kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Sat, 28 Sep 2019 07:56:33 GMT</date><size>3244kb</size><source_type>D</source_type></version><title>Value Propagation for Decentralized Networked Deep Multi-agent\\n  Reinforcement Learning</title><authors>Chao Qu, Shie Mannor, Huan Xu, Yuan Qi, Le Song, Junwu Xiong</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the networked multi-agent reinforcement learning (MARL) problem\\nin a fully decentralized setting, where agents learn to coordinate to achieve\\nthe joint success. This problem is widely encountered in many areas including\\ntraffic control, distributed control, and smart grids. We assume that the\\nreward function for each agent can be different and observed only locally by\\nthe agent itself. Furthermore, each agent is located at a node of a\\ncommunication network and can exchanges information only with its neighbors.\\nUsing softmax temporal consistency and a decentralized optimization method, we\\nobtain a principled and data-efficient iterative algorithm. In the first step\\nof each iteration, an agent computes its local policy and value gradients and\\nthen updates only policy parameters. In the second step, the agent propagates\\nto its neighbors the messages based on its value function and then updates its\\nown value function. Hence we name the algorithm value propagation. We prove a\\nnon-asymptotic convergence rate 1/T with the nonlinear function approximation.\\nTo the best of our knowledge, it is the first MARL algorithm with convergence\\nguarantee in the control, off-policy and non-linear function approximation\\nsetting. We empirically demonstrate the effectiveness of our approach in\\nexperiments.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.09491</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.09491</id><submitter>Stanislav Fort</submitter><version version=\"v1\"><date>Mon, 28 Jan 2019 02:49:46 GMT</date><size>4444kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 14:29:53 GMT</date><size>5773kb</size><source_type>D</source_type></version><title>Stiffness: A New Perspective on Generalization in Neural Networks</title><authors>Stanislav Fort, Pawe{\\\\l} Krzysztof Nowak, Stanislaw Jastrzebski, Srini\\n  Narayanan</authors><categories>cs.LG cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate neural network training and generalization using the concept\\nof stiffness. We measure how stiff a network is by looking at how a small\\ngradient step on one example affects the loss on another example. In\\nparticular, we study how stiffness depends on 1) class membership, 2) distance\\nbetween data points in the input space, 3) training iteration, and 4) learning\\nrate. We experiment on MNIST, FASHION MNIST, and CIFAR-10 using fully-connected\\nand convolutional neural networks. Our results demonstrate that stiffness is a\\nuseful concept for diagnosing and characterizing generalization. We observe\\nthat small learning rates reliably lead to higher stiffness at a given epoch as\\nwell as at a given training loss. In addition, we measure how stiffness between\\ntwo data points depends on their mutual input space distance, and establish the\\nconcept of a dynamical critical length that characterizes the distance over\\nwhich data points react similarly to gradient updates. The dynamical critical\\nlength decreases with training and the higher the learning rate, the smaller\\nthe critical length.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.09735</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.09735</id><submitter>Emiliano De Cristofaro</submitter><version version=\"v1\"><date>Mon, 28 Jan 2019 15:29:22 GMT</date><size>5015kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 29 Jan 2019 16:45:49 GMT</date><size>5015kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 17 May 2019 21:05:03 GMT</date><size>5012kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Mon, 29 Jul 2019 16:03:38 GMT</date><size>5005kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Fri, 4 Oct 2019 12:59:30 GMT</date><size>5005kb</size><source_type>D</source_type></version><title>&quot;And We Will Fight For Our Race!&quot; A Measurement Study of Genetic Testing\\n  Conversations on Reddit and 4chan</title><authors>Alexandros Mittos, Savvas Zannettou, Jeremy Blackburn, Emiliano De\\n  Cristofaro</authors><categories>cs.CY</categories><comments>This is the full version of the paper, with same title, appearing in\\n  the 14th AAAI Conference on Web and Social Media (ICWSM 2020). Please cite\\n  the ICWSM version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Progress in genomics has enabled the emergence of a booming market for\\n&quot;direct-to-consumer&quot; genetic testing. Nowadays, companies like 23andMe and\\nAncestryDNA provide affordable health, genealogy, and ancestry reports, and\\nhave already tested tens of millions of customers. At the same time, alt- and\\nfar-right groups have also taken an interest in genetic testing, using them to\\nattack minorities and prove their genetic &quot;purity.&quot; In this paper, we present a\\nmeasurement study shedding light on how genetic testing is being discussed on\\nWeb communities in Reddit and 4chan. We collect 1.3M comments posted over 27\\nmonths on the two platforms, using a set of 280 keywords related to genetic\\ntesting. We then use NLP and computer vision tools to identify trends, themes,\\nand topics of discussion.\\n  Our analysis shows that genetic testing attracts a lot of attention on Reddit\\nand 4chan, with discussions often including highly toxic language expressed\\nthrough hateful, racist, and misogynistic comments. In particular, on 4chan\\'s\\npolitically incorrect board (/pol/), content from genetic testing conversations\\ninvolves several alt-right personalities and openly antisemitic rhetoric, often\\nconveyed through memes. Finally, we find that discussions build around user\\ngroups, from technology enthusiasts to communities promoting fringe political\\nviews.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.09849</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.09849</id><submitter>Vahid Partovi Nia</submitter><version version=\"v1\"><date>Mon, 28 Jan 2019 17:42:10 GMT</date><size>532kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 21:40:39 GMT</date><size>658kb</size><source_type>D</source_type></version><title>Activation Adaptation in Neural Networks</title><authors>Farnoush Farhadi, Vahid Partovi Nia, Andrea Lodi</authors><categories>cs.LG stat.ML</categories><msc-class>92B20, 68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many neural network architectures rely on the choice of the activation\\nfunction for each hidden layer. Given the activation function, the neural\\nnetwork is trained over the bias and the weight parameters. The bias catches\\nthe center of the activation, and the weights capture the scale. Here we\\npropose to train the network over a shape parameter as well. This view allows\\neach neuron to tune its own activation function and adapt the neuron curvature\\ntowards a better prediction. This modification only adds one further equation\\nto the back-propagation for each neuron. Re-formalizing activation functions as\\nCDF generalizes the class of activation function extensively. We aimed at\\ngeneralizing an extensive class of activation functions to study: i) skewness\\nand ii) smoothness of activation functions. Here we introduce adaptive Gumbel\\nactivation function as a bridge between Gumbel and sigmoid. A similar approach\\nis used to invent a smooth version of ReLU. Our comparison with common\\nactivation functions suggests different data representation especially in early\\nneural network layers. This adaptation also provides prediction improvement.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.09972</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.09972</id><submitter>David Mac\\\\^edo</submitter><version version=\"v1\"><date>Mon, 28 Jan 2019 19:55:42 GMT</date><size>3040kb</size><source_type>D</source_type></version><title>Heartbeat Anomaly Detection using Adversarial Oversampling</title><authors>Jefferson L. P. Lima, David Mac\\\\^edo, Cleber Zanchettin</authors><categories>cs.LG cs.AI cs.CV cs.NE stat.ML</categories><journal-ref>2019 International Joint Conference on Neural Networks (IJCNN)</journal-ref><doi>10.1109/IJCNN.2019.8852242</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cardiovascular diseases are one of the most common causes of death in the\\nworld. Prevention, knowledge of previous cases in the family, and early\\ndetection is the best strategy to reduce this fact. Different machine learning\\napproaches to automatic diagnostic are being proposed to this task. As in most\\nhealth problems, the imbalance between examples and classes is predominant in\\nthis problem and affects the performance of the automated solution. In this\\npaper, we address the classification of heartbeats images in different\\ncardiovascular diseases. We propose a two-dimensional Convolutional Neural\\nNetwork for classification after using a InfoGAN architecture for generating\\nsynthetic images to unbalanced classes. We call this proposal Adversarial\\nOversampling and compare it with the classical oversampling methods as SMOTE,\\nADASYN, and RandomOversampling. The results show that the proposed approach\\nimproves the classifier performance for the minority classes without harming\\nthe performance in the balanced classes.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.10187</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.10187</id><submitter>Matthias Hofer</submitter><version version=\"v1\"><date>Tue, 29 Jan 2019 09:20:37 GMT</date><size>8233kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 11 Feb 2019 10:06:29 GMT</date><size>8233kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 4 Mar 2019 17:13:28 GMT</date><size>4912kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Tue, 1 Oct 2019 13:38:25 GMT</date><size>4917kb</size><source_type>D</source_type></version><title>Iterative Learning Control for Fast and Accurate Position Tracking with\\n  an Articulated Soft Robotic Arm</title><authors>Matthias Hofer, Lukas Spannagl and Raffaello D\\'Andrea</authors><categories>cs.RO</categories><comments>This work has been accepted to the 2019 IEEE/RSJ International\\n  Conference on Intelligent Robots and Systems (IROS) for publication.\\n  Copyright may be transferred without notice, after which this version may no\\n  longer be accessible</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the application of an iterative learning control scheme\\nto improve the position tracking performance for an articulated soft robotic\\narm during aggressive maneuvers. Two antagonistically arranged, inflatable\\nbellows actuate the robotic arm and provide high compliance while enabling fast\\nactuation. Switching valves are used for pressure control of the soft\\nactuators. A norm-optimal iterative learning control scheme based on a linear\\nmodel of the system is presented and applied in parallel with a feedback\\ncontroller. The learning scheme is experimentally evaluated on an aggressive\\ntrajectory involving set point shifts of 60 degrees within 0.2 seconds. The\\neffectiveness of the learning approach is demonstrated by a reduction of the\\nroot-mean-square tracking error from 13 degrees to less than 2 degrees after\\napplying the learning scheme for less than 30 iterations.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.10615</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.10615</id><submitter>Shale Xiong</submitter><version version=\"v1\"><date>Tue, 29 Jan 2019 23:47:02 GMT</date><size>463kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 14:48:40 GMT</date><size>431kb</size></version><title>Data Consistency in Transactional Storage Systems: a Centralised\\n  Approach</title><authors>Shale Xiong, Andrea Cerone, Azalea Raad, Philippa Gardner</authors><categories>cs.LO cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an interleaving operational semantics for describing the\\nclient-observable behaviour of atomic transactions on distributed key-value\\nstores. Our semantics builds on abstract states comprising centralised, global\\nkey-value stores and partial client views. We provide operational definitions\\nof consistency models for our key-value stores which are shown to be equivalent\\nto the well-known declarative definitions of consistency model for execution\\ngraphs. We explore two immediate applications of our semantics: specific\\nprotocols of geo-replicated databases (e.g. COPS) and partitioned databases\\n(e.g. Clock-SI) can be shown to be correct for a specific consistency model by\\nembedding them in our centralised semantics; programs can be directly shown to\\nhave invariant properties such as robustness results against a weak consistency\\nmodel.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.11133</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.11133</id><submitter>Tao Li</submitter><version version=\"v1\"><date>Thu, 17 Jan 2019 01:26:44 GMT</date><size>634kb</size></version><title>Multiclass Information Flow Propagation Control under Vehicle-to-Vehicle\\n  Communication Environments</title><authors>Jian Wang, Srinivas Peeta, Lili Lu, Tao Li</authors><categories>cs.SY cs.NI</categories><comments>Transportation Research Part B: Methodological, 2019</comments><doi>10.1016/j.trb.2019.09.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most existing models for information flow propagation in a vehicle-to-vehicle\\n(V2V) communications environment are descriptive. They lack capabilities to\\ncontrol information flow, which may preclude their ability to meet application\\nneeds, including the need to propagate different information types\\nsimultaneously to different target locations within corresponding time delay\\nbounds. This study proposes a queuing-based modeling approach to control the\\npropagation of information flow of multiple classes. Two control parameters\\nassociated with a vehicle are leveraged to control the propagation performance\\nof different information classes. A two-layer model is developed to\\ncharacterize the information flow propagation wave (IFPW) under the designed\\nqueuing strategy. The upper layer is formulated as integro-differential\\nequations to characterize the spatiotemporal information dissemination due to\\nV2V communication. The lower layer characterizes the traffic flow dynamics\\nusing the Lighthill-Whitham-Richards model. The analytical solution of the\\nasymptotic density of informed vehicles and the necessary condition for\\nexistence of the IFPW are derived for homogeneous traffic conditions. Numerical\\nexperiments provide insights on the impact of the mean communication service\\nrate on information spread and its spatial coverage. Further, a numerical\\nsolution method is developed to solve the two-layer model, which aids in\\nestimating the impacts of the control parameters in the queuing strategy on the\\nIFPW speed under homogeneous and heterogeneous conditions. The proposed\\nmodeling approach enables controlling the propagation of information of\\ndifferent information classes to meet application needs, which can assist\\ntraffic managers to design effective and efficient traffic management and\\ncontrol strategies under V2V communications.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1901.11371</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1901.11371</id><submitter>Haizhao Yang</submitter><version version=\"v1\"><date>Thu, 31 Jan 2019 14:27:13 GMT</date><size>2490kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 29 Jul 2019 22:29:09 GMT</date><size>4769kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 4 Oct 2019 21:51:33 GMT</date><size>2896kb</size><source_type>D</source_type></version><title>A Hierarchical Butterfly LU Preconditioner for Two-Dimensional\\n  Electromagnetic Scattering Problems Involving Open Surfaces</title><authors>Yang Liu and Haizhao Yang</authors><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a hierarchical interpolative decomposition butterfly-LU\\nfactorization (H-IDBF-LU) preconditioner for solving two-dimensional\\nelectric-field integral equations (EFIEs) in electromagnetic scattering\\nproblems of perfect electrically conducting objects with open surfaces.\\nH-IDBF-LU leverages the interpolative decomposition butterfly factorization\\n(IDBF) to compress dense blocks of the discretized EFIE operator to expedite\\nits application; this compressed operator also serves as an approximate LU\\nfactorization of the EFIE operator leading to an efficient preconditioner in\\niterative solvers. Both the memory requirement and computational cost of the\\nH-IDBF-LU solver scale as $O(N\\\\log^2 N)$ in one iteration; the total number of\\niterations required for a reasonably good accuracy scales as $O(1)$ to\\n$O(\\\\log^2N)$ in all of our numerical tests. The efficacy and accuracy of the\\nproposed preconditioned iterative solver are demonstrated via its application\\nto a broad range of scatterers involving up to $100$ million unknowns.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.00071</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.00071</id><submitter>Nidham Gazagnadou</submitter><version version=\"v1\"><date>Thu, 31 Jan 2019 21:14:19 GMT</date><size>746kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 31 Jul 2019 11:14:28 GMT</date><size>33337kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 18 Sep 2019 09:38:32 GMT</date><size>11800kb</size><source_type>D</source_type></version><title>Optimal mini-batch and step sizes for SAGA</title><authors>Nidham Gazagnadou and Robert M. Gower and Joseph Salmon</authors><categories>math.OC cs.LG stat.ML</categories><comments>34 pages, 27 figures</comments><msc-class>90C15, 90C25, 68W20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently it has been shown that the step sizes of a family of variance\\nreduced gradient methods called the JacSketch methods depend on the expected\\nsmoothness constant. In particular, if this expected smoothness constant could\\nbe calculated a priori, then one could safely set much larger step sizes which\\nwould result in a much faster convergence rate. We fill in this gap, and\\nprovide simple closed form expressions for the expected smoothness constant and\\ncareful numerical experiments verifying these bounds. Using these bounds, and\\nsince the SAGA algorithm is part of this JacSketch family, we suggest a new\\nstandard practice for setting the step sizes and mini-batch size for SAGA that\\nare competitive with a numerical grid search. Furthermore, we can now show that\\nthe total complexity of the SAGA algorithm decreases linearly in the mini-batch\\nsize up to a pre-defined value: the optimal mini-batch size. This is a rare\\nresult in the stochastic variance reduced literature, only previously shown for\\nthe Katyusha algorithm. Finally we conjecture that this is the case for many\\nother stochastic variance reduced methods and that our bounds and analysis of\\nthe expected smoothness constant is key to extending these results.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.00230</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.00230</id><submitter>Mladen Kova\\\\v{c}evi\\\\\\'c</submitter><version version=\"v1\"><date>Fri, 1 Feb 2019 08:50:50 GMT</date><size>12kb</size></version><version version=\"v2\"><date>Thu, 6 Jun 2019 07:44:17 GMT</date><size>12kb</size></version><title>Some Enumeration Problems in the Duplication-Loss Model of Genome\\n  Rearrangement</title><authors>Mladen Kova\\\\v{c}evi\\\\\\'c, Sanja Brdar, Vladimir Crnojevi\\\\\\'c</authors><categories>cs.DM cs.IT math.IT q-bio.GN q-bio.QM</categories><comments>5 pages (double-column). To be presented at the 2019 IEEE\\n  International Symposium on Information Theory (ISIT), Paris, France</comments><msc-class>05A05, 68R05, 92B99, 92D20, 94B25</msc-class><doi>10.1109/ISIT.2019.8849847</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tandem-duplication-random-loss (TDRL) is an important genome rearrangement\\noperation studied in evolutionary biology. This paper investigates some of the\\nformal properties of TDRL operations on the symmetric group (the space of\\npermutations over an $ n $-set). In particular, the cardinality of `balls\\' of\\nradius one in the TDRL metric, as well as the cardinality of the maximum\\nintersection of two such balls, are determined. The corresponding problems for\\nthe so-called mirror (or palindromic) TDRL rearrangement operations are also\\nsolved. The results represent an initial step in the study of error correction\\nand reconstruction problems in this context and are of potential interest in\\nDNA-based data storage applications.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.00658</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.00658</id><submitter>Pedro Cisneros-Velarde</submitter><version version=\"v1\"><date>Sat, 2 Feb 2019 07:19:31 GMT</date><size>1689kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 28 Jul 2019 23:23:27 GMT</date><size>1682kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 4 Oct 2019 03:11:17 GMT</date><size>1691kb</size><source_type>D</source_type></version><title>Polarization and Fluctuations in Signed Social Networks</title><authors>Pedro Cisneros-Velarde, Kevin S. Chan, Francesco Bullo</authors><categories>cs.SI physics.soc-ph</categories><comments>15 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much recent research on social networks has focused on the modeling and\\nanalysis of how opinions evolve as a function of interpersonal relationships.\\nIt is also of great interest to model and understand the implications of\\nfriendly and antagonistic relationships. In this paper, we propose a new,\\nsimple and intuitive model that incorporates the socio-psychological phenomenon\\nof the boomerang effect in opinion dynamics. We establish that, under certain\\nconditions on the structure of the signed network that corresponds to the\\nso-called structural balance property, the opinions in the network polarize.\\nCompared to other models in the literature, our model displays a richer and\\nperhaps more intuitive behavior of the opinions when the social network does\\nnot satisfy structural balance. In particular, we analyze signed networks in\\nwhich the opinions show persistent fluctuations (including the case of the\\nso-called clustering balance).\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.00993</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.00993</id><submitter>Kai Sun</submitter><version version=\"v1\"><date>Sun, 3 Feb 2019 23:44:10 GMT</date><size>31kb</size></version><version version=\"v2\"><date>Wed, 14 Aug 2019 20:32:42 GMT</date><size>1011kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 1 Oct 2019 20:25:33 GMT</date><size>605kb</size><source_type>D</source_type></version><title>Improving Question Answering with External Knowledge</title><authors>Xiaoman Pan, Kai Sun, Dian Yu, Jianshu Chen, Heng Ji, Claire Cardie,\\n  Dong Yu</authors><categories>cs.CL</categories><comments>Accepted to MRQA (2019)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on multiple-choice question answering (QA) tasks in subject areas\\nsuch as science, where we require both broad background knowledge and the facts\\nfrom the given subject-area reference corpus. In this work, we explore simple\\nyet effective methods for exploiting two sources of external knowledge for\\nsubject-area QA. The first enriches the original subject-area reference corpus\\nwith relevant text snippets extracted from an open-domain resource (i.e.,\\nWikipedia) that cover potentially ambiguous concepts in the question and answer\\noptions. As in other QA research, the second method simply increases the amount\\nof training data by appending additional in-domain subject-area instances.\\n  Experiments on three challenging multiple-choice science QA tasks (i.e.,\\nARC-Easy, ARC-Challenge, and OpenBookQA) demonstrate the effectiveness of our\\nmethods: in comparison to the previous state-of-the-art, we obtain absolute\\ngains in accuracy of up to 8.1%, 13.0%, and 12.8%, respectively. While we\\nobserve consistent gains when we introduce knowledge from Wikipedia, we find\\nthat employing additional QA training instances is not uniformly helpful:\\nperformance degrades when the added instances exhibit a higher level of\\ndifficulty than the original training data. As one of the first studies on\\nexploiting unstructured external knowledge for subject-area QA, we hope our\\nmethods, observations, and discussion of the exposed limitations may shed light\\non further developments in the area.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.01219</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.01219</id><submitter>Joseph Lam-Weil</submitter><version version=\"v1\"><date>Fri, 1 Feb 2019 12:42:12 GMT</date><size>562kb</size></version><version version=\"v2\"><date>Wed, 2 Oct 2019 14:59:51 GMT</date><size>62kb</size></version><title>Local minimax rates for closeness testing of discrete distributions</title><authors>Joseph Lam-Weil, Alexandra Carpentier, Bharath K. Sriperumbudur</authors><categories>math.ST cs.IT cs.LG math.IT stat.ML stat.TH</categories><comments>62 pages</comments><msc-class>62F03, 62G10, 62F35</msc-class><acm-class>G.3; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the closeness testing (or two-sample testing) problem in the\\nPoisson vector model - which is known to be asymptotically equivalent to the\\nmodel of multinomial distributions. The goal is to distinguish whether two data\\nsamples are drawn from the same unspecified distribution, or whether their\\nrespective distributions are separated in $L_1$-norm. In this paper, we focus\\non adapting the rate to the shape of the underlying distributions, i.e. we\\nconsider a local minimax setting. We provide, to the best of our knowledge, the\\nfirst local minimax rate for the separation distance up to logarithmic factors,\\ntogether with a test that achieves it. In view of the rate, closeness testing\\nturns out to be substantially harder than the related one-sample testing\\nproblem over a wide range of cases.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.01333</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.01333</id><submitter>Barbara McGillivray</submitter><version version=\"v1\"><date>Mon, 4 Feb 2019 17:44:20 GMT</date><size>290kb</size></version><version version=\"v2\"><date>Thu, 21 Feb 2019 15:06:23 GMT</date><size>333kb</size></version><version version=\"v3\"><date>Thu, 12 Sep 2019 17:55:16 GMT</date><size>381kb</size></version><version version=\"v4\"><date>Fri, 4 Oct 2019 17:07:07 GMT</date><size>384kb</size></version><title>The relationship between usage and citations in an open access mega\\n  journal</title><authors>Barbara McGillivray (The Alan Turing Institute and University of\\n  Cambridge) and Mathias Astell (Hindawi Limited)</authors><categories>cs.DL stat.AP</categories><comments>22 pages, 7 figures. Scientometrics (2019)</comments><doi>10.1007/s11192-019-03228-3</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  How do the level of usage of an article, the timeframe of its usage and its\\nsubject area relate to the number of citations it accrues? This paper aims to\\nanswer this question through an observational study of usage and citation data\\ncollected about the multidisciplinary, open access mega-journal Scientific\\nReports. This observational study answers these questions using the following\\nmethods: an overlap analysis of most read and top-cited articles; Spearman\\ncorrelation tests between total citation counts over two years and usage over\\nvarious timeframes; a comparison of first months of citation for most read and\\nall articles; a Wilcoxon test on the distribution of total citations of early\\ncited articles and the distribution of total citations of all other articles.\\nAll analyses were performed using the programming language R. As Scientific\\nReports is a multidisciplinary journal covering all natural and clinical\\nsciences, we also looked at the differences across subjects. We found a\\nmoderate correlation between usage in the first year and citations in the first\\ntwo years since publication, and that articles with high usage in the first 6\\nmonths are more likely to have their first citation earlier (Wilcoxon=1811500,\\np &lt; 0.0001), which is also related to higher citations in the first two years\\n(Wilcoxon=8071200, p &lt; 0.0001). As this final assertion is inferred based on\\nthe results of the other elements of this paper, it requires further analysis.\\nMoreover, our choice of a 2 year window for our analysis did not consider the\\narticles\\' citation half-life, and our use of Scientific Reports (a journal that\\nis atypical compared to most academic journals) as the source of the articles\\nanalysed has likely played a role in our findings, and so analysing a longer\\ntimeframe and carrying out similar analysis on a different journal (or group of\\njournals) may lead to different conclusions.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.01436</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.01436</id><submitter>Luis Scoccola</submitter><version version=\"v1\"><date>Mon, 4 Feb 2019 19:25:30 GMT</date><size>294kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 30 Apr 2019 21:15:55 GMT</date><size>842kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sat, 28 Sep 2019 16:29:15 GMT</date><size>77kb</size><source_type>D</source_type></version><title>Visualization tools for parameter selection in cluster analysis</title><authors>Alexander Rolle and Luis Scoccola</authors><categories>stat.ML cs.LG</categories><comments>6 pages</comments><msc-class>62H30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an algorithm, HPREF (Hierarchical Partitioning by Repeated\\nFeatures), that produces a hierarchical partition of a set of clusterings of a\\nfixed dataset, such as sets of clusterings produced by running a clustering\\nalgorithm with a range of parameters. This gives geometric structure to such\\nsets of clustering, and can be used to visualize the set of results one obtains\\nby running a clustering algorithm with a range of parameters.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.01487</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.01487</id><submitter>Ivo D\\\\&quot;untsch</submitter><version version=\"v1\"><date>Mon, 4 Feb 2019 22:47:12 GMT</date><size>19kb</size></version><title>Confusion matrices and rough set data analysis</title><authors>Ivo D\\\\&quot;untsch and G\\\\&quot;unther Gediga</authors><categories>cs.LG stat.ML</categories><comments>Equal authorship implied. To appear in the Proceedings of the 2019\\n  International Conference on Pattern Recognition and Intelligent Systems (PRIS\\n  2019)</comments><doi>10.1088/1742-6596/1229/1/012055</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A widespread approach in machine learning to evaluate the quality of a\\nclassifier is to cross -- classify predicted and actual decision classes in a\\nconfusion matrix, also called error matrix. A classification tool which does\\nnot assume distributional parameters but only information contained in the data\\nis based on the rough set data model which assumes that knowledge is given only\\nup to a certain granularity. Using this assumption and the technique of\\nconfusion matrices, we define various indices and classifiers based on rough\\nconfusion matrices.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.01716</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.01716</id><submitter>Adrien Laurent</submitter><version version=\"v1\"><date>Tue, 5 Feb 2019 14:49:50 GMT</date><size>599kb</size></version><version version=\"v2\"><date>Sat, 5 Oct 2019 10:03:39 GMT</date><size>472kb</size></version><title>Multirevolution integrators for differential equations with fast\\n  stochastic oscillations</title><authors>Adrien Laurent, Gilles Vilmart</authors><categories>math.NA cs.NA</categories><comments>27 pages</comments><msc-class>60H35, 35Q55, 34E13</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new methodology based on the multirevolution idea for\\nconstructing integrators for stochastic differential equations in the situation\\nwhere the fast oscillations themselves are driven by a Stratonovich noise.\\nApplications include in particular highly-oscillatory Kubo oscillators and\\nspatial discretizations of the nonlinear Schr\\\\&quot;odinger equation with fast white\\nnoise dispersion. We construct a method of weak order two with computational\\ncost and accuracy both independent of the stiffness of the oscillations. A\\ngeometric modification that conserves exactly quadratic invariants is also\\npresented.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.02234</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.02234</id><submitter>Shaofeng Zou</submitter><version version=\"v1\"><date>Wed, 6 Feb 2019 15:33:45 GMT</date><size>57kb</size></version><version version=\"v2\"><date>Fri, 4 Oct 2019 18:10:46 GMT</date><size>32kb</size></version><title>Finite-Sample Analysis for SARSA with Linear Function Approximation</title><authors>Shaofeng Zou and Tengyu Xu and Yingbin Liang</authors><categories>cs.LG stat.ML</categories><comments>To appear in NeurIPS 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SARSA is an on-policy algorithm to learn a Markov decision process policy in\\nreinforcement learning. We investigate the SARSA algorithm with linear function\\napproximation under the non-i.i.d.\\\\ data, where a single sample trajectory is\\navailable. With a Lipschitz continuous policy improvement operator that is\\nsmooth enough, SARSA has been shown to converge asymptotically\\n\\\\cite{perkins2003convergent,melo2008analysis}. However, its non-asymptotic\\nanalysis is challenging and remains unsolved due to the non-i.i.d. samples and\\nthe fact that the behavior policy changes dynamically with time. In this paper,\\nwe develop a novel technique to explicitly characterize the stochastic bias of\\na type of stochastic approximation procedures with time-varying Markov\\ntransition kernels. Our approach enables non-asymptotic convergence analyses of\\nthis type of stochastic approximation algorithms, which may be of independent\\ninterest. Using our bias characterization technique and a gradient descent type\\nof analysis, we provide the finite-sample analysis on the mean square error of\\nthe SARSA algorithm. We then further study a fitted SARSA algorithm, which\\nincludes the original SARSA algorithm and its variant in\\n\\\\cite{perkins2003convergent} as special cases. This fitted SARSA algorithm\\nprovides a more general framework for \\\\textit{iterative} on-policy fitted\\npolicy iteration, which is more memory and computationally efficient. For this\\nfitted SARSA algorithm, we also provide its finite-sample analysis.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.02598</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.02598</id><submitter>Matilda Rhode</submitter><version version=\"v1\"><date>Thu, 7 Feb 2019 13:01:59 GMT</date><size>1462kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 10:07:05 GMT</date><size>909kb</size><source_type>D</source_type></version><title>Distillation for run-time malware process detection and automated\\n  process killing</title><authors>Matilda Rhode, Pete Burnap, Kevin Jones</authors><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Adversaries are increasingly motivated to spend energy trying to evade\\nautomatic malware detection tools. Dynamic analysis examines the behavioural\\ntrace of malware, which is difficult to obfuscate, but the time required for\\ndynamic analysis means it is not typically used in practice for endpoint\\nprotection but rather as an analysis tool. This paper presents a run-time model\\nto detect malicious processes and automatically kill them as they run on a real\\nendpoint in use. This approach enables dynamic analysis to be used to prevent\\nharm to the endpoint, rather than to analyse the cause of damage after the\\nevent. Run-time detection introduces the risk of malicious damage to the\\nendpoint and necessitates that malicious processes are detected and killed as\\nearly as possible to minimise the opportunities for damage to take place. A\\ndistilled machine learning model is used to improve inference speed whilst\\nbenefiting from the parameters learned by larger, more computationally\\nintensive model. This paper is the first to focus on tangible benefits of\\nprocess killing to the user, showing that the distilled model is able to\\nprevent 86.34% of files being corrupted by ransomware whilst maintaining a low\\nfalse positive rate for unseen benignware of 4.72%.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.03088</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.03088</id><submitter>Hasan Asy\\'ari Arief</submitter><version version=\"v1\"><date>Fri, 8 Feb 2019 14:20:42 GMT</date><size>1922kb</size></version><title>Addressing Overfitting on Pointcloud Classification using Atrous XCRF</title><authors>Hasan Asyari Arief, Ulf Geir Indahl, Geir-Harald Strand, H{\\\\aa}vard\\n  Tveite</authors><categories>cs.CV</categories><journal-ref>ISPRS Journal of Photogrammetry and Remote Sensing Volume 155,\\n  September 2019, Pages 90-101</journal-ref><doi>10.1016/j.isprsjprs.2019.07.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in techniques for automated classification of pointcloud data\\nintroduce great opportunities for many new and existing applications. However,\\nwith a limited number of labeled points, automated classification by a machine\\nlearning model is prone to overfitting and poor generalization. The present\\npaper addresses this problem by inducing controlled noise (on a trained model)\\ngenerated by invoking conditional random field similarity penalties using\\nnearby features. The method is called Atrous XCRF and works by forcing a\\ntrained model to respect the similarity penalties provided by unlabeled data.\\nIn a benchmark study carried out using the ISPRS 3D labeling dataset, our\\ntechnique achieves 84.97% in term of overall accuracy, and 71.05% in term of F1\\nscore. The result is on par with the current best model for the benchmark\\ndataset and has the highest value in term of F1 score.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.03129</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.03129</id><submitter>Amirata Ghorbani</submitter><version version=\"v1\"><date>Thu, 7 Feb 2019 03:18:54 GMT</date><size>8699kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 10 Jun 2019 18:53:03 GMT</date><size>5148kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 8 Oct 2019 09:28:43 GMT</date><size>4337kb</size><source_type>D</source_type></version><title>Towards Automatic Concept-based Explanations</title><authors>Amirata Ghorbani, James Wexler, James Zou, Been Kim</authors><categories>stat.ML cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interpretability has become an important topic of research as more machine\\nlearning (ML) models are deployed and widely used to make important decisions.\\n  Most of the current explanation methods provide explanations through feature\\nimportance scores, which identify features that are important for each\\nindividual input. However, how to systematically summarize and interpret such\\nper sample feature importance scores itself is challenging. In this work, we\\npropose principles and desiderata for \\\\emph{concept} based explanation, which\\ngoes beyond per-sample features to identify higher-level human-understandable\\nconcepts that apply across the entire dataset. We develop a new algorithm, ACE,\\nto automatically extract visual concepts. Our systematic experiments\\ndemonstrate that \\\\alg discovers concepts that are human-meaningful, coherent\\nand important for the neural network\\'s predictions.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.03438</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.03438</id><submitter>Emil Saucan</submitter><version version=\"v1\"><date>Sat, 9 Feb 2019 16:14:10 GMT</date><size>196kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 27 Sep 2019 19:35:42 GMT</date><size>301kb</size><source_type>D</source_type></version><title>Metric Curvatures and their Applications 2: Metric Ricci Curvature and\\n  Flow</title><authors>Emil Saucan</authors><categories>math.MG cs.CG cs.GR</categories><comments>40 pages, 2 figures. Important correction made, figure added</comments><msc-class>Primary: 53C44, 52C26, 68U05, Secondary: 65D18, 51K10, 57R40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this second part of our overview of the different metric curvatures and\\ntheir various applications, we concentrate on the Ricci curvature and flow for\\npolyhedral surfaces and higher dimensional manifolds, and we largely review our\\nprevious studies on the subject, based upon Wald\\'s curvature. In addition to\\nour previous metric approaches to the discretization of Ricci curvature, we\\nconsider yet another one, based on the Haantjes curvature, interpreted as a\\ngeodesic curvature. We also try to understand the mathematical reasons behind\\nthe recent proliferation of discretizations of Ricci curvature. Furthermore, we\\npropose another approach to the metrization of Ricci curvature, based on\\nForman\\'s discretization, and in particular we propose on that uses our graph\\nversion of Forman\\'s Ricci curvature.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.03568</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.03568</id><submitter>Markus Lohrey</submitter><version version=\"v1\"><date>Sun, 10 Feb 2019 10:30:46 GMT</date><size>45kb</size></version><version version=\"v2\"><date>Tue, 12 Mar 2019 05:20:18 GMT</date><size>46kb</size></version><version version=\"v3\"><date>Thu, 4 Apr 2019 18:37:57 GMT</date><size>51kb</size></version><version version=\"v4\"><date>Tue, 1 Oct 2019 11:58:08 GMT</date><size>51kb</size></version><title>Balancing Straight-Line Programs</title><authors>Moses Ganardi, Artur Je\\\\.z and Markus Lohrey</authors><categories>cs.DS</categories><comments>An extended abstract of this paper appears in the Proceedings of FOCS\\n  2019</comments><msc-class>68P05, 68W32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that a context-free grammar of size $m$ that produces a single\\nstring $w$ (such a grammar is also called a string straight-line program) can\\nbe transformed in linear time into a context-free grammar for $w$ of size\\n$\\\\mathcal{O}(m)$, whose unique derivation tree has depth $\\\\mathcal{O}(\\\\log\\n|w|)$. This solves an open problem in the area of grammar-based compression.\\nSimilar results are shown for two formalism for grammar-based tree compression:\\ntop dags and forest straight-line programs. These balancing results are all\\ndeduced from a single meta theorem stating that the depth of an algebraic\\ncircuit over an algebra with a certain finite base property can be reduced to\\n$\\\\mathcal{O}(\\\\log n)$ with the cost of a constant multiplicative size increase.\\nHere, $n$ refers to the size of the unfolding (or unravelling) of the circuit.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.03607</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.03607</id><submitter>Pascal Vontobel</submitter><version version=\"v1\"><date>Sun, 10 Feb 2019 14:24:49 GMT</date><size>24kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 26 Jul 2019 15:57:04 GMT</date><size>26kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sun, 6 Oct 2019 13:03:36 GMT</date><size>27kb</size><source_type>D</source_type></version><title>Quantum Measurement as Marginalization and Nested Quantum Systems</title><authors>Hans-Andrea Loeliger, Pascal O. Vontobel</authors><categories>quant-ph cs.IT math.IT</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In prior work, we have shown how the basic concepts and terms of quantum\\nmechanics relate to factorizations and marginals of complex-valued quantum mass\\nfunctions, which are generalizations of joint probability mass functions. In\\nthis paper, using quantum mass functions, we discuss the realization of\\nmeasurements in terms of unitary interactions and marginalizations. It follows\\nthat classical measurement results strictly belong to local models, i.e.,\\nmarginals of more detailed models. Classical variables that are created by\\nmarginalization do not exist in the unmarginalized model, and different\\nmarginalizations may yield incompatible classical variables. These observations\\nare illustrated by the Frauchiger-Renner paradox, which is analyzed (and\\nresolved) in terms of quantum mass functions. Throughout, the paper uses factor\\ngraphs to represent quantum systems/models with multiple measurements at\\ndifferent points in time.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.03740</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.03740</id><submitter>Bin Liu</submitter><version version=\"v1\"><date>Mon, 11 Feb 2019 05:46:11 GMT</date><size>459kb</size></version><version version=\"v2\"><date>Wed, 13 Feb 2019 15:51:31 GMT</date><size>460kb</size></version><version version=\"v3\"><date>Thu, 14 Feb 2019 02:02:52 GMT</date><size>459kb</size></version><version version=\"v4\"><date>Mon, 30 Sep 2019 07:45:25 GMT</date><size>446kb</size></version><title>Harnessing Low-Fidelity Data to Accelerate Bayesian Optimization via\\n  Posterior Regularization</title><authors>Bin Liu</authors><categories>cs.LG stat.ML</categories><comments>7 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian optimization (BO) is a powerful paradigm for derivative-free global\\noptimization of a black-box objective function (BOF) that is expensive to\\nevaluate. However, the overhead of BO can still be prohibitive for problems\\nwith highly expensive function evaluations. In this paper, we investigate how\\nto reduce the required number of function evaluations for BO without compromise\\nin solution quality. We explore the idea of posterior regularization to harness\\nlow fidelity (LF) data within the Gaussian process upper confidence bound\\n(GP-UCB) framework. The LF data can arise from previous evaluations of an LF\\napproximation of the BOF or of a related optimization task. An extra GP model\\ncalled LF-GP is trained to fit the LF data. We develop a dynamic weighted\\nproduct of experts (DW-POE) fusion operator. The regularization is induced from\\nthis operator on the posterior of the BOF. The impact of the LF GP model on the\\nresulting regularized posterior is adaptively adjusted via Bayesian formalism.\\nExtensive experimental results on benchmark BOF optimization tasks demonstrate\\nthe superior performance of the proposed algorithm over state-of-the-art.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.04168</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.04168</id><submitter>Qiang Sun</submitter><version version=\"v1\"><date>Fri, 8 Feb 2019 03:22:15 GMT</date><size>1009kb</size><source_type>D</source_type></version><title>A robust and non-singular formulation of the boundary integral method\\n  for the potential problem</title><authors>Q. Sun, E. Klaseboer, B. C. Khoo and D. Y. C. Chan</authors><categories>math.NA cs.CE cs.NA physics.flu-dyn</categories><journal-ref>Engineering Analysis with Boundary Elements 43 (2014) 117</journal-ref><doi>10.1016/j.enganabound.2014.03.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A non-singular formulation of the boundary integral method (BIM) is presented\\nfor the Laplace equation whereby the well-known singularities that arise from\\nthe fundamental solution are eliminated analytically. A key advantage of this\\napproach is that numerical errors that arise due to the proximity of nodes\\nlocated on osculating boundaries are suppressed. This is particularly relevant\\nin multi-scale problems where high accuracy is required without undue increase\\nin computational cost when the spacing between boundaries become much smaller\\nthan their characteristic dimensions. The elimination of the singularities\\nmeans that standard quadrature can be used to evaluate the surface integrals\\nand this results in about 60% savings in coding effort. The new formulation\\nalso affords a numerically robust way to calculate the potential close to the\\nboundaries. Detailed implementations of this approach are illustrated with\\nproblems involving osculating boundaries, 2D domains with corners and a wave\\ndrag problem in a 3D semi-infinite domain. The explicit formulation of problems\\nwith axial symmetry is also given.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.04595</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.04595</id><submitter>Mark Newman</submitter><version version=\"v1\"><date>Tue, 12 Feb 2019 19:08:28 GMT</date><size>42kb</size></version><title>Spectra of networks containing short loops</title><authors>M. E. J. Newman</authors><categories>cs.SI physics.soc-ph</categories><comments>8 pages, 4 figures</comments><journal-ref>Phys. Rev. E 100, 012314 (2019)</journal-ref><doi>10.1103/PhysRevE.100.012314</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spectrum of the adjacency matrix plays several important roles in the\\nmathematical theory of networks and in network data analysis, for example in\\npercolation theory, community detection, centrality measures, and the theory of\\ndynamical systems on networks. A number of methods have been developed for the\\nanalytic computation of network spectra, but they typically assume that\\nnetworks are locally tree-like, meaning that the local neighborhood of any node\\ntakes the form of a tree, free of short loops. Empirically observed networks,\\nby contrast, often have many short loops. Here we develop an approach for\\ncalculating the spectra of networks with short loops using a message passing\\nmethod. We give example applications to some previously studied classes of\\nnetworks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.04753</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.04753</id><submitter>Xinpeng Xu Dr.</submitter><version version=\"v1\"><date>Wed, 13 Feb 2019 05:52:13 GMT</date><size>1179kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 12 Jun 2019 00:46:39 GMT</date><size>3075kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sat, 5 Oct 2019 00:55:25 GMT</date><size>4689kb</size><source_type>D</source_type></version><title>Defect removal by solvent vapor annealing in thin films of lamellar\\n  diblock copolymers</title><authors>Xinpeng Xu, Xingkun Man, Masao Doi, Zhong-can Ou-Yang, David Andelman</authors><categories>cond-mat.soft cs.NA math.NA nlin.PS</categories><comments>14 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solvent vapor annealing (SVA) is known to be a simple, low-cost and highly\\nefficient technique to produce defect-free diblock copolymer (BCP) thin films.\\nNot only can the solvent weaken the BCP segmental interactions, but it can vary\\nthe characteristic spacing of the BCP microstructures. We carry out systematic\\ntheoretical studies on the effect of adding solvent into lamellar BCP thin\\nfilms on the defect removal close to the BCP order-disorder transition. We find\\nthat the increase of the lamellar spacing, as is induced by addition of\\nsolvent, facilitates more efficient removal of defects. The stability of a\\nparticular defect in a lamellar BCP thin film is given in terms of two key\\ncontrollable parameters: the amount of BCP swelling and solvent evaporation\\nrate. Our results highlight the SVA mechanism for obtaining defect-free BCP\\nthin films, as is highly desired in nanolithography and other industrial\\napplications.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.05023</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.05023</id><submitter>Youye Xie</submitter><version version=\"v1\"><date>Wed, 13 Feb 2019 17:34:02 GMT</date><size>111kb</size></version><version version=\"v2\"><date>Tue, 13 Aug 2019 00:51:07 GMT</date><size>187kb</size><source_type>D</source_type></version><title>Simultaneous Sparse Recovery and Blind Demodulation</title><authors>Youye Xie, Michael B. Wakin, Gongguo Tang</authors><categories>cs.IT math.IT</categories><comments>16 pages, 10 figures</comments><doi>10.1109/TSP.2019.2935910</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of finding a sparse signal decomposition in an overcomplete\\ndictionary is made more complicated when the signal undergoes an unknown\\nmodulation (or convolution in the complementary Fourier domain). Such\\nsimultaneous sparse recovery and blind demodulation problems appear in many\\napplications including medical imaging, super resolution, self-calibration,\\netc. In this paper, we consider a more general sparse recovery and blind\\ndemodulation problem in which each atom comprising the signal undergoes a\\ndistinct modulation process. Under the assumption that the modulating waveforms\\nlive in a known common subspace, we employ the lifting technique and recast\\nthis problem as the recovery of a column-wise sparse matrix from structured\\nlinear measurements. In this framework, we accomplish sparse recovery and blind\\ndemodulation simultaneously by minimizing the induced atomic norm, which in\\nthis problem corresponds to the block $\\\\ell_1$ norm minimization. For perfect\\nrecovery in the noiseless case, we derive near optimal sample complexity bounds\\nfor Gaussian and random Fourier overcomplete dictionaries. We also provide\\nbounds on recovering the column-wise sparse matrix in the noisy case. Numerical\\nsimulations illustrate and support our theoretical results.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.05045</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.05045</id><submitter>David Mguni</submitter><version version=\"v1\"><date>Wed, 13 Feb 2019 18:19:18 GMT</date><size>137kb</size></version><version version=\"v2\"><date>Thu, 21 Feb 2019 11:13:32 GMT</date><size>137kb</size></version><version version=\"v3\"><date>Tue, 8 Oct 2019 17:40:45 GMT</date><size>489kb</size></version><title>Cutting Your Losses: Learning Fault-Tolerant Control and Optimal\\n  Stopping under Adverse Risk</title><authors>David Mguni</authors><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been a surge in interest in safe and robust techniques\\nwithin reinforcement learning (RL). Current notions of risk in RL fail to\\ncapture the potential for systemic failures such as abrupt stoppages from\\nsystem failures or surpassing of safety thresholds and the appropriate\\nresponsive controls in such instances. We propose a novel approach to risk\\nminimisation within RL in which, in addition to taking actions that maximise\\nits expected return, the controller learns a policy that is robust against\\nstoppages due to an adverse event such as an abrupt failure. The results of the\\npaper cover fault-tolerant control in \\\\textit{worst-case scenarios} under\\nrandom stopping and optimal stopping, all in unknown environments. By\\ndemonstrating that the class of problems is represented by a variant of\\nstochastic games, we prove the existence of a solution which is a unique fixed\\npoint equilibrium of the game and characterise the optimal controller\\nbehaviour. We then introduce a value function approximation algorithm that\\nconverges to the solution through simulation in unknown environments.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.05132</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.05132</id><submitter>Wei Cai</submitter><version version=\"v1\"><date>Wed, 13 Feb 2019 21:06:24 GMT</date><size>2676kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 18:20:16 GMT</date><size>731kb</size><source_type>D</source_type></version><title>Fast Multipole Method For 3-D Helmholtz Equation In Layered Media</title><authors>Bo Wang, Wenzhong Zhang, and Wei Cai</authors><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a fast multipole method (FMM) is proposed to compute\\nlong-range interactions of wave sources embedded in 3-D layered media. The\\nlayered media Green\\'s function for the Helmholtz equation, which satisfies the\\ntransmission conditions at material interfaces, is decomposed into a free space\\ncomponent and four types of reaction field components arising from wave\\nreflections and transmissions through the layered media. The proposed algorithm\\nis a combination of the classic FMM for the free space component and FMMs\\nspecifically designed for the four types reaction components, made possible by\\nnew multipole expansions (MEs) and local expansions (LEs) as well as the\\nmultipole-to-local translation (M2L) operators for the reaction field\\ncomponents. { Moreover, equivalent polarization source can be defined for each\\nreaction component based on the convergence analysis of its ME. The FMMs for\\nthe reaction components, implemented with the target particles and equivalent\\npolarization sources, are found to be much more efficient than the classic FMM\\nfor the free space component due to the fact that the equivalent polarization\\nsources and the target particles are always separated by a material interface.}\\nAs a result, the FMM algorithm developed for layered media has a similar\\ncomputational cost as that for the free space. Numerical results validate the\\nfast convergence of the MEs and the $O(N)$ complexity of the FMM for\\ninteractions of low-frequency wave sources in 3-D layered media.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.05184</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.05184</id><submitter>Shuang Qiu</submitter><version version=\"v1\"><date>Thu, 14 Feb 2019 01:50:38 GMT</date><size>729kb</size></version><version version=\"v2\"><date>Mon, 7 Oct 2019 13:26:33 GMT</date><size>734kb</size></version><title>A Covariance-Based Hybrid Channel Feedback in FDD Massive MIMO Systems</title><authors>Shuang Qiu, David Gesbert, Da Chen, Tao Jiang</authors><categories>cs.IT math.IT</categories><comments>31 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel covariance-based channel feedback mechanism is\\ninvestigated for frequency division duplexing (FDD) massive multi-input\\nmulti-output (MIMO) systems. The concept capitalizes on the notion of user\\nstatistical separability which was hinted in several prior works in the massive\\nantenna regime but not fully exploited so far. We here propose a hybrid\\nstatistical-instantaneous feedback mechanism where the users are separated into\\ntwo classes of feedback design based on their channel covariance. Under the\\nhybrid framework, each user either operates on a statistical feedback mode or\\nquantized instantaneous channel feedback mode depending on their so-called\\nstatistical isolability. The key challenge lies in the design of a\\ncovariance-aware classification algorithm which can handle the complex mutual\\ninteractions between all users. The classification is derived from rate bound\\nprinciples. A suitable precoding method is also devised under the mixed\\nstatistical and instantaneous feedback model. Simulations are performed to\\nvalidate our analytical results and illustrate the sum rate advantages of the\\nproposed feedback scheme under a global feedback overhead constraint.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.05238</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.05238</id><submitter>Shuang Li</submitter><version version=\"v1\"><date>Thu, 14 Feb 2019 06:35:43 GMT</date><size>48kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 4 Sep 2019 22:42:24 GMT</date><size>51kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 8 Oct 2019 15:55:13 GMT</date><size>51kb</size><source_type>D</source_type></version><title>Atomic Norm Denoising for Complex Exponentials with Unknown Waveform\\n  Modulations</title><authors>Shuang Li, Michael B. Wakin, and Gongguo Tang</authors><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-stationary blind super-resolution is an extension of the traditional\\nsuper-resolution problem, which deals with the problem of recovering fine\\ndetails from coarse measurements. The non-stationary blind super-resolution\\nproblem appears in many applications including radar imaging, 3D\\nsingle-molecule microscopy, computational photography, etc. There is a growing\\ninterest in solving non-stationary blind super-resolution task with convex\\nmethods due to their robustness to noise and strong theoretical guarantees.\\nMotivated by the recent work on atomic norm minimization in blind inverse\\nproblems, we focus here on the signal denoising problem in non-stationary blind\\nsuper-resolution. In particular, we use an atomic norm regularized\\nleast-squares problem to denoise a sum of complex exponentials with unknown\\nwaveform modulations. We quantify how the mean square error depends on the\\nnoise variance and the true signal parameters. Numerical experiments are also\\nimplemented to illustrate the theoretical result.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.06002</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.06002</id><submitter>Matthew Aldridge</submitter><version version=\"v1\"><date>Fri, 15 Feb 2019 23:04:24 GMT</date><size>1130kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 22:43:13 GMT</date><size>1227kb</size><source_type>D</source_type></version><title>Group testing: an information theory perspective</title><authors>Matthew Aldridge, Oliver Johnson, Jonathan Scarlett</authors><categories>cs.IT cs.DM math.IT math.PR math.ST stat.TH</categories><comments>Survey paper, 140 pages, 19 figures. To be published in Foundations\\n  and Trends in Communications and Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The group testing problem concerns discovering a small number of defective\\nitems within a large population by performing tests on pools of items. A test\\nis positive if the pool contains at least one defective, and negative if it\\ncontains no defectives. This is a sparse inference problem with a combinatorial\\nflavour, with applications in medical testing, biology, telecommunications,\\ninformation technology, data science, and more. In this monograph, we survey\\nrecent developments in the group testing problem from an information-theoretic\\nperspective. We cover several related developments: efficient algorithms with\\npractical storage and computation requirements, achievability bounds for\\noptimal decoding methods, and algorithm-independent converse bounds. We assess\\nthe theoretical guarantees not only in terms of scaling laws, but also in terms\\nof the constant factors, leading to the notion of the &quot;rate&quot; of group testing,\\nindicating the amount of information learned per test. Considering both\\nnoiseless and noisy settings, we identify several regimes where existing\\nalgorithms are provably optimal or near-optimal, as well as regimes where there\\nremains greater potential for improvement. In addition, we survey results\\nconcerning a number of variations on the standard group testing problem,\\nincluding partial recovery criteria, adaptive algorithms with a limited number\\nof stages, constrained test designs, and sublinear-time algorithms.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.06196</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.06196</id><submitter>Thijs Laarhoven</submitter><version version=\"v1\"><date>Sun, 17 Feb 2019 03:35:23 GMT</date><size>71kb</size><source_type>D</source_type></version><title>Nearest neighbor decoding for Tardos fingerprinting codes</title><authors>Thijs Laarhoven</authors><categories>cs.CR cs.CC cs.CG cs.DS</categories><comments>6 pages, 1 figure, 2 tables</comments><journal-ref>ACM Workshop on Information Hiding and Multimedia Security\\n  (IH&amp;MMSec), pp. 182-187, 2019</journal-ref><doi>10.1145/3335203.3335732</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past decade, various improvements have been made to Tardos\\'\\ncollusion-resistant fingerprinting scheme [Tardos, STOC 2003], ultimately\\nresulting in a good understanding of what is the minimum code length required\\nto achieve collusion-resistance. In contrast, decreasing the cost of the actual\\ndecoding algorithm for identifying the potential colluders has received less\\nattention, even though previous results have shown that using joint decoding\\nstrategies, deemed too expensive for decoding, may lead to better code lengths.\\nMoreover, in dynamic settings a fast decoder may be required to provide answers\\nin real-time, further raising the question whether the decoding costs of\\nscore-based fingerprinting schemes can be decreased with a smarter decoding\\nalgorithm. In this paper we show how to model the decoding step of score-based\\nfingerprinting as a nearest neighbor search problem, and how this relation\\nallows us to apply techniques from the field of (approximate) nearest neighbor\\nsearching to obtain decoding times which are sublinear in the total number of\\nusers. As this does not affect the encoding and embedding steps, this decoding\\nmechanism can easily be deployed within existing fingerprinting schemes, and\\nthis may bring a truly efficient joint decoder closer to reality. Besides the\\napplication to fingerprinting, similar techniques can be used to decrease the\\ndecoding costs of group testing methods, which may be of independent interest.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.06918</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.06918</id><submitter>Seojin Bang</submitter><version version=\"v1\"><date>Tue, 19 Feb 2019 06:42:05 GMT</date><size>749kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 08:24:25 GMT</date><size>3498kb</size><source_type>D</source_type></version><title>Explaining a black-box using Deep Variational Information Bottleneck\\n  Approach</title><authors>Seojin Bang, Pengtao Xie, Heewook Lee, Wei Wu, and Eric Xing</authors><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Interpretable machine learning has gained much attention recently. Briefness\\nand comprehensiveness are necessary in order to provide a large amount of\\ninformation concisely when explaining a black-box decision system. However,\\nexisting interpretable machine learning methods fail to consider briefness and\\ncomprehensiveness simultaneously, leading to redundant explanations. We propose\\nthe variational information bottleneck for interpretation, VIBI, a\\nsystem-agnostic interpretable method that provides a brief but comprehensive\\nexplanation. VIBI adopts an information theoretic principle, information\\nbottleneck principle, as a criterion for finding such explanations. For each\\ninstance, VIBI selects key features that are maximally compressed about an\\ninput (briefness), and informative about a decision made by a black-box system\\non that input (comprehensive). We evaluate VIBI on three datasets and compare\\nwith state-of-the-art interpretable machine learning methods in terms of both\\ninterpretability and fidelity evaluated by human and quantitative metrics\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.07263</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.07263</id><submitter>Amirhossein Taghvaei</submitter><version version=\"v1\"><date>Tue, 19 Feb 2019 20:05:09 GMT</date><size>213kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 06:40:49 GMT</date><size>268kb</size><source_type>D</source_type></version><title>Diffusion map-based algorithm for Gain function approximation in the\\n  Feedback Particle Filter</title><authors>Amirhossein Taghvaei, Prashant G. Mehta, Sean P. Meyn</authors><categories>math.OC cs.NA math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feedback particle filter (FPF) is a numerical algorithm to approximate the\\nsolution of the nonlinear filtering problem in continuous-time settings. In any\\nnumerical implementation of the FPF algorithm, the main challenge is to\\nnumerically approximate the so-called gain function. A numerical algorithm for\\ngain function approximation is the subject of this paper. The exact gain\\nfunction is the solution of a Poisson equation involving a probability-weighted\\nLaplacian $\\\\Delta_\\\\rho$. The numerical problem is to approximate this solution\\nusing {\\\\em only} finitely many particles sampled from the probability\\ndistribution $\\\\rho$. A diffusion map-based algorithm was proposed by the\\nauthors in a prior work to solve this problem. The algorithm is named as such\\nbecause it involves, as an intermediate step, a diffusion map approximation of\\nthe exact semigroup $e^{\\\\Delta_\\\\rho}$. The original contribution of this paper\\nis to carry out a rigorous error analysis of the diffusion map-based algorithm.\\nThe error is shown to include two components: bias and variance. The bias\\nresults from the diffusion map approximation of the exact semigroup. The\\nvariance arises because of finite sample size. Scalings and upper bounds are\\nderived for bias and variance. These bounds are then illustrated with numerical\\nexperiments that serve to emphasize the effects of problem dimension and sample\\nsize. The proposed algorithm is applied to two filtering examples and\\ncomparisons provided with the sequential importance resampling (SIR) particle\\nfilter.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.07286</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.07286</id><submitter>Ching-An Cheng</submitter><version version=\"v1\"><date>Tue, 19 Feb 2019 21:07:10 GMT</date><size>393kb</size></version><version version=\"v2\"><date>Wed, 2 Oct 2019 20:12:19 GMT</date><size>69kb</size></version><title>Online Learning with Continuous Variations: Dynamic Regret and\\n  Reductions</title><authors>Ching-An Cheng, Jonathan Lee, Ken Goldberg, Byron Boots</authors><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online learning is a powerful tool for analyzing iterative algorithms.\\nHowever, the classic adversarial setup sometimes fails to capture certain\\nregularity in online problems in practice. Motivated by this, we establish a\\nnew setup, called Continuous Online Learning (COL), where the gradient of\\nonline loss function changes continuously across rounds with respect to the\\nlearner\\'s decisions. We show that COL covers and more appropriately describes\\nmany interesting applications, from general equilibrium problems (EPs) to\\noptimization in episodic MDPs. In particular, we show monotone EPs admits a\\nreduction to achieving sublinear static regret in COL. Using this new setup, we\\nrevisit the difficulty of sublinear dynamic regret. We prove a fundamental\\nequivalence between achieving sublinear dynamic regret in COL and solving\\ncertain EPs. With this insight, we offer conditions for efficient algorithms\\nthat achieve sublinear dynamic regret, even when the losses are chosen\\nadaptively without any a priori variation budget. Furthermore, we show for COL\\na reduction from dynamic regret to both static regret and convergence in the\\nassociated EP, allowing us to analyze the dynamic regret of many existing\\nalgorithms.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.07346</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.07346</id><submitter>Carlo Tiseo</submitter><version version=\"v1\"><date>Tue, 19 Feb 2019 23:32:02 GMT</date><size>1032kb</size></version><version version=\"v2\"><date>Thu, 11 Apr 2019 12:17:29 GMT</date><size>1032kb</size></version><version version=\"v3\"><date>Tue, 23 Apr 2019 18:23:34 GMT</date><size>918kb</size></version><title>Analytic Model for Quadruped Locomotion Task-Space Planning</title><authors>Carlo Tiseo, Sethu Vijayakumar, Michael Mistry</authors><categories>cs.RO</categories><comments>Accepted to be Published in 2019, 41th Annual International\\n  Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),\\n  Berlin Germany</comments><journal-ref>2019 41st Annual International Conference of the IEEE Engineering\\n  in Medicine and Biology Society (EMBC)</journal-ref><doi>10.1109/EMBC.2019.8857345</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the extensive presence of the legged locomotion in animals, it is\\nextremely challenging to be reproduced with robots. Legged locomotion is an\\ndynamic task which benefits from a planning that takes advantage of the\\ngravitational pull on the system. However, the computational cost of such\\noptimization rapidly increases with the complexity of kinematic structures,\\nrendering impossible real-time deployment in unstructured environments. This\\npaper proposes a simplified method that can generate desired centre of mass and\\nfeet trajectory for quadrupeds. The model describes a quadruped as two bipeds\\nconnected via their centres of mass, and it is based on the extension of an\\nalgebraic bipedal model that uses the topology of the gravitational attractor\\nto describe bipedal locomotion strategies. The results show that the model\\ngenerates trajectories that agrees with previous studies. The model will be\\ndeployed in the future as seed solution for whole-body trajectory optimization\\nin the attempt to reduce the computational cost and obtain real-time planning\\nof complex action in challenging environments.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.07698</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.07698</id><submitter>Cong Ma</submitter><version version=\"v1\"><date>Wed, 20 Feb 2019 18:51:50 GMT</date><size>112kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 14:01:40 GMT</date><size>177kb</size><source_type>D</source_type></version><title>Noisy Matrix Completion: Understanding Statistical Guarantees for Convex\\n  Relaxation via Nonconvex Optimization</title><authors>Yuxin Chen, Yuejie Chi, Jianqing Fan, Cong Ma, Yuling Yan</authors><categories>stat.ML cs.IT cs.LG math.IT math.OC math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies noisy low-rank matrix completion: given partial and noisy\\nentries of a large low-rank matrix, the goal is to estimate the underlying\\nmatrix faithfully and efficiently. Arguably one of the most popular paradigms\\nto tackle this problem is convex relaxation, which achieves remarkable efficacy\\nin practice. However, the theoretical support of this approach is still far\\nfrom optimal in the noisy setting, falling short of explaining its empirical\\nsuccess.\\n  We make progress towards demystifying the practical efficacy of convex\\nrelaxation vis-\\\\`a-vis random noise. When the rank and the condition number of\\nthe unknown matrix are bounded by a constant, we demonstrate that the convex\\nprogramming approach achieves near-optimal estimation errors --- in terms of\\nthe Euclidean loss, the entrywise loss, and the spectral norm loss --- for a\\nwide range of noise levels. All of this is enabled by bridging convex\\nrelaxation with the nonconvex Burer-Monteiro approach, a seemingly distinct\\nalgorithmic paradigm that is provably robust against noise. More specifically,\\nwe show that an approximate critical point of the nonconvex formulation serves\\nas an extremely tight approximation of the convex solution, thus allowing us to\\ntransfer the desired statistical guarantees of the nonconvex approach to its\\nconvex counterpart.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.08055</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.08055</id><submitter>David Cerna</submitter><version version=\"v1\"><date>Thu, 21 Feb 2019 14:01:12 GMT</date><size>44kb</size></version><version version=\"v2\"><date>Wed, 2 Oct 2019 11:53:32 GMT</date><size>42kb</size></version><title>Schematic Refutations of Formula Schemata</title><authors>David Cerna, Alexander Leitsch, and Anela Lolic</authors><categories>cs.LO</categories><comments>32 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proof schemata are infinite sequences of proofs which are defined\\ninductively. In this paper we present a general framework for schemata of\\nterms, formulas and unifiers and define a resolution calculus for schemata of\\nquantifier-free formulas. The new calculus generalizes and improves former\\napproaches to schematic deduction. As an application of the method we present a\\nschematic refutation formalizing a proof of a weak form of the pigeon hole\\nprinciple.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.08149</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.08149</id><submitter>Paul Gastin</submitter><version version=\"v1\"><date>Thu, 21 Feb 2019 17:23:30 GMT</date><size>149kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 12 Aug 2019 10:18:37 GMT</date><size>99kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 30 Sep 2019 16:18:55 GMT</date><size>102kb</size><source_type>D</source_type></version><title>Aperiodic Weighted Automata and Weighted First-Order Logic</title><authors>Manfred Droste and Paul Gastin</authors><categories>cs.FL cs.LO</categories><comments>An extended abstract of the paper appeared at MFCS\\'19</comments><doi>10.4230/LIPIcs.MFCS.2019.76</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By fundamental results of Sch\\\\&quot;utzenberger, McNaughton and Papert from the\\n1970s, the classes of first-order definable and aperiodic languages coincide.\\nHere, we extend this equivalence to a quantitative setting. For this, weighted\\nautomata form a general and widely studied model. We define a suitable notion\\nof a weighted first-order logic. Then we show that this weighted first-order\\nlogic and aperiodic polynomially ambiguous weighted automata have the same\\nexpressive power. Moreover, we obtain such equivalence results for suitable\\nweighted sublogics and finitely ambiguous or unambiguous aperiodic weighted\\nautomata. Our results hold for general weight structures, including all\\nsemirings, average computations of costs, bounded lattices, and others.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.08283</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.08283</id><submitter>Babak Salimi</submitter><version version=\"v1\"><date>Thu, 21 Feb 2019 22:13:29 GMT</date><size>838kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 26 Feb 2019 00:20:52 GMT</date><size>840kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 15 Jul 2019 23:34:33 GMT</date><size>2959kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Mon, 22 Jul 2019 01:35:59 GMT</date><size>965kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Tue, 1 Oct 2019 19:37:23 GMT</date><size>3053kb</size><source_type>D</source_type></version><title>Capuchin: Causal Database Repair for Algorithmic Fairness</title><authors>Babak Salimi, Luke Rodriguez, Bill Howe, Dan Suciu</authors><categories>cs.DB cs.AI</categories><journal-ref>Proceedings of the 2019 International Conference on Management of\\n  Data. ACM, 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fairness is increasingly recognized as a critical component of machine\\nlearning systems. However, it is the underlying data on which these systems are\\ntrained that often reflect discrimination, suggesting a database repair\\nproblem. Existing treatments of fairness rely on statistical correlations that\\ncan be fooled by statistical anomalies, such as Simpson\\'s paradox. Proposals\\nfor causality-based definitions of fairness can correctly model some of these\\nsituations, but they require specification of the underlying causal models. In\\nthis paper, we formalize the situation as a database repair problem, proving\\nsufficient conditions for fair classifiers in terms of admissible variables as\\nopposed to a complete causal model. We show that these conditions correctly\\ncapture subtle fairness violations. We then use these conditions as the basis\\nfor database repair algorithms that provide provable fairness guarantees about\\nclassifiers trained on their training labels. We evaluate our algorithms on\\nreal data, demonstrating improvement over the state of the art on multiple\\nfairness metrics proposed in the literature while retaining high utility.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.08404</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.08404</id><submitter>Chinmay Maheshwari</submitter><version version=\"v1\"><date>Fri, 22 Feb 2019 08:55:21 GMT</date><size>875kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 00:20:25 GMT</date><size>1437kb</size><source_type>D</source_type></version><title>On optimal multiplexing of an ensemble of discrete-time constrained\\n  control systems on matrix Lie groups</title><authors>Chinmay Maheshwari, Sukumar Srikant, and Debasish Chatterjee</authors><categories>cs.SY math.DS math.OC</categories><comments>29 pages, 7 figures</comments><msc-class>93B27</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a constrained optimal control problem for an ensemble of control\\nsystems. Each sub-system (or plant) evolves on a matrix Lie group, and must\\nsatisfy given state and control action constraints pointwise in time. In\\naddition, certain multiplexing requirement is imposed: the controller must be\\nshared between the plants in the sense that at any time instant the control\\nsignal may be sent to only one plant. We provide first-order necessary\\nconditions for optimality in the form of suitable Pontryagin maximum principle\\nin this problem. Detailed numerical experiments are presented for a system of\\ntwo satellites performing energy optimal maneuvers under the preceding family\\nof constraints.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.08692</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.08692</id><submitter>Rafael Boloix-Tortosa</submitter><version version=\"v1\"><date>Fri, 22 Feb 2019 22:31:23 GMT</date><size>8619kb</size><source_type>D</source_type></version><title>The Generalized Complex Kernel Least-Mean-Square Algorithm</title><authors>Rafael Boloix-Tortosa, Juan Jos\\\\\\'e Murillo-Fuentes, Sotirios A.\\n  Tsaftaris</authors><categories>stat.ML cs.LG</categories><comments>Submitted to IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2019.2937289</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel adaptive kernel based regression method for complex-valued\\nsignals: the generalized complex-valued kernel least-mean-square (gCKLMS). We\\nborrow from the new results on widely linear reproducing kernel Hilbert space\\n(WL-RKHS) for nonlinear regression and complex-valued signals, recently\\nproposed by the authors. This paper shows that in the adaptive version of the\\nkernel regression for complex-valued signals we need to include another kernel\\nterm, the so-called pseudo-kernel. This new solution is endowed with better\\nrepresentation capabilities in complex-valued fields, since it can efficiently\\ndecouple the learning of the real and the imaginary part. Also, we review\\nprevious realizations of the complex KLMS algorithm and its augmented version\\nto prove that they can be rewritten as particular cases of the gCKLMS.\\nFurthermore, important conclusions on the kernels design are drawn that help to\\ngreatly improve the convergence of the algorithms. In the experiments, we\\nrevisit the nonlinear channel equalization problem to highlight the better\\nconvergence of the gCKLMS compared to previous solutions. Also, the flexibility\\nof the proposed generalized approach is tested in a second experiment with\\nnon-independent real and imaginary parts. The results illustrate the\\nsignificant performance improvements of the gCKLMS approach when the\\ncomplex-valued signals have different properties for the real and imaginary\\nparts.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.09700</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.09700</id><submitter>Ryoma Sato</submitter><version version=\"v1\"><date>Tue, 26 Feb 2019 02:01:26 GMT</date><size>38kb</size></version><version version=\"v2\"><date>Thu, 3 Oct 2019 12:40:45 GMT</date><size>153kb</size></version><title>Learning to Sample Hard Instances for Graph Algorithms</title><authors>Ryoma Sato, Makoto Yamada, Hisashi Kashima</authors><categories>cs.LG stat.ML</categories><comments>16 pages, 4 figures, accepted by ACML 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hard instances, which require a long time for a specific algorithm to solve,\\nhelp (1) analyze the algorithm for accelerating it and (2) build a good\\nbenchmark for evaluating the performance of algorithms. There exist several\\nefforts for automatic generation of hard instances. For example, evolutionary\\nalgorithms have been utilized to generate hard instances. However, they\\ngenerate only finite number of hard instances. The merit of such methods is\\nlimited because it is difficult to extract meaningful patterns from small\\nnumber of instances. We seek for a probabilistic generator of hard instances.\\nOnce the generative distribution of hard instances is obtained, we can sample a\\nvariety of hard instances to build a benchmark, and we can extract meaningful\\npatterns of hard instances from sampled instances. The existing methods for\\nmodeling the hard instance distribution rely on parameters or rules that are\\nfound by domain experts; however, they are specific to the problem. Hence, it\\nis challenging to model the distribution for general cases. In this paper, we\\nfocus on graph problems. We propose HiSampler, the hard instance sampler, to\\nmodel the hard instance distribution of graph algorithms. HiSampler makes it\\npossible to obtain the distribution of hard instances without hand-engineered\\nfeatures. To the best of our knowledge, this is the first method to learn the\\ndistribution of hard instances using machine learning. Through experiments, we\\ndemonstrate that our proposed method can generate instances that are a few to\\nseveral orders of magnitude harder than the random-based approach in many\\nsettings. In particular, our method outperforms rule-based algorithms in the\\n3-coloring problem.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.09707</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.09707</id><submitter>Qunliang Xing</submitter><version version=\"v1\"><date>Tue, 26 Feb 2019 02:35:55 GMT</date><size>9099kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 15 Aug 2019 15:52:25 GMT</date><size>8225kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 4 Oct 2019 05:20:30 GMT</date><size>8283kb</size><source_type>D</source_type></version><title>MFQE 2.0: A New Approach for Multi-frame Quality Enhancement on\\n  Compressed Video</title><authors>Zhenyu Guan, Qunliang Xing, Mai Xu, Ren Yang, Tie Liu, Zulin Wang</authors><categories>cs.CV cs.MM</categories><comments>Accepted by TPAMI on September 26, 2019. arXiv admin note: text\\n  overlap with arXiv:1803.04680</comments><doi>10.1109/TPAMI.2019.2944806</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The past few years have witnessed great success in applying deep learning to\\nenhance the quality of compressed image/video. The existing approaches mainly\\nfocus on enhancing the quality of a single frame, not considering the\\nsimilarity between consecutive frames. Since heavy fluctuation exists across\\ncompressed video frames as investigated in this paper, frame similarity can be\\nutilized for quality enhancement of low-quality frames given their neighboring\\nhigh-quality frames. This task is Multi-Frame Quality Enhancement (MFQE).\\nAccordingly, this paper proposes an MFQE approach for compressed video, as the\\nfirst attempt in this direction. In our approach, we firstly develop a\\nBidirectional Long Short-Term Memory (BiLSTM) based detector to locate Peak\\nQuality Frames (PQFs) in compressed video. Then, a novel Multi-Frame\\nConvolutional Neural Network (MF-CNN) is designed to enhance the quality of\\ncompressed video, in which the non-PQF and its nearest two PQFs are the input.\\nIn MF-CNN, motion between the non-PQF and PQFs is compensated by a motion\\ncompensation subnet. Subsequently, a quality enhancement subnet fuses the\\nnon-PQF and compensated PQFs, and then reduces the compression artifacts of the\\nnon-PQF. Also, PQF quality is enhanced in the same way. Finally, experiments\\nvalidate the effectiveness and generalization ability of our MFQE approach in\\nadvancing the state-of-the-art quality enhancement of compressed video. The\\ncode of our MFQE approach is available at\\nhttps://github.com/RyanXingQL/MFQEv2.0.git.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.09713</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.09713</id><submitter>Khalil Mrini</submitter><version version=\"v1\"><date>Tue, 26 Feb 2019 02:54:03 GMT</date><size>133kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 5 Oct 2019 05:45:39 GMT</date><size>183kb</size><source_type>D</source_type></version><title>Interpretable Structure-aware Document Encoders with Hierarchical\\n  Attention</title><authors>Khalil Mrini, Claudiu Musat, Michael Baeriswyl, Martin Jaggi</authors><categories>cs.CL cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method to create document representations that reflect their\\ninternal structure. We modify Tree-LSTMs to hierarchically merge basic elements\\nsuch as words and sentences into blocks of increasing complexity. Our Structure\\nTree-LSTM implements a hierarchical attention mechanism over individual\\ncomponents and combinations thereof. We thus emphasize the usefulness of\\nTree-LSTMs for texts larger than a sentence. We show that structure-aware\\nencoders can be used to improve the performance of document classification. We\\ndemonstrate that our method is resilient to changes to the basic building\\nblocks, as it performs well with both sentence and word embeddings. The\\nStructure Tree-LSTM outperforms all the baselines on two datasets by leveraging\\nstructural clues. We show our model\\'s interpretability by visualizing how our\\nmodel distributes attention inside a document. On a third dataset from the\\nmedical domain, our model achieves competitive performance with the state of\\nthe art. This result shows the Structure Tree-LSTM can leverage dependency\\nrelations other than text structure, such as a set of reports on the same\\npatient.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.10042</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.10042</id><submitter>Andrew Carr</submitter><version version=\"v1\"><date>Tue, 26 Feb 2019 16:39:42 GMT</date><size>887kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 21:13:48 GMT</date><size>840kb</size><source_type>D</source_type></version><title>Graph Neural Processes: Towards Bayesian Graph Neural Networks</title><authors>Andrew Carr and David Wingate</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Graph Neural Processes (GNP), inspired by the recent work in\\nconditional and latent neural processes. A Graph Neural Process is defined as a\\nConditional Neural Process that operates on arbitrary graph data. It takes\\nfeatures of sparsely observed context points as input, and outputs a\\ndistribution over target points. We demonstrate graph neural processes in edge\\nimputation and discuss benefits and drawbacks of the method for other\\napplication areas. One major benefit of GNPs is the ability to quantify\\nuncertainty in deep learning on graph structures. An additional benefit of this\\nmethod is the ability to extend graph neural networks to inputs of dynamic\\nsized graphs.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.10160</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.10160</id><submitter>Scott Burns</submitter><version version=\"v1\"><date>Tue, 26 Feb 2019 19:00:29 GMT</date><size>931kb</size></version><version version=\"v2\"><date>Thu, 28 Feb 2019 14:13:27 GMT</date><size>942kb</size></version><version version=\"v3\"><date>Wed, 10 Apr 2019 09:15:48 GMT</date><size>942kb</size></version><version version=\"v4\"><date>Thu, 18 Apr 2019 14:02:37 GMT</date><size>944kb</size></version><version version=\"v5\"><date>Tue, 23 Apr 2019 10:30:20 GMT</date><size>943kb</size></version><version version=\"v6\"><date>Tue, 21 May 2019 12:12:41 GMT</date><size>944kb</size></version><version version=\"v7\"><date>Thu, 4 Jul 2019 12:11:12 GMT</date><size>945kb</size></version><version version=\"v8\"><date>Sat, 28 Sep 2019 17:58:06 GMT</date><size>961kb</size></version><title>Chromatic Adaptation Transform by Spectral Reconstruction (Preprint)</title><authors>Scott A Burns</authors><categories>cs.GR</categories><comments>Ver 2 adds the abstract. Ver 3 gives attribution to Eq 1. Ver 4 adds\\n  publication notice. Ver 5 corrects Table 4. Ver 6 adds email address, date,\\n  and updates publication notice. Ver 7 adds link to full text of the final\\n  published version at Color Res Appl. Ver 8 adds citation of final publication</comments><journal-ref>Color Res Appl. 2019;44(5):682-693</journal-ref><doi>10.1002/col.22384</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A color appearance model (CAM) is an advanced colorimetric tool used to\\npredict color appearance under a wide variety of viewing conditions. A\\nchromatic adaptation transform (CAT) is an integral part of a CAM. Its role is\\nto predict &quot;corresponding colors,&quot; that is, a pair of colors that have the same\\ncolor appearance when viewed under different illuminants, after partial or full\\nadaptation to each illuminant. Modern CATs perform well when applied to a\\nlimited range of illuminant pairs and a limited range of source (test) colors.\\nHowever, they can fail if operated outside these ranges. For imaging\\napplications, it is important to have a CAT that can operate on any real color\\nand illuminant pair without failure. This paper proposes a new CAT that does\\nnot operate on the standard von Kries model of adaptation. Instead it relies on\\nspectral reconstruction and how these reconstructions behave with respect to\\ndifferent illuminants. It is demonstrated that the proposed CAT is immune to\\nsome of the limitations of existing CATs (such as producing colors with\\nnegative tristimulus values). The proposed CAT does not use established\\nempirical corresponding-color datasets to optimize performance, as most modern\\nCATs do, yet it performs as well as or better than the most recent CATs when\\ntested against the corresponding-color datasets. This increase in robustness\\ncomes at the expense of additional complexity and computational effort. If\\nrobustness is of prime importance, then the proposed method may be justifiable.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.10170</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.10170</id><submitter>Haizhao Yang</submitter><version version=\"v1\"><date>Tue, 26 Feb 2019 19:10:58 GMT</date><size>1687kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 10 Apr 2019 08:45:42 GMT</date><size>1555kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 13 May 2019 11:54:00 GMT</date><size>3160kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Mon, 7 Oct 2019 18:43:26 GMT</date><size>1566kb</size><source_type>D</source_type></version><title>Nonlinear Approximation via Compositions</title><authors>Zuowei Shen and Haizhao Yang and Shijun Zhang</authors><categories>cs.LG stat.ML</categories><journal-ref>Neural Networks, Volume 119, November 2019, Pages 74-84</journal-ref><doi>10.1016/j.neunet.2019.07.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the approximation efficiency of function compositions in nonlinear\\napproximation, especially the case when compositions are implemented using\\nmulti-layer feed-forward neural networks (FNNs) with ReLU activation functions.\\nThe central question of interest is what are the advantages of function\\ncompositions in generating dictionaries and what is the optimal implementation\\nof function compositions via ReLU FNNs, especially in modern computing\\narchitecture. This question is answered by studying the $N$-term approximation\\nrate, which is the decrease in error versus the number of computational nodes\\n(neurons) in the approximant, together with parallel efficiency for the first\\ntime.\\n  First, for an arbitrary function $f$ on $[0,1]$, regardless of its smoothness\\nand even the continuity, if $f$ can be approximated via nonlinear approximation\\nusing one-hidden-layer ReLU FNNs with an approximation rate $O(N^{-\\\\eta})$, we\\nquantitatively show that dictionaries with function compositions via deep ReLU\\nFNNs can improve the approximation rate to $O(N^{-2\\\\eta})$. Second, for\\nH{\\\\&quot;o}lder continuous functions of order $\\\\alpha$ with a uniform Lipchitz\\nconstant $\\\\omega$ on a $d$-dimensional cube, we show that the $N$-term\\napproximation via ReLU FNNs with two or three function compositions can achieve\\nan approximation rate $O( N^{-2\\\\alpha/d})$. The approximation rate can be\\nimproved to $O(L^{-2\\\\alpha/d})$ by composing $L$ times, if $N$ is fixed and\\nsufficiently large; but further compositions cannot achieve the approximation\\nrate $O(N^{-\\\\alpha L/d})$. Finally, considering the computational efficiency\\nper training iteration in parallel computing, FNNs with $O(1)$ hidden layers\\nare an optimal choice for approximating H{\\\\&quot;o}lder continuous functions if\\ncomputing resources are enough.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.10184</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.10184</id><submitter>Sergio Grammatico</submitter><version version=\"v1\"><date>Tue, 26 Feb 2019 19:52:01 GMT</date><size>40kb</size></version><version version=\"v2\"><date>Tue, 1 Oct 2019 12:13:03 GMT</date><size>48kb</size></version><title>Convergence in uncertain linear systems</title><authors>Filippo Fabiani, Giuseppe Belgioioso, Franco Blanchini, Patrizio\\n  Colaneri, Sergio Grammatico</authors><categories>math.OC cs.SY math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State convergence is essential in several scientific areas, e.g. multi-agent\\nconsensus/disagreement, distributed optimization, monotone game theory,\\nmulti-agent learning over time-varying networks. This paper is the first on\\nstate convergence in both continuous- and discrete-time linear systems affected\\nby polytopic uncertainty. First, we characterize state convergence in linear\\ntime invariant systems via equivalent necessary and sufficient conditions. In\\nthe presence of uncertainty, we complement the canonical definition of (weak)\\nconvergence with a stronger notion of convergence, which requires the existence\\nof a common kernel among the generator matrices of the difference/differential\\ninclusion (strong convergence). We investigate under which conditions the two\\ndefinitions are equivalent. Then, we characterize weak and strong convergence\\nby means of Lyapunov and LaSalle arguments, (linear) matrix inequalities and\\nseparability of the eigenvalues of the generator matrices. We also show that,\\nunlike asymptotic stability, state convergence lacks of duality.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.10482</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.10482</id><submitter>Ruiying Geng</submitter><version version=\"v1\"><date>Wed, 27 Feb 2019 12:16:55 GMT</date><size>428kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 09:21:49 GMT</date><size>413kb</size><source_type>D</source_type></version><title>Induction Networks for Few-Shot Text Classification</title><authors>Ruiying Geng, Binhua Li, Yongbin Li, Xiaodan Zhu, Ping Jian and Jian\\n  Sun</authors><categories>cs.CL</categories><comments>7 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text classification tends to struggle when data is deficient or when it needs\\nto adapt to unseen classes. In such challenging scenarios, recent studies have\\nused meta-learning to simulate the few-shot task, in which new queries are\\ncompared to a small support set at the sample-wise level. However, this\\nsample-wise comparison may be severely disturbed by the various expressions in\\nthe same class. Therefore, we should be able to learn a general representation\\nof each class in the support set and then compare it to new queries. In this\\npaper, we propose a novel Induction Network to learn such a generalized\\nclass-wise representation, by innovatively leveraging the dynamic routing\\nalgorithm in meta-learning. In this way, we find the model is able to induce\\nand generalize better. We evaluate the proposed model on a well-studied\\nsentiment classification dataset (English) and a real-world dialogue intent\\nclassification dataset (Chinese). Experiment results show that on both\\ndatasets, the proposed model significantly outperforms the existing\\nstate-of-the-art approaches, proving the effectiveness of class-wise\\ngeneralization in few-shot text classification.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.10645</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.10645</id><submitter>Kurniawan Irianto</submitter><version version=\"v1\"><date>Wed, 27 Feb 2019 17:25:56 GMT</date><size>932kb</size><source_type>D</source_type></version><title>S-PRAC: Fast Partial Packet Recovery with Network Coding in Very Noisy\\n  Wireless Channels</title><authors>Kurniawan D. Irianto, Juan A. Cabrera, Giang T. Nguyen, Hani Salah,\\n  and Frank H.P. Fitzek</authors><categories>cs.NI</categories><doi>10.1109/WD.2019.8734223</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Well-known error detection and correction solutions in wireless\\ncommunications are slow or incur high transmission overhead. Recently, notable\\nsolutions like PRAC and DAPRAC, implementing partial packet recovery with\\nnetwork coding, could address these problems. However, they perform slowly when\\nthere are many errors. We propose S-PRAC, a fast scheme for partial packet\\nrecovery, particularly designed for very noisy wireless channels. S-PRAC\\nimproves on DAPRAC. It divides each packet into segments consisting of a fixed\\nnumber of small RLNC encoded symbols and then attaches a CRC code to each\\nsegment and one to each coded packet. Extensive simulations show that S-PRAC\\ncan detect and correct errors quickly. It also outperforms DAPRAC significantly\\nwhen the number of errors is high.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.10646</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.10646</id><submitter>Adish Singla</submitter><version version=\"v1\"><date>Wed, 27 Feb 2019 17:27:30 GMT</date><size>666kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 09:14:26 GMT</date><size>3793kb</size><source_type>D</source_type></version><title>Unifying Ensemble Methods for Q-learning via Social Choice Theory</title><authors>Rishav Chourasia, Adish Singla</authors><categories>cs.AI</categories><comments>Learning with Rich Experience (LIRE) Workshop, NeurIPS 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ensemble methods have been widely applied in Reinforcement Learning (RL) in\\norder to enhance stability, increase convergence speed, and improve\\nexploration. These methods typically work by employing an aggregation mechanism\\nover actions of different RL algorithms. We show that a variety of these\\nmethods can be unified by drawing parallels from committee voting rules in\\nSocial Choice Theory. We map the problem of designing an action aggregation\\nmechanism in an ensemble method to a voting problem which, under different\\nvoting rules, yield popular ensemble-based RL algorithms like Majority Voting\\nQ-learning or Bootstrapped Q-learning. Our unification framework, in turn,\\nallows us to design new ensemble-RL algorithms with better performance. For\\ninstance, we map two diversity-centered committee voting rules, namely Single\\nNon-Transferable Voting Rule and Chamberlin-Courant Rule, into new RL\\nalgorithms that demonstrate excellent exploratory behavior in our experiments.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1902.10654</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1902.10654</id><submitter>Karoliina Lehtinen</submitter><version version=\"v1\"><date>Wed, 27 Feb 2019 17:39:31 GMT</date><size>34kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 28 Feb 2019 10:08:52 GMT</date><size>34kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 1 Mar 2019 09:14:55 GMT</date><size>35kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Thu, 3 Oct 2019 14:54:20 GMT</date><size>41kb</size><source_type>D</source_type></version><title>Register Games</title><authors>Karoliina Lehtinen and Udi Boker</authors><categories>cs.FL</categories><comments>To appear in Logical Methods in Computer Science</comments><msc-class>68Q45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The complexity of parity games is a long standing open problem that saw a\\nmajor breakthrough in 2017 when two quasi-polynomial algorithms were published.\\nThis article presents a third, independent approach to solving parity games in\\nquasi-polynomial time, based on the notion of register game, a parameterised\\nvariant of a parity game. The analysis of register games leads to a\\nquasi-polynomial algorithm for parity games, a polynomial algorithm for\\nrestricted classes of parity games and a novel measure of complexity, the\\nregister index, which aims to capture the combined complexity of the priority\\nassignement and the underlying game graph.\\n  We further present a translation of alternating parity word automata into\\nalternating weak automata with only a quasi-polynomial increase in size, based\\non register games; this improves on the previous exponential translation.\\n  We also use register games to investigate the parity index hierarchy: while\\nfor words the index hierarchy of alternating parity automata collapses to the\\nweak level, and for trees it is strict, for structures between trees and words,\\nit collapses logarithmically, in the sense that any parity tree automaton of\\nsize n is equivalent, on these particular classes of structures, to an\\nautomaton with a number of priorities logarithmic in n.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.00704</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.00704</id><submitter>Binghan He</submitter><version version=\"v1\"><date>Sat, 2 Mar 2019 13:41:52 GMT</date><size>1735kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 6 Mar 2019 20:21:14 GMT</date><size>1735kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 3 Oct 2019 17:46:39 GMT</date><size>605kb</size><source_type>D</source_type></version><title>Complex Stiffness Model of Physical Human-Robot Interaction:\\n  Implications for Control of Performance Augmentation Exoskeletons</title><authors>Binghan He, Huang Huang, Gray C. Thomas and Luis Sentis</authors><categories>cs.RO</categories><comments>Accepted for publication in IEEE/RSJ International Conference on\\n  Intelligent Robots and Systems (IROS) Copyright 2019 IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human joint dynamic stiffness plays an important role in the stability of\\nperformance augmentation exoskeletons. In this paper, we consider a new\\nfrequency domain model of the human joint dynamics which features a complex\\nvalue stiffness. This complex stiffness consists of a real stiffness and a\\nhysteretic damping. We use it to explain the dynamic behaviors of the human\\nconnected to the exoskeleton, in particular the observed non-zero low frequency\\nphase shift and the near constant damping ratio of the resonant as stiffness\\nand inertia vary. We validate this concept by experimenting with an elbow-joint\\nexoskeleton testbed on a subject while modifying joint stiffness behavior,\\nexoskeleton inertia, and strength augmentation gains. We compare three\\ndifferent models of elbow-joint dynamic stiffness: a model with real stiffness,\\nviscous damping and inertia, a model with complex stiffness and inertia, and a\\nmodel combining the previous two models. Our results show that the hysteretic\\ndamping term improves modeling accuracy, using a statistical F-test. Moreover\\nthis improvement is statistically more significant than using classical viscous\\ndamping term. In addition, we experimentally observe a linear relationship\\nbetween the hysteretic damping and the real part of the stiffness which allows\\nus to simplify the complex stiffness model as a 1-parameter system. Ultimately,\\nwe design a fractional order controller to demonstrate how human hysteretic\\ndamping behavior can be exploited to improve strength amplification performance\\nwhile maintaining stability.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.00820</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.00820</id><submitter>Md Jahidul Islam</submitter><version version=\"v1\"><date>Sun, 3 Mar 2019 03:55:03 GMT</date><size>6055kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 03:23:10 GMT</date><size>7569kb</size><source_type>D</source_type></version><title>Robot-to-Robot Relative Pose Estimation using Humans as Markers</title><authors>Md Jahidul Islam, Jiawei Mo and Junaed Sattar</authors><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a method to determine the 3D relative pose of pairs\\nof communicating robots by using human pose-based key-points as\\ncorrespondences. We adopt a `leader-follower\\' framework, where at first, the\\nleader robot visually detects and triangulates the key-points using the\\nstate-of-the-art pose detector named OpenPose. Afterward, the follower robots\\nmatch the corresponding 2D projections on their respective calibrated cameras\\nand find their relative poses by solving the perspective-n-point (PnP) problem.\\nIn the proposed method, we design an efficient person re-identification\\ntechnique for associating the mutually visible humans in the scene.\\nAdditionally, we present an iterative optimization algorithm to refine the\\nassociated key-points based on their local structural properties in the image\\nspace. We demonstrate that these refinement processes are essential to\\nestablish accurate key-point correspondences across viewpoints. Furthermore, we\\nevaluate the performance of the proposed relative pose estimation method\\nthrough several experiments conducted in terrestrial and underwater\\nenvironments. Finally, we discuss the relevant operational challenges of this\\napproach and analyze its feasibility for multi-robot cooperative systems in\\nhuman-dominated social settings and feature-deprived environments such as\\nunderwater.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.00843</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.00843</id><submitter>Xiang Liu</submitter><version version=\"v1\"><date>Sun, 3 Mar 2019 06:34:24 GMT</date><size>20kb</size></version><version version=\"v2\"><date>Sat, 5 Oct 2019 00:39:30 GMT</date><size>90kb</size></version><title>Multiple Learning for Regression in big data</title><authors>Xiang Liu, Ziyang Tang, Huyunting Huang, Tonglin Zhang, Baijian Yang</authors><categories>cs.LG cs.DC stat.ML</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regression problems that have closed-form solutions are well understood and\\ncan be easily implemented when the dataset is small enough to be all loaded\\ninto the RAM. Challenges arise when data is too big to be stored in RAM to\\ncompute the closed form solutions. Many techniques were proposed to overcome or\\nalleviate the memory barrier problem but the solutions are often local optimal.\\nIn addition, most approaches require accessing the raw data again when updating\\nthe models. Parallel computing clusters are also expected if multiple models\\nneed to be computed simultaneously. We propose multiple learning approaches\\nthat utilize an array of sufficient statistics (SS) to address this big data\\nchallenge. This memory oblivious approach breaks the memory barrier when\\ncomputing regressions with closed-form solutions, including but not limited to\\nlinear regression, weighted linear regression, linear regression with Box-Cox\\ntransformation (Box-Cox regression) and ridge regression models. The\\ncomputation and update of the SS array can be handled at per row level or per\\nmini-batch level. And updating a model is as easy as matrix addition and\\nsubtraction. Furthermore, multiple SS arrays for different models can be easily\\ncomputed simultaneously to obtain multiple models at one pass through the\\ndataset. We implemented our approaches on Spark and evaluated over the\\nsimulated datasets. Results showed our approaches can achieve closed-form\\nsolutions of multiple models at the cost of half training time of the\\ntraditional methods for a single model.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.00979</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.00979</id><submitter>Sarthak Chatterjee</submitter><version version=\"v1\"><date>Sun, 3 Mar 2019 20:09:39 GMT</date><size>72kb</size></version><version version=\"v2\"><date>Tue, 1 Oct 2019 17:48:29 GMT</date><size>72kb</size></version><title>Analysis of Gradient-Based Expectation-Maximization-Like Algorithms via\\n  Integral Quadratic Constraints</title><authors>Sarthak Chatterjee, Orlando Romero, S\\\\\\'ergio Pequito</authors><categories>math.OC cs.LG cs.SY math.DS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Expectation-Maximization (EM) algorithm is one of the most popular\\nmethods used to solve the problem of distribution-based clustering in\\nunsupervised learning. In this paper, we propose an analysis of a generalized\\nEM (GEM) algorithm and a designed EM-like algorithm, as linear time-invariant\\n(LTI) systems with a feedback nonlinearity, and by leveraging tools from robust\\ncontrol theory, particularly integral quadratic constraints (IQCs). Towards\\nthis goal, we investigate the absolute stability of dynamical systems of the\\nabove form with a sector-bounded feedback nonlinearity, that represent the\\naforementioned algorithms. This analysis allows us to craft a strongly convex\\nobjective function, which led to the design of the aforementioned novel EM-like\\nalgorithm for Gaussian mixture models (GMMs). Furthermore, it allows us to\\nestablish bounds on the convergence rates of the studied algorithms. In\\nparticular, the derived bounds for our proposed EM-like algorithm generalize\\nbounds found in the literature for the EM algorithm on GMMs, and our analysis\\nof an existing gradient ascent GEM algorithm based on the $Q$-function allowed\\nus to approximately recover bounds found in the literature.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.01373</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.01373</id><submitter>Shayegan Omidshafiei</submitter><version version=\"v1\"><date>Mon, 4 Mar 2019 17:13:40 GMT</date><size>2667kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 12 Mar 2019 11:25:21 GMT</date><size>3385kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sun, 19 May 2019 17:10:21 GMT</date><size>3385kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Fri, 4 Oct 2019 15:22:09 GMT</date><size>3385kb</size><source_type>D</source_type></version><title>$\\\\alpha$-Rank: Multi-Agent Evaluation by Evolution</title><authors>Shayegan Omidshafiei, Christos Papadimitriou, Georgios Piliouras, Karl\\n  Tuyls, Mark Rowland, Jean-Baptiste Lespiau, Wojciech M. Czarnecki, Marc\\n  Lanctot, Julien Perolat, and Remi Munos</authors><categories>cs.MA cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce $\\\\alpha$-Rank, a principled evolutionary dynamics methodology\\nfor the evaluation and ranking of agents in large-scale multi-agent\\ninteractions, grounded in a novel dynamical game-theoretic solution concept\\ncalled Markov-Conley chains (MCCs). The approach leverages continuous- and\\ndiscrete-time evolutionary dynamical systems applied to empirical games, and\\nscales tractably in the number of agents, the type of interactions, and the\\ntype of empirical games (symmetric and asymmetric). Current models are\\nfundamentally limited in one or more of these dimensions and are not guaranteed\\nto converge to the desired game-theoretic solution concept (typically the Nash\\nequilibrium). $\\\\alpha$-Rank provides a ranking over the set of agents under\\nevaluation and provides insights into their strengths, weaknesses, and\\nlong-term dynamics. This is a consequence of the links we establish to the MCC\\nsolution concept when the underlying evolutionary model\\'s ranking-intensity\\nparameter, $\\\\alpha$, is chosen to be large, which exactly forms the basis of\\n$\\\\alpha$-Rank. In contrast to the Nash equilibrium, which is a static concept\\nbased on fixed points, MCCs are a dynamical solution concept based on the\\nMarkov chain formalism, Conley\\'s Fundamental Theorem of Dynamical Systems, and\\nthe core ingredients of dynamical systems: fixed points, recurrent sets,\\nperiodic orbits, and limit cycles. $\\\\alpha$-Rank runs in polynomial time with\\nrespect to the total number of pure strategy profiles, whereas computing a Nash\\nequilibrium for a general-sum game is known to be intractable. We introduce\\nproofs that not only provide a unifying perspective of existing continuous- and\\ndiscrete-time evolutionary evaluation models, but also reveal the formal\\nunderpinnings of the $\\\\alpha$-Rank methodology. We empirically validate the\\nmethod in several domains including AlphaGo, AlphaZero, MuJoCo Soccer, and\\nPoker.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.01648</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.01648</id><submitter>Tianyi Li</submitter><version version=\"v1\"><date>Tue, 5 Mar 2019 03:43:10 GMT</date><size>1227kb</size></version><title>A DenseNet Based Approach for Multi-Frame In-Loop Filter in HEVC</title><authors>Tianyi Li, Mai Xu, Ren Yang and Xiaoming Tao</authors><categories>cs.CV</categories><comments>10 pages, 4 figures. Accepted by Data Compression Conference 2019</comments><journal-ref>Data Compression Conference 2019</journal-ref><doi>10.1109/TIP.2019.2921877</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High efficiency video coding (HEVC) has brought outperforming efficiency for\\nvideo compression. To reduce the compression artifacts of HEVC, we propose a\\nDenseNet based approach as the in-loop filter of HEVC, which leverages multiple\\nadjacent frames to enhance the quality of each encoded frame. Specifically, the\\nhigher-quality frames are found by a reference frame selector (RFS). Then, a\\ndeep neural network for multi-frame in-loop filter (named MIF-Net) is developed\\nto enhance the quality of each encoded frame by utilizing the spatial\\ninformation of this frame and the temporal information of its neighboring\\nhigher-quality frames. The MIF-Net is built on the recently developed DenseNet,\\nbenefiting from the improved generalization capacity and computational\\nefficiency. Finally, experimental results verify the effectiveness of our\\nmulti-frame in-loop filter, outperforming the HM baseline and other\\nstate-of-the-art approaches.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.02114</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.02114</id><submitter>Jo\\\\~ao Silv\\\\\\'erio</submitter><version version=\"v1\"><date>Tue, 5 Mar 2019 23:45:20 GMT</date><size>8020kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 11 Mar 2019 13:55:14 GMT</date><size>8019kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 30 Sep 2019 16:23:55 GMT</date><size>4570kb</size><source_type>D</source_type></version><title>Uncertainty-Aware Imitation Learning using Kernelized Movement\\n  Primitives</title><authors>Jo\\\\~ao Silv\\\\\\'erio, Yanlong Huang, Fares J. Abu-Dakka, Leonel Rozo and\\n  Darwin G. Caldwell</authors><categories>cs.RO cs.LG</categories><comments>Published in the proceedings of IROS 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the past few years, probabilistic approaches to imitation learning\\nhave earned a relevant place in the literature. One of their most prominent\\nfeatures, in addition to extracting a mean trajectory from task demonstrations,\\nis that they provide a variance estimation. The intuitive meaning of this\\nvariance, however, changes across different techniques, indicating either\\nvariability or uncertainty. In this paper we leverage kernelized movement\\nprimitives (KMP) to provide a new perspective on imitation learning by\\npredicting variability, correlations and uncertainty about robot actions. This\\nrich set of information is used in combination with optimal controller fusion\\nto learn actions from data, with two main advantages: i) robots become safe\\nwhen uncertain about their actions and ii) they are able to leverage partial\\ndemonstrations, given as elementary sub-tasks, to optimally perform a higher\\nlevel, more complex task. We showcase our approach in a painting task, where a\\nhuman user and a KUKA robot collaborate to paint a wooden board. The task is\\ndivided into two sub-tasks and we show that using our approach the robot\\nbecomes compliant (hence safe) outside the training regions and executes the\\ntwo sub-tasks with optimal gains.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.02133</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.02133</id><submitter>Yunfan Liu</submitter><version version=\"v1\"><date>Wed, 6 Mar 2019 01:31:30 GMT</date><size>2830kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 01:51:48 GMT</date><size>2843kb</size><source_type>D</source_type></version><title>Age Progression and Regression with Spatial Attention Modules</title><authors>Qi Li, Yunfan Liu, Zhenan Sun</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Age progression and regression refers to aesthetically render-ing a given\\nface image to present effects of face aging and rejuvenation, respectively.\\nAlthough numerous studies have been conducted in this topic, there are two\\nmajor problems: 1) multiple models are usually trained to simulate different\\nage mappings, and 2) the photo-realism of generated face images is heavily\\ninfluenced by the variation of training images in terms of pose, illumination,\\nand background. To address these issues, in this paper, we propose a framework\\nbased on conditional Generative Adversarial Networks (cGANs) to achieve age\\nprogression and regression simultaneously. Particularly, since face aging and\\nrejuvenation are largely different in terms of image translation patterns, we\\nmodel these two processes using two separate generators, each dedicated to one\\nage changing process. In addition, we exploit spatial attention mechanisms to\\nlimit image modifications to regions closely related to age changes, so that\\nimages with high visual fidelity could be synthesized for in-the-wild cases.\\nExperiments on multiple datasets demonstrate the ability of our model in\\nsynthesizing lifelike face images at desired ages with personalized features\\nwell preserved, and keeping age-irrelevant regions unchanged.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.02423</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.02423</id><submitter>Milena Veneva</submitter><version version=\"v1\"><date>Mon, 4 Mar 2019 19:44:24 GMT</date><size>3764kb</size><source_type>D</source_type></version><title>Performance Analysis of Effective Symbolic Methods for Solving Band\\n  Matrix SLAEs</title><authors>Milena Veneva and Alexander Ayriyan</authors><categories>cs.MS</categories><comments>7 pages, 9 tables, 4 figures</comments><doi>10.1051/epjconf/201921405004</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents an experimental performance study of implementations of\\nthree symbolic algorithms for solving band matrix systems of linear algebraic\\nequations with heptadiagonal, pentadiagonal, and tridiagonal coefficient\\nmatrices. The only assumption on the coefficient matrix in order for the\\nalgorithms to be stable is nonsingularity. These algorithms are implemented\\nusing the GiNaC library of C++ and the SymPy library of Python, considering\\nfive different data storing classes. Performance analysis of the\\nimplementations is done using the high-performance computing (HPC) platforms\\n&quot;HybriLIT&quot; and &quot;Avitohol&quot;. The experimental setup and the results from the\\nconducted computations on the individual computer systems are presented and\\ndiscussed. An analysis of the three algorithms is performed.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.02858</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.02858</id><submitter>Daniel Tenbrinck</submitter><version version=\"v1\"><date>Thu, 7 Mar 2019 12:03:44 GMT</date><size>3696kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 8 Mar 2019 12:14:59 GMT</date><size>3696kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 30 Sep 2019 10:01:43 GMT</date><size>4836kb</size><source_type>D</source_type></version><title>Variational Graph Methods for Efficient Point Cloud Sparsification</title><authors>Daniel Tenbrinck, Fjedor Gaede, Martin Burger</authors><categories>math.NA cs.DM cs.DS cs.NA math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years new application areas have emerged in which one aims to\\ncapture the geometry of objects by means of three-dimensional point clouds.\\nOften the obtained data consist of a dense sampling of the object\\'s surface,\\ncontaining many redundant 3D points. These unnecessary data samples lead to\\nhigh computational effort in subsequent processing steps. Thus, point cloud\\nsparsification or compression is often applied as a preprocessing step. The two\\nstandard methods to compress dense 3D point clouds are random subsampling and\\napproximation schemes based on hierarchical tree structures, e.g., octree\\nrepresentations. However, both approaches give little flexibility for adjusting\\npoint cloud compression based on a-priori knowledge on the geometry of the\\nscanned object. Furthermore, these methods lead to suboptimal approximations if\\nthe 3D point cloud data is prone to noise. In this paper we propose a\\nvariational method defined on finite weighted graphs, which allows to sparsify\\na given 3D point cloud while giving the flexibility to control the appearance\\nof the resulting approximation based on the chosen regularization functional.\\nThe main contribution in this paper is a novel coarse-to-fine optimization\\nscheme for point cloud sparsification, inspired by the efficiency of the\\nrecently proposed Cut Pursuit algorithm for total variation denoising. This\\nstrategy gives a substantial speed up in computing sparse point clouds compared\\nto a direct application on all points as done in previous works and renders\\nvariational methods now applicable for this task. We compare different settings\\nfor our point cloud sparsification method both on unperturbed as well as noisy\\n3D point cloud data.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.03085</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.03085</id><submitter>Anum Ali</submitter><version version=\"v1\"><date>Thu, 7 Mar 2019 18:23:04 GMT</date><size>5394kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 15:22:52 GMT</date><size>8540kb</size><source_type>D</source_type></version><title>Non-Stationarities in Extra-Large Scale Massive MIMO</title><authors>Elisabeth De Carvalho and Anum Ali and Abolfazl Amiri and Marko\\n  Angjelichinoski and Robert W. Heath Jr</authors><categories>cs.IT eess.SP math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO, a key technology for increasing area spectral efficiency in\\ncellular systems, was developed assuming moderately sized apertures. In this\\npaper, we argue that massive MIMO systems behave differently in large-scale\\nregimes due to spatial non-stationarity. In the large-scale regime, with arrays\\nof around fifty wavelengths, the terminals see the whole array but\\nnon-stationarities occur because different regions of the array see different\\npropagation paths. At even larger dimensions, which we call the extra-large\\nscale regime, terminals see a portion of the array and inside the first type of\\nnon-stationarities might occur. We show that the non-stationarity properties of\\nthe massive MIMO channel changes several important MIMO design aspects. In\\nsimulations, we demonstrate how non-stationarity is a curse when neglected but\\na blessing when embraced in terms of computational load and multi-user\\ntransceiver design.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.03196</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.03196</id><submitter>Matteo Magnani</submitter><version version=\"v1\"><date>Thu, 7 Mar 2019 21:54:18 GMT</date><size>555kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 19 Jun 2019 21:37:28 GMT</date><size>1190kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sat, 5 Oct 2019 20:31:25 GMT</date><size>488kb</size></version><title>An Analysis of the Consequences of the General Data Protection\\n  Regulation (GDPR) on Social Network Research</title><authors>Andreas Kotsios and Matteo Magnani and Luca Rossi and Irina Shklovski\\n  and Davide Vega</authors><categories>cs.SI</categories><comments>To appear on ACM Transactions on Social Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article examines the principles outlined in the General Data Protection\\nRegulation (GDPR) in the context of social network data. We provide both a\\npractical guide to GDPR-compliant social network data processing, covering\\naspects such as data collection, consent, anonymization and data analysis, and\\na broader discussion of the problems emerging when the general principles on\\nwhich the regulation is based are instantiated to this research area.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.03978</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.03978</id><submitter>Yidong Luo</submitter><version version=\"v1\"><date>Sun, 10 Mar 2019 12:24:27 GMT</date><size>205kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 2 Apr 2019 06:53:59 GMT</date><size>115kb</size></version><version version=\"v3\"><date>Wed, 10 Apr 2019 05:32:20 GMT</date><size>115kb</size></version><version version=\"v4\"><date>Tue, 7 May 2019 12:16:01 GMT</date><size>115kb</size></version><version version=\"v5\"><date>Fri, 17 May 2019 05:01:46 GMT</date><size>115kb</size></version><version version=\"v6\"><date>Sun, 30 Jun 2019 03:47:28 GMT</date><size>115kb</size></version><version version=\"v7\"><date>Sat, 20 Jul 2019 04:48:20 GMT</date><size>115kb</size></version><version version=\"v8\"><date>Sat, 5 Oct 2019 04:23:13 GMT</date><size>110kb</size></version><title>Galerkin Method with Trigonometric Basis on Stable Numerical\\n  Differentiation</title><authors>Yidong Luo</authors><categories>math.NA cs.NA</categories><comments>Eighth Version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the $ p $ ($ p=1,2,3 $) order numerical differentiation\\non function $ y $ in $ (0,2\\\\pi) $. They are transformed into corresponding\\nFredholm integral equation of the first kind. Computational schemes with\\nanalytic solution formulas are designed using Galerkin method on trigonometric\\nbasis. Convergence and divergence are all analysed in Corollaries 5.1, 5.2, and\\na-priori error estimate is uniformly obtained in Theorem 6.1, 7.1, 7.2.\\nTherefore, the algorithm achieves the optimal convergence rate $ O(\\n\\\\delta^{\\\\frac{2\\\\mu}{2\\\\mu+1}} ) \\\\ (\\\\mu = \\\\frac{1}{2} \\\\ \\\\textrm{or} \\\\ 1)$ with\\nperiodic Sobolev source condition of order $ 2\\\\mu p $. Besides, we indicate a\\nnoise-independent a-priori parameter choice when the function $ y $ possesses\\nthe form of \\\\begin{equation*}\\n  \\\\sum^{p-1}_{k=0} a_k t^k + \\\\sum^{N_1}_{k=1} b_k \\\\cos k t + \\\\sum^{N_2}_{k=1}\\nc_k \\\\sin k t, \\\\ b_{N_1}, c_{N_2} \\\\neq 0, \\\\end{equation*} In particular, in\\nnumerical differentiations for functions above, good filtering effect (error\\napproaches 0) is displayed with corresponding parameter choice. In addition,\\nseveral numerical examples are given to show that even derivatives with\\ndiscontinuity can be recovered well.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.04057</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.04057</id><submitter>Joeri Hermans</submitter><version version=\"v1\"><date>Sun, 10 Mar 2019 20:51:02 GMT</date><size>2090kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 13:06:38 GMT</date><size>6967kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 8 Oct 2019 08:29:11 GMT</date><size>7011kb</size><source_type>D</source_type></version><title>Likelihood-free MCMC with Amortized Approximate Likelihood Ratios</title><authors>Joeri Hermans, Volodimir Begy, Gilles Louppe</authors><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Posterior inference with an intractable likelihood is becoming an\\nincreasingly common task in scientific domains which rely on sophisticated\\ncomputer simulations. Typically, these mechanistic models do not admit\\ntractable densities forcing practitioners to rely on approximations during\\ninference. This work proposes a novel approach to address the intractability of\\nthe likelihood and the marginal model. We achieve this by learning a flexible\\nestimator which approximates the likelihood-to-evidence ratio. The resulting\\namortized ratio estimator is embedded in MCMC samplers such as\\nMetropolis-Hastings and Hamiltonian Monte Carlo to approximate the\\nlikelihood-ratio between consecutive states in the Markov chain, allowing us to\\ndraw samples from the intractable posterior. Techniques are presented to\\nimprove the numerical stability. We demonstrate our approach on a variety of\\nbenchmarks and compare against well-established approximate inference\\ntechniques. Scientific applications in high energy and astrophysics with\\nhigh-dimensional observations show its applicability.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.04239</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.04239</id><submitter>Lin Gao</submitter><version version=\"v1\"><date>Mon, 11 Mar 2019 12:09:04 GMT</date><size>227kb</size></version><version version=\"v2\"><date>Tue, 9 Jul 2019 19:19:51 GMT</date><size>228kb</size></version><version version=\"v3\"><date>Thu, 3 Oct 2019 15:43:02 GMT</date><size>227kb</size></version><title>Multiobject fusion with minimum information loss</title><authors>Lin Gao and Giorgio Battistelli and Luigi Chisci</authors><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generalized covariance intersection (GCI) has been effective in fusing\\nmultiobject densities from multiple agents for multitarget tracking and mapping\\npurposes. From an information-theoretic viewpoint, it has been shown that GCI\\nfusion essentially minimizes the weighted information gain (WIG) from local\\ndensities to the fused one. In this paper, the interest is in the fusion rule\\nthat dually minimizes the weighted information loss (WIL) and it turns out that\\nsuch a fusion rule is consistent with the so-called linear opinion pool (LOP).\\nHowever, the LOP cannot be directly applied to multiobject fusion since the\\nresulting fused multiobject density (FMD), in general, no longer belongs to the\\nsame family of the local ones, thus it cannot be utilized as prior information\\nfor the next recursion in the context of Bayesian multiobject filtering. In\\norder to overcome such a difficulty, the principle of minimizing WIL is further\\nexploited in that the optimal FMD in the same family of the local ones is\\nlooked for. Implementation issues relative to the proposed minimum WIL (MWIL)\\nfusion rule are discussed. Finally, the performance of the MWIL rule is\\nassessed via simulation experiments concerning distributed multitarget tracking\\nover a wireless sensor network.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.04483</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.04483</id><submitter>Xin Wang</submitter><version version=\"v1\"><date>Mon, 11 Mar 2019 17:59:29 GMT</date><size>436kb</size><source_type>AD</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 09:49:40 GMT</date><size>228kb</size><source_type>AD</source_type></version><title>Quantifying the magic of quantum channels</title><authors>Xin Wang, Mark M. Wilde, Yuan Su</authors><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>44 pages, 7 figures; v2 close to published version</comments><journal-ref>New Journal of Physics 21 103002, 2019</journal-ref><doi>10.1088/1367-2630/ab451d</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To achieve universal quantum computation via general fault-tolerant schemes,\\nstabilizer operations must be supplemented with other non-stabilizer quantum\\nresources. Motivated by this necessity, we develop a resource theory for magic\\nquantum channels to characterize and quantify the quantum &quot;magic&quot; or\\nnon-stabilizerness of noisy quantum circuits. For qudit quantum computing with\\nodd dimension $d$, it is known that quantum states with non-negative Wigner\\nfunction can be efficiently simulated classically. First, inspired by this\\nobservation, we introduce a resource theory based on completely\\npositive-Wigner-preserving quantum operations as free operations, and we show\\nthat they can be efficiently simulated via a classical algorithm. Second, we\\nintroduce two efficiently computable magic measures for quantum channels,\\ncalled the mana and thauma of a quantum channel. As applications, we show that\\nthese measures not only provide fundamental limits on the distillable magic of\\nquantum channels, but they also lead to lower bounds for the task of\\nsynthesizing non-Clifford gates. Third, we propose a classical algorithm for\\nsimulating noisy quantum circuits, whose sample complexity can be quantified by\\nthe mana of a quantum channel. We further show that this algorithm can\\noutperform another approach for simulating noisy quantum circuits, based on\\nchannel robustness. Finally, we explore the threshold of non-stabilizerness for\\nbasic quantum circuits under depolarizing noise.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.04933</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.04933</id><submitter>Sander Dieleman</submitter><version version=\"v1\"><date>Wed, 6 Mar 2019 22:13:52 GMT</date><size>7073kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 17:55:59 GMT</date><size>9563kb</size><source_type>D</source_type></version><title>Hierarchical Autoregressive Image Models with Auxiliary Decoders</title><authors>Jeffrey De Fauw, Sander Dieleman, Karen Simonyan</authors><categories>cs.CV cs.LG stat.ML</categories><comments>Updated: added human evaluation results, incorporated review feedback</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autoregressive generative models of images tend to be biased towards\\ncapturing local structure, and as a result they often produce samples which are\\nlacking in terms of large-scale coherence. To address this, we propose two\\nmethods to learn discrete representations of images which abstract away local\\ndetail. We show that autoregressive models conditioned on these representations\\ncan produce high-fidelity reconstructions of images, and that we can train\\nautoregressive priors on these representations that produce samples with\\nlarge-scale coherence. We can recursively apply the learning procedure,\\nyielding a hierarchy of progressively more abstract image representations. We\\ntrain hierarchical class-conditional autoregressive models on the ImageNet\\ndataset and demonstrate that they are able to generate realistic images at\\nresolutions of 128$\\\\times$128 and 256$\\\\times$256 pixels. We also perform a\\nhuman evaluation study comparing our models with both adversarial and\\nlikelihood-based state-of-the-art generative models.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.05044</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.05044</id><submitter>S. Mazdak Abulnaga</submitter><version version=\"v1\"><date>Tue, 12 Mar 2019 16:48:23 GMT</date><size>4471kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 14 Mar 2019 23:52:38 GMT</date><size>4471kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 30 Sep 2019 18:30:15 GMT</date><size>2082kb</size><source_type>D</source_type></version><title>Placental Flattening via Volumetric Parameterization</title><authors>S. Mazdak Abulnaga, Esra Abaci Turk, Mikhail Bessmeltsev, P. Ellen\\n  Grant, Justin Solomon, Polina Golland</authors><categories>cs.CV</categories><comments>MICCAI 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a volumetric mesh-based algorithm for flattening the placenta to a\\ncanonical template to enable effective visualization of local anatomy and\\nfunction. Monitoring placental function in vivo promises to support pregnancy\\nassessment and to improve care outcomes. We aim to alleviate visualization and\\ninterpretation challenges presented by the shape of the placenta when it is\\nattached to the curved uterine wall. To do so, we flatten the volumetric mesh\\nthat captures placental shape to resemble the well-studied ex vivo shape. We\\nformulate our method as a map from the in vivo shape to a flattened template\\nthat minimizes the symmetric Dirichlet energy to control distortion throughout\\nthe volume. Local injectivity is enforced via constrained line search during\\ngradient descent. We evaluate the proposed method on 28 placenta shapes\\nextracted from MRI images in a clinical study of placental function. We achieve\\nsub-voxel accuracy in mapping the boundary of the placenta to the template\\nwhile successfully controlling distortion throughout the volume. We illustrate\\nhow the resulting mapping of the placenta enhances visualization of placental\\nanatomy and function. Our code is freely available at\\nhttps://github.com/mabulnaga/placenta-flattening .\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.05567</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.05567</id><submitter>Maxim Gonchar</submitter><version version=\"v1\"><date>Wed, 13 Mar 2019 16:09:08 GMT</date><size>94kb</size><source_type>D</source_type></version><title>GNA: new framework for statistical data analysis</title><authors>Anna Fatkina, Maxim Gonchar, Anastasia Kalitkina, Liudmila Kolupaeva,\\n  Dmitry Naumov, Dmitry Selivanov, Konstantin Treskov</authors><categories>cs.MS</categories><comments>9 pages, 3 figures, CHEP 2018, submitted to EPJ Web of Conferences</comments><doi>10.1051/epjconf/201921405024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report on the status of GNA --- a new framework for fitting large-scale\\nphysical models. GNA utilizes the data flow concept within which a model is\\nrepresented by a directed acyclic graph. Each node is an operation on an array\\n(matrix multiplication, derivative or cross section calculation, etc). The\\nframework enables the user to create flexible and efficient large-scale lazily\\nevaluated models, handle large numbers of parameters, propagate parameters\\'\\nuncertainties while taking into account possible correlations between them, fit\\nmodels, and perform statistical analysis. The main goal of the paper is to give\\nan overview of the main concepts and methods as well as reasons behind their\\ndesign. Detailed technical information is to be published in further works.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.05898</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.05898</id><submitter>Siqi Wang</submitter><version version=\"v1\"><date>Thu, 14 Mar 2019 10:24:57 GMT</date><size>697kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 25 Jul 2019 07:43:41 GMT</date><size>711kb</size><source_type>D</source_type></version><title>High-Throughput CNN Inference on Embedded ARM big.LITTLE Multi-Core\\n  Processors</title><authors>Siqi Wang, Gayathri Ananthanarayanan, Yifan Zeng, Neeraj Goel, Anuj\\n  Pathania, Tulika Mitra</authors><categories>cs.LG cs.DC cs.PF</categories><comments>14 pages, submitted to the Transactions on Computer-Aided Design of\\n  Integrated Circuits and Systems</comments><doi>10.1109/TCAD.2019.2944584</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IoT Edge intelligence requires Convolutional Neural Network (CNN) inference\\nto take place in the edge devices itself. ARM big.LITTLE architecture is at the\\nheart of prevalent commercial edge devices. It comprises of single-ISA\\nheterogeneous cores grouped into multiple homogeneous clusters that enable\\npower and performance trade-offs. All cores are expected to be simultaneously\\nemployed in inference to attain maximal throughput. However, high communication\\noverhead involved in parallelization of computations from convolution kernels\\nacross clusters is detrimental to throughput. We present an alternative\\nframework called Pipe-it that employs pipelined design to split convolutional\\nlayers across clusters while limiting parallelization of their respective\\nkernels to the assigned cluster. We develop a performance-prediction model that\\nutilizes only the convolutional layer descriptors to predict the execution time\\nof each layer individually on all permitted core configurations (type and\\ncount). Pipe-it then exploits the predictions to create a balanced pipeline\\nusing an efficient design space exploration algorithm. Pipe-it on average\\nresults in a 39% higher throughput than the highest antecedent throughput.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.05946</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.05946</id><submitter>Tim C Kietzmann</submitter><version version=\"v1\"><date>Thu, 14 Mar 2019 12:43:38 GMT</date><size>5447kb</size></version><version version=\"v2\"><date>Tue, 8 Oct 2019 08:45:53 GMT</date><size>2516kb</size></version><title>Recurrence is required to capture the representational dynamics of the\\n  human visual system</title><authors>Tim C Kietzmann, Courtney J Spoerer, Lynn S\\\\&quot;orensen, Radoslaw M\\n  Cichy, Olaf Hauk, and Nikolaus Kriegeskorte</authors><categories>q-bio.NC cs.CV cs.LG</categories><comments>https://www.pnas.org/content/early/2019/10/04/1905544116.short?rss=1</comments><journal-ref>Proceedings of the National Academy of Sciences, p. 1-10 (2019)</journal-ref><doi>10.1073/pnas.1905544116</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The human visual system is an intricate network of brain regions that enables\\nus to recognize the world around us. Despite its abundant lateral and feedback\\nconnections, object processing is commonly viewed and studied as a feedforward\\nprocess. Here, we measure and model the rapid representational dynamics across\\nmultiple stages of the human ventral stream using time-resolved brain imaging\\nand deep learning. We observe substantial representational transformations\\nduring the first 300 ms of processing within and across ventral-stream regions.\\nCategorical divisions emerge in sequence, cascading forward and in reverse\\nacross regions, and Granger causality analysis suggests bidirectional\\ninformation flow between regions. Finally, recurrent deep neural network models\\nclearly outperform parameter-matched feedforward models in terms of their\\nability to capture the multi-region cortical dynamics. Targeted virtual cooling\\nexperiments on the recurrent deep network models further substantiate the\\nimportance of their lateral and top-down connections. These results establish\\nthat recurrent models are required to understand information processing in the\\nhuman ventral stream.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.06327</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.06327</id><submitter>Ho-Chun Herbert Chang</submitter><version version=\"v1\"><date>Fri, 15 Mar 2019 02:26:40 GMT</date><size>1460kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 4 Jun 2019 16:37:37 GMT</date><size>2199kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 17 Jul 2019 14:19:16 GMT</date><size>2537kb</size><source_type>D</source_type></version><title>Co-Contagion Diffusion on Multilayer Networks</title><authors>Ho-Chun Herbert Chang and Feng Fu</authors><categories>cs.SI nlin.AO physics.soc-ph</categories><journal-ref>Applied Network Science. 4.78 (2019)</journal-ref><doi>10.1007/s41109-019-0176-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study examines the interface of three elements during co-contagion\\ndiffusion: the \\\\textbf{synergy} between contagions, the \\\\textbf{dormancy} rate\\nof each individual contagion, and the \\\\textbf{multiplex network topology}.\\nDormancy is defined as a weaker form of &quot;immunity,&quot; where dormant nodes no\\nlonger actively participate in diffusion, but are still susceptible to\\ninfection. The proposed model extends the literature on threshold models, and\\ndemonstrates intricate interdependencies between different graph structures.\\nOur simulations show that first, the faster contagion induces branching on the\\nslower contagion; second, shorter characteristic path lengths diminish the\\nimpact of dormancy in lowering diffusion. Third, when two long-range graphs are\\npaired, the faster contagion depends on both dormancy rates, whereas the slower\\ncontagion depends only on its own; fourth, synergistic contagions are less\\nsensitive to dormancy, and have a wider window to diffuse. Furthermore, when\\nlong-range and spatially constrained graphs are paired, ring vaccination occurs\\non the spatial graph and produces partial diffusion, due to dormant,\\nsurrounding nodes. The spatial contagion depends on both dormancy rates whereas\\nthe long-range contagion depends on only its own.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.06341</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.06341</id><submitter>Ruiqin Zhao</submitter><version version=\"v1\"><date>Fri, 15 Mar 2019 03:19:55 GMT</date><size>1535kb</size><source_type>D</source_type></version><title>Time Reversal based MAC for Multi-Hop Underwater Acoustic Networks</title><authors>Ruiqin Zhao, Hao Long, Octavia A. Dobre, Xiaohong Shen, Telex M. N.\\n  Ngatched, and Haodi Mei</authors><categories>cs.NI</categories><comments>16 pages,16 figures</comments><doi>10.1109/JSYST.2018.2890101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constrained-energy underwater acoustic nodes are typically connected via a\\nmulti-hop underwater acoustic network (MHUAN) to cover a broad marine region.\\nRecently, protocols for efficiently connecting such nodes have received\\nconsiderable attention. In this paper, we show that the time reversal (TR)\\nprocess plays an important role in the medium access control (MAC) because of\\nits physical capability to exploit the multi-path energy from the richly\\nscattering underwater environment, as well as to focus the signal energy in\\nboth spatial and temporal domains. In MHUANs, with severe multi-path\\npropagation at the physical layer, the active TR process spatially focuses the\\nsignals to the location of the intended receiver; this significantly diminishes\\nthe interference among parallel links. We propose an active TR-based MAC\\nprotocol for MHUANs, with the aim of minimizing collision and maximizing\\nchannel utilization simultaneously. Furthermore, by considering the impact of\\nthe cross-correlation between different links on the TR-based medium access, we\\nderive the threshold of the link cross-correlation to resolve collision caused\\nby the high cross-correlation between realistic links. We perform simulations\\nusing the OPNET and BELLHOP environments, and show that the proposed TR-based\\nMAC results in significantly improved throughput, decreased delay, and reduced\\ndata drop ratio in MHUANs.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.06407</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.06407</id><submitter>Fr\\\\\\'ed\\\\\\'eric Recoules</submitter><version version=\"v1\"><date>Fri, 15 Mar 2019 08:39:27 GMT</date><size>74kb</size></version><version version=\"v2\"><date>Tue, 1 Oct 2019 05:49:20 GMT</date><size>401kb</size><source_type>D</source_type></version><title>Get rid of inline assembly through verification-oriented lifting</title><authors>Fr\\\\\\'ed\\\\\\'eric Recoules, S\\\\\\'ebastien Bardin, Richard Bonichon, Laurent\\n  Mounier, Marie-Laure Potet</authors><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formal methods for software development have made great strides in the last\\ntwo decades, to the point that their application in safety-critical embedded\\nsoftware is an undeniable success. Their extension to non-critical software is\\none of the notable forthcoming challenges. For example, C programmers regularly\\nuse inline assembly for low-level optimizations and system primitives. This\\nusually results in driving state-of-the-art formal analyzers developed for C\\nineffective. We thus propose TInA, an automated, generic, trustable and\\nverification-oriented lifting technique turning inline assembly into\\nsemantically equivalent C code, in order to take advantage of existing C\\nanalyzers. Extensive experiments on real-world C code with inline assembly\\n(including GMP and ffmpeg) show the feasibility and benefits of TInA.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.06741</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.06741</id><submitter>Xi Xiong</submitter><version version=\"v1\"><date>Fri, 15 Mar 2019 18:41:15 GMT</date><size>7471kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 27 Sep 2019 23:14:18 GMT</date><size>1142kb</size><source_type>D</source_type></version><title>Analysis of a Stochastic Model for Coordinated Platooning of Heavy-duty\\n  Vehicles</title><authors>Xi Xiong and Erdong Xiao and Li Jin</authors><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Platooning of heavy-duty vehicles (HDVs) is a key component of smart and\\nconnected highways and is expected to bring remarkable fuel savings and\\nemission reduction. In this paper, we study the coordination of HDV platooning\\non a highway section. We model the arrival of HDVs as a Poisson process.\\nMultiple HDVs are merged into one platoon if their headways are below a given\\nthreshold. The merging is done by accelerating the following vehicles to catch\\nup with the leading ones. We characterize the following random variables: (i)\\nplatoon size, (ii) headway between platoons, and (iii) travel time increment\\ndue to platoon formation. We formulate and solve an optimization problem to\\ndetermine the headway threshold for platooning that leads to minimal cost (time\\nplus fuel). We also compare our results with that from Simulation of Urban\\nMObility (SUMO).\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.06822</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.06822</id><submitter>Jose Armando Oviedo</submitter><version version=\"v1\"><date>Fri, 15 Mar 2019 22:18:31 GMT</date><size>48kb</size></version><version version=\"v2\"><date>Wed, 24 Jul 2019 05:26:23 GMT</date><size>80kb</size></version><version version=\"v3\"><date>Tue, 8 Oct 2019 08:12:17 GMT</date><size>86kb</size></version><title>Fundamentals of Power Allocation Strategies for Downlink Multi-user NOMA\\n  with Target Rates</title><authors>Jose Armando Oviedo, Hamid R. Sadjadpour</authors><categories>cs.IT math.IT</categories><comments>This paper has been submitted to an IEEE journal for publication; 37\\n  pages, 6 figures</comments><msc-class>94A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For downlink multi-user non-orthogonal multiple access (NOMA) systems with\\nsuccessive interference cancellation (SIC) receivers, and a base-station not\\npossessing the instantaneous channel gains, the fundamental relationship\\nbetween the target rates and power allocation is investigated. It is proven\\nthat the total interference from signals not removed by SIC has a fundamental\\nupper-limit which is a function of the target rates, and the outage probability\\nis one when exceeding this limit. The concept of well-behaved power allocation\\nstrategies is defined, and its properties are proven to be derived solely based\\non the target rates. The existence of power allocation strategies that enable\\nNOMA to outperform OMA in per-user outage probability is proven, and are always\\nwell-behaved for the case when the outage probability performance of NOMA and\\nOMA are equal for all users. The proposed SIC decoding order is then shown to\\nthe most energy efficient. The derivation of well-behaved power allocation\\nstrategies that have improved outage probability performance over OMA for each\\nuser is outlined. Simulations validate the theoretical results, demonstrating\\nthat NOMA systems can always outperform OMA systems in outage probability\\nperformance, without relying on the exact channel gains.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.06836</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.06836</id><submitter>Lakshmanan Nataraj</submitter><version version=\"v1\"><date>Fri, 15 Mar 2019 23:24:08 GMT</date><size>1829kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 00:54:21 GMT</date><size>1829kb</size><source_type>D</source_type></version><title>Detecting GAN generated Fake Images using Co-occurrence Matrices</title><authors>Lakshmanan Nataraj, Tajuddin Manhar Mohammed, Shivkumar\\n  Chandrasekaran, Arjuna Flenner, Jawadul H. Bappy, Amit K. Roy-Chowdhury, B.\\n  S. Manjunath</authors><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advent of Generative Adversarial Networks (GANs) has brought about\\ncompletely novel ways of transforming and manipulating pixels in digital\\nimages. GAN based techniques such as Image-to-Image translations, DeepFakes,\\nand other automated methods have become increasingly popular in creating fake\\nimages. In this paper, we propose a novel approach to detect GAN generated fake\\nimages using a combination of co-occurrence matrices and deep learning. We\\nextract co-occurrence matrices on three color channels in the pixel domain and\\ntrain a model using a deep convolutional neural network (CNN) framework.\\nExperimental results on two diverse and challenging GAN datasets comprising\\nmore than 56,000 images based on unpaired image-to-image translations (cycleGAN\\n[1]) and facial attributes/expressions (StarGAN [2]) show that our approach is\\npromising and achieves more than 99% classification accuracy in both datasets.\\nFurther, our approach also generalizes well and achieves good results when\\ntrained on one dataset and tested on the other.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.06958</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.06958</id><submitter>Yang Wang</submitter><version version=\"v1\"><date>Sat, 16 Mar 2019 16:54:57 GMT</date><size>5429kb</size></version><title>Early-career setback and future career impact</title><authors>Yang Wang, Benjamin F. Jones, Dashun Wang</authors><categories>physics.soc-ph cs.DL</categories><doi>10.1038/s41467-019-12189-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Setbacks are an integral part of a scientific career, yet little is known\\nabout whether an early-career setback may augment or hamper an individual\\'s\\nfuture career impact. Here we examine junior scientists applying for U.S.\\nNational Institutes of Health (NIH) R01 grants. By focusing on grant proposals\\nthat fell just below and just above the funding threshold, we compare\\n&quot;near-miss&quot; with &quot;near-win&quot; individuals to examine longer-term career outcomes.\\nOur analyses reveal that an early-career near miss has powerful, opposing\\neffects. On one hand, it significantly increases attrition, with one near miss\\npredicting more than a 10% chance of disappearing permanently from the NIH\\nsystem. Yet, despite an early setback, individuals with near misses\\nsystematically outperformed those with near wins in the longer run, as their\\npublications in the next ten years garnered substantially higher impact. We\\nfurther find that this performance advantage seems to go beyond a screening\\nmechanism, whereby a more selected fraction of near-miss applicants remained\\nthan the near winners, suggesting that early-career setback appears to cause a\\nperformance improvement among those who persevere. Overall, the findings are\\nconsistent with the concept that &quot;what doesn\\'t kill me makes me stronger.&quot;\\nWhereas science is often viewed as a setting where early success begets future\\nsuccess, our findings unveil an intimate yet previously unknown relationship\\nwhere early-career setback can become a marker for future achievement, which\\nmay have broad implications for identifying, training and nurturing junior\\nscientists whose career will have lasting impact.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.06996</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.06996</id><submitter>Cong Xie</submitter><version version=\"v1\"><date>Sat, 16 Mar 2019 22:25:20 GMT</date><size>117kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 5 Apr 2019 16:53:40 GMT</date><size>269kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 1 Oct 2019 20:12:18 GMT</date><size>269kb</size><source_type>D</source_type></version><title>SLSGD: Secure and Efficient Distributed On-device Machine Learning</title><authors>Cong Xie, Sanmi Koyejo, Indranil Gupta</authors><categories>cs.LG cs.DC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider distributed on-device learning with limited communication and\\nsecurity requirements. We propose a new robust distributed optimization\\nalgorithm with efficient communication and attack tolerance. The proposed\\nalgorithm has provable convergence and robustness under non-IID settings.\\nEmpirical results show that the proposed algorithm stabilizes the convergence\\nand tolerates data poisoning on a small number of workers.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.07364</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.07364</id><submitter>Jihwan Moon</submitter><version version=\"v1\"><date>Mon, 18 Mar 2019 11:08:35 GMT</date><size>314kb</size></version><version version=\"v2\"><date>Tue, 19 Mar 2019 07:20:35 GMT</date><size>314kb</size></version><title>Online Reinforcement Learning of X-Haul Content Delivery Mode in Fog\\n  Radio Access Networks</title><authors>Jihwan Moon, Osvaldo Simeone, Seok-Hwan Park, Inkyu Lee</authors><categories>eess.SP cs.NI</categories><comments>12 pages, 2 figures</comments><doi>10.1109/LSP.2019.2932193</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a Fog Radio Access Network (F-RAN) with a Base Band Unit (BBU) in\\nthe cloud and multiple cache-enabled enhanced Remote Radio Heads (eRRHs). The\\nsystem aims at delivering contents on demand with minimal average latency from\\na time-varying library of popular contents. Information about uncached\\nrequested files can be transferred from the cloud to the eRRHs by following\\neither backhaul or fronthaul modes. The backhaul mode transfers fractions of\\nthe requested files, while the fronthaul mode transmits quantized baseband\\nsamples as in Cloud-RAN (C-RAN). The backhaul mode allows the caches of the\\neRRHs to be updated, which may lower future delivery latencies. In contrast,\\nthe fronthaul mode enables cooperative C-RAN transmissions that may reduce the\\ncurrent delivery latency. Taking into account the trade-off between current and\\nfuture delivery performance, this paper proposes an adaptive selection method\\nbetween the two delivery modes to minimize the long-term delivery latency.\\nAssuming an unknown and time-varying popularity model, the method is based on\\nmodel-free Reinforcement Learning (RL). Numerical results confirm the\\neffectiveness of the proposed RL scheme.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.07390</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.07390</id><submitter>Jorge Gonz\\\\\\'alez Ordiano</submitter><version version=\"v1\"><date>Mon, 18 Mar 2019 12:41:32 GMT</date><size>151kb</size></version><title>Probabilistic Energy Forecasting using Quantile Regressions based on a\\n  new Nearest Neighbors Quantile Filter</title><authors>Jorge \\\\\\'Angel Gonz\\\\\\'alez Ordiano (1), Lutz Gr\\\\&quot;oll (1), Ralf Mikut\\n  (1), Veit Hagenmeyer (1) ((1) Institute for Automation and Applied\\n  Informatics, Karlsruhe Institute of Technology)</authors><categories>cs.LG stat.ML</categories><comments>36 pages, 5 figures, 5 tables</comments><doi>10.1016/j.ijforecast.2019.06.003</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Parametric quantile regressions are a useful tool for creating probabilistic\\nenergy forecasts. Nonetheless, since classical quantile regressions are trained\\nusing a non-differentiable cost function, their creation using complex data\\nmining techniques (e.g., artificial neural networks) may be complicated. This\\narticle presents a method that uses a new nearest neighbors quantile filter to\\nobtain quantile regressions independently of the utilized data mining technique\\nand without the non-differentiable cost function. Thereafter, a validation of\\nthe presented method using the dataset of the Global Energy Forecasting\\nCompetition of 2014 is undertaken. The results show that the presented method\\nis able to solve the competition\\'s task with a similar accuracy and in a\\nsimilar time as the competition\\'s winner, but requiring a much less powerful\\ncomputer. This property may be relevant in an online forecasting service for\\nwhich the fast computation of probabilistic forecasts using not so powerful\\nmachines is required.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.07497</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.07497</id><submitter>Nguyen Huu Phong</submitter><version version=\"v1\"><date>Mon, 18 Mar 2019 15:12:13 GMT</date><size>1841kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 2 Apr 2019 07:09:02 GMT</date><size>1841kb</size><source_type>D</source_type></version><title>Advanced Capsule Networks via Context Awareness</title><authors>Nguyen Huu Phong and Bernardete Ribeiro</authors><categories>cs.LG stat.ML</categories><comments>12 pages</comments><doi>10.1007/978-3-030-30487-4_14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Capsule Networks (CN) offer new architectures for Deep Learning (DL)\\ncommunity. Though its effectiveness has been demonstrated in MNIST and\\nsmallNORB datasets, the networks still face challenges in other datasets for\\nimages with distinct contexts. In this research, we improve the design of CN\\n(Vector version) namely we expand more Pooling layers to filter image\\nbackgrounds and increase Reconstruction layers to make better image\\nrestoration. Additionally, we perform experiments to compare accuracy and speed\\nof CN versus DL models. In DL models, we utilize Inception V3 and DenseNet V201\\nfor powerful computers besides NASNet, MobileNet V1 and MobileNet V2 for small\\nand embedded devices. We evaluate our models on a fingerspelling alphabet\\ndataset from American Sign Language (ASL). The results show that CNs perform\\ncomparably to DL models while dramatically reducing training time. We also make\\na demonstration and give a link for the purpose of illustration.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.07832</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.07832</id><submitter>Zhe Chen</submitter><version version=\"v1\"><date>Tue, 19 Mar 2019 04:48:39 GMT</date><size>2089kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 16 Apr 2019 13:42:07 GMT</date><size>3464kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 14 May 2019 07:08:59 GMT</date><size>5816kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Tue, 8 Oct 2019 07:37:26 GMT</date><size>5817kb</size><source_type>D</source_type></version><title>Low-Rank Discriminative Least Squares Regression for Image\\n  Classification</title><authors>Zhe Chen, Xiao-Jun Wu and Josef Kittler</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latest least squares regression (LSR) methods mainly try to learn slack\\nregression targets to replace strict zero-one labels. However, the difference\\nof intra-class targets can also be highlighted when enlarging the distance\\nbetween different classes, and roughly persuing relaxed targets may lead to the\\nproblem of overfitting. To solve above problems, we propose a low-rank\\ndiscriminative least squares regression model (LRDLSR) for multi-class image\\nclassification. Specifically, LRDLSR class-wisely imposes low-rank constraint\\non the intra-class regression targets to encourage its compactness and\\nsimilarity. Moreover, LRDLSR introduces an additional regularization term on\\nthe learned targets to avoid the problem of overfitting. These two improvements\\nare helpful to learn a more discriminative projection for regression and thus\\nachieving better classification performance. Experimental results over a range\\nof image databases demonstrate the effectiveness of the proposed LRDLSR method.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.07833</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.07833</id><submitter>Zhe Chen</submitter><version version=\"v1\"><date>Tue, 19 Mar 2019 04:53:48 GMT</date><size>7323kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 09:21:00 GMT</date><size>8710kb</size><source_type>D</source_type></version><title>Fisher Discriminative Least Square Regression with Self-Adaptive\\n  Weighting for Face Recognition</title><authors>Zhe Chen, Xiao-Jun Wu, and Josef Kittler</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a supervised classification method, least square regression (LSR) has\\nshown promising performance in multiclass face recognition tasks. However, the\\nlatest LSR based classification methods mainly focus on learning a relaxed\\nregression target to replace traditional zero-one label matrix while ignoring\\nthe discriminability of transformed features. Based on the assumption that the\\ntransformed features of samples from the same class have similar structure\\nwhile those of samples from different classes are uncorrelated, in this paper\\nwe propose a novel discriminative LSR method based on the Fisher discrimination\\ncriterion (FDLSR), where the projected features have small within-class scatter\\nand large inter-class scatter simultaneously. Moreover, different from other\\nmethods, we explore relax regression from the view of transformed features\\nrather than the regression targets. Specifically, we impose a dynamic\\nnon-negative weight matrix on the transformed features to enlarge the margin\\nbetween the true and the false classes by self-adaptively assigning appropriate\\nweights to different features. Above two factors can encourage the learned\\ntransformation for regression to be more discriminative and thus achieving\\nbetter classification performance. Extensive experiments on various databases\\ndemonstrate that the proposed FDLSR method achieves superior performance to\\nother state-of-the-art LSR based classification methods.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.07836</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.07836</id><submitter>Zhe Chen</submitter><version version=\"v1\"><date>Tue, 19 Mar 2019 05:08:21 GMT</date><size>1940kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 09:23:04 GMT</date><size>2799kb</size><source_type>D</source_type></version><title>Non-negative representation based discriminative dictionary learning for\\n  face recognition</title><authors>Zhe Chen, Xiao-Jun Wu and Josef Kittler</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a non-negative representation based discriminative\\ndictionary learning algorithm (NRDL) for multicategory face classification. In\\ncontrast to traditional dictionary learning methods, NRDL investigates the use\\nof non-negative representation (NR), which contributes to learning\\ndiscriminative dictionary atoms. In order to make the learned dictionary more\\nsuitable for classification, NRDL seamlessly incorporates nonnegative\\nrepresentation constraint, discriminative dictionary learning and linear\\nclassifier training into a unified model. Specifically, NRDL introduces a\\npositive constraint on representation matrix to find distinct atoms from\\nheterogeneous training samples, which results in sparse and discriminative\\nrepresentation. Moreover, a discriminative dictionary encouraging function is\\nproposed to enhance the uniqueness of class-specific sub-dictionaries.\\nMeanwhile, an inter-class incoherence constraint and a compact graph based\\nregularization term are constructed to respectively improve the\\ndiscriminability of learned classifier. Experimental results on several\\nbenchmark face data sets verify the advantages of our NRDL algorithm over the\\nstate-of-the-art dictionary learning methods.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.08374</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.08374</id><submitter>Joackim Bernier</submitter><version version=\"v1\"><date>Wed, 20 Mar 2019 07:52:36 GMT</date><size>1199kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 9 Jul 2019 06:47:42 GMT</date><size>1199kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 30 Sep 2019 10:09:41 GMT</date><size>308kb</size><source_type>D</source_type></version><title>Long-time behavior of second order linearized Vlasov-Poisson equations\\n  near a homogeneous equilibrium</title><authors>Joackim Bernier (MINGUS, IRMAR), Michel Mehrenberger (I2M)</authors><categories>math.NA cs.NA math.AP</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The asymptotic behavior of the solutions of the second order linearized\\nVlasov-Poisson system around homogeneous equilibria is derived. It provides a\\nfine description of some nonlinear and multidimensional phenomena such as the\\nexistence of Best frequencies. Numerical results for the 1Dx1D and 2Dx2D\\nVlasov-Poisson system illustrate the effectiveness of this approach.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.08481</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.08481</id><submitter>Jonathan Williams</submitter><version version=\"v1\"><date>Wed, 20 Mar 2019 12:35:17 GMT</date><size>811kb</size><source_type>D</source_type></version><title>Three-dimensional Segmentation of Trees Through a Flexible Multi-Class\\n  Graph Cut Algorithm (MCGC)</title><authors>Jonathan Williams, Carola-Bibiane Sch\\\\&quot;onlieb, Tom Swinfield, Juheon\\n  Lee, Xiaohao Cai, Lan Qie, David A. Coomes</authors><categories>cs.CV cs.LG</categories><doi>10.1109/TGRS.2019.2940146</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developing a robust algorithm for automatic individual tree crown (ITC)\\ndetection from laser scanning datasets is important for tracking the responses\\nof trees to anthropogenic change. Such approaches allow the size, growth and\\nmortality of individual trees to be measured, enabling forest carbon stocks and\\ndynamics to be tracked and understood. Many algorithms exist for structurally\\nsimple forests including coniferous forests and plantations. Finding a robust\\nsolution for structurally complex, species-rich tropical forests remains a\\nchallenge; existing segmentation algorithms often perform less well than simple\\narea-based approaches when estimating plot-level biomass. Here we describe a\\nMulti-Class Graph Cut (MCGC) approach to tree crown delineation. This uses\\nlocal three-dimensional geometry and density information, alongside knowledge\\nof crown allometries, to segment individual tree crowns from LiDAR point\\nclouds. Our approach robustly identifies trees in the top and intermediate\\nlayers of the canopy, but cannot recognise small trees. From these\\nthree-dimensional crowns, we are able to measure individual tree biomass.\\nComparing these estimates to those from permanent inventory plots, our\\nalgorithm is able to produce robust estimates of hectare-scale carbon density,\\ndemonstrating the power of ITC approaches in monitoring forests. The\\nflexibility of our method to add additional dimensions of information, such as\\nspectral reflectance, make this approach an obvious avenue for future\\ndevelopment and extension to other sources of three-dimensional data, such as\\nstructure from motion datasets.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.08605</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.08605</id><submitter>Rui Gao</submitter><version version=\"v1\"><date>Wed, 20 Mar 2019 16:38:22 GMT</date><size>1078kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 27 Jun 2019 11:12:17 GMT</date><size>825kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 2 Aug 2019 16:50:05 GMT</date><size>1556kb</size><source_type>D</source_type></version><title>Iterated Extended Kalman Smoother-based Variable Splitting for\\n  $L_1$-Regularized State Estimation</title><authors>Rui Gao and Filip Tronarp and Simo S\\\\&quot;arkk\\\\&quot;a</authors><categories>cs.IT math.IT stat.ME</categories><comments>16 pages, 9 figures</comments><doi>10.1109/TSP.2019.2935868</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new framework for solving state estimation\\nproblems with an additional sparsity-promoting $L_1$-regularizer term. We first\\nformulate such problems as minimization of the sum of linear or nonlinear\\nquadratic error terms and an extra regularizer, and then present novel\\nalgorithms which solve the linear and nonlinear cases. The methods are based on\\na combination of the iterated extended Kalman smoother and variable splitting\\ntechniques such as alternating direction method of multipliers (ADMM). We\\npresent a general algorithmic framework for variable splitting methods, where\\nthe iterative steps involving minimization of the nonlinear quadratic terms can\\nbe computed efficiently by iterated smoothing. Due to the use of state\\nestimation algorithms, the proposed framework has a low per-iteration time\\ncomplexity, which makes it suitable for solving a large-scale or\\nhigh-dimensional state estimation problem. We also provide convergence results\\nfor the proposed algorithms. The experiments show the promising performance and\\nspeed-ups provided by the methods.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.08732</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.08732</id><submitter>Massimiliano Di Ventra</submitter><version version=\"v1\"><date>Mon, 18 Mar 2019 20:28:02 GMT</date><size>320kb</size><source_type>D</source_type></version><title>Digital Memcomputing: from Logic to Dynamics to Topology</title><authors>Massimiliano Di Ventra and Igor V. Ovchinnikov</authors><categories>cs.ET cond-mat.mes-hall quant-ph</categories><doi>10.1016/j.aop.2019.167935</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital memcomputing machines (DMMs) are a class of computational machines\\ndesigned to solve combinatorial optimization problems. A practical realization\\nof DMMs can be accomplished via electrical circuits of highly non-linear,\\npoint-dissipative dynamical systems engineered so that periodic orbits and\\nchaos can be avoided. A given logic problem is first mapped into this type of\\ndynamical system whose point attractors represent the solutions of the original\\nproblem. A DMM then finds the solution via a succession of elementary\\ninstantons whose role is to eliminate solitonic configurations of logical\\ninconsistency (&quot;logical defects&quot;) from the circuit. By employing a\\nsupersymmetric theory of dynamics, a DMM can be described by a cohomological\\nfield theory that allows for computation of certain topological matrix elements\\non instantons that have the mathematical meaning of intersection numbers on\\ninstantons. We discuss the &quot;dynamical&quot; meaning of these matrix elements, and\\nargue that the number of elementary instantons needed to reach the solution\\ncannot exceed the number of state variables of DMMs, which in turn can only\\ngrow at most polynomially with the size of the problem. These results shed\\nfurther light on the relation between logic, dynamics and topology in digital\\nmemcomputing.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.08784</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.08784</id><submitter>Sangjae Bae</submitter><version version=\"v1\"><date>Thu, 21 Mar 2019 00:31:50 GMT</date><size>1103kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 25 Mar 2019 09:13:10 GMT</date><size>1103kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sun, 29 Sep 2019 06:34:15 GMT</date><size>3365kb</size><source_type>D</source_type></version><title>Real-time Ecological Velocity Planning for Plug-in Hybrid Vehicles with\\n  Partial Communication to Traffic Lights</title><authors>Sangjae Bae, Yongkeun Choi, Yeojun Kim, Jacopo Guanetti, Francesco\\n  Borrelli, and Scott Moura</authors><categories>cs.SY</categories><comments>To appear at 2019 IEEE Conference on Decision and Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the design of an ecological adaptive cruise controller\\n(ECO-ACC) for a plug-in hybrid vehicle (PHEV) which exploits automated driving\\nand connectivity. Most existing papers for ECO-ACC focus on a short-sighted\\ncontrol scheme. A two-level control framework for long-sighted ECO-ACC was only\\nrecently introduced. However, that work is based on a deterministic traffic\\nsignal phase and timing (SPaT) over the entire route. In practice, connectivity\\nwith traffic lights may be limited by communication range, e.g. just one\\nupcoming traffic light. We propose a two-level receding-horizon control\\nframework for long-sighted ECO-ACC that exploits deterministic SPaT for the\\nupcoming traffic light, and utilizes historical SPaT for other traffic lights\\nwithin a receding control horizon. We also incorporate a powertrain control\\nmechanism to enhance PHEV energy prediction accuracy. Hardware-in-the-loop\\nsimulation results validate the energy savings of the receding-horizon control\\nframework in various traffic scenarios.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.08901</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.08901</id><submitter>Anton Martinsson</submitter><version version=\"v1\"><date>Thu, 21 Mar 2019 09:57:30 GMT</date><size>3225kb</size><source_type>D</source_type></version><title>Transferability of Operational Status Classification Models Among\\n  Different Wind Turbine Typesq</title><authors>Z. Trstanova, A. Martinsson, C. Matthews, S. Jimenez, B. Leimkuhler,\\n  T. Van Delft, M. Wilkinson</authors><categories>stat.ML cs.LG</categories><comments>9 pages</comments><doi>10.1088/1742-6596/1222/1/012041</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A detailed understanding of wind turbine performance status classification\\ncan improve operations and maintenance in the wind energy industry. Due to\\ndifferent engineering properties of wind turbines, the standard supervised\\nlearning models used for classification do not generalize across data sets\\nobtained from different wind sites. We propose two methods to deal with the\\ntransferability of the trained models: first, data normalization in the form of\\npower curve alignment, and second, a robust method based on convolutional\\nneural networks and feature-space extension. We demonstrate the success of our\\nmethods on real-world data sets with industrial applications.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.09029</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.09029</id><submitter>Leo Duan</submitter><version version=\"v1\"><date>Thu, 21 Mar 2019 14:37:17 GMT</date><size>6268kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 21:21:33 GMT</date><size>7459kb</size><source_type>D</source_type></version><title>Latent Simplex Position Model: High Dimensional Multi-view Clustering\\n  with Uncertainty Quantification</title><authors>Leo L Duan</authors><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High dimensional data often contain multiple facets, and several clustering\\npatterns can co-exist under different variable subspaces, also known as the\\nviews. While multi-view clustering algorithms were proposed, the uncertainty\\nquantification remains difficult --- a particular challenge is in the high\\ncomplexity of estimating the cluster assignment probability under each view,\\nand sharing information among views. In this article, we propose an approximate\\nBayes approach --- treating the similarity matrices generated over the views as\\nrough first-stage estimates for the co-assignment probabilities; in its\\nKullback-Leibler neighborhood, we obtain a refined low-rank matrix, formed by\\nthe pairwise product of simplex coordinates. Interestingly, each simplex\\ncoordinate directly encodes the cluster assignment uncertainty. For multi-view\\nclustering, we let each view draw a parameterization from a few candidates,\\nleading to dimension reduction. With high model flexibility, the estimation can\\nbe efficiently carried out as a continuous optimization problem, hence enjoys\\ngradient-based computation. The theory establishes the connection of this model\\nto a random partition distribution under multiple views. Compared to\\nsingle-view clustering approaches, substantially more interpretable results are\\nobtained when clustering brains from a human traumatic brain injury study,\\nusing high-dimensional gene expression data.\\n  KEY WORDS: Co-regularized Clustering, Consensus, PAC-Bayes, Random Cluster\\nGraph, Variable Selection\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.09177</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.09177</id><submitter>Matthew Fickus</submitter><version version=\"v1\"><date>Thu, 21 Mar 2019 18:24:34 GMT</date><size>39kb</size></version><version version=\"v2\"><date>Fri, 4 Oct 2019 13:06:38 GMT</date><size>39kb</size></version><title>Harmonic equiangular tight frames comprised of regular simplices</title><authors>Matthew Fickus and Courtney A. Schmitt</authors><categories>math.FA cs.IT math.CO math.IT</categories><msc-class>42C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An equiangular tight frame (ETF) is a sequence of unit-norm vectors in a\\nEuclidean space whose coherence achieves equality in the Welch bound, and thus\\nyields an optimal packing in a projective space. A regular simplex is a simple\\ntype of ETF in which the number of vectors is one more than the dimension of\\nthe underlying space. More sophisticated examples include harmonic ETFs which\\nequate to difference sets in finite abelian groups. Recently, it was shown that\\nsome harmonic ETFs are comprised of regular simplices. In this paper, we\\ncontinue the investigation into these special harmonic ETFs. We begin by\\ncharacterizing when the subspaces that are spanned by the ETF\\'s regular\\nsimplices form an equi-isoclinic tight fusion frame (EITFF), which is a type of\\noptimal packing in a Grassmannian space. We shall see that every difference set\\nthat produces an EITFF in this way also yields a complex circulant conference\\nmatrix. Next, we consider a subclass of these difference sets that can be\\nfactored in terms of a smaller difference set and a relative difference set. It\\nturns out that these relative difference sets lend themselves to a second,\\nrelated and yet distinct, construction of complex circulant conference\\nmatrices. Finally, we provide explicit infinite families of ETFs to which this\\ntheory applies.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.09215</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.09215</id><submitter>Adam Oberman</submitter><version version=\"v1\"><date>Thu, 21 Mar 2019 19:48:45 GMT</date><size>467kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 16:37:47 GMT</date><size>6270kb</size><source_type>D</source_type></version><title>Empirical confidence estimates for classification by deep neural\\n  networks</title><authors>Chris Finlay and Adam M. Oberman</authors><categories>stat.ML cs.LG</categories><comments>16 pages, 6 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How well can we estimate the probability that the classification predicted by\\na deep neural network is correct (or in the Top 5)? It is well-known that the\\nsoftmax values of the network are not estimates of the probabilities of class\\nlabels. However, there is a misconception that these values are not\\ninformative. We define the notion of \\\\emph{implied loss} and prove that if an\\nuncertainty measure is an implied loss, then low uncertainty means high\\nprobability of correct (or top $k$) classification on the test set. We\\ndemonstrate empirically that these values can be used to measure the confidence\\nthat the classification is correct. Our method is simple to use on existing\\nnetworks: we proposed confidence measures for Top $k$ which can be evaluated by\\nbinning values on the test set.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.09395</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.09395</id><submitter>Kurt Izak Cabanilla</submitter><version version=\"v1\"><date>Fri, 22 Mar 2019 08:15:35 GMT</date><size>1754kb</size></version><version version=\"v2\"><date>Wed, 24 Apr 2019 07:45:26 GMT</date><size>1854kb</size></version><version version=\"v3\"><date>Mon, 7 Oct 2019 07:23:35 GMT</date><size>2142kb</size></version><title>Forecasting, Causality, and Impulse Response with Neural Vector\\n  Autoregressions</title><authors>Kurt Izak Cabanilla and Kevin Thomas Go</authors><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Incorporating nonlinearity is paramount to predicting the future states of a\\ndynamical system, its response to shocks, and its underlying causal network.\\nHowever, most existing methods for causality detection and impulse response,\\nsuch as Vector Autoregression (VAR), assume linearity and are thus unable to\\ncapture the complexity. Here, we introduce a vector autoencoder nonlinear\\nautoregression neural network (VANAR) capable of both automatic time series\\nfeature extraction for its inputs and functional form estimation. We evaluate\\nVANAR in three ways: first in terms of pure forecast accuracy, second in terms\\nof detecting the correct causality between variables, and lastly in terms of\\nimpulse response where we model trajectories given external shocks. These tests\\nwere performed on a simulated nonlinear chaotic system and an empirical system\\nusing Philippine macroeconomic data. Results show that VANAR significantly\\noutperforms VAR in the forecast and causality tests. VANAR has consistently\\nsuperior accuracy even over state of the art models such as SARIMA and TBATS.\\nFor the impulse response test, both models fail to predict the shocked\\ntrajectories of the nonlinear chaotic system. VANAR was robust in its ability\\nto model a wide variety of dynamics, from chaotic, high noise, and low data\\nenvironments to macroeconomic systems.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.09760</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.09760</id><submitter>Jaejun Yoo</submitter><version version=\"v1\"><date>Sat, 23 Mar 2019 05:02:00 GMT</date><size>4490kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 08:49:37 GMT</date><size>13032kb</size><source_type>D</source_type></version><title>Photorealistic Style Transfer via Wavelet Transforms</title><authors>Jaejun Yoo, Youngjung Uh, Sanghyuk Chun, Byeongkyu Kang, Jung-Woo Ha</authors><categories>cs.CV</categories><comments>Accepted to ICCV 2019, Code and data: https://github.com/ClovaAI/WCT2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent style transfer models have provided promising artistic results.\\nHowever, given a photograph as a reference style, existing methods are limited\\nby spatial distortions or unrealistic artifacts, which should not happen in\\nreal photographs. We introduce a theoretically sound correction to the network\\narchitecture that remarkably enhances photorealism and faithfully transfers the\\nstyle. The key ingredient of our method is wavelet transforms that naturally\\nfits in deep networks. We propose a wavelet corrected transfer based on\\nwhitening and coloring transforms (WCT$^2$) that allows features to preserve\\ntheir structural information and statistical properties of VGG feature space\\nduring stylization. This is the first and the only end-to-end model that can\\nstylize a $1024\\\\times1024$ resolution image in 4.7 seconds, giving a pleasing\\nand photorealistic quality without any post-processing. Last but not least, our\\nmodel provides a stable video stylization without temporal constraints. Our\\ncode, generated images, and pre-trained models are all available at\\nhttps://github.com/ClovaAI/WCT2.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.09954</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.09954</id><submitter>Cong Ling</submitter><version version=\"v1\"><date>Sun, 24 Mar 2019 09:35:41 GMT</date><size>598kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 13:36:24 GMT</date><size>535kb</size><source_type>D</source_type></version><title>Semantically Secure Lattice Codes for Compound MIMO Channels</title><authors>Antonio Campello, Cong Ling and Jean-Claude Belfiore</authors><categories>cs.IT math.IT</categories><comments>IEEE Trans. Information Theory, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider compound multi-input multi-output (MIMO) wiretap channels where\\nminimal channel state information at the transmitter (CSIT) is assumed. Code\\nconstruction is given for the special case of isotropic mutual information,\\nwhich serves as a conservative strategy for general cases. Using the flatness\\nfactor for MIMO channels, we propose lattice codes universally achieving the\\nsecrecy capacity of compound MIMO wiretap channels up to a constant gap\\n(measured in nats) that is equal to the number of transmit antennas. The\\nproposed approach improves upon existing works on secrecy coding for MIMO\\nwiretap channels from an error probability perspective, and establishes\\ninformation theoretic security (in fact semantic security). We also give an\\nalgebraic construction to reduce the code design complexity, as well as the\\ndecoding complexity of the legitimate receiver. Thanks to the algebraic\\nstructures of number fields and division algebras, our code construction for\\ncompound MIMO wiretap channels can be reduced to that for Gaussian wiretap\\nchannels, up to some additional gap to secrecy capacity.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.10081</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.10081</id><submitter>Ortho Flint</submitter><version version=\"v1\"><date>Sun, 24 Mar 2019 23:38:27 GMT</date><size>384kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 26 Mar 2019 19:11:40 GMT</date><size>384kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 28 Mar 2019 15:34:37 GMT</date><size>384kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Thu, 18 Apr 2019 02:52:42 GMT</date><size>384kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Sun, 28 Apr 2019 13:28:25 GMT</date><size>384kb</size><source_type>D</source_type></version><version version=\"v6\"><date>Thu, 6 Jun 2019 14:07:02 GMT</date><size>379kb</size><source_type>D</source_type></version><version version=\"v7\"><date>Mon, 8 Jul 2019 12:33:07 GMT</date><size>379kb</size><source_type>D</source_type></version><version version=\"v8\"><date>Sat, 5 Oct 2019 16:47:14 GMT</date><size>383kb</size><source_type>D</source_type></version><title>Determining satisfiability of 3-SAT in polynomial time</title><authors>Ortho Flint, Asanka Wickramasinghe, Jason Brasse, Christopher Fowler</authors><categories>cs.DS</categories><comments>Versions 1-7 are missing cases in the theorem. Version 8 is complete</comments><msc-class>68R05</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we provide a polynomial time (and space), algorithm that\\ndetermines satisfiability of 3-SAT. The complexity analysis for the algorithm\\ntakes into account no efficiency and yet provides a low enough bound, that\\nefficient versions are practical with respect to today\\'s hardware. We accompany\\nthis paper with a serial version of the algorithm without non-trivial\\nefficiencies (link: polynomial3sat.org).\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.10559</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.10559</id><submitter>Luis A. Pineda</submitter><version version=\"v1\"><date>Mon, 25 Mar 2019 19:25:16 GMT</date><size>620kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 5 Oct 2019 17:20:01 GMT</date><size>702kb</size><source_type>D</source_type></version><title>The Mode of Computing</title><authors>Luis A. Pineda</authors><categories>cs.AI</categories><comments>35 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Turing Machine is the paradigmatic case of computing machines, but there\\nare others, such as Artificial Neural Networks, Table Computing,\\nRelational-Indeterminate Computing and diverse forms of analogical computing,\\neach of which based on a particular underlying intuition of the phenomenon of\\ncomputing. This variety can be captured in terms of system levels,\\nre-interpreting and generalizing Newell\\'s hierarchy, which includes the\\nknowledge level at the top and the symbol level immediately below it. In this\\nre-interpretation the knowledge level consists of human knowledge and the\\nsymbol level is generalized into a new level that here is called The Mode of\\nComputing. Natural computing performed by the brains of humans and non-human\\nanimals with a developed enough neural system should be understood in terms of\\na hierarchy of system levels too. By analogy from standard computing machinery\\nthere must be a system level above the neural circuitry levels and directly\\nbelow the knowledge level that is named here The mode of Natural Computing. A\\ncentral question for Cognition is the characterization of this mode. The Mode\\nof Computing provides a novel perspective on the phenomena of computing,\\ninterpreting, the representational and non-representational views of cognition,\\nand consciousness.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.10782</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.10782</id><submitter>Dinh-Cuong Hoang</submitter><version version=\"v1\"><date>Tue, 26 Mar 2019 10:42:09 GMT</date><size>5121kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 19 Aug 2019 16:43:39 GMT</date><size>5880kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 30 Sep 2019 17:30:30 GMT</date><size>5880kb</size><source_type>D</source_type></version><title>High-quality Instance-aware Semantic 3D Map Using RGB-D Camera</title><authors>Dinh-Cuong Hoang, Todor Stoyanov, Achim J. Lilienthal</authors><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a mapping system capable of constructing detailed instance-level\\nsemantic models of room-sized indoor environments by means of an RGB-D camera.\\nIn this work, we integrate deep-learning-based instance segmentation and\\nclassification into a state of the art RGB-D SLAM system. We leverage the\\npipeline of ElasticFusion [1] as a backbone and propose modifications of the\\nregistration cost function to make full use of the instance class labels in the\\nprocess. The proposed objective function features tunable weights for the\\ndepth, appearance, and semantic information channels, which can be learned from\\ndata. The resulting system is capable of producing accurate semantic maps of\\nroom-sized environments, as well as reconstructing highly detailed object-level\\nmodels. The developed method has been verified through experimental validation\\non the TUM RGB-D SLAM benchmark and the YCB video dataset. Our results\\nconfirmed that the proposed system performs favorably in terms of trajectory\\nestimation, surface reconstruction, and segmentation quality in comparison to\\nother state-of-the-art systems.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.11059</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.11059</id><submitter>Linnan Wang</submitter><version version=\"v1\"><date>Tue, 26 Mar 2019 04:54:53 GMT</date><size>7838kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 02:04:45 GMT</date><size>7590kb</size><source_type>D</source_type></version><title>AlphaX: eXploring Neural Architectures with Deep Neural Networks and\\n  Monte Carlo Tree Search</title><authors>Linnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian, Rodrigo Fonseca</authors><categories>cs.CV</categories><comments>another search algorithm for NAS. arXiv admin note: substantial text\\n  overlap with arXiv:1805.07440</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural Architecture Search (NAS) has shown great success in automating the\\ndesign of neural networks, but the prohibitive amount of computations behind\\ncurrent NAS methods requires further investigations in improving the sample\\nefficiency and the network evaluation cost to get better results in a shorter\\ntime. In this paper, we present a novel scalable Monte Carlo Tree Search (MCTS)\\nbased NAS agent, named AlphaX, to tackle these two aspects. AlphaX improves the\\nsearch efficiency by adaptively balancing the exploration and exploitation at\\nthe state level, and by a Meta-Deep Neural Network (DNN) to predict network\\naccuracies for biasing the search toward a promising region. To amortize the\\nnetwork evaluation cost, AlphaX accelerates MCTS rollouts with a distributed\\ndesign and reduces the number of epochs in evaluating a network by transfer\\nlearning guided with the tree structure in MCTS. In 12 GPU days and 1000\\nsamples, AlphaX found an architecture that reaches 97.84\\\\% top-1 accuracy on\\nCIFAR-10, and 75.5\\\\% top-1 accuracy on ImageNet, exceeding SOTA NAS methods in\\nboth the accuracy and sampling efficiency. Particularly, we also evaluate\\nAlphaX on NASBench-101, a large scale NAS dataset; AlphaX is 3x and 2.8x more\\nsample efficient than Random Search and Regularized Evolution in finding the\\nglobal optimum. Finally, we show the searched architecture improves a variety\\nof vision applications from Neural Style Transfer, to Image Captioning and\\nObject Detection.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1903.11452</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1903.11452</id><submitter>Antonio Scala PhD</submitter><version version=\"v1\"><date>Wed, 27 Mar 2019 14:28:17 GMT</date><size>502kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 27 Sep 2019 22:13:29 GMT</date><size>2029kb</size><source_type>D</source_type></version><title>Lexical convergence and collective identities within Facebook echo\\n  chambers</title><authors>Emanuele Brugnoli, Matteo Cinelli, Fabiana Zollo, Walter\\n  Quattrociocchi, Antonio Scala</authors><categories>cs.SI physics.soc-ph</categories><comments>22 pages, 11 figures</comments><msc-class>91F20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies, targeting Facebook, showed the tendency of users to interact\\nwith information adhering to their preferred narrative and to ignore dissenting\\ninformation. Primarily driven by confirmation bias, users tend to join\\nhomogeneous and polarized clusters (echo chambers) where they cooperate to\\nframe and reinforce a like-minded system of beliefs, thus facilitating\\nmisinformation cascades. To gain a deeper understanding of these phenomena, in\\nthis work we analyze the lexicons used by the communities of users emerging on\\nFacebook around two very conflicting narratives: science and conspiracy. We\\nshow how the words exhibiting a significant differentiation in frequency from\\none community to another, provide important insights about the kind of\\ninformation processed by the two groups of users and about the overall\\nsentiment expressed in their comments. Furthermore, by focusing on comment\\nthreads, a context of interaction mediated by the posts, we observe a strong\\npositive correlation between the lexical convergence of co-commenters and their\\nnumber of interactions, both in the case of co-commenters polarized towards the\\nsame content as well as in the case of co-commenters with opposing\\npolarization. Nevertheless, the analysis of how lexical convergence evolves\\nthrough time suggests that such a trend is a proxy of the emergence of\\ncollective identities in the case of co-commenters polarized toward the same\\ncontent, whereas during cross interactions it is more likely due to the need\\nfor a common vocabulary to achieve communication than to real opinion\\nconvergence. Nonetheless, the fact that even users with opposing views try to\\ncoordinate their lexical choices when joining a discussion, suggests that a\\ndialogue between competing parties is possible and indeed it should be\\nstimulated in an attempt to smooth polarization and to reduce both the risk and\\nthe consequences of misinformation.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.00014</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.00014</id><submitter>Daniel Muthukrishna</submitter><version version=\"v1\"><date>Fri, 29 Mar 2019 18:00:00 GMT</date><size>3329kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 16:14:46 GMT</date><size>3381kb</size><source_type>D</source_type></version><title>RAPID: Early Classification of Explosive Transients using Deep Learning</title><authors>Daniel Muthukrishna, Gautham Narayan, Kaisey S. Mandel, Rahul Biswas,\\n  Ren\\\\\\'ee Hlo\\\\v{z}ek</authors><categories>astro-ph.IM astro-ph.HE cs.LG stat.ML</categories><comments>Accepted version. 28 pages, 16 figures, 2 tables, PASP Special Issue\\n  on Methods for Time-Domain Astrophysics. Submitted: 13 December 2018,\\n  Accepted: 26 March 2019</comments><journal-ref>PASP 131, 118002 (2019)</journal-ref><doi>10.1088/1538-3873/ab1609</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present RAPID (Real-time Automated Photometric IDentification), a novel\\ntime-series classification tool capable of automatically identifying transients\\nfrom within a day of the initial alert, to the full lifetime of a light curve.\\nUsing a deep recurrent neural network with Gated Recurrent Units (GRUs), we\\npresent the first method specifically designed to provide early classifications\\nof astronomical time-series data, typing 12 different transient classes. Our\\nclassifier can process light curves with any phase coverage, and it does not\\nrely on deriving computationally expensive features from the data, making RAPID\\nwell-suited for processing the millions of alerts that ongoing and upcoming\\nwide-field surveys such as the Zwicky Transient Facility (ZTF), and the Large\\nSynoptic Survey Telescope (LSST) will produce. The classification accuracy\\nimproves over the lifetime of the transient as more photometric data becomes\\navailable, and across the 12 transient classes, we obtain an average area under\\nthe receiver operating characteristic curve of 0.95 and 0.98 at early and late\\nepochs, respectively. We demonstrate RAPID\\'s ability to effectively provide\\nearly classifications of observed transients from the ZTF data stream. We have\\nmade RAPID available as an open-source software package\\n(https://astrorapid.readthedocs.io) for machine learning-based alert-brokers to\\nuse for the autonomous and quick classification of several thousand light\\ncurves within a few seconds.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.00055</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.00055</id><submitter>Ivo Trowitzsch</submitter><version version=\"v1\"><date>Fri, 29 Mar 2019 19:04:59 GMT</date><size>1772kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 12:10:23 GMT</date><size>2102kb</size><source_type>D</source_type></version><title>Joining Sound Event Detection and Localization Through Spatial\\n  Segregation</title><authors>Ivo Trowitzsch, Christopher Schymura, Dorothea Kolossa, Klaus\\n  Obermayer</authors><categories>cs.SD eess.AS</categories><comments>Revision after feedback of reviewers; submitted for publication to\\n  IEEE/ACM Transactions on Audio, Speech, and Language Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identification and localization of sounds are both integral parts of\\ncomputational auditory scene analysis. Although each can be solved separately,\\nthe goal of forming coherent auditory objects and achieving a comprehensive\\nspatial scene understanding suggests pursuing a joint solution of the two\\nproblems. This work presents an approach that robustly binds localization with\\nthe detection of sound events in a binaural robotic system. Both tasks are\\njoined through the use of spatial stream segregation which produces\\nprobabilistic time-frequency masks for individual sources attributable to\\nseparate locations, enabling segregated sound event detection operating on\\nthese streams. We use simulations of a comprehensive suite of test scenes with\\nmultiple co-occurring sound sources, and propose performance measures for\\nsystematic investigation of the impact of scene complexity on this segregated\\ndetection of sound types. Analyzing the effect of spatial scene arrangement, we\\nshow how a robot could facilitate high performance through optimal head\\nrotation. Furthermore, we investigate the performance of segregated detection\\ngiven possible localization error as well as error in the estimation of number\\nof active sources. Our analysis demonstrates that the proposed approach is an\\neffective method to obtain joint sound event location and type information\\nunder a wide range of conditions.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.00708</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.00708</id><submitter>Kolja Thormann</submitter><version version=\"v1\"><date>Mon, 1 Apr 2019 11:52:27 GMT</date><size>144kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 8 Jul 2019 13:13:22 GMT</date><size>160kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 8 Oct 2019 13:03:32 GMT</date><size>160kb</size><source_type>D</source_type></version><title>Optimal Fusion of Elliptic Extended Target Estimates based on the\\n  Wasserstein Distance</title><authors>Kolja Thormann and Marcus Baum</authors><categories>eess.SP cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the fusion of multiple estimates of a spatially extended\\nobject, where the object extent is modeled as an ellipse parameterized by the\\norientation and semiaxes lengths. For this purpose, we propose a novel\\nsystematic approach that employs a distance measure for ellipses, i.e., the\\nGaussian Wasserstein distance, as a cost function. We derive an explicit\\napproximate expression for the Minimum Mean Gaussian Wasserstein distance\\n(MMGW) estimate. Based on the concept of a MMGW estimator, we develop efficient\\nmethods for the fusion of extended target estimates. The proposed fusion\\nmethods are evaluated in a simulated experiment and the benefits of the novel\\nmethods are discussed.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.00865</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.00865</id><submitter>Benjamin Guedj</submitter><version version=\"v1\"><date>Mon, 1 Apr 2019 14:10:21 GMT</date><size>2065kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 18:54:55 GMT</date><size>1638kb</size><source_type>D</source_type></version><title>Non-linear aggregation of filters to improve image denoising</title><authors>Benjamin Guedj and Juliette Rengot</authors><categories>stat.ML cs.CV cs.LG eess.IV</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We introduce a novel aggregation method to efficiently perform image\\ndenoising. Preliminary filters are aggregated in a non-linear fashion, using a\\nnew metric of pixel proximity based on how the pool of filters reaches a\\nconsensus. We provide a theoretical bound to support our aggregation scheme,\\nits numerical performance is illustrated and we show that the aggregate\\nsignificantly outperforms each of the preliminary filters.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.01326</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.01326</id><submitter>Thu Nguyen-Phuoc</submitter><version version=\"v1\"><date>Tue, 2 Apr 2019 10:36:01 GMT</date><size>4497kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 10:41:28 GMT</date><size>3832kb</size><source_type>D</source_type></version><title>HoloGAN: Unsupervised learning of 3D representations from natural images</title><authors>Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt,\\n  Yong-Liang Yang</authors><categories>cs.CV</categories><comments>International Conference on Computer Vision ICCV 2019. For project\\n  page, see\\n  https://www.monkeyoverflow.com/#/hologan-unsupervised-learning-of-3d-representations-from-natural-images/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel generative adversarial network (GAN) for the task of\\nunsupervised learning of 3D representations from natural images. Most\\ngenerative models rely on 2D kernels to generate images and make few\\nassumptions about the 3D world. These models therefore tend to create blurry\\nimages or artefacts in tasks that require a strong 3D understanding, such as\\nnovel-view synthesis. HoloGAN instead learns a 3D representation of the world,\\nand to render this representation in a realistic manner. Unlike other GANs,\\nHoloGAN provides explicit control over the pose of generated objects through\\nrigid-body transformations of the learnt 3D features. Our experiments show that\\nusing explicit 3D features enables HoloGAN to disentangle 3D pose and identity,\\nwhich is further decomposed into shape and appearance, while still being able\\nto generate images with similar or higher visual quality than other generative\\nmodels. HoloGAN can be trained end-to-end from unlabelled 2D images only.\\nParticularly, we do not require pose labels, 3D shapes, or multiple views of\\nthe same objects. This shows that HoloGAN is the first generative model that\\nlearns 3D representations from natural images in an entirely unsupervised\\nmanner.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.01392</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.01392</id><submitter>Fuad A. Ghaleb Dr</submitter><version version=\"v1\"><date>Tue, 2 Apr 2019 13:11:14 GMT</date><size>2213kb</size></version><title>Context-Aware Misbehavior Detection Scheme for Vehicular Ad Hoc Networks\\n  using Sequential Analysis of the Temporal and Spatial Correlation of the\\n  Cooperative Awareness Messages</title><authors>Fuad A. Ghaleb</authors><categories>cs.CY</categories><doi>10.1016/j.vehcom.2019.100186</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular ad hoc Networks (VANETs) are emerged mainly to improve road safety,\\ntraffic efficiency, and passenger comfort. The performance of most VANET\\napplications relies on the availability of accurate and recent\\nmobility-information, shared by neighboring vehicles. However, sharing false\\nmobility information can disrupt any potential VANET application. Misbehavior\\ndetection is an important security component. However, existing misbehavior\\ndetection solutions lack considering the high dynamicity of vehicular context\\nwhich leads to low detection accuracy and high false alarms. The use of\\npredefined and static security thresholds are the main drawbacks of the\\nexisting solutions. In this paper, a context-aware misbehavior detection scheme\\n(CAMDS) is proposed using sequential analysis of temporal and spatial\\nproperties of mobility information. A dynamic context reference is constructed\\nonline and timely updated using statistical techniques. Firstly, the Kalman\\nfilter algorithm is used to track the mobility information received from\\nneighboring vehicles. Then, the innovation errors of the Kalman filter are\\nutilized to construct a temporal consistency assessment model for each vehicle\\nusing Box-plot. Then, the Hampel filter is used to construct a spatial\\nconsistency assessment model that represents the context reference model.\\nSimilarly, plausibility assessment reference models are built online and timely\\nupdated using the Hampel filter and by utilizing the consistency assessment\\nreference model of neighboring information. Finally, a message is classified as\\nsuspicious if its consistency and plausibility scores deviate much from the\\ncontext reference model. The proposed context-aware scheme achieved a 73%\\nreduction in false Alarm rate while it achieves a 37% improvement in the\\ndetection rate. This proves the effectiveness of the proposed scheme compared\\nwith the existing static solutions.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.01608</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.01608</id><submitter>Arman Cohan</submitter><version version=\"v1\"><date>Tue, 2 Apr 2019 18:22:09 GMT</date><size>133kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 16:37:20 GMT</date><size>133kb</size><source_type>D</source_type></version><title>Structural Scaffolds for Citation Intent Classification in Scientific\\n  Publications</title><authors>Arman Cohan, Waleed Ammar, Madeleine van Zuylen, and Field Cady</authors><categories>cs.CL</categories><comments>NAACL 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying the intent of a citation in scientific papers (e.g., background\\ninformation, use of methods, comparing results) is critical for machine reading\\nof individual publications and automated analysis of the scientific literature.\\nWe propose structural scaffolds, a multitask model to incorporate structural\\ninformation of scientific papers into citations for effective classification of\\ncitation intents. Our model achieves a new state-of-the-art on an existing ACL\\nanthology dataset (ACL-ARC) with a 13.3% absolute increase in F1 score, without\\nrelying on external linguistic resources or hand-engineered features as done in\\nexisting methods. In addition, we introduce a new dataset of citation intents\\n(SciCite) which is more than five times larger and covers multiple scientific\\ndomains compared with existing datasets. Our code and data are available at:\\nhttps://github.com/allenai/scicite.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.01821</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.01821</id><submitter>Kyung-Su Kim</submitter><version version=\"v1\"><date>Wed, 3 Apr 2019 07:55:22 GMT</date><size>56kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 4 Jun 2019 08:27:44 GMT</date><size>56kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 13 Aug 2019 04:52:40 GMT</date><size>66kb</size><source_type>D</source_type></version><title>Fourier Phase Retrieval with Extended Support Estimation via Deep Neural\\n  Network</title><authors>Kyung-Su Kim, Sae-Young Chung</authors><categories>stat.ML cs.LG</categories><doi>10.1109/LSP.2019.2935814</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of sparse phase retrieval from Fourier transform\\nmagnitudes to recover the $k$-sparse signal vector and its support\\n$\\\\mathcal{T}$. We exploit extended support estimate $\\\\mathcal{E}$ with size\\nlarger than $k$ satisfying $\\\\mathcal{E} \\\\supseteq \\\\mathcal{T}$ and obtained by\\na trained deep neural network (DNN). To make the DNN learnable, it provides\\n$\\\\mathcal{E}$ as the union of equivalent solutions of $\\\\mathcal{T}$ by\\nutilizing modulo Fourier invariances. Set $\\\\mathcal{E}$ can be estimated with\\nshort running time via the DNN, and support $\\\\mathcal{T}$ can be determined\\nfrom the DNN output rather than from the full index set by applying hard\\nthresholding to $\\\\mathcal{E}$. Thus, the DNN-based extended support estimation\\nimproves the reconstruction performance of the signal with a low complexity\\nburden dependent on $k$. Numerical results verify that the proposed scheme has\\na superior performance with lower complexity compared to local search-based\\ngreedy sparse phase retrieval and a state-of-the-art variant of the Fienup\\nmethod.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.01831</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.01831</id><submitter>Cai Shaofeng</submitter><version version=\"v1\"><date>Wed, 3 Apr 2019 08:16:24 GMT</date><size>1628kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 08:49:08 GMT</date><size>2231kb</size><source_type>D</source_type></version><title>Model Slicing for Supporting Complex Analytics with Elastic Inference\\n  Cost and Resource Constraints</title><authors>Shaofeng Cai, Gang Chen, Beng Chin Ooi, Jinyang Gao</authors><categories>cs.LG cs.DB cs.PF</categories><comments>14 pages, 8 figures. arXiv admin note: text overlap with\\n  arXiv:1706.02093 by other authors</comments><doi>10.14778/3364324.3364325</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning models have been used to support analytics beyond simple\\naggregation, where deeper and wider models have been shown to yield great\\nresults. These models consume a huge amount of memory and computational\\noperations. However, most of the large-scale industrial applications are often\\ncomputational budget constrained. In practice, the peak workload of inference\\nservice could be 10x higher than the average cases, with the presence of\\nunpredictable extreme cases. Lots of computational resources could be wasted\\nduring off-peak hours and the system may crash when the workload exceeds system\\ncapacity. How to support deep learning services with dynamic workload\\ncost-efficiently remains a challenging problem. In this paper, we address the\\nchallenge with a general and novel training scheme called model slicing, which\\nenables deep learning models to provide predictions within the prescribed\\ncomputational resource budget dynamically. Model slicing could be viewed as an\\nelastic computation solution without requiring more computational resources.\\nSuccinctly, each layer in the model is divided into groups of contiguous block\\nof basic components (i.e. neurons in dense layers and channels in convolutional\\nlayers), and then partially ordered relation is introduced to these groups by\\nenforcing that groups participated in each forward pass always starts from the\\nfirst group to the dynamically-determined rightmost group. Trained by\\ndynamically indexing the rightmost group with a single parameter slice rate,\\nthe network is engendered to build up group-wise and residual representation.\\nThen during inference, a sub-model with fewer groups can be readily deployed\\nfor efficiency whose computation is roughly quadratic to the width controlled\\nby the slice rate. Extensive experiments show that models trained with model\\nslicing can effectively support on-demand workload with elastic inference cost.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.02021</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.02021</id><submitter>James Smith</submitter><version version=\"v1\"><date>Wed, 3 Apr 2019 14:25:08 GMT</date><size>1483kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 18:02:19 GMT</date><size>2859kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 3 Oct 2019 13:43:48 GMT</date><size>2856kb</size><source_type>D</source_type></version><title>Unsupervised Progressive Learning and the STAM Architecture</title><authors>James Smith, Constantine Dovrolis</authors><categories>cs.LG q-bio.NC stat.ML</categories><comments>Under peer-review</comments><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We first pose the Unsupervised Progressive Learning (UPL) problem: learning\\nsalient representations from a non-stationary stream of unlabeled data in which\\nthe number of object classes increases with time. If some limited labeled data\\nis also available, those representations can be associated with specific\\nclasses, thus enabling classification tasks. To solve the UPL problem, we\\npropose an architecture that involves an online clustering module, called\\nSelf-Taught Associative Memory (STAM). Layered hierarchies of STAM modules\\nlearn based on a combination of online clustering, novelty detection,\\nforgetting outliers, and storing only prototypical representations rather than\\nspecific examples. The goal of this paper is to introduce the UPL problem,\\ndescribe the STAM architecture, and evaluate the latter in the UPL context.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.02057</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.02057</id><submitter>Kaidi Xu</submitter><version version=\"v1\"><date>Wed, 3 Apr 2019 15:25:21 GMT</date><size>8183kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 18:32:03 GMT</date><size>5702kb</size><source_type>D</source_type></version><title>Interpreting Adversarial Examples by Activation Promotion and\\n  Suppression</title><authors>Kaidi Xu, Sijia Liu, Gaoyuan Zhang, Mengshu Sun, Pu Zhao, Quanfu Fan,\\n  Chuang Gan, Xue Lin</authors><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is widely known that convolutional neural networks (CNNs) are vulnerable\\nto adversarial examples: images with imperceptible perturbations crafted to\\nfool classifiers. However, interpretability of these perturbations is less\\nexplored in the literature. This work aims to better understand the roles of\\nadversarial perturbations and provide visual explanations from pixel, image and\\nnetwork perspectives. We show that adversaries have a promotion-suppression\\neffect (PSE) on neurons\\' activations and can be primarily categorized into\\nthree types: i) suppression-dominated perturbations that mainly reduce the\\nclassification score of the true label, ii) promotion-dominated perturbations\\nthat focus on boosting the confidence of the target label, and iii) balanced\\nperturbations that play a dual role in suppression and promotion. We also\\nprovide image-level interpretability of adversarial examples. This links PSE of\\npixel-level perturbations to class-specific discriminative image regions\\nlocalized by class activation mapping (Zhou et al. 2016). Further, we examine\\nthe adversarial effect through network dissection (Bau et al. 2017), which\\noffers concept-level interpretability of hidden units. We show that there\\nexists a tight connection between the units\\' sensitivity to adversarial attacks\\nand their interpretability on semantic concepts. Lastly, we provide some new\\ninsights from our interpretation to improve the adversarial robustness of\\nnetworks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.02291</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.02291</id><submitter>Rohit Agrawal</submitter><version version=\"v1\"><date>Thu, 4 Apr 2019 01:03:19 GMT</date><size>8kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 16:25:55 GMT</date><size>11kb</size><source_type>D</source_type></version><title>Multinomial Concentration in Relative Entropy at the Ratio of Alphabet\\n  and Sample Sizes</title><authors>Rohit Agrawal</authors><categories>cs.IT math.IT math.PR math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the moment generating function of the Kullback-Leibler\\ndivergence between the empirical distribution of $n$ independent samples from a\\ndistribution $P$ over a finite alphabet of size $k$ (e.g. a multinomial\\ndistribution) and $P$ itself is no more than that of a gamma distribution with\\nshape $k - 1$ and rate $n$. The resulting exponential concentration inequality\\nbecomes meaningful (less than 1) when the divergence $\\\\varepsilon$ is larger\\nthan $(k-1)/n$, whereas the standard method of types bound requires\\n$\\\\varepsilon &gt; \\\\frac{1}{n} \\\\cdot \\\\log{\\\\binom{n+k-1}{k-1}} \\\\geq (k-1)/n \\\\cdot\\n\\\\log(1 + n/(k-1))$, thus saving a factor of order $\\\\log(n/k)$ in the standard\\nregime of parameters where $n\\\\gg k$. Our proof proceeds via a simple reduction\\nto the case $k = 2$ of a binary alphabet (e.g. a binomial distribution), and\\nhas the property that improvements in the case of $k = 2$ directly translate to\\nimprovements for general $k$.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.02343</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.02343</id><submitter>Chunting Zhou</submitter><version version=\"v1\"><date>Thu, 4 Apr 2019 04:36:11 GMT</date><size>715kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 13 Apr 2019 03:47:04 GMT</date><size>1428kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 30 Apr 2019 21:08:13 GMT</date><size>1428kb</size><source_type>D</source_type></version><title>Density Matching for Bilingual Word Embedding</title><authors>Chunting Zhou, Xuezhe Ma, Di Wang, Graham Neubig</authors><categories>cs.CL cs.LG</categories><comments>Accepted by NAACL-HLT 2019</comments><journal-ref>NAACL 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent approaches to cross-lingual word embedding have generally been based\\non linear transformations between the sets of embedding vectors in the two\\nlanguages. In this paper, we propose an approach that instead expresses the two\\nmonolingual embedding spaces as probability densities defined by a Gaussian\\nmixture model, and matches the two densities using a method called normalizing\\nflow. The method requires no explicit supervision, and can be learned with only\\na seed dictionary of words that have identical strings. We argue that this\\nformulation has several intuitively attractive properties, particularly with\\nthe respect to improving robustness and generalization to mappings between\\ndifficult language pairs or word pairs. On a benchmark data set of bilingual\\nlexicon induction and cross-lingual word similarity, our approach can achieve\\ncompetitive or superior performance compared to state-of-the-art published\\nresults, with particularly strong results being found on etymologically distant\\nand/or morphologically rich languages.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.02902</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.02902</id><submitter>Pedro Cisneros-Velarde</submitter><version version=\"v1\"><date>Fri, 5 Apr 2019 07:24:35 GMT</date><size>45kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 09:50:15 GMT</date><size>51kb</size><source_type>D</source_type></version><title>Signed Network Formation Games and Clustering Balance</title><authors>Pedro Cisneros-Velarde, Francesco Bullo</authors><categories>cs.SI physics.soc-ph</categories><comments>15 pages, 3 figures</comments><msc-class>91A80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a signed network formation game, in which pairs of individuals\\nstrategically change the signs of the edges in a complete network. These\\nindividuals are members of a social network who strategically reduce cognitive\\ndissonances by changing their interpersonal appraisals. We characterize the\\nbest-response dynamics for this game and prove that its implementation \\\\pc{can}\\ndynamically drive the network to a sociologically meaningful sign configuration\\ncalled clustering balance. In this configuration, agents in the social network\\nform one or more clusters that have positive relationships among their members\\nbut negative relationships among members of other clusters. In the past,\\nvarious researchers in the fields of psycho-sociology, political science, and\\nphysics have looked at models that explain the generation of up to two\\nclusters. Our work contributes to these fields by proposing a simple model that\\ngenerates a broader class of signed networks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.03167</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.03167</id><submitter>Sergey Zakharov</submitter><version version=\"v1\"><date>Fri, 5 Apr 2019 17:16:09 GMT</date><size>7393kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 18:49:11 GMT</date><size>5285kb</size><source_type>D</source_type></version><title>HomebrewedDB: RGB-D Dataset for 6D Pose Estimation of 3D Objects</title><authors>Roman Kaskman, Sergey Zakharov, Ivan Shugurov, Slobodan Ilic</authors><categories>cs.CV cs.RO</categories><comments>ICCVW 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Among the most important prerequisites for creating and evaluating 6D object\\npose detectors are datasets with labeled 6D poses. With the advent of deep\\nlearning, demand for such datasets is growing continuously. Despite the fact\\nthat some of exist, they are scarce and typically have restricted setups, such\\nas a single object per sequence, or they focus on specific object types, such\\nas textureless industrial parts. Besides, two significant components are often\\nignored: training using only available 3D models instead of real data and\\nscalability, i.e. training one method to detect all objects rather than\\ntraining one detector per object. Other challenges, such as occlusions,\\nchanging light conditions and changes in object appearance, as well precisely\\ndefined benchmarks are either not present or are scattered among different\\ndatasets. In this paper we present a dataset for 6D pose estimation that covers\\nthe above-mentioned challenges, mainly targeting training from 3D models (both\\ntextured and textureless), scalability, occlusions, and changes in light\\nconditions and object appearance. The dataset features 33 objects (17 toy, 8\\nhousehold and 8 industry-relevant objects) over 13 scenes of various\\ndifficulty. We also present a set of benchmarks to test various desired\\ndetector properties, particularly focusing on scalability with respect to the\\nnumber of objects and resistance to changing light conditions, occlusions and\\nclutter. We also set a baseline for the presented benchmarks using a\\nstate-of-the-art DPOD detector. Considering the difficulty of making such\\ndatasets, we plan to release the code allowing other researchers to extend this\\ndataset or make their own datasets in the future.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.03208</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.03208</id><submitter>Qing Liu</submitter><version version=\"v1\"><date>Fri, 5 Apr 2019 18:04:40 GMT</date><size>4327kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 22 Aug 2019 03:33:21 GMT</date><size>1880kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 27 Sep 2019 18:14:45 GMT</date><size>1881kb</size><source_type>D</source_type></version><title>Semantic-Aware Knowledge Preservation for Zero-Shot Sketch-Based Image\\n  Retrieval</title><authors>Qing Liu, Lingxi Xie, Huiyu Wang, Alan Yuille</authors><categories>cs.CV</categories><comments>To appear in ICCV 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sketch-based image retrieval (SBIR) is widely recognized as an important\\nvision problem which implies a wide range of real-world applications. Recently,\\nresearch interests arise in solving this problem under the more realistic and\\nchallenging setting of zero-shot learning. In this paper, we investigate this\\nproblem from the viewpoint of domain adaptation which we show is critical in\\nimproving feature embedding in the zero-shot scenario. Based on a framework\\nwhich starts with a pre-trained model on ImageNet and fine-tunes it on the\\ntraining set of SBIR benchmark, we advocate the importance of preserving\\npreviously acquired knowledge, e.g., the rich discriminative features learned\\nfrom ImageNet, to improve the model\\'s transfer ability. For this purpose, we\\ndesign an approach named Semantic-Aware Knowledge prEservation (SAKE), which\\nfine-tunes the pre-trained model in an economical way and leverages semantic\\ninformation, e.g., inter-class relationship, to achieve the goal of knowledge\\npreservation. Zero-shot experiments on two extended SBIR datasets, TU-Berlin\\nand Sketchy, verify the superior performance of our approach. Extensive\\ndiagnostic experiments validate that knowledge preserved benefits SBIR in\\nzero-shot settings, as a large fraction of the performance gain is from the\\nmore properly structured feature embedding for photo images. Code is available\\nat: https://github.com/qliu24/SAKE.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.03406</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.03406</id><submitter>Luca Sanguinetti</submitter><version version=\"v1\"><date>Sat, 6 Apr 2019 10:07:02 GMT</date><size>1133kb</size></version><version version=\"v2\"><date>Wed, 29 May 2019 11:37:03 GMT</date><size>1977kb</size></version><version version=\"v3\"><date>Sat, 5 Oct 2019 17:08:27 GMT</date><size>1382kb</size><source_type>D</source_type></version><title>Towards Massive MIMO 2.0: Understanding spatial correlation,\\n  interference suppression, and pilot contamination</title><authors>Luca Sanguinetti and Emil Bj\\\\&quot;ornson and Jakob Hoydis</authors><categories>eess.SP cs.IT math.IT</categories><comments>26 pages, 16 figures, IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the seminal paper by Marzetta from 2010, Massive MIMO has changed from\\nbeing a theoretical concept with an infinite number of antennas to a practical\\ntechnology. The key concepts are adopted in 5G and base stations (BSs) with\\n$M=64$ full-digital transceivers have been commercially deployed in sub-6\\\\,GHz\\nbands. The fast progress was enabled by many solid research contributions of\\nwhich the vast majority assume spatially uncorrelated channels and signal\\nprocessing schemes developed for single-cell operation. These assumptions make\\nthe performance analysis and optimization of Massive MIMO tractable but have\\nthree major caveats: 1) practical channels are spatially correlated; 2) large\\nperformance gains can be obtained by multicell processing, without BS\\ncooperation; 3) the interference caused by pilot contamination creates a finite\\ncapacity limit, as $M\\\\to\\\\infty$. There is a thin line of papers that avoided\\nthese caveats, but the results are easily missed. Hence, this tutorial article\\nexplains the importance of considering spatial channel correlation and using\\nsignal processing schemes designed for multicell networks. We present recent\\nresults on the fundamental limits of Massive MIMO, which are not determined by\\npilot contamination but the ability to acquire channel statistics. These\\nresults will guide the journey towards the next level of Massive MIMO, which we\\ncall ``Massive MIMO 2.0\\'\\'.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.03515</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.03515</id><submitter>Micha{\\\\l} Zaj\\\\k{a}c</submitter><version version=\"v1\"><date>Sat, 6 Apr 2019 19:10:05 GMT</date><size>815kb</size><source_type>D</source_type></version><title>Split Batch Normalization: Improving Semi-Supervised Learning under\\n  Domain Shift</title><authors>Micha{\\\\l} Zaj\\\\k{a}c, Konrad Zolna, Stanis{\\\\l}aw Jastrz\\\\k{e}bski</authors><categories>cs.LG stat.ML</categories><comments>Under review for ECML PKDD 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work has shown that using unlabeled data in semi-supervised learning\\nis not always beneficial and can even hurt generalization, especially when\\nthere is a class mismatch between the unlabeled and labeled examples. We\\ninvestigate this phenomenon for image classification on the CIFAR-10 and the\\nImageNet datasets, and with many other forms of domain shifts applied (e.g.\\nsalt-and-pepper noise). Our main contribution is Split Batch Normalization\\n(Split-BN), a technique to improve SSL when the additional unlabeled data comes\\nfrom a shifted distribution. We achieve it by using separate batch\\nnormalization statistics for unlabeled examples. Due to its simplicity, we\\nrecommend it as a standard practice. Finally, we analyse how domain shift\\naffects the SSL training process. In particular, we find that during training\\nthe statistics of hidden activations in late layers become markedly different\\nbetween the unlabeled and the labeled examples.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.03858</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.03858</id><submitter>Alexander Wein</submitter><version version=\"v1\"><date>Mon, 8 Apr 2019 06:26:35 GMT</date><size>34kb</size></version><version version=\"v2\"><date>Tue, 1 Oct 2019 14:35:50 GMT</date><size>40kb</size></version><title>The Kikuchi Hierarchy and Tensor PCA</title><authors>Alexander S. Wein, Ahmed El Alaoui, Cristopher Moore</authors><categories>cs.DS cond-mat.stat-mech cs.LG math.ST stat.ML stat.TH</categories><comments>42 pages. This version adds results on odd-order tensor PCA and\\n  even-arity XOR refutation</comments><msc-class>68Q87</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the tensor PCA (principal component analysis) problem, we propose a new\\nhierarchy of increasingly powerful algorithms with increasing runtime. Our\\nhierarchy is analogous to the sum-of-squares (SOS) hierarchy but is instead\\ninspired by statistical physics and related algorithms such as belief\\npropagation and AMP (approximate message passing). Our level-$\\\\ell$ algorithm\\ncan be thought of as a linearized message-passing algorithm that keeps track of\\n$\\\\ell$-wise dependencies among the hidden variables. Specifically, our\\nalgorithms are spectral methods based on the Kikuchi Hessian, which generalizes\\nthe well-studied Bethe Hessian to the higher-order Kikuchi free energies.\\n  It is known that AMP, the flagship algorithm of statistical physics, has\\nsubstantially worse performance than SOS for tensor PCA. In this work we\\n\\'redeem\\' the statistical physics approach by showing that our hierarchy gives a\\npolynomial-time algorithm matching the performance of SOS. Our hierarchy also\\nyields a continuum of subexponential-time algorithms, and we prove that these\\nachieve the same (conjecturally optimal) tradeoff between runtime and\\nstatistical power as SOS. Our proofs are much simpler than prior work, and also\\napply to the related problem of refuting random $k$-XOR formulas. The results\\nwe present here apply to tensor PCA for tensors of all orders, and to $k$-XOR\\nwhen $k$ is even.\\n  Our methods suggest a new avenue for systematically obtaining optimal\\nalgorithms for Bayesian inference problems, and our results constitute a step\\ntoward unifying the statistical physics and sum-of-squares approaches to\\nalgorithm design.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.03973</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.03973</id><submitter>Tao Lei</submitter><version version=\"v1\"><date>Mon, 8 Apr 2019 11:56:07 GMT</date><size>8280kb</size><source_type>D</source_type></version><title>Adaptive Morphological Reconstruction for Seeded Image Segmentation</title><authors>Tao Lei, Xiaohong Jia, Tongliang Liu, Shigang Liu, Hongying Meng, and\\n  Asoke K. Nandi</authors><categories>cs.CV</categories><doi>10.1109/TIP.2019.2920514</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Morphological reconstruction (MR) is often employed by seeded image\\nsegmentation algorithms such as watershed transform and power watershed as it\\nis able to filter seeds (regional minima) to reduce over-segmentation. However,\\nMR might mistakenly filter meaningful seeds that are required for generating\\naccurate segmentation and it is also sensitive to the scale because a\\nsingle-scale structuring element is employed. In this paper, a novel adaptive\\nmorphological reconstruction (AMR) operation is proposed that has three\\nadvantages. Firstly, AMR can adaptively filter useless seeds while preserving\\nmeaningful ones. Secondly, AMR is insensitive to the scale of structuring\\nelements because multiscale structuring elements are employed. Finally, AMR has\\ntwo attractive properties: monotonic increasingness and convergence that help\\nseeded segmentation algorithms to achieve a hierarchical segmentation.\\nExperiments clearly demonstrate that AMR is useful for improving algorithms of\\nseeded image segmentation and seed-based spectral segmentation. Compared to\\nseveral state-of-the-art algorithms, the proposed algorithms provide better\\nsegmentation results requiring less computing time. Source code is available at\\nhttps://github.com/SUST-reynole/AMR.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.04563</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.04563</id><submitter>Giuseppe Rodriguez</submitter><version version=\"v1\"><date>Tue, 9 Apr 2019 09:45:06 GMT</date><size>379kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 16 May 2019 09:58:31 GMT</date><size>3012kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 3 Oct 2019 11:11:12 GMT</date><size>6490kb</size><source_type>D</source_type></version><title>Inversion of multi-configuration complex EMI data with minimum gradient\\n  support regularization. A case study</title><authors>Gian Piero Deidda, Patricia Diaz de Alba, Giuseppe Rodriguez, Giulio\\n  Vignoli</authors><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frequency-domain electromagnetic instruments allow the collection of data in\\ndifferent configurations, that is, varying the inter-coil spacing, the\\nfrequency, and the height above the ground. This makes these tools very\\npractical, also because of their handy size, for the characterization of the\\nnear surface in many fields of applications, for example, precision\\nagriculture, pollution assessments, shallow geological investigations. To this\\nend, the inversion of either the real (in-phase) or the imaginary (quadrature)\\ncomponent of the signal has already been studied. Furthermore, in many\\nsituations a regularization scheme retrieving smooth solutions is blindly\\napplied, without taking into account the prior available knowledge.. The\\npresent work discusses an algorithm for the inversion of the complex signal in\\nits entirety, as well as a regularization method promoting the sparsity of the\\nreconstructed electrical conductivity distribution. This regularization\\nstrategy incorporates a minimum gradient support stabilizer into a truncated\\ngeneralized singular value decomposition scheme. The results of the\\nimplementation of this sparsity enhancing regularization at each step of a\\ndamped Gauss--Newton inversion algorithm (based on a nonlinear forward model)\\nare compared against the associated solutions obtained via a standard smooth\\nstabilizer. An approach to estimate the depth of investigation (DOI), that is,\\nthe maximum depth that can be investigated by a chosen instrument configuration\\nin a particular experimental setting, is also discussed. The effectiveness and\\nlimitations of the whole inversion algorithm are demonstrated on synthetic and\\nreal datasets.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.04744</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.04744</id><submitter>Pierluigi Zama Ramirez</submitter><version version=\"v1\"><date>Tue, 9 Apr 2019 15:48:44 GMT</date><size>4571kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 15:27:16 GMT</date><size>8673kb</size><source_type>D</source_type></version><title>Learning Across Tasks and Domains</title><authors>Pierluigi Zama Ramirez, Alessio Tonioni, Samuele Salti, Luigi Di\\n  Stefano</authors><categories>cs.CV</categories><comments>Accepted at ICCV 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent works have proven that many relevant visual tasks are closely related\\none to another. Yet, this connection is seldom deployed in practice due to the\\nlack of practical methodologies to transfer learned concepts across different\\ntraining processes. In this work, we introduce a novel adaptation framework\\nthat can operate across both task and domains. Our framework learns to transfer\\nknowledge across tasks in a fully supervised domain (e.g., synthetic data) and\\nuse this knowledge on a different domain where we have only partial supervision\\n(e.g., real data). Our proposal is complementary to existing domain adaptation\\ntechniques and extends them to cross tasks scenarios providing additional\\nperformance gains. We prove the effectiveness of our framework across two\\nchallenging tasks (i.e., monocular depth estimation and semantic segmentation)\\nand four different domains (Synthia, Carla, Kitti, and Cityscapes).\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.05016</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.05016</id><submitter>Mohammad Javad Khojasteh</submitter><version version=\"v1\"><date>Wed, 10 Apr 2019 06:25:53 GMT</date><size>3715kb</size></version><version version=\"v2\"><date>Tue, 1 Oct 2019 20:35:50 GMT</date><size>4761kb</size></version><title>Theory and implementation of event-triggered stabilization over digital\\n  channels</title><authors>Mohammad Javad Khojasteh, Mojtaba Hedayatpour, Massimo Franceschetti</authors><categories>cs.SY cs.IT math.IT math.OC</categories><journal-ref>2019 IEEE 58th Conference on Decision and Control (CDC)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of event-triggered control, the timing of the triggering\\nevents carries information about the state of the system that can be used for\\nstabilization. At each triggering event, not only can information be\\ntransmitted by the message content (data payload) but also by its timing. We\\ndemonstrate this in the context of stabilization of a laboratory-scale inverted\\npendulum around its equilibrium point over a digital communication channel with\\nbounded unknown delay. Our event-triggering control strategy encodes timing\\ninformation by transmitting in a state-dependent fashion and can achieve\\nstabilization using a data payload transmission rate lower than what the\\ndata-rate theorem prescribes for classical periodic control policies that do\\nnot exploit timing information. Through experimental results, we show that as\\nthe delay in the communication channel increases, a higher data payload\\ntransmission rate is required to fulfill the proposed event-triggering policy\\nrequirements. This confirms the theoretical intuition that a larger delay\\nbrings a larger uncertainty about the value of the state at the controller, as\\nless timing information is carried in the communication. In addition, our\\nresults also provide a novel encoding-decoding scheme to achieve input-to-state\\npractically stability (ISpS) for nonlinear continuous-time systems under\\nappropriate assumptions.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.05308</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.05308</id><submitter>Davy Weissenbacher</submitter><version version=\"v1\"><date>Wed, 10 Apr 2019 17:18:17 GMT</date><size>1090kb</size></version><version version=\"v2\"><date>Mon, 30 Sep 2019 16:34:33 GMT</date><size>1090kb</size></version><title>Deep Neural Networks Ensemble for Detecting Medication Mentions in\\n  Tweets</title><authors>Davy Weissenbacher, Abeed Sarker, Ari Klein, Karen O\\'Connor, Arjun\\n  Magge Ranganatha, Graciela Gonzalez-Hernandez</authors><categories>cs.CL cs.IR cs.LG</categories><comments>This is a pre-copy-editing, author-produced PDF of an article\\n  accepted for publication in JAMIA following peer review. The definitive\\n  publisher-authenticated version is &quot;D. Weissenbacher, A. Sarker, A. Klein, K.\\n  O\\'Connor, A. Magge, G. Gonzalez-Hernandez, Deep neural networks ensemble for\\n  detecting medication mentions in tweets, Journal of the American Medical\\n  Informatics Association, ocz156, 2019&quot;</comments><journal-ref>Journal of the American Medical Informatics Association, ocz156,\\n  2019</journal-ref><doi>10.1093/jamia/ocz156</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective: After years of research, Twitter posts are now recognized as an\\nimportant source of patient-generated data, providing unique insights into\\npopulation health. A fundamental step to incorporating Twitter data in\\npharmacoepidemiological research is to automatically recognize medication\\nmentions in tweets. Given that lexical searches for medication names may fail\\ndue to misspellings or ambiguity with common words, we propose a more advanced\\nmethod to recognize them. Methods: We present Kusuri, an Ensemble Learning\\nclassifier, able to identify tweets mentioning drug products and dietary\\nsupplements. Kusuri (&quot;medication&quot; in Japanese) is composed of two modules.\\nFirst, four different classifiers (lexicon-based, spelling-variant-based,\\npattern-based and one based on a weakly-trained neural network) are applied in\\nparallel to discover tweets potentially containing medication names. Second, an\\nensemble of deep neural networks encoding morphological, semantical and\\nlong-range dependencies of important words in the tweets discovered is used to\\nmake the final decision. Results: On a balanced (50-50) corpus of 15,005\\ntweets, Kusuri demonstrated performances close to human annotators with 93.7%\\nF1-score, the best score achieved thus far on this corpus. On a corpus made of\\nall tweets posted by 113 Twitter users (98,959 tweets, with only 0.26%\\nmentioning medications), Kusuri obtained 76.3% F1-score. There is not a prior\\ndrug extraction system that compares running on such an extremely unbalanced\\ndataset. Conclusion: The system identifies tweets mentioning drug names with\\nperformance high enough to ensure its usefulness and ready to be integrated in\\nlarger natural language processing systems.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.05394</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.05394</id><submitter>Marco Huber</submitter><version version=\"v1\"><date>Wed, 10 Apr 2019 19:11:47 GMT</date><size>1462kb</size></version><version version=\"v2\"><date>Thu, 3 Oct 2019 19:57:24 GMT</date><size>1402kb</size></version><title>Enhancing Decision Tree based Interpretation of Deep Neural Networks\\n  through L1-Orthogonal Regularization</title><authors>Nina Schaaf, Marco F. Huber, and Johannes Maucher</authors><categories>cs.LG stat.ML</categories><comments>8 pages, 18th IEEE International Conference on Machine Learning and\\n  Applications (ICMLA) 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One obstacle that so far prevents the introduction of machine learning models\\nprimarily in critical areas is the lack of explainability. In this work, a\\npracticable approach of gaining explainability of deep artificial neural\\nnetworks (NN) using an interpretable surrogate model based on decision trees is\\npresented. Simply fitting a decision tree to a trained NN usually leads to\\nunsatisfactory results in terms of accuracy and fidelity. Using L1-orthogonal\\nregularization during training, however, preserves the accuracy of the NN,\\nwhile it can be closely approximated by small decision trees. Tests with\\ndifferent data sets confirm that L1-orthogonal regularization yields models of\\nlower complexity and at the same time higher fidelity compared to other\\nregularizers.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.05530</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.05530</id><submitter>Xiang Ren</submitter><version version=\"v1\"><date>Thu, 11 Apr 2019 04:45:42 GMT</date><size>338kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 4 Jun 2019 19:06:37 GMT</date><size>273kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 8 Oct 2019 03:32:40 GMT</date><size>425kb</size><source_type>D</source_type></version><title>Recurrent Event Network: Global Structure Inference over Temporal\\n  Knowledge Graph</title><authors>Woojeong Jin, He Jiang, Meng Qu, Tong Chen, Changlin Zhang, Pedro\\n  Szekely, Xiang Ren</authors><categories>cs.LG cs.AI cs.CL stat.ML</categories><comments>10 pages, 5 figures, short version is accepted at ICLR 2019 RLGM\\n  Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling dynamically-evolving, multi-relational graph data has received a\\nsurge of interests with the rapid growth of heterogeneous event data. However,\\npredicting future events on such data requires global structure inference over\\ntime and the ability to integrate temporal and structural information, which\\nare not yet well understood. We present Recurrent Event Network (RE-Net), a\\nnovel autoregressive architecture for modeling temporal sequences of\\nmulti-relational graphs (e.g., temporal knowledge graph), which can perform\\nsequential, global structure inference over future time stamps to predict new\\nevents. RE-Net employs a recurrent event encoder to model the temporally\\nconditioned joint probability distribution for the event sequences, and equips\\nthe event encoder with a neighborhood aggregator for modeling the concurrent\\nevents within a time window associated with each entity. We apply teacher\\nforcing for model training over historical data, and infer graph sequences over\\nfuture time stamps by sampling from the learned joint distribution in a\\nsequential manner. We evaluate the proposed method via temporal link prediction\\non five public datasets. Extensive experiments demonstrate the strength of\\nRE-Net, especially on multi-step inference over future time stamps. Code and\\ndata can be found at https://github.com/INK-USC/RE-Net .\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.05657</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.05657</id><submitter>Brynjulf Owren</submitter><version version=\"v1\"><date>Thu, 11 Apr 2019 12:15:00 GMT</date><size>4841kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 22 Sep 2019 17:28:02 GMT</date><size>7223kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 30 Sep 2019 21:33:06 GMT</date><size>7226kb</size><source_type>D</source_type></version><title>Deep learning as optimal control problems: models and numerical methods</title><authors>Martin Benning and Elena Celledoni and Matthias J. Ehrhardt and\\n  Brynjulf Owren and Carola-Bibiane Sch\\\\&quot;onlieb</authors><categories>math.OC cs.LG cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider recent work of Haber and Ruthotto 2017 and Chang et al. 2018,\\nwhere deep learning neural networks have been interpreted as discretisations of\\nan optimal control problem subject to an ordinary differential equation\\nconstraint. We review the first order conditions for optimality, and the\\nconditions ensuring optimality after discretisation. This leads to a class of\\nalgorithms for solving the discrete optimal control problem which guarantee\\nthat the corresponding discrete necessary conditions for optimality are\\nfulfilled. The differential equation setting lends itself to learning\\nadditional parameters such as the time discretisation. We explore this\\nextension alongside natural constraints (e.g. time steps lie in a simplex). We\\ncompare these deep learning algorithms numerically in terms of induced flow and\\ngeneralisation ability.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.05995</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.05995</id><submitter>Shelby Kimmel</submitter><version version=\"v1\"><date>Fri, 12 Apr 2019 00:56:28 GMT</date><size>20kb</size></version><title>Applications of the quantum algorithm for st-connectivity</title><authors>Kai DeLorenzo, Shelby Kimmel, R. Teal Witter</authors><categories>quant-ph cs.CC cs.DS</categories><journal-ref>TQC 2019</journal-ref><doi>10.4230/LIPIcs.TQC.2019.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present quantum algorithms for various problems related to graph\\nconnectivity. We give simple and query-optimal algorithms for cycle detection\\nand odd-length cycle detection (bipartiteness) using a reduction to\\nst-connectivity. Furthermore, we show that our algorithm for cycle detection\\nhas improved performance under the promise of large circuit rank or a small\\nnumber of edges. We also provide algorithms for detecting even-length cycles\\nand for estimating the circuit rank of a graph. All of our algorithms have\\nlogarithmic space complexity.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.06097</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.06097</id><submitter>Jun-Ho Choi</submitter><version version=\"v1\"><date>Fri, 12 Apr 2019 08:37:17 GMT</date><size>9027kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 05:11:06 GMT</date><size>7953kb</size><source_type>D</source_type></version><title>Evaluating Robustness of Deep Image Super-Resolution against Adversarial\\n  Attacks</title><authors>Jun-Ho Choi, Huan Zhang, Jun-Hyuk Kim, Cho-Jui Hsieh, Jong-Seok Lee</authors><categories>cs.CV</categories><comments>Accepted in ICCV 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single-image super-resolution aims to generate a high-resolution version of a\\nlow-resolution image, which serves as an essential component in many computer\\nvision applications. This paper investigates the robustness of deep\\nlearning-based super-resolution methods against adversarial attacks, which can\\nsignificantly deteriorate the super-resolved images without noticeable\\ndistortion in the attacked low-resolution images. It is demonstrated that\\nstate-of-the-art deep super-resolution methods are highly vulnerable to\\nadversarial attacks. Different levels of robustness of different methods are\\nanalyzed theoretically and experimentally. We also present analysis on\\ntransferability of attacks, and feasibility of targeted attacks and universal\\nattacks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.06601</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.06601</id><submitter>Yunus Can G\\\\&quot;ultekin</submitter><version version=\"v1\"><date>Sat, 13 Apr 2019 23:13:45 GMT</date><size>1373kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 23 Apr 2019 10:21:37 GMT</date><size>2691kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 3 Jul 2019 13:33:55 GMT</date><size>849kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Thu, 19 Sep 2019 10:02:55 GMT</date><size>839kb</size><source_type>D</source_type></version><title>Introducing Enumerative Sphere Shaping for Optical Communication Systems\\n  with Short Blocklengths</title><authors>Abdelkerim Amari, Sebastiaan Goossens, Yunus Can Gultekin, Olga\\n  Vassilieva, Inwoong Kim, Tadashi Ikeuchi, Chigo Okonkwo, Frans M. J. Willems,\\n  and Alex Alvarado</authors><categories>cs.IT math.IT</categories><comments>9 pages, 12 figures</comments><doi>10.1109/JLT.2019.2943938</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic shaping based on constant composition distribution matching\\n(CCDM) has received considerable attention as a way to increase the capacity of\\nfiber optical communication systems. CCDM suffers from significant rate loss at\\nshort blocklengths and requires long blocklengths to achieve high shaping gain,\\nwhich makes its implementation very challenging. In this paper, we propose to\\nuse enumerative sphere shaping (ESS) and investigate its performance for the\\nnonlinear fiber optical channel. ESS has lower rate loss than CCDM at the same\\nshaping rate, which makes it a suitable candidate to be implemented in\\nreal-time high-speed optical systems. In this paper, we first show that finite\\nblocklength ESS and CCDM exhibit higher effective signal-to-noise ratio than\\ntheir infinite blocklength counterparts. These results show that for the\\nnonlinear fiber optical channel, large blocklengths should be avoided. We then\\nshow that for a 400 Gb/s dual-polarization 64-QAM WDM transmission system, ESS\\nwith short blocklengths outperforms both uniform signaling and CCDM. Gains in\\nterms of both bit-metric decoding rate and bit-error rate are presented. ESS\\nwith a blocklength of 200 is shown to provide an extension reach of about 200\\nkm in comparison with CCDM with the same blocklength. The obtained reach\\nincrease of ESS with a blocklength of 200 over uniform signaling is\\napproximately 450 km (approximately 19%)\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.06786</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.06786</id><submitter>Sarah Bechtle</submitter><version version=\"v1\"><date>Mon, 15 Apr 2019 00:02:27 GMT</date><size>3067kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 00:34:22 GMT</date><size>1448kb</size><source_type>D</source_type></version><title>Curious iLQR: Resolving Uncertainty in Model-based RL</title><authors>Sarah Bechtle, Yixin Lin, Akshara Rai, Ludovic Righetti, Franziska\\n  Meier</authors><categories>cs.RO cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Curiosity as a means to explore during reinforcement learning problems has\\nrecently become very popular. However, very little progress has been made in\\nutilizing curiosity for learning control. In this work, we propose a\\nmodel-based reinforcement learning (MBRL) framework that combines Bayesian\\nmodeling of the system dynamics with curious iLQR, an iterative LQR approach\\nthat considers model uncertainty. During trajectory optimization the curious\\niLQR attempts to minimize both the task-dependent cost and the uncertainty in\\nthe dynamics model. We demonstrate the approach on reaching tasks with 7-DoF\\nmanipulators in simulation and on a real robot. Our experiments show that MBRL\\nwith curious iLQR reaches desired end-effector targets more reliably and with\\nless system rollouts when learning a new task from scratch, and that the\\nlearned model generalizes better to new reaching tasks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.06965</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.06965</id><submitter>Nikolaj Takata M\\\\&quot;ucke</submitter><version version=\"v1\"><date>Mon, 15 Apr 2019 11:09:02 GMT</date><size>371kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 12:53:03 GMT</date><size>298kb</size><source_type>D</source_type></version><title>Reduced Order Modeling for Nonlinear PDE-constrained Optimization using\\n  Neural Networks</title><authors>Nikolaj Takata M\\\\&quot;ucke, Lasse Hjuler Christiansen, Allan Peter\\n  Karup-Engsig, John Bagterp J{\\\\o}rgensen</authors><categories>math.NA cs.NA math.AP math.OC</categories><comments>Accepted for publishing at the 58th IEEE Conference on Decision and\\n  Control, Nice, France, 11-13 December, https://cdc2019.ieeecss.org/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonlinear model predictive control (NMPC) often requires real-time solution\\nto optimization problems. However, in cases where the mathematical model is of\\nhigh dimension in the solution space, e.g. for solution of partial differential\\nequations (PDEs), black-box optimizers are rarely sufficient to get the\\nrequired online computational speed. In such cases one must resort to\\ncustomized solvers. This paper present a new solver for nonlinear\\ntime-dependent PDE-constrained optimization problems. It is composed of a\\nsequential quadratic programming (SQP) scheme to solve the PDE-constrained\\nproblem in an offline phase, a proper orthogonal decomposition (POD) approach\\nto identify a lower dimensional solution space, and a neural network (NN) for\\nfast online evaluations. The proposed method is showcased on a regularized\\nleast-square optimal control problem for the viscous Burgers\\' equation. It is\\nconcluded that significant online speed-up is achieved, compared to\\nconventional methods using SQP and finite elements, at a cost of a prolonged\\noffline phase and reduced accuracy.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.07027</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.07027</id><submitter>Felipe S. Abrah\\\\~ao</submitter><version version=\"v1\"><date>Mon, 8 Apr 2019 23:32:45 GMT</date><size>36kb</size></version><version version=\"v2\"><date>Mon, 22 Apr 2019 20:04:48 GMT</date><size>37kb</size></version><version version=\"v3\"><date>Fri, 4 Oct 2019 21:07:08 GMT</date><size>37kb</size></version><title>Learning the undecidable from networked systems</title><authors>Felipe S. Abrah\\\\~ao, \\\\\\'Itala M. Loffredo D\\'Ottaviano, Klaus Wehmuth,\\n  Francisco Ant\\\\^onio D\\\\\\'oria, Artur Ziviani</authors><categories>cs.DC cs.MA cs.SI math.LO</categories><comments>Revised preprint version for publication as a book chapter</comments><msc-class>68Q30, 03D32, 68R10, 05C30, 05C78, 05C75, 05C60, 05C80, 05C82,\\n  94A15, 68Q01, 03D10, 03D32, 03D35, 03D80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a theoretical investigation of computation beyond the\\nTuring barrier from emergent behavior in distributed systems. In particular, we\\npresent an algorithmic network that is a mathematical model of a networked\\npopulation of randomly generated computable systems with a fixed communication\\nprotocol. Then, in order to solve an undecidable problem, we study how nodes\\n(i.e., Turing machines or computable systems) can harness the power of the\\nmetabiological selection and the power of information sharing (i.e.,\\ncommunication) through the network. Formally, we show that there is a pervasive\\nnetwork topological condition, in particular, the small-diameter phenomenon,\\nthat ensures that every node becomes capable of solving the halting problem for\\nevery program with a length upper bounded by a logarithmic order of the\\npopulation size. In addition, we show that this result implies the existence of\\na central node capable of emergently solving the halting problem in the minimum\\nnumber of communication rounds. Furthermore, we introduce an\\nalgorithmic-informational measure of synergy for networked computable systems,\\nwhich we call local algorithmic synergy. Then, we show that such algorithmic\\nnetwork can produce an arbitrarily large value of expected local algorithmic\\nsynergy.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.07117</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.07117</id><submitter>Milo Viviani</submitter><version version=\"v1\"><date>Fri, 12 Apr 2019 11:11:45 GMT</date><size>1147kb</size></version><version version=\"v2\"><date>Fri, 19 Apr 2019 11:32:22 GMT</date><size>1100kb</size></version><version version=\"v3\"><date>Mon, 6 May 2019 09:43:25 GMT</date><size>587kb</size></version><version version=\"v4\"><date>Fri, 4 Oct 2019 20:33:29 GMT</date><size>593kb</size></version><title>A minimal-variable symplectic method for isospectral flows</title><authors>Milo Viviani</authors><categories>math.NA cs.NA</categories><comments>17 pages, 9 figures</comments><msc-class>37M15 65P10 37J15 53D20 70H06</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Isospectral flows are abundant in mathematical physics; the rigid body, the\\nthe Toda lattice, the Brockett flow, the Heisenberg spin chain, and the point\\nvortex dynamics, to mention but a few. Their connection on the one hand with\\nintegrable systems and, on the other, with Lie--Poisson systems motivates the\\nresearch for optimal numerical schemes to solve them. Several works about\\nnumerical methods to integrate isospectral flows have produced a large\\nvarieties of solutions to this problem. However, many of these algorithms are\\nnot intrinsically defined in the space where the equations take place and/or\\nrely on computationally heavy transformations. In the literature, only few\\nexamples of numerical methods avoiding these issues are known, for instance,\\nthe \\\\textit{spherical midpoint method} on $\\\\SO(3)$. In this paper we introduce\\na new minimal-variable, second order, numerical integrator for isospectral\\nflows intrinsically defined on quadratic Lie algebras and symmetric matrices.\\nThe algorithm is isospectral for general isospectral flows and Lie--Poisson\\npreserving when the isospectral flow is Hamiltonian. The simplicity of the\\nscheme, together with its structure-preserving properties, makes it a\\ncompetitive alternative to those already present in literature.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.07272</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.07272</id><submitter>Aleksandrs Slivkins</submitter><version version=\"v1\"><date>Mon, 15 Apr 2019 18:17:01 GMT</date><size>510kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 29 Apr 2019 20:45:01 GMT</date><size>510kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 25 Jun 2019 14:39:03 GMT</date><size>536kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Sun, 15 Sep 2019 02:06:22 GMT</date><size>557kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Mon, 30 Sep 2019 00:15:42 GMT</date><size>543kb</size><source_type>D</source_type></version><title>Introduction to Multi-Armed Bandits</title><authors>Aleksandrs Slivkins</authors><categories>cs.LG cs.AI cs.DS stat.ML</categories><comments>The manuscript is complete, but comments are very welcome! To be\\n  published with Foundations and Trends in Machine Learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-armed bandits a simple but very powerful framework for algorithms that\\nmake decisions over time under uncertainty. An enormous body of work has\\naccumulated over the years, covered in several books and surveys. This book\\nprovides a more introductory, textbook-like treatment of the subject. Each\\nchapter tackles a particular line of work, providing a self-contained,\\nteachable technical introduction and a brief review of the further\\ndevelopments. The chapters are as follows: stochastic bandits, lower bounds;\\nBayesian bandits and Thompson Sampling; Lipschitz Bandits; full Feedback and\\nadversarial costs; adversarial bandits; linear costs and semi-bandits;\\ncontextual Bandits; bandits and games; bandits with knapsacks; bandits and\\nincentives.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.08535</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.08535</id><submitter>Paria Jamshid Lou</submitter><version version=\"v1\"><date>Wed, 17 Apr 2019 23:30:17 GMT</date><size>83kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 26 Sep 2019 04:50:22 GMT</date><size>85kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 1 Oct 2019 00:00:11 GMT</date><size>85kb</size><source_type>D</source_type></version><title>Neural Constituency Parsing of Speech Transcripts</title><authors>Paria Jamshid Lou and Yufei Wang and Mark Johnson</authors><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the performance of a neural self-attentive parser on\\ntranscribed speech. Speech presents parsing challenges that do not appear in\\nwritten text, such as the lack of punctuation and the presence of speech\\ndisfluencies (including filled pauses, repetitions, corrections, etc.).\\nDisfluencies are especially problematic for conventional syntactic parsers,\\nwhich typically fail to find any EDITED disfluency nodes at all. This motivated\\nthe development of special disfluency detection systems, and special mechanisms\\nadded to parsers specifically to handle disfluencies. However, we show here\\nthat neural parsers can find EDITED disfluency nodes, and the best neural\\nparsers find them with an accuracy surpassing that of specialized disfluency\\ndetection systems, thus making these specialized mechanisms unnecessary. This\\npaper also investigates a modified loss function that puts more weight on\\nEDITED nodes. It also describes tree-transformations that simplify the\\ndisfluency detection task by providing alternative encodings of disfluencies\\nand syntactic information.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.08693</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.08693</id><submitter>Niki Kilbertus</submitter><version version=\"v1\"><date>Thu, 18 Apr 2019 11:13:43 GMT</date><size>846kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 6 Sep 2019 07:39:52 GMT</date><size>612kb</size><source_type>D</source_type></version><title>Convolutional neural networks: a magic bullet for gravitational-wave\\n  detection?</title><authors>Timothy D. Gebhard, Niki Kilbertus, Ian Harry, Bernhard Sch\\\\&quot;olkopf</authors><categories>astro-ph.IM astro-ph.HE cs.LG stat.ML</categories><comments>First two authors contributed equally; appeared at Phys. Rev. D</comments><journal-ref>Phys. Rev. D 100, 063015 (2019)</journal-ref><doi>10.1103/PhysRevD.100.063015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last few years, machine learning techniques, in particular\\nconvolutional neural networks, have been investigated as a method to replace or\\ncomplement traditional matched filtering techniques that are used to detect the\\ngravitational-wave signature of merging black holes. However, to date, these\\nmethods have not yet been successfully applied to the analysis of long\\nstretches of data recorded by the Advanced LIGO and Virgo gravitational-wave\\nobservatories. In this work, we critically examine the use of convolutional\\nneural networks as a tool to search for merging black holes. We identify the\\nstrengths and limitations of this approach, highlight some common pitfalls in\\ntranslating between machine learning and gravitational-wave astronomy, and\\ndiscuss the interdisciplinary challenges. In particular, we explain in detail\\nwhy convolutional neural networks alone cannot be used to claim a statistically\\nsignificant gravitational-wave detection. However, we demonstrate how they can\\nstill be used to rapidly flag the times of potential signals in the data for a\\nmore detailed follow-up. Our convolutional neural network architecture as well\\nas the proposed performance metrics are better suited for this task than a\\nstandard binary classifications scheme. A detailed evaluation of our approach\\non Advanced LIGO data demonstrates the potential of such systems as trigger\\ngenerators. Finally, we sound a note of caution by constructing adversarial\\nexamples, which showcase interesting &quot;failure modes&quot; of our model, where inputs\\nwith no visible resemblance to real gravitational-wave signals are identified\\nas such by the network with high confidence.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.08800</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.08800</id><submitter>Carlton Shepherd</submitter><version version=\"v1\"><date>Thu, 18 Apr 2019 14:21:05 GMT</date><size>2553kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 10 Jul 2019 11:04:40 GMT</date><size>2628kb</size><source_type>D</source_type></version><title>Privacy-Enhancing Context Authentication from Location-Sensitive Data</title><authors>Pradip Mainali, Carlton Shepherd and Fabien A. P. Petitcolas</authors><categories>cs.CR cs.LG</categories><comments>Accepted at the 2nd ACM International Workshop on Behavioral\\n  Authentication for System Security (BASS 2019)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new privacy-enhancing, context-aware user\\nauthentication system, ConSec, which uses a transformation of general\\nlocation-sensitive data, such as GPS location, barometric altitude and noise\\nlevels, collected from the user\\'s device, into a representation based on\\nlocality-sensitive hashing (LSH). The resulting hashes provide a dimensionality\\nreduction of the underlying data, which we leverage to model users\\' behaviour\\nfor authentication using machine learning. We present how ConSec supports\\nlearning from categorical and numerical data, while addressing a number of\\non-device and network-based threats. ConSec is implemented subsequently for the\\nAndroid platform and evaluated using data collected from 35 users, which is\\nfollowed by a security and privacy analysis. We demonstrate that LSH presents a\\nuseful approach for context authentication from location-sensitive data without\\ndirectly utilising plain measurements.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.08824</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.08824</id><submitter>Mathias Ramparison</submitter><version version=\"v1\"><date>Thu, 18 Apr 2019 14:57:46 GMT</date><size>83kb</size></version><version version=\"v2\"><date>Sun, 29 Sep 2019 15:41:34 GMT</date><size>847kb</size><source_type>D</source_type></version><title>Parametric updates in parametric timed automata</title><authors>\\\\\\'Etienne Andr\\\\\\'e, Didier Lime, Mathias Ramparison</authors><categories>cs.FL</categories><comments>This is the extended version of the manuscript of the same name in\\n  the proceedings of the 39th International Conference on Formal Techniques for\\n  Distributed Objects, Components, and Systems (FORTE 2019). This work is\\n  partially supported by the ANR national research program PACS\\n  (ANR-14-CE28-0002), and ERATO HASUO Metamathematics for Systems Design\\n  Project (No. JPMJER1603), JST</comments><journal-ref>Springer LNCS 11535, pages 39-56, 2019</journal-ref><doi>10.1007/978-3-030-21759-4_3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new class of Parametric Timed Automata (PTAs) where we allow\\nclocks to be compared to parameters in guards, as in classic PTAs, but also to\\nbe updated to parameters. We focus here on the EF-emptiness problem: &quot;is the\\nset of parameter valuations for which some given location is reachable in the\\ninstantiated timed automaton empty?&quot;. This problem is well-known to be\\nundecidable for PTAs, and so it is for our extension. Nonetheless, if we update\\nall clocks each time we compare a clock with a parameter and each time we\\nupdate a clock to a parameter, we obtain a syntactic subclass for which we can\\ndecide the EF-emptiness problem and even perform the exact synthesis of the set\\nof rational valuations such that a given location is reachable. To the best of\\nour knowledge, this is the first non-trivial subclass of PTAs, actually even\\nextended with parametric updates, for which this is possible.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.08988</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.08988</id><submitter>Mhashilkar, Parag A.</submitter><version version=\"v1\"><date>Thu, 18 Apr 2019 19:55:10 GMT</date><size>442kb</size></version><title>HEPCloud, an Elastic Hybrid HEP Facility using an Intelligent Decision\\n  Support System</title><authors>Parag Mhashilkar, Mine Altunay, Eileen Berman, David Dagenhart, Stuart\\n  Fuess, Burt Holzman, James Kowalkowski, Dmitry Litvintsev, Qiming Lu,\\n  Alexander Moibenko, Marc Paterno, Panagiotis Spentzouris, Steven Timm,\\n  Anthony Tiradani, Eric Vaandering (Fermilab), John Hover, and Jose Caballero\\n  Bejar (Brookhaven)</authors><categories>cs.DC</categories><comments>8 pages</comments><proxy>Fermilab Proxy</proxy><report-no>FERMILAB-CONF-18-658-CD</report-no><doi>10.1051/epjconf/201921403060</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  HEPCloud is rapidly becoming the primary system for provisioning compute\\nresources for all Fermilab-affiliated experiments. In order to reliably meet\\nthe peak demands of the next generation of High Energy Physics experiments,\\nFermilab must plan to elastically expand its computational capabilities to\\ncover the forecasted need. Commercial cloud and allocation-based High\\nPerformance Computing (HPC) resources both have explicit and implicit costs\\nthat must be considered when deciding when to provision these resources, and at\\nwhich scale. In order to support such provisioning in a manner consistent with\\norganizational business rules and budget constraints, we have developed a\\nmodular intelligent decision support system (IDSS) to aid in the automatic\\nprovisioning of resources spanning multiple cloud providers, multiple HPC\\ncenters, and grid computing federations. In this paper, we discuss the goals\\nand architecture of the HEPCloud Facility, the architecture of the IDSS, and\\nour early experience in using the IDSS for automated facility expansion both at\\nFermi and Brookhaven National Laboratory.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.09077</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.09077</id><submitter>Shijie Wu</submitter><version version=\"v1\"><date>Fri, 19 Apr 2019 04:45:44 GMT</date><size>182kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 17:22:55 GMT</date><size>132kb</size><source_type>D</source_type></version><title>Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT</title><authors>Shijie Wu and Mark Dredze</authors><categories>cs.CL</categories><comments>EMNLP 2019 Camera Ready</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pretrained contextual representation models (Peters et al., 2018; Devlin et\\nal., 2018) have pushed forward the state-of-the-art on many NLP tasks. A new\\nrelease of BERT (Devlin, 2018) includes a model simultaneously pretrained on\\n104 languages with impressive performance for zero-shot cross-lingual transfer\\non a natural language inference task. This paper explores the broader\\ncross-lingual potential of mBERT (multilingual) as a zero shot language\\ntransfer model on 5 NLP tasks covering a total of 39 languages from various\\nlanguage families: NLI, document classification, NER, POS tagging, and\\ndependency parsing. We compare mBERT with the best-published methods for\\nzero-shot cross-lingual transfer and find mBERT competitive on each task.\\nAdditionally, we investigate the most effective strategy for utilizing mBERT in\\nthis manner, determine to what extent mBERT generalizes away from language\\nspecific features, and measure factors that influence cross-lingual transfer.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.09416</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.09416</id><submitter>Thomas Durieux</submitter><version version=\"v1\"><date>Sat, 20 Apr 2019 07:59:27 GMT</date><size>120kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 20:11:05 GMT</date><size>177kb</size><source_type>D</source_type></version><title>An Analysis of 35+ Million Jobs of Travis CI</title><authors>Thomas Durieux, Rui Abreu, Martin Monperrus, Tegawend\\\\\\'e F.\\n  Bissyand\\\\\\'e, Lu\\\\\\'is Cruz</authors><categories>cs.SE</categories><journal-ref>Proceedings of the International Conference on Software\\n  Maintenance and Evolution (ICSME), 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Travis CI handles automatically thousands of builds every day to, amongst\\nother things, provide valuable feedback to thousands of open-source developers.\\nIn this paper, we investigate Travis CI to firstly understand who is using it,\\nand when they start to use it. Secondly, we investigate how the developers use\\nTravis CI and finally, how frequently the developers change the Travis CI\\nconfigurations. We observed during our analysis that the main users of Travis\\nCI are corporate users such as Microsoft. And the programming languages used in\\nTravis CI by those users do not follow the same popularity trend than on\\nGitHub, for example, Python is the most popular language on Travis CI, but it\\nis only the third one on GitHub. We also observe that Travis CI is set up on\\naverage seven days after the creation of the repository and the jobs are still\\nmainly used (60%) to run tests. And finally, we observe that 7.34% of the\\ncommits modify the Travis CI configuration. We share the biggest benchmark of\\nTravis CI jobs (to our knowledge): it contains 35,793,144 jobs from 272,917\\ndifferent GitHub projects.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.09563</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.09563</id><submitter>Himanshu Tyagi</submitter><version version=\"v1\"><date>Sun, 21 Apr 2019 08:07:30 GMT</date><size>156kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 13:29:56 GMT</date><size>159kb</size><source_type>D</source_type></version><title>Communication for Generating Correlation: A Unifying Survey</title><authors>Madhu Sudan, Himanshu Tyagi, Shun Watanabe</authors><categories>cs.IT math.IT</categories><comments>A review article to appear in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of manipulating correlated random variables in a distributed setting\\nhas received attention in the fields of both Information Theory and Computer\\nScience. Often shared correlations can be converted, using a little amount of\\ncommunication, into perfectly shared uniform random variables. Such perfect\\nshared randomness, in turn, enables the solutions of many tasks. Even the\\nreverse conversion of perfectly shared uniform randomness into variables with a\\ndesired form of correlation turns out to be insightful and technically useful.\\nIn this survey article, we describe progress-to-date on such problems and lay\\nout pertinent measures, achievability results, limits of performance, and point\\nto new directions.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.09675</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.09675</id><submitter>Felix Wu</submitter><version version=\"v1\"><date>Sun, 21 Apr 2019 23:08:53 GMT</date><size>410kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 16:52:00 GMT</date><size>1605kb</size><source_type>D</source_type></version><title>BERTScore: Evaluating Text Generation with BERT</title><authors>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, Yoav\\n  Artzi</authors><categories>cs.CL</categories><comments>Code available at https://github.com/Tiiiger/bert_score</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose BERTScore, an automatic evaluation metric for text generation.\\nAnalogously to common metrics, BERTScore computes a similarity score for each\\ntoken in the candidate sentence with each token in the reference sentence.\\nHowever, instead of exact matches, we compute token similarity using contextual\\nembeddings. We evaluate using the outputs of 363 machine translation and image\\ncaptioning systems. BERTScore correlates better with human judgments and\\nprovides stronger model selection performance than existing metrics. Finally,\\nwe use an adversarial paraphrase detection task to show that BERTScore is more\\nrobust to challenging examples when compared to existing metrics.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.09925</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.09925</id><submitter>Irwan Bello</submitter><version version=\"v1\"><date>Mon, 22 Apr 2019 15:31:15 GMT</date><size>831kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 5 Aug 2019 20:10:55 GMT</date><size>846kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 14 Aug 2019 16:38:19 GMT</date><size>847kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Sat, 28 Sep 2019 01:12:10 GMT</date><size>847kb</size><source_type>D</source_type></version><title>Attention Augmented Convolutional Networks</title><authors>Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, Quoc V. Le</authors><categories>cs.CV</categories><comments>ICCV 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional networks have been the paradigm of choice in many computer\\nvision applications. The convolution operation however has a significant\\nweakness in that it only operates on a local neighborhood, thus missing global\\ninformation. Self-attention, on the other hand, has emerged as a recent advance\\nto capture long range interactions, but has mostly been applied to sequence\\nmodeling and generative modeling tasks. In this paper, we consider the use of\\nself-attention for discriminative visual tasks as an alternative to\\nconvolutions. We introduce a novel two-dimensional relative self-attention\\nmechanism that proves competitive in replacing convolutions as a stand-alone\\ncomputational primitive for image classification. We find in control\\nexperiments that the best results are obtained when combining both convolutions\\nand self-attention. We therefore propose to augment convolutional operators\\nwith this self-attention mechanism by concatenating convolutional feature maps\\nwith a set of feature maps produced via self-attention. Extensive experiments\\nshow that Attention Augmentation leads to consistent improvements in image\\nclassification on ImageNet and object detection on COCO across many different\\nmodels and scales, including ResNets and a state-of-the art mobile constrained\\nnetwork, while keeping the number of parameters similar. In particular, our\\nmethod achieves a $1.3\\\\%$ top-1 accuracy improvement on ImageNet classification\\nover a ResNet50 baseline and outperforms other attention mechanisms for images\\nsuch as Squeeze-and-Excitation. It also achieves an improvement of 1.4 mAP in\\nCOCO Object Detection on top of a RetinaNet baseline.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.10089</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.10089</id><submitter>Fernando Gama</submitter><version version=\"v1\"><date>Mon, 22 Apr 2019 23:17:22 GMT</date><size>740kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 12:47:53 GMT</date><size>784kb</size><source_type>D</source_type></version><title>Controllability of Bandlimited Graph Processes Over Random Time Varying\\n  Graphs</title><authors>Fernando Gama, Elvin Isufi, Alejandro Ribeiro, Geert Leus</authors><categories>cs.SY</categories><comments>Submitted to IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Controllability of complex networks arises in many technological problems\\ninvolving social, financial, road, communication, and smart grid networks. In\\nmany practical situations, the underlying topology might change randomly with\\ntime, due to link failures such as changing friendships, road blocks or sensor\\nmalfunctions. Thus, it leads to poorly controlled dynamics if randomness is not\\nproperly accounted for. We consider the problem of controlling the network\\nstate when the topology varies randomly with time. Our problem concerns target\\nstates that are bandlimited over the graph; these are states that have nonzero\\nfrequency content only on a specific graph frequency band. We thus leverage\\ngraph signal processing and exploit the bandlimited model to drive the network\\nstate from a fixed set of control nodes. When controlling the state from a few\\nnodes, we observe that spurious, out-of-band frequency content is created.\\nTherefore, we focus on controlling the network state over the desired frequency\\nband, and then use a graph filter to get rid of the unwanted frequency content.\\nTo account for the topological randomness, we develop the concept of\\ncontrollability in the mean, which consists of driving the expected network\\nstate towards the target state. A detailed mean squared error analysis is\\nperformed to quantify the statistical deviation between the final controlled\\nstate on a particular graph realization and the actual target state. Finally,\\nwe propose different control strategies and evaluate their effectiveness on\\nsynthetic network models and social networks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.10158</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.10158</id><submitter>Sasinee Pruekprasert</submitter><version version=\"v1\"><date>Tue, 23 Apr 2019 05:38:25 GMT</date><size>235kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 22:35:00 GMT</date><size>221kb</size><source_type>D</source_type></version><title>Decision Making for Autonomous Vehicles at Unsignalized Intersection in\\n  Presence of Malicious Vehicles</title><authors>Sasinee Pruekprasert, Xiaoyi Zhang, J\\\\\\'er\\\\\\'emy Dubut, Chao Huang,\\n  Masako Kishida</authors><categories>cs.SY</categories><comments>IEEE Conference on Intelligent Transportation Systems (ITSC), 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the decision making of autonomous vehicles in\\nan unsignalized intersection in presence of malicious vehicles, which are\\nvehicles that do not respect the law by not using the proper rules of the right\\nof way. Each vehicle computes its control input as a Nash equilibrium of a game\\ndetermined by the priority order based on its own belief: each of non-malicious\\nvehicle bases its order on the law, while a malicious one considers itself as\\nhaving priority. To illustrate our method, we provide numerical simulations,\\nwith different scenarios given by different cases of malicious vehicles.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.10276</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.10276</id><submitter>Xiaoshen Song</submitter><version version=\"v1\"><date>Tue, 23 Apr 2019 12:31:50 GMT</date><size>331kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 09:54:24 GMT</date><size>334kb</size><source_type>D</source_type></version><title>Fully-/Partially-Connected Hybrid Beamforming Architectures for mmWave\\n  MU-MIMO</title><authors>Xiaoshen Song, Thomas K\\\\&quot;uhne, Giuseppe Caire</authors><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid digital analog (HDA) beamforming has attracted considerable attention\\nin practical implementation of millimeter wave (mmWave) multiuser\\nmultiple-input multiple-output (MU-MIMO) systems due to the low power\\nconsumption with respect to its fully digital baseband counterpart. The\\nimplementation cost, performance, and power efficiency of HDA beamforming\\ndepends on the level of connectivity and reconfigurability of the analog\\nbeamforming network. In this paper, we investigate the performance of two\\ntypical architectures that can be regarded as extreme cases, namely, the\\nfully-connected (FC) and the one-stream-per-subarray (OSPS) architectures. In\\nthe FC architecture each RF antenna port is connected to all antenna elements\\nof the array, while in the OSPS architecture the RF antenna ports are connected\\nto disjoint subarrays. We jointly consider the initial beam acquisition and\\ndata communication phases, such that the latter takes place by using the beam\\ndirection information obtained by the former. We use the state-of-the-art beam\\nalignment (BA) scheme previously proposed by the authors and consider a family\\nof MU-MIMO precoding schemes well adapted to the beam information extracted\\nfrom the BA phase. We also evaluate the power efficiency of the two HDA\\narchitectures taking into account the power dissipation at different hardware\\ncomponents as well as the power backoff under typical power amplifier\\nconstraints. Numerical results show that the two architectures achieve similar\\nsum spectral efficiency, while the OSPS architecture is advantageous with\\nrespect to the FC case in terms of hardware complexity and power efficiency, at\\nthe sole cost of a slightly longer BA time-to-acquisition due to its reduced\\nbeam angle resolution.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.10614</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.10614</id><submitter>Cole Comfort</submitter><version version=\"v1\"><date>Wed, 24 Apr 2019 02:43:08 GMT</date><size>57kb</size></version><version version=\"v2\"><date>Mon, 30 Sep 2019 22:44:05 GMT</date><size>52kb</size></version><title>Circuit Relations for Real Stabilizers: Towards TOF+H</title><authors>Cole Comfort</authors><categories>quant-ph cs.LO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The real stabilizer fragment of quantum mechanics was shown to have a\\ncomplete axiomatization in terms of the angle-free fragment of the ZX-calculus.\\nThis fragment of the ZX-calculus---although abstractly elegant---is stated in\\nterms of identities, such as spider fusion which generally do not have\\ninterpretations as circuit transformations.\\n  We complete the category CNOT generated by the controlled not gate and the\\ncomputational ancillary bits, presented by circuit relations, to the real\\nstabilizer fragment of quantum mechanics. This is performed first, by adding\\nthe Hadamard gate and the scalar sqrt 2 as generators. We then construct\\ntranslations to and from the angle-free fragment of the ZX-calculus, showing\\nthat they are inverses.\\n  We then discuss how this could potentially lead to a complete axiomatization,\\nin terms of circuit relations, for the approximately universal fragment of\\nquantum mechanics generated by the Toffoli gate, Hadamard gate and\\ncomputational ancillary bits.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.10874</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.10874</id><submitter>Zhaoji Zhang</submitter><version version=\"v1\"><date>Wed, 24 Apr 2019 15:32:05 GMT</date><size>614kb</size><source_type>D</source_type></version><title>Fixed-Symbol Aided Random Access Scheme for Machine-to-Machine\\n  Communications</title><authors>Zhaoji Zhang, Ying Li, Lei Liu, and Wei Hou</authors><categories>cs.IT math.IT</categories><comments>15 pages, 9 figures</comments><journal-ref>IEEE Access, vol. 7, pp. 52913-52928, 2019</journal-ref><doi>10.1109/ACCESS.2019.2912448</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The massiveness of devices in crowded Machine-to-Machine (M2M) communications\\nbrings new challenges to existing random-access (RA) schemes, such as heavy\\nsignaling overhead and severe access collisions. In order to reduce the\\nsignaling overhead, we propose a fixed-symbol aided RA scheme where active\\ndevices access the network in a grant-free method, i.e., data packets are\\ndirectly transmitted in randomly chosen slots. To further address the access\\ncollision which impedes the activity detection, one fixed symbol is inserted\\ninto each transmitted data packet in the proposed scheme. An iterative message\\npassing based activity detection (MP-AD) algorithm is performed upon the\\nreceived signal of this fixed symbol to detect the device activity in each\\nslot. In addition, the deep neural network-aided MP-AD (DNN-MP-AD) algorithm is\\nfurther designed to alleviate the correlation problem of the iterative message\\npassing process. In the DNN-MP-AD algorithm, the iterative message passing\\nprocess is transferred from a factor graph to a DNN. Weights are imposed on the\\nmessages in the DNN and further trained to improve the accuracy of the device\\nactivity detection. Finally, numerical simulations are provided for the\\nthroughput of the proposed RA scheme, the accuracy of the proposed MP-AD\\nalgorithm, as well as the improvement brought by the DNN-MP-AD algorithm.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.11263</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.11263</id><submitter>Mantas Mikaitis</submitter><version version=\"v1\"><date>Thu, 25 Apr 2019 11:26:40 GMT</date><size>709kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 2 Aug 2019 16:20:46 GMT</date><size>648kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 12 Aug 2019 11:38:58 GMT</date><size>647kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Mon, 30 Sep 2019 08:42:45 GMT</date><size>647kb</size><source_type>D</source_type></version><title>Stochastic rounding and reduced-precision fixed-point arithmetic for\\n  solving neural ODEs</title><authors>Michael Hopkins and Mantas Mikaitis and Dave R. Lester and Steve\\n  Furber</authors><categories>cs.DS cs.MS</categories><comments>Submitted to Philosophical Transactions of the Royal Society A</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although double-precision floating-point arithmetic currently dominates\\nhigh-performance computing, there is increasing interest in smaller and simpler\\narithmetic types. The main reasons are potential improvements in energy\\nefficiency and memory footprint and bandwidth. However, simply switching to\\nlower-precision types typically results in increased numerical errors. We\\ninvestigate approaches to improving the accuracy of reduced-precision\\nfixed-point arithmetic types, using examples in an important domain for\\nnumerical computation in neuroscience: the solution of Ordinary Differential\\nEquations (ODEs). The Izhikevich neuron model is used to demonstrate that\\nrounding has an important role in producing accurate spike timings from\\nexplicit ODE solution algorithms. In particular, fixed-point arithmetic with\\nstochastic rounding consistently results in smaller errors compared to single\\nprecision floating-point and fixed-point arithmetic with round-to-nearest\\nacross a range of neuron behaviours and ODE solvers. A computationally much\\ncheaper alternative is also investigated, inspired by the concept of dither\\nthat is a widely understood mechanism for providing resolution below the least\\nsignificant bit (LSB) in digital signal processing. These results will have\\nimplications for the solution of ODEs in other subject areas, and should also\\nbe directly relevant to the huge range of practical problems that are\\nrepresented by Partial Differential Equations (PDEs).\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.11538</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.11538</id><submitter>Shuhang Chen</submitter><version version=\"v1\"><date>Thu, 25 Apr 2019 18:57:49 GMT</date><size>343kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 1 May 2019 18:39:03 GMT</date><size>343kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 30 Sep 2019 16:37:22 GMT</date><size>345kb</size><source_type>D</source_type></version><title>Zap Q-Learning for Optimal Stopping Time Problems</title><authors>Shuhang Chen, Adithya M. Devraj, Ana Bu\\\\v{s}i\\\\\\'c, Sean P. Meyn</authors><categories>cs.SY cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective in this paper is to obtain fast converging reinforcement\\nlearning algorithms to approximate solutions to the problem of discounted cost\\noptimal stopping in an irreducible, uniformly ergodic Markov chain, evolving on\\na compact subset of $\\\\mathbb{R}^n$. We build on the dynamic programming\\napproach taken by Tsitsikilis and Van Roy, wherein they propose a Q-learning\\nalgorithm to estimate the optimal state-action value function, which then\\ndefines an optimal stopping rule. We provide insights as to why the convergence\\nrate of this algorithm can be slow, and propose a fast-converging alternative,\\nthe &quot;Zap-Q-learning&quot; algorithm, designed to achieve optimal rate of\\nconvergence. For the first time, we prove the convergence of the Zap-Q-learning\\nalgorithm under the assumption of linear function approximation setting. We use\\nODE analysis for the proof, and the optimal asymptotic variance property of the\\nalgorithm is reflected via fast convergence in a finance example.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.11812</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.11812</id><submitter>George K. Thiruvathukal</submitter><version version=\"v1\"><date>Fri, 26 Apr 2019 12:52:02 GMT</date><size>2615kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 27 Sep 2019 22:26:39 GMT</date><size>2634kb</size><source_type>D</source_type></version><title>A Benchmarking Study to Evaluate Apache Spark on Large-Scale\\n  Supercomputers</title><authors>George K. Thiruvathukal and Cameron Christensen and Xiaoyong Jin and\\n  Fran\\\\c{c}ois Tessier and Venkatram Vishwanath</authors><categories>cs.DC cs.SE</categories><comments>Submitted to IEEE Cloud 2019</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  As dataset sizes increase, data analysis tasks in high performance computing\\n(HPC) are increasingly dependent on sophisticated dataflows and out-of-core\\nmethods for efficient system utilization. In addition, as HPC systems grow,\\nmemory access and data sharing are becoming performance bottlenecks. Cloud\\ncomputing employs a data processing paradigm typically built on a loosely\\nconnected group of low-cost computing nodes without relying upon shared storage\\nand/or memory. Apache Spark is a popular engine for large-scale data analysis\\nin the cloud, which we have successfully deployed via job submission scripts on\\nproduction clusters.\\n  In this paper, we describe common parallel analysis dataflows for both\\nMessage Passing Interface (MPI) and cloud based applications. We developed an\\neffective benchmark to measure the performance characteristics of these tasks\\nusing both types of systems, specifically comparing MPI/C-based analyses with\\nSpark. The benchmark is a data processing pipeline representative of a typical\\nanalytics framework implemented using map-reduce. In the case of Spark, we also\\nconsider whether language plays a role by writing tests using both Python and\\nScala, a language built on the Java Virtual Machine (JVM). We include\\nperformance results from two large systems at Argonne National Laboratory\\nincluding Theta, a Cray XC40 supercomputer on which our experiments run with\\n65,536 cores (1024 nodes with 64 cores each). The results of our experiments\\nare discussed in the context of their applicability to future HPC\\narchitectures. Beyond understanding performance, our work demonstrates that\\ntechnologies such as Spark, while typically aimed at multi-tenant cloud-based\\nenvironments, show promise for data analysis needs in a traditional\\nclustering/supercomputing environment.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.12058</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.12058</id><submitter>Muhan Zhang</submitter><version version=\"v1\"><date>Fri, 26 Apr 2019 21:58:46 GMT</date><size>2525kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 6 Oct 2019 03:08:54 GMT</date><size>1804kb</size><source_type>D</source_type></version><title>Inductive Matrix Completion Based on Graph Neural Networks</title><authors>Muhan Zhang, Yixin Chen</authors><categories>cs.IR cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an inductive matrix completion model without using side\\ninformation. By factorizing the (rating) matrix into the product of\\nlow-dimensional latent embeddings of rows (users) and columns (items), a\\nmajority of existing matrix completion methods are \\\\textit{transductive}, since\\nthe learned embeddings cannot generalize to unseen rows/columns or to new\\nmatrices. To make matrix completion \\\\textit{inductive}, content (side\\ninformation), such as user\\'s age or movie\\'s genre, has to be used previously.\\nHowever, high-quality content is not always available, and can be hard to\\nextract. Under the extreme setting where not any side information is available\\nother than the matrix to complete, can we still learn an inductive matrix\\ncompletion model? In this paper, we investigate this seemingly impossible\\nproblem and propose an Inductive Graph-based Matrix Completion (IGMC) model\\nwithout using any side information. It trains a graph neural network (GNN)\\nbased purely on local subgraphs around (user, item) pairs generated from the\\nrating matrix and maps these subgraphs to their corresponding ratings. Our\\nmodel achieves highly competitive performance with state-of-the-art\\ntransductive baselines. In addition, since our model is inductive, it can\\ngeneralize to users/items unseen during the training (given that their ratings\\nexist), and can even transfer to new tasks. Our transfer learning experiments\\nshow that a model trained out of the MovieLens dataset can be directly used to\\npredict Douban movie ratings and works surprisingly well.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.12156</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.12156</id><submitter>Arne Meier</submitter><version version=\"v1\"><date>Sat, 27 Apr 2019 13:12:28 GMT</date><size>139kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 09:25:01 GMT</date><size>298kb</size><source_type>D</source_type></version><title>Parameterised Counting Classes: Tail Versus Reductions</title><authors>Anselm Haak and Arne Meier and Om Prakash and Raghavendra Rao B. V</authors><categories>cs.LO cs.CC</categories><comments>extended content, title reflecting new results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stockhusen and Tantau (IPEC 2013) defined the operators paraW- and paraBeta-\\nfor parameterised space complexity classes by allowing bounded nondeterminism\\nwith multiple read and read-once access, respectively. Using these operators,\\nthey characterised the complexity for many parameterisations of natural\\nproblems on graphs. In this article, we study the counting versions of such\\noperators and introduce variants based on tail-nondeterminism, paraW[1]- and\\nparaBetaTail-, in the setting of parameterised logarithmic space. Initially, we\\nexamine closure properties of such classes under the central reductions as well\\nas arithmetic operations. We prove that the closure of the class\\n#paraBetaTail-L under parsimonious parameterised logspace reductions coincides\\nwith #paraBeta-L. We identify natural path counting problems in digraphs that\\nare complete for the newly introduced classes #paraW-L and #paraBeta-L. We\\nstudy the complexity of counting variants of model checking problems for\\nspecific classes of FO-formulas, and find complete versions for #paraBetaTail-L\\nand #paraW[1]-L. Furthermore, we present a counting variant of a parameterised\\nhomomorphism problem, where the input structure is a coloured path, which is\\ncomplete for the class #paraBeta-L. Afterwards, we show that the complexity of\\na parameterised variant of the determinant function is #paraBetaTail-L-hard and\\ncan be written as the difference of two functions in #paraBetaTail-L for 0/1\\nmatrices. Also, we characterise the new complexity classes in terms of\\nbranching programs.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.12200</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.12200</id><submitter>Anmol Sharma</submitter><version version=\"v1\"><date>Sat, 27 Apr 2019 20:15:15 GMT</date><size>1186kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 23 Aug 2019 19:08:43 GMT</date><size>2431kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 2 Oct 2019 00:20:13 GMT</date><size>2513kb</size><source_type>D</source_type></version><title>Missing MRI Pulse Sequence Synthesis using Multi-Modal Generative\\n  Adversarial Network</title><authors>Anmol Sharma, Ghassan Hamarneh</authors><categories>eess.IV cs.AI cs.CV cs.LG stat.ML</categories><comments>Accepted for publication in IEEE Transactions on Medical Imaging</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Magnetic resonance imaging (MRI) is being increasingly utilized to assess,\\ndiagnose, and plan treatment for a variety of diseases. The ability to\\nvisualize tissue in varied contrasts in the form of MR pulse sequences in a\\nsingle scan provides valuable insights to physicians, as well as enabling\\nautomated systems performing downstream analysis. However many issues like\\nprohibitive scan time, image corruption, different acquisition protocols, or\\nallergies to certain contrast materials may hinder the process of acquiring\\nmultiple sequences for a patient. This poses challenges to both physicians and\\nautomated systems since complementary information provided by the missing\\nsequences is lost. In this paper, we propose a variant of generative\\nadversarial network (GAN) capable of leveraging redundant information contained\\nwithin multiple available sequences in order to generate one or more missing\\nsequences for a patient scan. The proposed network is designed as a\\nmulti-input, multi-output network which combines information from all the\\navailable pulse sequences, implicitly infers which sequences are missing, and\\nsynthesizes the missing ones in a single forward pass. We demonstrate and\\nvalidate our method on two brain MRI datasets each with four sequences, and\\nshow the applicability of the proposed method in simultaneously synthesizing\\nall missing sequences in any possible scenario where either one, two, or three\\nof the four sequences may be missing. We compare our approach with competing\\nunimodal and multi-modal methods, and show that we outperform both\\nquantitatively and qualitatively.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.12348</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.12348</id><submitter>Lvbang Tang</submitter><version version=\"v1\"><date>Sun, 28 Apr 2019 17:13:49 GMT</date><size>2441kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 14 Aug 2019 15:02:06 GMT</date><size>2441kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 27 Sep 2019 05:31:16 GMT</date><size>2161kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Mon, 30 Sep 2019 06:43:19 GMT</date><size>2161kb</size><source_type>D</source_type></version><title>Real-time Trajectory Generation for Quadrotors using B-spline based\\n  Non-uniform Kinodynamic Search</title><authors>Lvbang Tang, Hesheng Wang and Peng Li</authors><categories>cs.RO</categories><comments>7 pages,6 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a time-efficient approach to generate safe, smooth\\nand dynamically feasible trajectories for quadrotors in obstacle-cluttered\\nenvironment. By using the uniform B-spline to represent trajectories, we\\ntransform the trajectory planning to a graph-search problem of B-spline control\\npoints in discretized space. Highly strict convex hull property of B-spline is\\nderived to guarantee the dynamical feasibility of the entire trajectory. A\\nnovel non-uniform kinodynamic search strategy is adopted, and the step length\\nis dynamically adjusted during the search process according to the Euclidean\\nsigned distance field (ESDF), making the trajectory achieve reasonable\\ntime-allocation and be away from obstacles. Non-static initial and goal states\\nare allowed, therefore it can be used for online local replanning as well as\\nglobal planning. Extensive simulation and hardware experiments show that our\\nmethod achieves higher performance comparing with the state-of-the-art method.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.12728</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.12728</id><submitter>Alessio Mazzetto</submitter><version version=\"v1\"><date>Mon, 29 Apr 2019 14:12:15 GMT</date><size>27kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 19:58:38 GMT</date><size>511kb</size></version><title>Accurate MapReduce Algorithms for $k$-median and $k$-means in General\\n  Metric Spaces</title><authors>Alessio Mazzetto, Andrea Pietracaprina, Geppino Pucci</authors><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Center-based clustering is a fundamental primitive for data analysis and\\nbecomes very challenging for large datasets. In this paper, we focus on the\\npopular $k$-median and $k$-means variants which, given a set $P$ of points from\\na metric space and a parameter $k&lt;|P|$, require to identify a set $S$ of $k$\\ncenters minimizing, respectively, the sum of the distances and of the squared\\ndistances of all points in $P$ from their closest centers. Our specific focus\\nis on general metric spaces, for which it is reasonable to require that the\\ncenters belong to the input set (i.e., $S \\\\subseteq P$). We present\\ncoreset-based 3-round distributed approximation algorithms for the above\\nproblems using the MapReduce computational model. The algorithms are rather\\nsimple and obliviously adapt to the intrinsic complexity of the dataset,\\ncaptured by the doubling dimension $D$ of the metric space. Remarkably, the\\nalgorithms attain approximation ratios that can be made arbitrarily close to\\nthose achievable by the best known polynomial-time sequential approximations,\\nand they are very space efficient for small $D$, requiring local memory sizes\\nsubstantially sublinear in the input size. To the best of our knowledge, no\\nprevious distributed approaches were able to attain similar quality-performance\\nguarantees in general metric spaces.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.12848</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.12848</id><submitter>Qizhe Xie</submitter><version version=\"v1\"><date>Mon, 29 Apr 2019 17:56:59 GMT</date><size>82kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 10 Jul 2019 17:53:48 GMT</date><size>566kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 26 Sep 2019 15:32:11 GMT</date><size>1094kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Mon, 30 Sep 2019 15:40:40 GMT</date><size>1094kb</size><source_type>D</source_type></version><title>Unsupervised Data Augmentation for Consistency Training</title><authors>Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, Quoc V. Le</authors><categories>cs.LG cs.AI cs.CL cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semi-supervised learning lately has shown much promise in improving deep\\nlearning models when labeled data is scarce. Common among recent approaches is\\nthe use of consistency training on a large amount of unlabeled data to\\nconstrain model predictions to be invariant to input noise. In this work, we\\npresent a new perspective on how to effectively noise unlabeled examples and\\nargue that the quality of noising, specifically those produced by advanced data\\naugmentation methods, plays a crucial role in semi-supervised learning. By\\nsubstituting simple noising operations with advanced data augmentation methods,\\nour method brings substantial improvements across six language and three vision\\ntasks under the same consistency training framework. On the IMDb text\\nclassification dataset, with only 20 labeled examples, our method achieves an\\nerror rate of 4.20, outperforming the state-of-the-art model trained on 25,000\\nlabeled examples. On a standard semi-supervised learning benchmark, CIFAR-10,\\nour method outperforms all previous approaches and achieves an error rate of\\n2.7% with only 4,000 examples, nearly matching the performance of models\\ntrained on 50,000 labeled examples. Our method also combines well with transfer\\nlearning, e.g., when finetuning from BERT, and yields improvements in high-data\\nregime, such as ImageNet, whether when there is only 10% labeled data or when a\\nfull labeled set with 1.3M extra unlabeled examples is used. Code is available\\nat https://github.com/google-research/uda.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.12919</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.12919</id><submitter>Mike Thelwall Prof</submitter><version version=\"v1\"><date>Mon, 29 Apr 2019 19:29:02 GMT</date><size>823kb</size></version><version version=\"v2\"><date>Wed, 2 Oct 2019 06:04:27 GMT</date><size>905kb</size></version><title>Female citation impact superiority 1996-2018 in six out of seven\\n  English-speaking nations</title><authors>Mike Thelwall</authors><categories>cs.DL</categories><comments>Thelwall, M. (in press). Female citation impact superiority 1996-2018\\n  in six out of seven English-speaking nations. Journal of the Association for\\n  Information Science and Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efforts to combat continuing gender inequalities in academia need to be\\ninformed by evidence about where differences occur. Citations are relevant as\\npotential evidence in appointment and promotion decisions, but it is unclear\\nwhether there have been historical gender differences in average citation\\nimpact that might explain the current shortfall of senior female academics.\\nThis study investigates the evolution of gender differences in citation impact\\n1996-2018 for six million articles from seven large English-speaking nations:\\nAustralia, Canada, Ireland, Jamaica, New Zealand, UK, and the USA. The results\\nshow that a small female citation advantage has been the norm over time for all\\nthese countries except the USA, where there has been no practical difference.\\nThe female citation advantage is largest, and statistically significant in most\\nyears, for Australia and the UK. This suggests that any academic bias against\\nciting female authored research cannot explain current employment inequalities.\\nNevertheless, comparisons using recent citation data, or avoiding it\\naltogether, during appointments or promotion may disadvantage females in some\\ncountries by underestimating the likely impact of their work, especially in the\\nlong term.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1904.13088</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1904.13088</id><submitter>Kaan Gen\\\\c{c}</submitter><version version=\"v1\"><date>Tue, 30 Apr 2019 07:46:51 GMT</date><size>144kb</size></version><version version=\"v2\"><date>Tue, 28 May 2019 19:54:33 GMT</date><size>143kb</size></version><version version=\"v3\"><date>Wed, 2 Oct 2019 18:42:54 GMT</date><size>150kb</size></version><title>Dependence-Aware, Unbounded Sound Predictive Race Detection</title><authors>Kaan Gen\\\\c{c}, Jake Roemer, Yufan Xu, Michael D. Bond (Ohio State\\n  University)</authors><categories>cs.PL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Data races are a real problem for parallel software, yet hard to detect.\\nSound predictive analysis observes a program execution and detects data races\\nthat exist in some other, unobserved execution. However, existing predictive\\nanalyses miss races because they do not scale to full program executions or do\\nnot precisely incorporate data and control dependence.\\n  This paper introduces two novel, sound predictive approaches that incorporate\\ndata and control dependence and handle full program executions. An evaluation\\nusing real, large Java programs shows that these approaches detect more data\\nraces than the closest related approaches, thus advancing the state of the art\\nin sound predictive race detection.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.00262</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.00262</id><submitter>Ricardo Ruiz Baier I</submitter><version version=\"v1\"><date>Wed, 1 May 2019 11:02:19 GMT</date><size>2909kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 20 May 2019 09:57:06 GMT</date><size>2910kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 7 Aug 2019 19:38:58 GMT</date><size>3718kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Tue, 8 Oct 2019 08:45:09 GMT</date><size>3718kb</size><source_type>D</source_type></version><title>An orthotropic electro-viscoelastic model for the heart with\\n  stress-assisted diffusion</title><authors>Adrienne Propp, Alessio Gizzi, Francesc Levrero-Florencio, Ricardo\\n  Ruiz-Baier</authors><categories>math.NA cs.NA q-bio.TO</categories><msc-class>65M60, 92C10, 74S05, 74F99, 74D10</msc-class><journal-ref>Biomechanics and Modeling in Mechanobiology, 2019</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose and analyse the properties of a new class of models for the\\nelectromechanics of cardiac tissue. The set of governing equations consists of\\nnonlinear elasticity using a viscoelastic and orthotropic exponential\\nconstitutive law (this is so for both active stress and active strain\\nformulations of active mechanics) coupled with a four-variable phenomenological\\nmodel for human cardiac cell electrophysiology, which produces an accurate\\ndescription of the action potential. The conductivities in the model of\\nelectric propagation are modified according to stress, inducing an additional\\ndegree of nonlinearity and anisotropy in the coupling mechanisms; and the\\nactivation model assumes a simplified stretch-calcium interaction generating\\nactive tension or active strain. The influence of the new terms in the\\nelectromechanical model is evaluated through a sensitivity analysis, and we\\nprovide numerical validation through a set of computational tests using a novel\\nmixed-primal finite element scheme.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.00502</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.00502</id><submitter>David Paulius</submitter><version version=\"v1\"><date>Wed, 1 May 2019 21:18:43 GMT</date><size>2953kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 04:29:36 GMT</date><size>2954kb</size><source_type>D</source_type></version><title>A Weighted Functional Object-Oriented Network for Task Planning</title><authors>David Paulius, Kelvin Sheng Pei Dong and Yu Sun</authors><categories>cs.RO cs.AI</categories><comments>Submitted to RA-L / ICRA 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prior to this work, we introduced the functional object-oriented network\\n(FOON) as a graphical knowledge representation for manipulations that can be\\nperformed by domestic robots. However, up to this point, we did not account for\\nreal robot task planning with FOON due to the difficulty in robots performing\\ncertain manipulations on its own due to physical limitations. We therefore\\npropose human-robot collaboration (HRC) as a solution to robotic programming\\nwith FOON. The knowledge retrieval procedure, used for acquiring a sequence\\nthat solves a given problem, is modified to include weights that reflect the\\nrobot\\'s chance of successfully executing motions. To make it easier for the\\nrobot, a human can assist to the minimal extent needed to perform the activity\\nto completion by delegating those actions with low success rates to the human\\nto do. From our experiments, we show that tasks can be completed successfully\\nwith the aid of the assistant and instruction from the robot while minimizing\\nthe effort needed from the human.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.00919</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.00919</id><submitter>Mohamed Baza</submitter><version version=\"v1\"><date>Thu, 2 May 2019 18:14:24 GMT</date><size>5969kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 5 Oct 2019 17:39:51 GMT</date><size>3060kb</size><source_type>D</source_type></version><title>Mimic Learning to Generate a Shareable Network Intrusion Detection Model</title><authors>Ahmed Shafee, Mohamed Baza, Douglas A. Talbert, Mostafa M. Fouda,\\n  Mahmoud Nabil, Mohamed Mahmoud</authors><categories>cs.CR cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purveyors of malicious network attacks continue to increase the complexity\\nand the sophistication of their techniques, and their ability to evade\\ndetection continues to improve as well. Hence, intrusion detection systems must\\nalso evolve to meet these increasingly challenging threats. Machine learning is\\noften used to support this needed improvement. However, training a good\\nprediction model can require a large set of labelled training data. Such\\ndatasets are difficult to obtain because privacy concerns prevent the majority\\nof intrusion detection agencies from sharing their sensitive data. In this\\npaper, we propose the use of mimic learning to enable the transfer of intrusion\\ndetection knowledge through a teacher model trained on private data to a\\nstudent model. This student model provides a mean of publicly sharing knowledge\\nextracted from private data without sharing the data itself. Our results\\nconfirm that the proposed scheme can produce a student intrusion detection\\nmodel that mimics the teacher model without requiring access to the original\\ndataset.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.00966</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.00966</id><submitter>Sahand Sharifzadeh</submitter><version version=\"v1\"><date>Thu, 2 May 2019 21:14:35 GMT</date><size>2245kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 13:33:27 GMT</date><size>6656kb</size><source_type>D</source_type></version><title>Improving Visual Relation Detection using Depth Maps</title><authors>Sahand Sharifzadeh, Sina Moayed Baharlou, Max Berrendorf, Rajat Koner,\\n  Volker Tresp</authors><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State of the art visual relation detection methods mostly rely on object\\ninformation extracted from RGB images such as predicted class probabilities, 2D\\nbounding boxes and feature maps. In this paper, we argue that the 3D positions\\nof objects in space can provide additional valuable information about object\\nrelations. This information helps not only to detect spatial relations, such as\\nstanding behind, but also non-spatial relations, such as holding. Since 3D\\ninformation of a scene is not easily accessible, we propose incorporating a\\npre-trained RGB-to-Depth model within visual relation detection frameworks. We\\ndiscuss different feature extraction strategies from depth maps and show their\\ncritical role in relation detection. Our experiments confirm that the\\nperformance of state-of-the-art visual relation detection approaches can\\nsignificantly be improved by utilizing depth map information.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.01296</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.01296</id><submitter>Nicholas Rhinehart</submitter><version version=\"v1\"><date>Fri, 3 May 2019 17:54:09 GMT</date><size>8267kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 7 May 2019 19:51:38 GMT</date><size>8264kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 30 Sep 2019 16:45:21 GMT</date><size>9253kb</size><source_type>D</source_type></version><title>PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings</title><authors>Nicholas Rhinehart, Rowan McAllister, Kris Kitani, Sergey Levine</authors><categories>cs.CV cs.AI cs.LG cs.RO stat.ML</categories><comments>To appear at the IEEE International Conference on Computer Vision\\n  (ICCV 2019). Website: https://sites.google.com/view/precog</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For autonomous vehicles (AVs) to behave appropriately on roads populated by\\nhuman-driven vehicles, they must be able to reason about the uncertain\\nintentions and decisions of other drivers from rich perceptual information.\\nTowards these capabilities, we present a probabilistic forecasting model of\\nfuture interactions between a variable number of agents. We perform both\\nstandard forecasting and the novel task of conditional forecasting, which\\nreasons about how all agents will likely respond to the goal of a controlled\\nagent (here, the AV). We train models on real and simulated data to forecast\\nvehicle trajectories given past positions and LIDAR. Our evaluation shows that\\nour model is substantially more accurate in multi-agent driving scenarios\\ncompared to existing state-of-the-art. Beyond its general ability to perform\\nconditional forecasting queries, we show that our model\\'s predictions of all\\nagents improve when conditioned on knowledge of the AV\\'s goal, further\\nillustrating its capability to model agent interactions.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.01373</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.01373</id><submitter>Kobbi Nissim</submitter><version version=\"v1\"><date>Fri, 3 May 2019 22:33:16 GMT</date><size>62kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 21:49:48 GMT</date><size>63kb</size><source_type>D</source_type></version><title>Exploring Differential Obliviousness</title><authors>Amos Beimel, Kobbi Nissim, Mohammad Zaheri</authors><categories>cs.CR cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent paper Chan et al. [SODA \\'19] proposed a relaxation of the notion\\nof (full) memory obliviousness, which was introduced by Goldreich and Ostrovsky\\n[J. ACM \\'96] and extensively researched by cryptographers. The new notion,\\ndifferential obliviousness, requires that any two neighboring inputs exhibit\\nsimilar memory access patterns, where the similarity requirement is that of\\ndifferential privacy. Chan et al. demonstrated that differential obliviousness\\nallows achieving improved efficiency for several algorithmic tasks, including\\nsorting, merging of sorted lists, and range query data structures.\\n  In this work, we continue the exploration and mapping of differential\\nobliviousness, focusing on algorithms that do not necessarily examine all their\\ninput. This choice is motivated by the fact that the existence of logarithmic\\noverhead ORAM protocols implies that differential obliviousness can yield at\\nmost a logarithmic improvement in efficiency for computations that need to\\nexamine all their input. In particular, we explore property testing, where we\\nshow that differential obliviousness yields an almost linear improvement in\\noverhead in the dense graph model, and at most quadratic improvement in the\\nbounded degree model. We also explore tasks where a non-oblivious algorithm\\nwould need to explore different portions of the input, where the latter would\\ndepend on the input itself, and where we show that such a behavior can be\\nmaintained under differential obliviousness, but not under full obliviousness.\\nOur examples suggest that there would be benefits in further exploring which\\nclass of computational tasks are amenable to differential obliviousness.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.01422</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.01422</id><submitter>Yushu Chen</submitter><version version=\"v1\"><date>Sat, 4 May 2019 03:39:30 GMT</date><size>902kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 14 May 2019 16:33:27 GMT</date><size>902kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 15 May 2019 06:02:12 GMT</date><size>0kb</size><source_type>I</source_type></version><version version=\"v4\"><date>Thu, 23 May 2019 18:19:31 GMT</date><size>903kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Sun, 29 Sep 2019 12:55:07 GMT</date><size>926kb</size><source_type>D</source_type></version><title>NAMSG: An Efficient Method For Training Neural Networks</title><authors>Yushu Chen, Hao Jing, Wenlai Zhao, Zhiqiang Liu, Liang Qiao, Wei Xue,\\n  Haohuan Fu, Guangwen Yang</authors><categories>cs.LG math.OC stat.ML</categories><comments>17 pages, 3 figures. The performance is improved significantly\\n  compared with former versions. In training logistic regression on MNIST and\\n  Resnet-20 on CIFAR10, the methods proposed converge 1 time faster than ADAM.\\n  In addition, a $O(log(T))$ regret bound for strongly convex functions is\\n  porposed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce NAMSG, an adaptive first-order algorithm for training neural\\nnetworks. The method is efficient in computation and memory, and is\\nstraightforward to implement. It computes the gradients at configurable remote\\nobservation points, in order to expedite the convergence by adjusting the step\\nsize for directions with different curvatures in the stochastic setting. It\\nalso scales the updating vector elementwise by a nonincreasing preconditioner\\nto take the advantages of AMSGRAD. We analyze the convergence properties for\\nboth convex and nonconvex problems by modeling the training process as a\\ndynamic system, and provide a strategy to select the observation factor without\\ngrid search. A data-dependent regret bound is proposed to guarantee the\\nconvergence in the convex setting. The method can further achieve a $O(log(T))$\\nregret bound for strongly convex functions. Experiments demonstrate that NAMSG\\nworks well in practical problems and compares favorably to popular adaptive\\nmethods, such as ADAM, NADAM, and AMSGRAD.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.01614</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.01614</id><submitter>Farhana Sultana</submitter><version version=\"v1\"><date>Sun, 5 May 2019 05:45:21 GMT</date><size>1205kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 10 May 2019 15:50:21 GMT</date><size>2527kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 1 Oct 2019 10:27:07 GMT</date><size>1052kb</size><source_type>D</source_type></version><title>A Review of Object Detection Models based on Convolutional Neural\\n  Network</title><authors>F. Sultana, A. Sufian, P. Dutta</authors><categories>cs.CV</categories><comments>17 pages, 11 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Neural Network (CNN) has become the state-of-the-art for object\\ndetection in image task. In this chapter, we have explained different\\nstate-of-the-art CNN based object detection models. We have made this review\\nwith categorization those detection models according to two different\\napproaches: two-stage approach and one-stage approach. Through this chapter, it\\nhas shown advancements in object detection models from R-CNN to latest\\nRefineDet. It has also discussed the model description and training details of\\neach model. Here, we have also drawn a comparison among those models.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.02113</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.02113</id><submitter>Daniel Riley</submitter><version version=\"v1\"><date>Mon, 6 May 2019 15:52:25 GMT</date><size>392kb</size><source_type>D</source_type></version><title>Multi-threaded Output in CMS using ROOT</title><authors>Daniel Riley, Christopher Jones</authors><categories>cs.DC</categories><comments>Submitted to CHEP 2018 - 23rd International Conference on Computing\\n  in High Energy and Nuclear Physics; 6 pages, 4 figures, uses webofc class</comments><doi>10.1051/epjconf/201921402016</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  CMS has worked aggressively to make use of multi-core architectures,\\nroutinely running 4- to 8-core production jobs in 2017. The primary impediment\\nto efficiently scaling beyond 8 cores has been our ROOT-based output module,\\nwhich has been necessarily single threaded. In this paper we explore the\\nchanges made to the CMS framework and our ROOT output module to overcome the\\nprevious scaling limits, using two new ROOT features: the\\n\\\\texttt{TBufferMerger} asynchronous file merger, and Implicit Multi-Threading.\\nWe examine the architecture of the new parallel output module, the specific\\naccommodations and modifications that were made to ensure compatibility with\\nthe CMS framework scheduler, and the performance characteristics of the new\\noutput module.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.02145</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.02145</id><submitter>S\\\\\\'ergio Medeiros</submitter><version version=\"v1\"><date>Mon, 6 May 2019 16:56:35 GMT</date><size>29kb</size></version><version version=\"v2\"><date>Tue, 1 Oct 2019 16:39:16 GMT</date><size>37kb</size></version><title>Automatic Syntax Error Reporting and Recovery in Parsing Expression\\n  Grammars</title><authors>S\\\\\\'ergio Queiroz de Medeiros, Gilney de Azevedo Alvez Junior, Fabio\\n  Mascarenhas</authors><categories>cs.PL cs.FL</categories><acm-class>D.3.4; D.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Error recovery is an essential feature for a parser that should be plugged in\\nIntegrated Development Environments (IDEs), which must build Abstract Syntax\\nTrees (ASTs) even for syntactically invalid programs in order to offer features\\nsuch as automated refactoring and code completion.\\n  Parsing Expressions Grammars (PEGs) are a formalism that naturally describes\\nrecursive top-down parsers using a restricted form of backtracking. Labeled\\nfailures are a conservative extension of PEGs that adds an error reporting\\nmechanism for PEG parsers, and these labels can also be associated with\\nrecovery expressions to provide an error recovery mechanism. These expressions\\ncan use the full expressivity of PEGs to recover from syntactic errors.\\n  Manually annotating a large grammar with labels and recovery expressions can\\nbe difficult. In this work, we present two approaches, Standard and Unique, to\\nautomatically annotate a PEG with labels, and to build their corresponding\\nrecovery expressions. The Standard approach annotates a grammar in a way\\nsimilar to manual annotation, but it may insert labels incorrectly, while the\\nUnique approach is more conservative to annotate a grammar and does not insert\\nlabels incorrectly.\\n  We evaluate both approaches by using them to generate error recovering\\nparsers for four programming languages: Titan, C, Pascal and Java. In our\\nevaluation, the parsers produced using the Standard approach, after a manual\\nintervention to remove the labels incorrectly added, gave an acceptable\\nrecovery for at least 70% of the files in each language. By it turn, the\\nacceptable recovery rate of the parsers produced via the Unique approach,\\nwithout the need of manual intervention, ranged from 41% to 76%.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.02463</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.02463</id><submitter>Isaac Dunn</submitter><version version=\"v1\"><date>Tue, 7 May 2019 10:54:43 GMT</date><size>2936kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 12:43:55 GMT</date><size>4879kb</size><source_type>D</source_type></version><title>Adaptive Generation of Unrestricted Adversarial Inputs</title><authors>Isaac Dunn, Hadrien Pouget, Tom Melham, Daniel Kroening</authors><categories>cs.LG cs.CR cs.CV stat.ML</categories><comments>Updated to include new results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural networks are vulnerable to adversarially-constructed perturbations of\\ntheir inputs. Most research so far has considered perturbations of a fixed\\nmagnitude under some $l_p$ norm. Although studying these attacks is valuable,\\nthere has been increasing interest in the construction of (and robustness to)\\nunrestricted attacks, which are not constrained to a small and rather\\nartificial subset of all possible adversarial inputs. We introduce a novel\\nalgorithm for generating such unrestricted adversarial inputs which, unlike\\nprior work, is adaptive: it is able to tune its attacks to the classifier being\\ntargeted. It also offers a 400-2,000x speedup over the existing state of the\\nart. We demonstrate our approach by generating unrestricted adversarial inputs\\nthat fool classifiers robust to perturbation-based attacks. We also show that,\\nby virtue of being adaptive and unrestricted, our attack is able to defeat\\nadversarial training against it.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.02791</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.02791</id><submitter>Jonathan Mailoa</submitter><version version=\"v1\"><date>Tue, 7 May 2019 19:54:59 GMT</date><size>2218kb</size></version><title>Fast Neural Network Approach for Direct Covariant Forces Prediction in\\n  Complex Multi-Element Extended Systems</title><authors>Jonathan P. Mailoa, Mordechai Kornbluth, Simon L. Batzner, Georgy\\n  Samsonidze, Stephen T. Lam, Chris Ablitt, Nicola Molinari, Boris Kozinsky</authors><categories>physics.comp-ph cs.NE</categories><journal-ref>Nature Machine Intelligence 1 (2019)</journal-ref><doi>10.1038/s42256-019-0098-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural network force field (NNFF) is a method for performing regression on\\natomic structure-force relationships, bypassing expensive quantum mechanics\\ncalculation which prevents the execution of long ab-initio quality molecular\\ndynamics simulations. However, most NNFF methods for complex multi-element\\natomic systems indirectly predict atomic force vectors by exploiting just\\natomic structure rotation-invariant features and the network-feature spatial\\nderivatives which are computationally expensive. We develop a staggered NNFF\\narchitecture exploiting both rotation-invariant and covariant features\\nseparately to directly predict atomic force vectors without using spatial\\nderivatives, thereby reducing expensive structural feature calculation by\\n~180-480x. This acceleration enables us to develop NNFF which directly predicts\\natomic forces in complex ternary and quaternary-element extended systems\\ncomprised of long polymer chains, amorphous oxide, and surface chemical\\nreactions. The staggered rotation-invariant-covariant architecture described\\nhere can also directly predict complex covariant vector outputs from local\\nphysical structures in domains beyond computational material science.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.03072</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.03072</id><submitter>Christoph L\\\\&quot;uscher</submitter><version version=\"v1\"><date>Wed, 8 May 2019 13:57:28 GMT</date><size>31kb</size></version><version version=\"v2\"><date>Wed, 3 Jul 2019 09:34:33 GMT</date><size>31kb</size></version><version version=\"v3\"><date>Thu, 25 Jul 2019 15:49:22 GMT</date><size>31kb</size></version><title>RWTH ASR Systems for LibriSpeech: Hybrid vs Attention -- w/o Data\\n  Augmentation</title><authors>Christoph L\\\\&quot;uscher, Eugen Beck, Kazuki Irie, Markus Kitza, Wilfried\\n  Michel, Albert Zeyer, Ralf Schl\\\\&quot;uter, Hermann Ney</authors><categories>cs.CL cs.SD eess.AS</categories><comments>Proceedings of INTERSPEECH 2019</comments><doi>10.21437/Interspeech.2019-1780</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present state-of-the-art automatic speech recognition (ASR) systems\\nemploying a standard hybrid DNN/HMM architecture compared to an attention-based\\nencoder-decoder design for the LibriSpeech task. Detailed descriptions of the\\nsystem development, including model design, pretraining schemes, training\\nschedules, and optimization approaches are provided for both system\\narchitectures. Both hybrid DNN/HMM and attention-based systems employ\\nbi-directional LSTMs for acoustic modeling/encoding. For language modeling, we\\nemploy both LSTM and Transformer based architectures. All our systems are built\\nusing RWTHs open-source toolkits RASR and RETURNN. To the best knowledge of the\\nauthors, the results obtained when training on the full LibriSpeech training\\nset, are the best published currently, both for the hybrid DNN/HMM and the\\nattention-based systems. Our single hybrid system even outperforms previous\\nresults obtained from combining eight single systems. Our comparison shows that\\non the LibriSpeech 960h task, the hybrid DNN/HMM system outperforms the\\nattention-based system by 15% relative on the clean and 40% relative on the\\nother test sets in terms of word error rate. Moreover, experiments on a reduced\\n100h-subset of the LibriSpeech training corpus even show a more pronounced\\nmargin between the hybrid DNN/HMM and attention-based architectures.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.03353</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.03353</id><submitter>Ioannis Panageas</submitter><version version=\"v1\"><date>Wed, 8 May 2019 21:11:50 GMT</date><size>49kb</size></version><version version=\"v2\"><date>Tue, 8 Oct 2019 10:26:36 GMT</date><size>35kb</size></version><title>Regression from Dependent Observations</title><authors>Constantinos Daskalakis, Nishanth Dikkala, Ioannis Panageas</authors><categories>cs.LG math.ST stat.ML stat.TH</categories><comments>33 pages, in proceedings of STOC 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The standard linear and logistic regression models assume that the response\\nvariables are independent, but share the same linear relationship to their\\ncorresponding vectors of covariates. The assumption that the response variables\\nare independent is, however, too strong. In many applications, these responses\\nare collected on nodes of a network, or some spatial or temporal domain, and\\nare dependent. Examples abound in financial and meteorological applications,\\nand dependencies naturally arise in social networks through peer effects.\\nRegression with dependent responses has thus received a lot of attention in the\\nStatistics and Economics literature, but there are no strong consistency\\nresults unless multiple independent samples of the vectors of dependent\\nresponses can be collected from these models. We present computationally and\\nstatistically efficient methods for linear and logistic regression models when\\nthe response variables are dependent on a network. Given one sample from a\\nnetworked linear or logistic regression model and under mild assumptions, we\\nprove strong consistency results for recovering the vector of coefficients and\\nthe strength of the dependencies, recovering the rates of standard regression\\nunder independent observations. We use projected gradient descent on the\\nnegative log-likelihood, or negative log-pseudolikelihood, and establish their\\nstrong convexity and consistency using concentration of measure for dependent\\nrandom variables.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.03406</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.03406</id><submitter>Francisco Sahli Costabal</submitter><version version=\"v1\"><date>Thu, 9 May 2019 01:52:47 GMT</date><size>7632kb</size><source_type>D</source_type></version><title>Multi-fidelity classification using Gaussian processes: accelerating the\\n  prediction of large-scale computational models</title><authors>Francisco Sahli Costabal, Paris Perdikaris, Ellen Kuhl and Daniel E.\\n  Hurtado</authors><categories>cs.LG stat.ML</categories><doi>10.1016/j.cma.2019.112602</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning techniques typically rely on large datasets to create\\naccurate classifiers. However, there are situations when data is scarce and\\nexpensive to acquire. This is the case of studies that rely on state-of-the-art\\ncomputational models which typically take days to run, thus hindering the\\npotential of machine learning tools. In this work, we present a novel\\nclassifier that takes advantage of lower fidelity models and inexpensive\\napproximations to predict the binary output of expensive computer simulations.\\nWe postulate an autoregressive model between the different levels of fidelity\\nwith Gaussian process priors. We adopt a fully Bayesian treatment for the\\nhyper-parameters and use Markov Chain Mont Carlo samplers. We take advantage of\\nthe probabilistic nature of the classifier to implement active learning\\nstrategies. We also introduce a sparse approximation to enhance the ability of\\nthemulti-fidelity classifier to handle large datasets. We test these\\nmulti-fidelity classifiers against their single-fidelity counterpart with\\nsynthetic data, showing a median computational cost reduction of 23% for a\\ntarget accuracy of 90%. In an application to cardiac electrophysiology, the\\nmulti-fidelity classifier achieves an F1 score, the harmonic mean of precision\\nand recall, of 99.6% compared to 74.1% of a single-fidelity classifier when\\nboth are trained with 50 samples. In general, our results show that the\\nmulti-fidelity classifiers outperform their single-fidelity counterpart in\\nterms of accuracy in all cases. We envision that this new tool will enable\\nresearchers to study classification problems that would otherwise be\\nprohibitively expensive. Source code is available at\\nhttps://github.com/fsahli/MFclass.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.03410</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.03410</id><submitter>Jonathan Scarlett</submitter><version version=\"v1\"><date>Thu, 9 May 2019 02:10:17 GMT</date><size>85kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 11 May 2019 01:13:51 GMT</date><size>85kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sun, 6 Oct 2019 11:02:45 GMT</date><size>174kb</size></version><title>Learning Erd\\\\H{o}s-R\\\\\\'enyi Random Graphs via Edge Detecting Queries</title><authors>Zihan Li, Matthias Fresacher, Jonathan Scarlett</authors><categories>cs.IT cs.DM cs.LG math.IT math.PR stat.ML</categories><comments>NeurIPS 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of learning an unknown graph via\\nqueries on groups of nodes, with the result indicating whether or not at least\\none edge is present among those nodes. While learning arbitrary graphs with $n$\\nnodes and $k$ edges is known to be hard in the sense of requiring $\\\\Omega(\\n\\\\min\\\\{ k^2 \\\\log n, n^2\\\\})$ tests (even when a small probability of error is\\nallowed), we show that learning an Erd\\\\H{o}s-R\\\\\\'enyi random graph with an\\naverage of $\\\\bar{k}$ edges is much easier; namely, one can attain\\nasymptotically vanishing error probability with only $O(\\\\bar{k}\\\\log n)$ tests.\\nWe establish such bounds for a variety of algorithms inspired by the group\\ntesting problem, with explicit constant factors indicating a near-optimal\\nnumber of tests, and in some cases asymptotic optimality including constant\\nfactors. In addition, we present an alternative design that permits a\\nnear-optimal sublinear decoding time of $O(\\\\bar{k} \\\\log^2 \\\\bar{k} + \\\\bar{k}\\n\\\\log n)$.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.03617</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.03617</id><submitter>Sungjae Cho</submitter><version version=\"v1\"><date>Thu, 9 May 2019 13:33:59 GMT</date><size>236kb</size></version><version version=\"v2\"><date>Thu, 30 May 2019 19:07:40 GMT</date><size>229kb</size></version><version version=\"v3\"><date>Wed, 2 Oct 2019 04:55:49 GMT</date><size>1432kb</size><source_type>D</source_type></version><title>Simulating Problem Difficulty in Arithmetic Cognition Through Dynamic\\n  Connectionist Models</title><authors>Sungjae Cho, Jaeseo Lim, Chris Hickey, Jung Ae Park, Byoung-Tak Zhang</authors><categories>cs.NE cs.AI cs.LG</categories><comments>7 pages; 15 figures; 5 tables; Published in the proceedings of the\\n  17th International Conference on Cognitive Modelling (ICCM 2019)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present study aims to investigate similarities between how humans and\\nconnectionist models experience difficulty in arithmetic problems. Problem\\ndifficulty was operationalized by the number of carries involved in solving a\\ngiven problem. Problem difficulty was measured in humans by response time, and\\nin models by computational steps. The present study found that both humans and\\nconnectionist models experience difficulty similarly when solving binary\\naddition and subtraction. Specifically, both agents found difficulty to be\\nstrictly increasing with respect to the number of carries. Another notable\\nsimilarity is that problem difficulty increases more steeply in subtraction\\nthan in addition, for both humans and connectionist models. Further\\ninvestigation on two model hyperparameters --- confidence threshold and hidden\\ndimension --- shows higher confidence thresholds cause the model to take more\\ncomputational steps to arrive at the correct answer. Likewise, larger hidden\\ndimensions cause the model to take more computational steps to correctly answer\\narithmetic problems; however, this effect by hidden dimensions is negligible.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.03696</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.03696</id><submitter>Amir Gholami</submitter><version version=\"v1\"><date>Mon, 29 Apr 2019 06:49:08 GMT</date><size>3103kb</size><source_type>D</source_type></version><title>HAWQ: Hessian AWare Quantization of Neural Networks with Mixed-Precision</title><authors>Zhen Dong, Zhewei Yao, Amir Gholami, Michael Mahoney, Kurt Keutzer</authors><categories>cs.CV</categories><journal-ref>ICCV 2019 paper</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model size and inference speed/power have become a major challenge in the\\ndeployment of Neural Networks for many applications. A promising approach to\\naddress these problems is quantization. However, uniformly quantizing a model\\nto ultra low precision leads to significant accuracy degradation. A novel\\nsolution for this is to use mixed-precision quantization, as some parts of the\\nnetwork may allow lower precision as compared to other layers. However, there\\nis no systematic way to determine the precision of different layers. A brute\\nforce approach is not feasible for deep networks, as the search space for\\nmixed-precision is exponential in the number of layers. Another challenge is a\\nsimilar factorial complexity for determining block-wise fine-tuning order when\\nquantizing the model to a target precision. Here, we introduce Hessian AWare\\nQuantization (HAWQ), a novel second-order quantization method to address these\\nproblems. HAWQ allows for the automatic selection of the relative quantization\\nprecision of each layer, based on the layer\\'s Hessian spectrum. Moreover, HAWQ\\nprovides a deterministic fine-tuning order for quantizing layers, based on\\nsecond-order information. We show the results of our method on Cifar-10 using\\nResNet20, and on ImageNet using Inception-V3, ResNet50 and SqueezeNext models.\\nComparing HAWQ with state-of-the-art shows that we can achieve similar/better\\naccuracy with $8\\\\times$ activation compression ratio on ResNet20, as compared\\nto DNAS~\\\\cite{wu2018mixed}, and up to $1\\\\%$ higher accuracy with up to $14\\\\%$\\nsmaller models on ResNet50 and Inception-V3, compared to recently proposed\\nmethods of RVQuant~\\\\cite{park2018value} and HAQ~\\\\cite{wang2018haq}.\\nFurthermore, we show that we can quantize SqueezeNext to just 1MB model size\\nwhile achieving above $68\\\\%$ top1 accuracy on ImageNet.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.04058</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.04058</id><submitter>Sam Sanders</submitter><version version=\"v1\"><date>Fri, 10 May 2019 10:37:01 GMT</date><size>96kb</size></version><version version=\"v2\"><date>Mon, 30 Sep 2019 13:01:42 GMT</date><size>57kb</size></version><title>Nets and Reverse Mathematics, a pilot study</title><authors>Sam Sanders</authors><categories>math.LO cs.LO</categories><comments>34 pages, 1 figure, to appear in \\'Computability\\'</comments><msc-class>03B30, 03D65, 03F35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nets are generalisations of sequences involving possibly uncountable index\\nsets; this notion was introduced about a century ago by Moore and Smith. They\\nalso established the generalisation to nets of various basic theorems of\\nanalysis due to Bolzano-Weierstrass, Dini, Arzela, and others. More recently,\\nnets are central to the development of domain theory, providing intuitive\\ndefinitions of the associated Scott and Lawson topologies, among others. This\\npaper deals with the Reverse Mathematics study of basic theorems about nets. We\\nrestrict ourselves to nets indexed by subsets of Baire space, and therefore\\nthird-order arithmetic, as such nets suffice to obtain our main results. Over\\nKohlenbach\\'s base theory of higher-order Reverse Mathematics, the\\nBolzano-Weierstrass theorem for nets implies the Heine-Borel theorem for\\nuncountable covers. We establish similar results for other basic theorems about\\nnets and even some equivalences, e.g. for Dini\\'s theorem for nets. Finally, we\\nshow that replacing nets by sequences is hard, but that replacing sequences by\\nnets can obviate the need for the Axiom of Choice, a foundational concern in\\ndomain theory. In an appendix, we study the power of more general index sets,\\nestablishing that the \\'size\\' of a net is directly proportional to the power of\\nthe associated convergence theorem.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.04577</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.04577</id><submitter>Suzan Verberne</submitter><version version=\"v1\"><date>Sat, 11 May 2019 19:05:20 GMT</date><size>97kb</size><source_type>D</source_type></version><title>Information search in a professional context - exploring a collection of\\n  professional search tasks</title><authors>Suzan Verberne, Jiyin He, Gineke Wiggers, Tony Russell-Rose, Udo\\n  Kruschwitz, Arjen P. de Vries</authors><categories>cs.IR cs.HC</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Search conducted in a work context is an everyday activity that has been\\naround since long before the Web was invented, yet we still seem to understand\\nlittle about its general characteristics. With this paper we aim to contribute\\nto a better understanding of this large but rather multi-faceted area of\\n`professional search\\'. Unlike task-based studies that aim at measuring the\\neffectiveness of search methods, we chose to take a step back by conducting a\\nsurvey among professional searchers to understand their typical search tasks.\\nBy doing so we offer complementary insights into the subject area. We asked our\\nrespondents to provide actual search tasks they have worked on, information\\nabout how these were conducted and details on how successful they eventually\\nwere. We then manually coded the collection of 56 search tasks with task\\ncharacteristics and relevance criteria, and used the coded dataset for\\nexploration purposes. Despite the relatively small scale of this study, our\\ndata provides enough evidence that professional search is indeed very different\\nfrom Web search in many key respects and that this is a field that offers many\\navenues for future research.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.04982</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.04982</id><submitter>Alexej Klushyn</submitter><version version=\"v1\"><date>Mon, 13 May 2019 11:42:27 GMT</date><size>8338kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 14 May 2019 13:33:08 GMT</date><size>8338kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 23 May 2019 14:42:37 GMT</date><size>8445kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Sun, 15 Sep 2019 16:36:19 GMT</date><size>8432kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Sat, 5 Oct 2019 16:44:42 GMT</date><size>8432kb</size><source_type>D</source_type></version><title>Learning Hierarchical Priors in VAEs</title><authors>Alexej Klushyn, Nutan Chen, Richard Kurle, Botond Cseke, Patrick van\\n  der Smagt</authors><categories>stat.ML cs.LG</categories><comments>Published at NeurIPS 2019 (spotlight)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to learn a hierarchical prior in the context of variational\\nautoencoders to avoid the over-regularisation resulting from a standard normal\\nprior distribution. To incentivise an informative latent representation of the\\ndata, we formulate the learning problem as a constrained optimisation problem\\nby extending the Taming VAEs framework to two-level hierarchical models. We\\nintroduce a graph-based interpolation method, which shows that the topology of\\nthe learned latent representation corresponds to the topology of the data\\nmanifold---and present several examples, where desired properties of latent\\nrepresentation such as smoothness and simple explanatory factors are learned by\\nthe prior.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.05094</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.05094</id><submitter>Pascal Koiran</submitter><version version=\"v1\"><date>Mon, 13 May 2019 15:38:09 GMT</date><size>33kb</size></version><version version=\"v2\"><date>Mon, 10 Jun 2019 19:51:01 GMT</date><size>33kb</size></version><version version=\"v3\"><date>Mon, 30 Sep 2019 12:50:46 GMT</date><size>34kb</size></version><title>Orthogonal tensor decomposition and orbit closures from a linear\\n  algebraic perspective</title><authors>Pascal Koiran</authors><categories>math.RA cs.CC math.AG</categories><comments>Final version taking the referee\\'s comments into account. In\\n  particular, Theorem 34 (formerly Theorem 30) was strengthened</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study orthogonal decompositions of symmetric and ordinary tensors using\\nmethods from linear algebra. For the field of real numbers we show that the\\nsets of decomposable tensors can be defined be equations of degree 2. This\\ngives a new proof of some of the results of Robeva and Boralevi et al.\\nOrthogonal decompositions over the field of complex numbers had not been\\nstudied previously; we give an explicit description of the set of decomposable\\ntensors using polynomial equalities and inequalities, and we begin a study of\\ntheir closures. The main open problem that arises from this work is to obtain a\\ncomplete description of the closures. This question is akin to that of\\ncharacterizing border rank of tensors in algebraic complexity. We give partial\\nresults using in particular a connection with approximate simultaneous\\ndiagonalization (the so-called &quot;ASD property&quot;).\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.05254</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.05254</id><submitter>Wei Quan Lim</submitter><version version=\"v1\"><date>Mon, 13 May 2019 19:23:36 GMT</date><size>124kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 08:27:48 GMT</date><size>153kb</size><source_type>D</source_type></version><title>Optimal Multithreaded Batch-Parallel 2-3 Trees</title><authors>Wei Quan Lim</authors><categories>cs.DS cs.DC</categories><acm-class>F.2.2; D.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a batch-parallel 2-3 tree T in an asynchronous dynamic\\nmultithreading model that supports searches, insertions and deletions in sorted\\nbatches and has essentially optimal parallelism, even under the restrictive\\nQRMW (queued read-modify-write) memory contention model where concurrent\\naccesses to the same memory location are queued and serviced one by one.\\n  Specifically, if T has n items, then performing an item-sorted batch (given\\nas a leaf-based balanced binary tree) of b operations on T takes O( b *\\nlog(n/b+1) + b ) work and O( log b + log n ) span (in the worst case as b,n -&gt;\\ninf). This is information-theoretically work-optimal for b &lt;= n, and also\\nspan-optimal for pointer-based structures. Moreover, it is easy to support\\noptimal intersection, union and difference of instances of T with sizes m &lt;= n,\\nnamely within O( m * log(n/m+1) ) work and O( log m + log n ) span.\\nFurthermore, T supports other batch operations that make it a very useful\\nbuilding block for parallel data structures.\\n  To the author\\'s knowledge, T is the first parallel sorted-set data structure\\nthat can be used in an asynchronous multi-processor machine under a memory\\nmodel with queued contention and yet have asymptotically optimal work and span.\\nIn fact, T is designed to have bounded contention and satisfy the claimed work\\nand span bounds regardless of the execution schedule.\\n  Since all data structures and algorithms in this paper fit into the dynamic\\nmultithreading paradigm, all their performance bounds are directly composable\\nwith those of other data structures and algorithms in the same model. Finally,\\nthe pipelining techniques in this paper are also likely to be very useful in\\nasynchronous parallelization of other recursive data structures.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.05264</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.05264</id><submitter>Claudio Conti</submitter><version version=\"v1\"><date>Mon, 13 May 2019 19:54:54 GMT</date><size>156kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 15 May 2019 17:38:45 GMT</date><size>108kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 3 Oct 2019 16:09:31 GMT</date><size>1134kb</size></version><title>Programming multi-level quantum gates in disordered computing reservoirs\\n  via machine learning and TensorFlow</title><authors>Giulia Marcucci and Davide Pierangeli and Pepijn Pinkse and Mehul\\n  Malik and Claudio Conti</authors><categories>quant-ph cs.LG physics.optics</categories><comments>Added a new section and a new figure about implementation of the\\n  gates by a single spatial light modulator. 9 pages and 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Novel machine learning computational tools open new perspectives for quantum\\ninformation systems. Here we adopt the open-source programming library\\nTensorFlow to design multi-level quantum gates including a computing reservoir\\nrepresented by a random unitary matrix. In optics, the reservoir is a\\ndisordered medium or a multi-modal fiber. We show that trainable operators at\\nthe input and the readout enable one to realize multi-level gates. We study\\nvarious qudit gates, including the scaling properties of the algorithms with\\nthe size of the reservoir. Despite an initial low slop learning stage,\\nTensorFlow turns out to be an extremely versatile resource for designing gates\\nwith complex media, including different models that use spatial light\\nmodulators with quantized modulation levels.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.05526</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.05526</id><submitter>Jiwei Li</submitter><version version=\"v1\"><date>Tue, 14 May 2019 11:39:43 GMT</date><size>2449kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 6 Oct 2019 07:54:54 GMT</date><size>2449kb</size><source_type>D</source_type></version><title>Is Word Segmentation Necessary for Deep Learning of Chinese\\n  Representations?</title><authors>Xiaoya Li, Yuxian Meng, Xiaofei Sun, Qinghong Han, Arianna Yuan and\\n  Jiwei Li</authors><categories>cs.CL cs.AI</categories><comments>to appear at ACL2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Segmenting a chunk of text into words is usually the first step of processing\\nChinese text, but its necessity has rarely been explored. In this paper, we ask\\nthe fundamental question of whether Chinese word segmentation (CWS) is\\nnecessary for deep learning-based Chinese Natural Language Processing. We\\nbenchmark neural word-based models which rely on word segmentation against\\nneural char-based models which do not involve word segmentation in four\\nend-to-end NLP benchmark tasks: language modeling, machine translation,\\nsentence matching/paraphrase and text classification. Through direct\\ncomparisons between these two types of models, we find that char-based models\\nconsistently outperform word-based models. Based on these observations, we\\nconduct comprehensive experiments to study why word-based models underperform\\nchar-based models in these deep learning-based NLP tasks. We show that it is\\nbecause word-based models are more vulnerable to data sparsity and the presence\\nof out-of-vocabulary (OOV) words, and thus more prone to overfitting. We hope\\nthis paper could encourage researchers in the community to rethink the\\nnecessity of word segmentation in deep learning-based Chinese Natural Language\\nProcessing. \\\\footnote{Yuxian Meng and Xiaoya Li contributed equally to this\\npaper.}\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.06845</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.06845</id><submitter>Friso H. Kingma</submitter><version version=\"v1\"><date>Thu, 16 May 2019 15:32:35 GMT</date><size>2544kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 5 Jun 2019 17:10:45 GMT</date><size>2545kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 10 Jun 2019 05:51:17 GMT</date><size>2545kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Wed, 2 Oct 2019 09:57:24 GMT</date><size>2575kb</size><source_type>D</source_type></version><title>Bit-Swap: Recursive Bits-Back Coding for Lossless Compression with\\n  Hierarchical Latent Variables</title><authors>Friso H. Kingma, Pieter Abbeel, Jonathan Ho</authors><categories>cs.LG cs.AI cs.IT math.IT stat.CO stat.ML</categories><comments>Accepted to ICML 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The bits-back argument suggests that latent variable models can be turned\\ninto lossless compression schemes. Translating the bits-back argument into\\nefficient and practical lossless compression schemes for general latent\\nvariable models, however, is still an open problem. Bits-Back with Asymmetric\\nNumeral Systems (BB-ANS), recently proposed by Townsend et al. (2019), makes\\nbits-back coding practically feasible for latent variable models with one\\nlatent layer, but it is inefficient for hierarchical latent variable models. In\\nthis paper we propose Bit-Swap, a new compression scheme that generalizes\\nBB-ANS and achieves strictly better compression rates for hierarchical latent\\nvariable models with Markov chain structure. Through experiments we verify that\\nBit-Swap results in lossless compression rates that are empirically superior to\\nexisting techniques. Our implementation is available at\\nhttps://github.com/fhkingma/bitswap.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.06853</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.06853</id><submitter>Tin Leelavimolsilp</submitter><version version=\"v1\"><date>Wed, 15 May 2019 13:35:15 GMT</date><size>192kb</size></version><version version=\"v2\"><date>Sun, 6 Oct 2019 14:04:53 GMT</date><size>412kb</size></version><title>Selfish Mining in Proof-of-Work Blockchain with Multiple Miners: An\\n  Empirical Evaluation</title><authors>Tin Leelavimolsilp, Long Tran-Thanh, Sebastian Stein, Viet Hung Nguyen</authors><categories>cs.CR cs.MA</categories><comments>Accepted in PRIMA2019</comments><doi>10.1007/978-3-030-33792-6_14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proof-of-Work blockchain, despite its numerous benefits, is still not an\\nentirely secure technology due to the existence of Selfish Mining (SM)\\nstrategies that can disrupt the system and its mining economy. While the effect\\nof SM has been studied mostly in a two-miners scenario, it has not been\\ninvestigated in a more practical context where there are multiple malicious\\nminers individually performing SM.\\n  To fill this gap, we carry out an empirical study that separately accounts\\nfor different numbers of SM miners (who always perform SM) and strategic miners\\n(who choose either SM or Nakamoto\\'s mining protocol depending on which\\nmaximises their individual mining reward).\\n  Our result shows that SM is generally more effective as the number of SM\\nminers increases, however its effectiveness does not vary in the presence of a\\nlarge number of strategic miners. Under specific mining power distributions, we\\nalso demonstrate that multiple miners can perform SM and simultaneously gain\\nhigher mining rewards than they should. Surprisingly, we also show that the\\nmore strategic miners there are, the more robust the systems become. Since\\nblockchain miners should naturally be seen as self-interested strategic miners,\\nour findings encourage blockchain system developers and engineers to attract as\\nmany miners as possible to prevent SM and similar behaviour.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.07139</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.07139</id><submitter>Ond\\\\v{r}ej Leng\\\\\\'al</submitter><version version=\"v1\"><date>Fri, 17 May 2019 07:26:49 GMT</date><size>131kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 15:10:27 GMT</date><size>70kb</size><source_type>D</source_type></version><title>Simulations in Rank-Based B\\\\&quot;uchi Automata Complementation (Technical\\n  Report)</title><authors>Yu-Fang Chen, Vojt\\\\v{e}ch Havlena, Ond\\\\v{r}ej Leng\\\\\\'al</authors><categories>cs.FL cs.LO</categories><comments>An extended version of a paper to appear in Proc. of APLAS\\'19</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complementation of B\\\\&quot;uchi automata is an essential technique used in some\\napproaches for termination analysis of programs. The long search for an optimal\\ncomplementation construction climaxed with the work of Schewe, who proposed a\\nworst-case optimal rank-based procedure that generates complements of a size\\nmatching the theoretical lower bound of $(0.76n)^n$, modulo a polynomial factor\\nof $O(n^2)$. Although worst-case optimal, the procedure in many cases produces\\nautomata that are unnecessarily large. In this paper, we propose several ways\\nof how to use the direct and delayed simulation relations to reduce the size of\\nthe automaton obtained in the rank-based complementation procedure. Our\\ntechniques are based on either (i) ignoring macrostates that cannot be used for\\naccepting a word in the complement or (ii) saturating macrostates with\\nsimulation-smaller states, in order to decrease their total number. We\\nexperimentally showed that our techniques can indeed considerably decrease the\\nsize of the output of the complementation.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.07424</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.07424</id><submitter>Mike Walmsley</submitter><version version=\"v1\"><date>Fri, 17 May 2019 18:20:35 GMT</date><size>7983kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 16:38:44 GMT</date><size>8179kb</size><source_type>D</source_type></version><title>Galaxy Zoo: Probabilistic Morphology through Bayesian CNNs and Active\\n  Learning</title><authors>Mike Walmsley, Lewis Smith, Chris Lintott, Yarin Gal, Steven Bamford,\\n  Hugh Dickinson, Lucy Fortson, Sandor Kruk, Karen Masters, Claudia Scarlata,\\n  Brooke Simmons, Rebecca Smethurst, Darryl Wright</authors><categories>astro-ph.GA cs.CV</categories><comments>Accepted by MNRAS. 21 pages, including appendices</comments><doi>10.1093/mnras/stz2816</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use Bayesian convolutional neural networks and a novel generative model of\\nGalaxy Zoo volunteer responses to infer posteriors for the visual morphology of\\ngalaxies. Bayesian CNN can learn from galaxy images with uncertain labels and\\nthen, for previously unlabelled galaxies, predict the probability of each\\npossible label. Our posteriors are well-calibrated (e.g. for predicting bars,\\nwe achieve coverage errors of 11.8% within a vote fraction deviation of 0.2)\\nand hence are reliable for practical use. Further, using our posteriors, we\\napply the active learning strategy BALD to request volunteer responses for the\\nsubset of galaxies which, if labelled, would be most informative for training\\nour network. We show that training our Bayesian CNNs using active learning\\nrequires up to 35-60% fewer labelled galaxies, depending on the morphological\\nfeature being classified. By combining human and machine intelligence, Galaxy\\nZoo will be able to classify surveys of any conceivable scale on a timescale of\\nweeks, providing massive and detailed morphology catalogues to support research\\ninto galaxy evolution.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.07443</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.07443</id><submitter>Tonmoy Saikia</submitter><version version=\"v1\"><date>Fri, 17 May 2019 19:05:25 GMT</date><size>2570kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 6 Oct 2019 20:19:32 GMT</date><size>2426kb</size><source_type>D</source_type></version><title>AutoDispNet: Improving Disparity Estimation With AutoML</title><authors>Tonmoy Saikia, Yassine Marrakchi, Arber Zela, Frank Hutter, Thomas\\n  Brox</authors><categories>cs.CV cs.AI cs.LG</categories><comments>In Proceedings of the 2019 IEEE International Conference on Computer\\n  Vision (ICCV)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much research work in computer vision is being spent on optimizing existing\\nnetwork architectures to obtain a few more percentage points on benchmarks.\\nRecent AutoML approaches promise to relieve us from this effort. However, they\\nare mainly designed for comparatively small-scale classification tasks. In this\\nwork, we show how to use and extend existing AutoML techniques to efficiently\\noptimize large-scale U-Net-like encoder-decoder architectures. In particular,\\nwe leverage gradient-based neural architecture search and Bayesian optimization\\nfor hyperparameter search. The resulting optimization does not require a\\nlarge-scale compute cluster. We show results on disparity estimation that\\nclearly outperform the manually optimized baseline and reach state-of-the-art\\nperformance.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.07475</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.07475</id><submitter>Rongjun Qin</submitter><version version=\"v1\"><date>Fri, 17 May 2019 20:56:52 GMT</date><size>1016kb</size></version><version version=\"v2\"><date>Sat, 5 Oct 2019 01:46:03 GMT</date><size>1219kb</size></version><title>Automated 3D recovery from very high resolution multi-view satellite\\n  images</title><authors>Rongjun Qin</authors><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an automated pipeline for processing multi-view satellite\\nimages to 3D digital surface models (DSM). The proposed pipeline performs\\nautomated geo-referencing and generates high-quality densely matched point\\nclouds. In particular, a novel approach is developed that fuses multiple depth\\nmaps derived by stereo matching to generate high-quality 3D maps. By learning\\ncritical configurations of stereo pairs from sample LiDAR data, we rank the\\nimage pairs based on the proximity of the results to the sample data. Multiple\\ndepth maps derived from individual image pairs are fused with an adaptive 3D\\nmedian filter that considers the image spectral similarities. We demonstrate\\nthat the proposed adaptive median filter generally delivers better results in\\ngeneral as compared to normal median filter, and achieved an accuracy of\\nimprovement of 0.36 meters RMSE in the best case. Results and analysis are\\nintroduced in detail.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.07665</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.07665</id><submitter>Shaoxiong Ji</submitter><version version=\"v1\"><date>Sun, 19 May 2019 00:06:02 GMT</date><size>272kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 5 Oct 2019 09:02:47 GMT</date><size>252kb</size><source_type>D</source_type></version><title>Knowledge Transferring via Model Aggregation for Online Social Care</title><authors>Shaoxiong Ji and Guodong Long and Shirui Pan and Tianqing Zhu and Jing\\n  Jiang and Sen Wang and Xue Li</authors><categories>cs.CR cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet and the Web are being increasingly used in proactive social care\\nto provide people, especially the vulnerable, with a better life and services,\\nand their derived social services generate enormous data. However, the strict\\nprotection of privacy makes user\\'s data become an isolated island and limits\\nthe predictive performance of standalone clients. To enable effective proactive\\nsocial care and knowledge sharing within intelligent agents, this paper\\ndevelops a knowledge transferring framework via model aggregation. Under this\\nframework, distributed clients perform on-device training, and a third-party\\nserver integrates multiple clients\\' models and redistributes to clients for\\nknowledge transferring among users. To improve the generalizability of the\\nknowledge sharing, we further propose a novel model aggregation algorithm,\\nnamely the average difference descent aggregation (AvgDiffAgg for short). In\\nparticular, to evaluate the effectiveness of the learning algorithm, we use a\\ncase study on the early detection and prevention of suicidal ideation, and the\\nexperiment results on four datasets derived from social communities demonstrate\\nthe effectiveness of the proposed learning method.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.08073</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.08073</id><submitter>Ali Mohammed</submitter><version version=\"v1\"><date>Mon, 20 May 2019 13:01:09 GMT</date><size>579kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 22 May 2019 10:37:12 GMT</date><size>583kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 4 Oct 2019 15:30:44 GMT</date><size>583kb</size><source_type>D</source_type></version><title>rDLB: A Novel Approach for Robust Dynamic Load Balancing of Scientific\\n  Applications with Parallel Independent Tasks</title><authors>Ali Mohammed, Aurelien Cavelan, and Florina M. Ciorba</authors><categories>cs.DC cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific applications often contain large and computationally intensive\\nparallel loops. Dynamic loop self scheduling (DLS) is used to achieve a\\nbalanced load execution of such applications on high performance computing\\n(HPC) systems. Large HPC systems are vulnerable to processors or node failures\\nand perturbations in the availability of resources. Most self-scheduling\\napproaches do not consider fault-tolerant scheduling or depend on failure or\\nperturbation detection and react by rescheduling failed tasks. In this work, a\\nrobust dynamic load balancing (rDLB) approach is proposed for the robust self\\nscheduling of independent tasks. The proposed approach is proactive and does\\nnot depend on failure or perturbation detection. The theoretical analysis of\\nthe proposed approach shows that it is linearly scalable and its cost decrease\\nquadratically by increasing the system size. rDLB is integrated into an MPI DLS\\nlibrary to evaluate its performance experimentally with two computationally\\nintensive scientific applications. Results show that rDLB enables the tolerance\\nof up to (P minus one) processor failures, where P is the number of processors\\nexecuting an application. In the presence of perturbations, rDLB boosted the\\nrobustness of DLS techniques up to 30 times and decreased application execution\\ntime up to 7 times compared to their counterparts without rDLB.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.08550</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.08550</id><submitter>Xiaoting Shao</submitter><version version=\"v1\"><date>Tue, 21 May 2019 11:13:17 GMT</date><size>2165kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 11:41:28 GMT</date><size>3951kb</size><source_type>D</source_type></version><title>Conditional Sum-Product Networks: Imposing Structure on Deep\\n  Probabilistic Architectures</title><authors>Xiaoting Shao, Alejandro Molina, Antonio Vergari, Karl Stelzner,\\n  Robert Peharz, Thomas Liebig, and Kristian Kersting</authors><categories>cs.LG stat.ML</categories><comments>13 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic graphical models are a central tool in AI; however, they are\\ngenerally not as expressive as deep neural models, and inference is notoriously\\nhard and slow. In contrast, deep probabilistic models such as sum-product\\nnetworks (SPNs) capture joint distributions in a tractable fashion, but still\\nlack the expressive power of intractable models based on deep neural networks.\\nTherefore, we introduce conditional SPNs (CSPNs), conditional density\\nestimators for multivariate and potentially hybrid domains which allow\\nharnessing the expressive power of neural networks while still maintaining\\ntractability guarantees. One way to implement CSPNs is to use an existing SPN\\nstructure and condition its parameters on the input, e.g., via a deep neural\\nnetwork. This approach, however, might misrepresent the conditional\\nindependence structure present in data. Consequently, we also develop a\\nstructure-learning approach that derives both the structure and parameters of\\nCSPNs from data. Our experimental evidence demonstrates that CSPNs are\\ncompetitive with other probabilistic models and yield superior performance on\\nmultilabel image classification compared to mean field and mixture density\\nnetworks. Furthermore, they can successfully be employed as building blocks for\\nstructured probabilistic models, such as autoregressive image models.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.08594</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.08594</id><submitter>Salimeh Yasaei Sekeh</submitter><version version=\"v1\"><date>Tue, 21 May 2019 12:59:30 GMT</date><size>3005kb</size><source_type>D</source_type></version><title>Geometric Estimation of Multivariate Dependency</title><authors>Salimeh Yasaei Sekeh and Alfred O. Hero</authors><categories>cs.LG cs.IT math.IT stat.ML</categories><comments>28 pages, 5 figures</comments><doi>10.3390/e21080787</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a geometric estimator of dependency between a pair of\\nmultivariate samples. The proposed estimator of dependency is based on a\\nrandomly permuted geometric graph (the minimal spanning tree) over the two\\nmultivariate samples. This estimator converges to a quantity that we call the\\ngeometric mutual information (GMI), which is equivalent to the Henze-Penrose\\ndivergence [1] between the joint distribution of the multivariate samples and\\nthe product of the marginals. The GMI has many of the same properties as\\nstandard MI but can be estimated from empirical data without density\\nestimation; making it scalable to large datasets. The proposed empirical\\nestimator of GMI is simple to implement, involving the construction of an MST\\nspanning over both the original data and a randomly permuted version of this\\ndata. We establish asymptotic convergence of the estimator and convergence\\nrates of the bias and variance for smooth multivariate density functions\\nbelonging to a H\\\\&quot;{o}lder class. We demonstrate the advantages of our proposed\\ngeometric dependency estimator in a series of experiments.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.08685</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.08685</id><submitter>Jen-Yen Chang</submitter><version version=\"v1\"><date>Tue, 21 May 2019 15:03:31 GMT</date><size>1130kb</size><source_type>D</source_type></version><title>Improved Optical Flow for Gesture-based Human-robot Interaction</title><authors>Jen-Yen Chang, Antonio Tejero-de-Pablos, Tatsuya Harada</authors><categories>cs.HC cs.CV cs.RO</categories><comments>Accepted by ICRA 2019 on Jan 31 2019</comments><doi>10.1109/ICRA.2019.8793825</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Gesture interaction is a natural way of communicating with a robot as an\\nalternative to speech. Gesture recognition methods leverage optical flow in\\norder to understand human motion. However, while accurate optical flow\\nestimation (i.e., traditional) methods are costly in terms of runtime, fast\\nestimation (i.e., deep learning) methods\\' accuracy can be improved. In this\\npaper, we present a pipeline for gesture-based human-robot interaction that\\nuses a novel optical flow estimation method in order to achieve an improved\\nspeed-accuracy trade-off. Our optical flow estimation method introduces four\\nimprovements to previous deep learning-based methods: strong feature\\nextractors, attention to contours, midway features, and a combination of these\\nthree. This results in a better understanding of motion, and a finer\\nrepresentation of silhouettes. In order to evaluate our pipeline, we generated\\nour own dataset, MIBURI, which contains gestures to command a house service\\nrobot. In our experiments, we show how our method improves not only optical\\nflow estimation, but also gesture recognition, offering a speed-accuracy\\ntrade-off more realistic for practical robot applications.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.08764</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.08764</id><submitter>Yihui Ren</submitter><version version=\"v1\"><date>Tue, 21 May 2019 17:33:19 GMT</date><size>8095kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 20:59:22 GMT</date><size>7215kb</size><source_type>D</source_type></version><title>Performance Analysis of Deep Learning Workloads on Leading-edge Systems</title><authors>Yihui Ren, Shinjae Yoo and Adolfy Hoisie</authors><categories>cs.PF cs.DC cs.LG</categories><comments>11 pages, 9 figures</comments><acm-class>D.4.8; C.1.2; I.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work examines the performance of leading-edge systems designed for\\nmachine learning computing, including the NVIDIA DGX-2, Amazon Web Services\\n(AWS) P3, IBM Power System Accelerated Compute Server AC922, and a\\nconsumer-grade Exxact TensorEX TS4 GPU server. Representative deep learning\\nworkloads from the fields of computer vision and natural language processing\\nare the focus of the analysis. Performance analysis is performed along with a\\nnumber of important dimensions. Performance of the communication interconnects\\nand large and high-throughput deep learning models are considered. Different\\npotential use models for the systems as standalone and in the cloud also are\\nexamined. The effect of various optimization of the deep learning models and\\nsystem configurations is included in the analysis.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.08841</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.08841</id><submitter>Arun Jambulapati</submitter><version version=\"v1\"><date>Tue, 21 May 2019 19:25:30 GMT</date><size>41kb</size></version><version version=\"v2\"><date>Fri, 27 Sep 2019 07:37:50 GMT</date><size>41kb</size></version><version version=\"v3\"><date>Mon, 30 Sep 2019 00:28:49 GMT</date><size>41kb</size></version><title>Parallel Reachability in Almost Linear Work and Square Root Depth</title><authors>Arun Jambulapati, Yang P. Liu, Aaron Sidford</authors><categories>cs.DS</categories><comments>38 pages. v2 fixes a small typo in Section 4 found by Aaron\\n  Bernstein. v3 fixes some overflow issues</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we provide a parallel algorithm that given any $n$-node\\n$m$-edge directed graph and source vertex $s$ computes all vertices reachable\\nfrom $s$ with $\\\\tilde{O}(m)$ work and $n^{1/2 + o(1)}$ depth with high\\nprobability in $n$ . This algorithm also computes a set of $\\\\tilde{O}(n)$ edges\\nwhich when added to the graph preserves reachability and ensures that the\\ndiameter of the resulting graph is at most $n^{1/2 + o(1)}$. Our result\\nimproves upon the previous best known almost linear work reachability algorithm\\ndue to Fineman which had depth $\\\\tilde{O}(n^{2/3})$.\\n  Further, we show how to leverage this algorithm to achieve improved\\ndistributed algorithms for single source reachability in the CONGEST model. In\\nparticular, we provide a distributed algorithm that given a $n$-node digraph of\\nundirected hop-diameter $D$ solves the single source reachability problem with\\n$\\\\tilde{O}(n^{1/2} + n^{1/3 + o(1)} D^{2/3})$ rounds of the communication in\\nthe CONGEST model with high probability in $n$. Our algorithm is nearly optimal\\nwhenever $D = O(n^{1/4 - \\\\epsilon})$ for any constant $\\\\epsilon &gt; 0$ and is the\\nfirst nearly optimal algorithm for general graphs whose diameter is\\n$\\\\Omega(n^\\\\delta)$ for any constant $\\\\delta$.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.08975</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.08975</id><submitter>Pedro Cisneros-Velarde</submitter><version version=\"v1\"><date>Wed, 22 May 2019 06:24:32 GMT</date><size>398kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 5 Oct 2019 10:07:02 GMT</date><size>442kb</size><source_type>D</source_type></version><title>Distributionally Robust Formulation and Model Selection for the\\n  Graphical Lasso</title><authors>Pedro Cisneros-Velarde, Sang-Yun Oh, Alexander Petersen</authors><categories>stat.ML cs.LG math.ST stat.TH</categories><msc-class>62F35, 62F40, 90C90</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building on a recent framework for distributionally robust optimization, we\\nconsider estimation of the inverse covariance matrix for multivariate data. We\\nprovide a novel notion of a Wasserstein ambiguity set specifically tailored to\\nthis estimation problem, leading to a tractable class of regularized\\nestimators. Special cases include penalized likelihood estimators for Gaussian\\ndata, specifically the graphical lasso estimator. As a consequence of this\\nformulation, the radius of the Wasserstein ambiguity set is directly related to\\nthe regularization parameter in the estimation problem. Using this\\nrelationship, the level of robustness of the estimation procedure can be shown\\nto correspond to the level of confidence with which the ambiguity set contains\\na distribution with the population covariance. Furthermore, a unique feature of\\nour formulation is that the radius can be expressed in closed-form as a\\nfunction of the ordinary sample covariance matrix. Taking advantage of this\\nfinding, we develop a simple algorithm to determine a regularization parameter\\nfor graphical lasso, using only the bootstrapped sample covariance matrices,\\nmeaning that computationally expensive repeated evaluation of the graphical\\nlasso algorithm is not necessary. Alternatively, the distributionally robust\\nformulation can also quantify the robustness of the corresponding estimator if\\none uses an off-the-shelf method such as cross-validation. Finally, we\\nnumerically study the obtained regularization criterion and analyze the\\nrobustness of other automated tuning procedures used in practice.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.09239</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.09239</id><submitter>Behzad Tabibian</submitter><version version=\"v1\"><date>Wed, 22 May 2019 16:55:24 GMT</date><size>1789kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 08:08:52 GMT</date><size>1789kb</size><source_type>D</source_type></version><title>Optimal Decision Making Under Strategic Behavior</title><authors>Moein Khajehnejad, Behzad Tabibian, Bernhard Sch\\\\&quot;olkopf, Adish\\n  Singla, Manuel Gomez-Rodriguez</authors><categories>cs.LG cs.CY stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are witnessing an increasing use of data-driven predictive models to\\ninform decisions. As decisions have implications for individuals and society,\\nthere is increasing pressure on decision makers to be transparent about their\\ndecision policies, models, and the features they use. At the same time,\\nindividuals may use knowledge, gained by transparency, to invest effort\\nstrategically in order to maximize their chances of receiving a beneficial\\ndecision. In this paper, our goal is to find decision policies that are optimal\\nin terms of utility in such a strategic setting. To this end, we first use the\\ntheory of optimal transport to characterize how strategic investment of effort\\nby individuals leads to a change in the feature distribution at a population\\nlevel. Then, we show that, in contrast with the non-strategic setting, optimal\\ndecision policies are stochastic, and we cannot expect to find them in\\npolynomial time. Finally, we derive an efficient greedy algorithm that is\\nguaranteed to find locally optimal decision policies in polynomial time.\\nExperiments on synthetic and real lending data illustrate our theoretical\\nfindings and show that the decision policies found by our greedy algorithm\\nachieve higher utility than deterministic threshold rules, which are optimal\\npolicies in a non-strategic setting.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.09608</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.09608</id><submitter>Yuantao Gu</submitter><version version=\"v1\"><date>Thu, 23 May 2019 12:11:02 GMT</date><size>32kb</size></version><version version=\"v2\"><date>Sat, 28 Sep 2019 19:24:43 GMT</date><size>44kb</size></version><title>Unraveling the Veil of Subspace RIP Through Near-Isometry on Subspaces</title><authors>Xingyu Xv and Gen Li and Yuantao Gu</authors><categories>cs.IT cs.LG math.IT</categories><comments>40 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dimensionality reduction is a popular approach to tackle high-dimensional\\ndata with low-dimensional nature. Subspace Restricted Isometry Property, a\\nnewly-proposed concept, has proved to be a useful tool in analyzing the effect\\nof dimensionality reduction algorithms on subspaces. In this paper, we provide\\na characterization of subspace Restricted Isometry Property, asserting that\\nmatrices which act as a near-isometry on low-dimensional subspaces possess\\nsubspace Restricted Isometry Property. This points out a unified approach to\\ndiscuss subspace Restricted Isometry Property. Its power is further\\ndemonstrated by the possibility to prove with this result the subspace RIP for\\na large variety of random matrices encountered in theory and practice,\\nincluding subgaussian matrices, partial Fourier matrices, partial Hadamard\\nmatrices, partial circulant/Toeplitz matrices, matrices with independent\\nstrongly regular rows (for instance, matrices with independent entries having\\nuniformly bounded $4+\\\\epsilon$ moments), and log-concave ensembles. Thus our\\nresult could extend the applicability of random projections in subspace-based\\nmachine learning algorithms including subspace clustering and allow for the\\napplication of some useful random matrices which are easier to implement on\\nhardware or are more efficient to compute.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.09864</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.09864</id><submitter>Varun Kumar</submitter><version version=\"v1\"><date>Thu, 23 May 2019 18:40:57 GMT</date><size>44kb</size></version><version version=\"v2\"><date>Tue, 4 Jun 2019 02:24:03 GMT</date><size>336kb</size></version><title>Why Didn\\'t You Listen to Me? Comparing User Control of Human-in-the-Loop\\n  Topic Models</title><authors>Varun Kumar, Alison Smith-Renner, Leah Findlater, Kevin Seppi and\\n  Jordan Boyd-Graber</authors><categories>cs.CL cs.HC cs.IR cs.LG</categories><comments>In proceedings of ACL 2019</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  To address the lack of comparative evaluation of Human-in-the-Loop Topic\\nModeling (HLTM) systems, we implement and evaluate three contrasting HLTM\\nmodeling approaches using simulation experiments. These approaches extend\\npreviously proposed frameworks, including constraints and informed prior-based\\nmethods. Users should have a sense of control in HLTM systems, so we propose a\\ncontrol metric to measure whether refinement operations\\' results match users\\'\\nexpectations. Informed prior-based methods provide better control than\\nconstraints, but constraints yield higher quality topics.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.09898</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.09898</id><submitter>Thodoris Lykouris</submitter><version version=\"v1\"><date>Thu, 23 May 2019 20:05:06 GMT</date><size>28kb</size></version><version version=\"v2\"><date>Sat, 28 Sep 2019 23:02:41 GMT</date><size>28kb</size></version><title>Feedback graph regret bounds for Thompson Sampling and UCB</title><authors>Thodoris Lykouris, Eva Tardos, Drishti Wali</authors><categories>cs.LG cs.DS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the stochastic multi-armed bandit problem with the graph-based\\nfeedback structure introduced by Mannor and Shamir. We analyze the performance\\nof the two most prominent stochastic bandit algorithms, Thompson Sampling and\\nUpper Confidence Bound (UCB), in the graph-based feedback setting. We show that\\nthese algorithms achieve regret guarantees that combine the graph structure and\\nthe gaps between the means of the arm distributions. Surprisingly this holds\\ndespite the fact that these algorithms do not explicitly use the graph\\nstructure to select arms; they observe the additional feedback but do not\\nexplore based on it. Towards this result we introduce a &quot;layering technique&quot;\\nhighlighting the commonalities in the two algorithms.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.09906</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.09906</id><submitter>Zhenyu Shou</submitter><version version=\"v1\"><date>Thu, 23 May 2019 20:24:29 GMT</date><size>11897kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 06:44:30 GMT</date><size>13447kb</size><source_type>D</source_type></version><title>Optimal Passenger-Seeking Policies on E-hailing Platforms Using Markov\\n  Decision Process and Imitation Learning</title><authors>Zhenyu Shou, Xuan Di, Jieping Ye, Hongtu Zhu, Hua Zhang, Robert\\n  Hampshire</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vacant taxi drivers\\' passenger seeking process in a road network generates\\nadditional vehicle miles traveled, adding congestion and pollution into the\\nroad network and the environment. This paper aims to employ a Markov Decision\\nProcess (MDP) to model idle e-hailing drivers\\' optimal sequential decisions in\\npassenger-seeking. Transportation network companies (TNC) or e-hailing (e.g.,\\nDidi, Uber) drivers exhibit different behaviors from traditional taxi drivers\\nbecause e-hailing drivers do not need to actually search for passengers.\\nInstead, they reposition themselves so that the matching platform can match a\\npassenger. Accordingly, we incorporate e-hailing drivers\\' new features into our\\nMDP model. The reward function used in the MDP model is uncovered by leveraging\\nan inverse reinforcement learning technique. We then use 44,160 Didi drivers\\'\\n3-day trajectories to train the model. To validate the effectiveness of the\\nmodel, a Monte Carlo simulation is conducted to simulate the performance of\\ndrivers under the guidance of the optimal policy, which is then compared with\\nthe performance of drivers following one baseline heuristic, namely, the local\\nhotspot strategy. The results show that our model is able to achieve a 17.5%\\nimprovement over the local hotspot strategy in terms of the rate of return. The\\nproposed MDP model captures the supply-demand ratio considering the fact that\\nthe number of drivers in this study is sufficiently large and thus the number\\nof unmatched orders is assumed to be negligible. To better incorporate the\\ncompetition among multiple drivers into the model, we have also devised and\\ncalibrated a dynamic adjustment strategy of the order matching probability.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.09980</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.09980</id><submitter>Jia Peng</submitter><version version=\"v1\"><date>Fri, 24 May 2019 00:33:17 GMT</date><size>3115kb</size></version><version version=\"v2\"><date>Wed, 26 Jun 2019 08:31:02 GMT</date><size>3020kb</size></version><title>Perception Evaluation -- A new solar image quality metric based on the\\n  multi-fractal property of texture features</title><authors>Yi Huang and Peng Jia and Dongmei Cai and Bojun Cai</authors><categories>astro-ph.IM astro-ph.SR cs.CV</categories><comments>15 pages, 13 figures, accepted by Solar Physics</comments><doi>10.1007/s11207-019-1524-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Next-generation ground-based solar observations require good image quality\\nmetrics for post-facto processing techniques. Based on the assumption that\\ntexture features in solar images are multi-fractal which can be extracted by a\\ntrained deep neural network as feature maps, a new reduced-reference objective\\nimage quality metric, the perception evaluation is proposed. The perception\\nevaluation is defined as cosine distance of Gram matrix between feature maps\\nextracted from high resolution reference image and that from blurred images. We\\nevaluate performance of the perception evaluation with simulated and real\\nobservation images. The results show that with a high resolution image as\\nreference, the perception evaluation can give robust estimate of image quality\\nfor solar images of different scenes.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.09982</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.09982</id><submitter>Tetsuya Sato</submitter><version version=\"v1\"><date>Fri, 24 May 2019 00:53:24 GMT</date><size>759kb</size></version><version version=\"v2\"><date>Tue, 8 Oct 2019 15:44:43 GMT</date><size>823kb</size><source_type>D</source_type></version><title>Hypothesis Testing Interpretations and Renyi Differential Privacy</title><authors>Borja Balle, Gilles Barthe, Marco Gaboardi, Justin Hsu and Tetsuya\\n  Sato</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential privacy is a de facto standard in data privacy, with\\napplications in the public and private sectors. A way to explain differential\\nprivacy, which is particularly appealing to statistician and social scientists\\nis by means of its statistical hypothesis testing interpretation. Informally,\\none cannot effectively test whether a specific individual has contributed her\\ndata by observing the output of a private mechanism---any test cannot have both\\nhigh significance and high power.\\n  In this paper, we identify some conditions under which a privacy definition\\ngiven in terms of a statistical divergence satisfies a similar interpretation.\\nThese conditions are useful to analyze the distinguishability power of\\ndivergences and we use them to study the hypothesis testing interpretation of\\nsome relaxations of differential privacy based on Renyi divergence. This\\nanalysis also results in an improved conversion rule between these definitions\\nand differential privacy.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.10041</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.10041</id><submitter>Yueming Lyu</submitter><version version=\"v1\"><date>Fri, 24 May 2019 05:40:05 GMT</date><size>727kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 07:32:04 GMT</date><size>730kb</size><source_type>D</source_type></version><title>Efficient Batch Black-box Optimization with Deterministic Regret Bounds</title><authors>Yueming Lyu, Yuan Yuan, Ivor W. Tsang</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we investigate black-box optimization from the perspective of\\nfrequentist kernel methods. We propose a novel batch optimization algorithm to\\njointly maximize the acquisition function and select points from a whole batch\\nin a holistic way. Theoretically, we derive regret bounds for both the\\nnoise-free and perturbation settings. Moreover, we analyze the property of the\\nadversarial regret that is required by robust initialization for Bayesian\\nOptimization (BO), and prove that the adversarial regret bounds decrease with\\nthe decrease of covering radius, which provides a criterion for generating\\n(initialization point set) to minimize the bound. We then propose fast\\nsearching algorithms to generate a point set with a small covering radius for\\nthe robust initialization. Experimental results on both synthetic benchmark\\nproblems and real-world problems show the effectiveness of the proposed\\nalgorithms.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.10427</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.10427</id><submitter>Jakub Tomczak Ph.D.</submitter><version version=\"v1\"><date>Fri, 24 May 2019 19:57:39 GMT</date><size>2132kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 13:15:48 GMT</date><size>2693kb</size><source_type>D</source_type></version><title>DIVA: Domain Invariant Variational Autoencoders</title><authors>Maximilian Ilse and Jakub M. Tomczak and Christos Louizos and Max\\n  Welling</authors><categories>stat.ML cs.LG</categories><comments>Code available at https://github.com/AMLab-Amsterdam/DIVA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of domain generalization, namely, how to learn\\nrepresentations given data from a set of domains that generalize to data from a\\npreviously unseen domain. We propose the Domain Invariant Variational\\nAutoencoder (DIVA), a generative model that tackles this problem by learning\\nthree independent latent subspaces, one for the domain, one for the class, and\\none for any residual variations. We highlight that due to the generative nature\\nof our model we can also incorporate unlabeled data from known or previously\\nunseen domains. To the best of our knowledge this has not been done before in a\\ndomain generalization setting. This property is highly desirable in fields like\\nmedical imaging where labeled data is scarce. We experimentally evaluate our\\nmodel on the rotated MNIST benchmark and a malaria cell images dataset where we\\nshow that (i) the learned subspaces are indeed complementary to each other,\\n(ii) we improve upon recent works on this task and (iii) incorporating\\nunlabelled data can boost the performance even further.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.10521</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.10521</id><submitter>Kyungwoo Song</submitter><version version=\"v1\"><date>Sat, 25 May 2019 05:10:01 GMT</date><size>1843kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 12:12:26 GMT</date><size>1519kb</size><source_type>D</source_type></version><title>Bivariate Beta LSTM</title><authors>Kyungwoo Song, JoonHo Jang, Seung jae Shin, Il-Chul Moon</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Long Short-Term Memory (LSTM) infers the long term dependency through a cell\\nstate maintained by the input and the forget gate structures, which models a\\ngate output as a value in [0,1] through a sigmoid function. However, due to the\\ngraduality of the sigmoid function, the sigmoid gate is not flexible in\\nrepresenting multi-modality or skewness. Besides, the previous models lack\\nmodeling on the correlation between the gates, which would be a new method to\\nadopt inductive bias for a relationship between previous and current input.\\nThis paper proposes a new gate structure with the bivariate Beta distribution.\\nThe proposed gate structure enables probabilistic modeling on the gates within\\nthe LSTM cell so that the modelers can customize the cell state flow with\\npriors and distributions. Moreover, we theoretically show the higher upper\\nbound of the gradient compared to the sigmoid function, and we empirically\\nobserved that the bivariate Beta distribution gate structure provides higher\\ngradient values in training. We demonstrate the effectiveness of bivariate Beta\\ngate structure on the sentence classification, image classification, polyphonic\\nmusic modeling, and image caption generation.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.10711</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.10711</id><submitter>Qiangeng Xu</submitter><version version=\"v1\"><date>Sun, 26 May 2019 01:58:28 GMT</date><size>5740kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 03:32:26 GMT</date><size>8785kb</size><source_type>D</source_type></version><title>DISN: Deep Implicit Surface Network for High-quality Single-view 3D\\n  Reconstruction</title><authors>Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir Mech, Ulrich Neumann</authors><categories>cs.CV</categories><journal-ref>33rd Annual Conference on Neural Information Processing Systems\\n  (NeurIPS 2019)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reconstructing 3D shapes from single-view images has been a long-standing\\nresearch problem. In this paper, we present DISN, a Deep Implicit Surface\\nNetwork which can generate a high-quality detail-rich 3D mesh from an 2D image\\nby predicting the underlying signed distance fields. In addition to utilizing\\nglobal image features, DISN predicts the projected location for each 3D point\\non the 2D image, and extracts local features from the image feature maps.\\nCombining global and local features significantly improves the accuracy of the\\nsigned distance field prediction, especially for the detail-rich areas. To the\\nbest of our knowledge, DISN is the first method that constantly captures\\ndetails such as holes and thin structures present in 3D shapes from single-view\\nimages. DISN achieves the state-of-the-art single-view reconstruction\\nperformance on a variety of shape categories reconstructed from both synthetic\\nand real images. Code is available at https://github.com/xharlie/DISN The\\nsupplementary can be found at\\nhttps://xharlie.github.io/images/neurips_2019_supp.pdf\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.10854</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.10854</id><submitter>Guy Hacohen</submitter><version version=\"v1\"><date>Sun, 26 May 2019 18:51:22 GMT</date><size>816kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 3 Jun 2019 07:58:46 GMT</date><size>817kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 3 Oct 2019 09:02:29 GMT</date><size>7065kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Mon, 7 Oct 2019 14:37:37 GMT</date><size>7065kb</size><source_type>D</source_type></version><title>All Neural Networks are Created Equal</title><authors>Guy Hacohen, Leshem Choshen, Daphna Weinshall</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the unresolved questions in deep learning is the nature of the\\nsolutions that are being discovered. We investigate the collection of solutions\\nreached by the same network architecture, with different random initialization\\nof weights and random mini-batches. These solutions are shown to be rather\\nsimilar - more often than not, each train and test example is either classified\\ncorrectly by all the networks, or by none at all. Surprisingly, all the network\\ninstances seem to share the same learning dynamics, whereby initially the same\\ntrain and test examples are correctly recognized by the learned model, followed\\nby other examples which are learned in roughly the same order. When extending\\nthe investigation to heterogeneous collections of neural network architectures,\\nonce again examples are seen to be learned in the same order irrespective of\\narchitecture, although the more powerful architecture may continue to learn and\\nthus achieve higher accuracy. This pattern of results remains true even when\\nthe composition of classes in the test set is unrelated to the train set, for\\nexample, when using out of sample natural images or even artificial images. To\\nshow the robustness of these phenomena we provide an extensive summary of our\\nempirical study, which includes hundreds of graphs describing tens of thousands\\nof networks with varying NN architectures, hyper-parameters and domains. We\\nalso discuss cases where this pattern of similarity breaks down, which show\\nthat the reported similarity is not an artifact of optimization by gradient\\ndescent. Rather, the observed pattern of similarity is characteristic of\\nlearning complex problems with big networks. Finally, we show that this pattern\\nof similarity seems to be strongly correlated with effective generalization.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.10900</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.10900</id><submitter>Varun Chandrasekaran</submitter><version version=\"v1\"><date>Sun, 26 May 2019 23:10:38 GMT</date><size>2483kb</size></version><version version=\"v2\"><date>Tue, 1 Oct 2019 17:00:15 GMT</date><size>6137kb</size><source_type>D</source_type></version><title>Enhancing ML Robustness Using Physical-World Constraints</title><authors>Varun Chandrasekaran, Brian Tang, Varsha Pendyala, Kassem Fawaz,\\n  Somesh Jha and Xi Wu</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in Machine Learning (ML) have demonstrated that neural\\nnetworks can exceed human performance in many tasks. While generalizing well\\nover natural inputs, neural networks are vulnerable to adversarial inputs -an\\ninput that is ``similar\\'\\' to the original input, but misclassified by the\\nmodel.\\n  Existing defenses focus on Lp-norm bounded adversaries that perturb ML inputs\\nin the digital space. In the real world, however, attackers can generate\\nadversarial perturbations that have a large Lp-norm in the digital space.\\nAdditionally, these defenses also come at a cost to accuracy, making their\\napplicability questionable in the real world.\\n  To defend models against such a powerful adversary, we leverage one\\nconstraint on its power: the perturbation should not change the human\\'s\\nperception of the physical information; the physical world places some\\nconstraints on the space of possible attacks. Two questions follow: how to\\nextract and model these constraints? and how to design a classification\\nparadigm that leverages these constraints to improve robustness accuracy\\ntrade-off?\\n  We observe that an ML model is typically a part of a larger system with\\naccess to different input modalities. Utilizing these modalities, we introduce\\ninvariants that limit the attacker\\'s action space. We design a hierarchical\\nclassification paradigm that enforces these invariants at inference time.\\n  As a case study, we implement and evaluate our proposal in the context of the\\nreal-world application of road sign classification because of its applicability\\nto autonomous driving. With access to different input modalities, such as\\nLiDAR, camera, and location we show how to extract invariants and develop a\\nhierarchical classifier. Our results on the KITTI and GTSRB datasets show that\\nwe can improve the robustness against physical attacks at minimal harm to\\naccuracy.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.10922</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.10922</id><submitter>Anthony Young</submitter><version version=\"v1\"><date>Wed, 15 May 2019 16:40:16 GMT</date><size>32kb</size></version><version version=\"v2\"><date>Wed, 2 Oct 2019 21:43:01 GMT</date><size>35kb</size></version><title>Applying Abstract Argumentation Theory to Cooperative Game Theory</title><authors>Anthony P. Young, David Kohan Marzagao and Josh Murphy</authors><categories>cs.GT cs.AI cs.MA</categories><comments>15 pages, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply ideas from abstract argumentation theory to study cooperative game\\ntheory. Building on Dung\\'s results in his seminal paper, we further the\\ncorrespondence between Dung\\'s four argumentation semantics and solution\\nconcepts in cooperative game theory by showing that complete extensions (the\\ngrounded extension) correspond to Roth\\'s subsolutions (respectively, the\\nsupercore). We then investigate the relationship between well-founded\\nargumentation frameworks and convex games, where in each case the semantics\\n(respectively, solution concepts) coincide; we prove that three-player convex\\ngames do not in general have well-founded argumentation frameworks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.10947</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.10947</id><submitter>Kenta Oono</submitter><version version=\"v1\"><date>Mon, 27 May 2019 02:59:06 GMT</date><size>480kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 04:45:55 GMT</date><size>585kb</size><source_type>D</source_type></version><title>Graph Neural Networks Exponentially Lose Expressive Power for Node\\n  Classification</title><authors>Kenta Oono, Taiji Suzuki</authors><categories>cs.LG stat.ML</categories><comments>9 pages, Supplemental material 24 pages</comments><msc-class>05C99, 62M45</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph Neural Networks (graph NNs) are a promising deep learning approach for\\nanalyzing graph-structured data. However, it is known that they do not improve\\n(or sometimes worsen) their predictive performance as we pile up many layers\\nand add non-lineality. To tackle this problem, we investigate the expressive\\npower of graph NNs via their asymptotic behaviors as the layer size tends to\\ninfinity. Our strategy is to generalize the forward propagation of a Graph\\nConvolutional Network (GCN), which is a popular graph NN variant, as a specific\\ndynamical system. In the case of a GCN, we show that when its weights satisfy\\nthe conditions determined by the spectra of the (augmented) normalized\\nLaplacian, its output exponentially approaches the set of signals that carry\\ninformation of the connected components and node degrees only for\\ndistinguishing nodes. Our theory enables us to relate the expressive power of\\nGCNs with the topological information of the underlying graphs inherent in the\\ngraph spectra. To demonstrate this, we characterize the asymptotic behavior of\\nGCNs on the Erd\\\\H{o}s -- R\\\\\\'{e}nyi graph. We show that when the Erd\\\\H{o}s --\\nR\\\\\\'{e}nyi graph is sufficiently dense and large, a broad range of GCNs on it\\nsuffers from the &quot;information loss&quot; in the limit of infinite layers with high\\nprobability. Based on the theory, we provide a principled guideline for weight\\nnormalization of graph NNs. We experimentally confirm that the proposed weight\\nscaling enhances the predictive performance of GCNs in real data. Code is\\navailable at https://github.com/delta2323/gnn-asymptotics.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.11075</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.11075</id><submitter>Petros Koumoutsakos</submitter><version version=\"v1\"><date>Mon, 27 May 2019 09:26:17 GMT</date><size>9138kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 07:59:26 GMT</date><size>5665kb</size><source_type>D</source_type></version><title>Machine Learning for Fluid Mechanics</title><authors>Steven Brunton and Bernd Noack and Petros Koumoutsakos</authors><categories>physics.flu-dyn cs.LG stat.ML</categories><comments>To appear in the Annual Reviews of Fluid Mechanics, 2020</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of fluid mechanics is rapidly advancing, driven by unprecedented\\nvolumes of data from experiments, field measurements, and large-scale\\nsimulations at multiple spatiotemporal scales. Machine learning presents us\\nwith a wealth of techniques to extract information from data that can be\\ntranslated into knowledge about the underlying fluid mechanics. Moreover,\\nmachine learning algorithms can augment domain knowledge and automate tasks\\nrelated to flow control and optimization. This article presents an overview of\\npast history, current developments, and emerging opportunities of machine\\nlearning for fluid mechanics. We outline fundamental machine learning\\nmethodologies and discuss their uses for understanding, modeling, optimizing,\\nand controlling fluid flows. The strengths and limitations of these methods are\\naddressed from the perspective of scientific inquiry that links data with\\nmodeling, experiments, and simulations. Machine learning provides a powerful\\ninformation processing framework that can augment, and possibly even transform,\\ncurrent lines of fluid mechanics research and industrial applications.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.11163</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.11163</id><submitter>Wojciech Michal Matkowski</submitter><version version=\"v1\"><date>Mon, 27 May 2019 12:22:31 GMT</date><size>1260kb</size></version><title>Giant Panda Face Recognition Using Small Dataset</title><authors>Wojciech Michal Matkowski, Adams Wai Kin Kong, Han Su, Peng Chen, Rong\\n  Hou, and Zhihe Zhang</authors><categories>cs.CV</categories><comments>Accepted in the IEEE 2019 International Conference on Image\\n  Processing (ICIP 2019), scheduled for 22-25 September 2019 in Taipei, Taiwan</comments><journal-ref>2019 IEEE International Conference on Image Processing (ICIP)</journal-ref><doi>10.1109/ICIP.2019.8803125</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Giant panda (panda) is a highly endangered animal. Significant efforts and\\nresources have been put on panda conservation. To measure effectiveness of\\nconservation schemes, estimating its population size in wild is an important\\ntask. The current population estimation approaches, including\\ncapture-recapture, human visual identification and collection of DNA from hair\\nor feces, are invasive, subjective, costly or even dangerous to the workers who\\nperform these tasks in wild. Cameras have been widely installed in the regions\\nwhere pandas live. It opens a new possibility for non-invasive image based\\npanda recognition. Panda face recognition is naturally a small dataset problem,\\nbecause of the number of pandas in the world and the number of qualified images\\ncaptured by the cameras in each encounter. In this paper, a panda face\\nrecognition algorithm, which includes alignment, large feature set extraction\\nand matching is proposed and evaluated on a dataset consisting of 163 images.\\nThe experimental results are encouraging.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.11166</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.11166</id><submitter>Johannes Blum</submitter><version version=\"v1\"><date>Mon, 27 May 2019 12:29:11 GMT</date><size>28kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 11:20:13 GMT</date><size>39kb</size><source_type>D</source_type></version><title>Hierarchy of Transportation Network Parameters and Hardness Results</title><authors>Johannes Blum</authors><categories>cs.DM cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The graph parameters highway dimension and skeleton dimension were introduced\\nto capture the properties of transportation networks. As many important\\noptimization problems like Travelling Salesperson, Steiner Tree or $k$-Center\\narise in such networks, it is worthwhile to study them on graphs of bounded\\nhighway or skeleton dimension.\\n  We investigate the relationships between mentioned parameters and how they\\nare related to other important graph parameters that have been applied\\nsuccessfully to various optimization problems. We show that the skeleton\\ndimension is incomparable to any of the parameters distance to linear forest,\\nbandwidth, treewidth and highway dimension and hence, it is worthwhile to study\\nmentioned problems also on graphs of bounded skeleton dimension. Moreover, we\\nprove that the skeleton dimension is upper bounded by the max leaf number and\\nthat for any graph on at least three vertices there are edge weights such that\\nboth parameters are equal.\\n  Then we show that computing the highway dimension according to most recent\\ndefinition is NP-hard, which answers an open question stated by Feldmann et al.\\nFinally we prove that on graphs $G=(V,E)$ of skeleton dimension\\n$\\\\mathcal{O}(\\\\log^2 \\\\vert V \\\\vert)$ it is NP-hard to approximate the $k$-Center\\nproblem within a factor less than $2$.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.11190</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.11190</id><submitter>Amir-Hossein Karimi</submitter><version version=\"v1\"><date>Mon, 27 May 2019 13:22:39 GMT</date><size>341kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 28 May 2019 08:00:19 GMT</date><size>341kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 8 Oct 2019 10:21:41 GMT</date><size>2980kb</size><source_type>D</source_type></version><title>Model-Agnostic Counterfactual Explanations for Consequential Decisions</title><authors>Amir-Hossein Karimi, Gilles Barthe, Borja Balle, Isabel Valera</authors><categories>cs.LG cs.AI cs.LO stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predictive models are being increasingly used to support consequential\\ndecision making at the individual level in contexts such as pretrial bail and\\nloan approval. As a result, there is increasing social and legal pressure to\\nprovide explanations that help the affected individuals not only to understand\\nwhy a prediction was output, but also how to act to obtain a desired outcome.\\nTo this end, several works have proposed optimization-based methods to generate\\nnearest counterfactual explanations. However, these methods are often\\nrestricted to a particular subset of models (e.g., decision trees or linear\\nmodels) and differentiable distance functions. In contrast, we build on\\nstandard theory and tools from formal verification and propose a novel\\nalgorithm that solves a sequence of satisfiability problems, where both the\\ndistance function (objective) and predictive model (constraints) are\\nrepresented as logic formulae. As shown by our experiments on real-world data,\\nour algorithm is: i) model-agnostic ({non-}linear, {non-}differentiable,\\n{non-}convex); ii) data-type-agnostic (heterogeneous features); iii)\\ndistance-agnostic ($\\\\ell_0, \\\\ell_1, \\\\ell_\\\\infty$, and combinations thereof);\\niv) able to generate plausible and diverse counterfactuals for any sample\\n(i.e., 100% coverage); and v) at provably optimal distances.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.11368</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.11368</id><submitter>Dingli Yu</submitter><version version=\"v1\"><date>Mon, 27 May 2019 17:52:28 GMT</date><size>333kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 29 May 2019 02:43:33 GMT</date><size>263kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 2 Oct 2019 03:30:12 GMT</date><size>404kb</size><source_type>D</source_type></version><title>Simple and Effective Regularization Methods for Training on Noisily\\n  Labeled Data with Generalization Guarantee</title><authors>Wei Hu, Zhiyuan Li, Dingli Yu</authors><categories>cs.LG cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over-parameterized deep neural networks trained by simple first-order methods\\nare known to be able to fit any labeling of data. Such over-fitting ability\\nhinders generalization when mislabeled training examples are present. On the\\nother hand, simple regularization methods like early-stopping can often achieve\\nhighly nontrivial performance on clean test data in these scenarios, a\\nphenomenon not theoretically understood. This paper proposes and analyzes two\\nsimple and intuitive regularization methods: (i) regularization by the distance\\nbetween the network parameters to initialization, and (ii) adding a trainable\\nauxiliary variable to the network output for each training example.\\nTheoretically, we prove that gradient descent training with either of these two\\nmethods leads to a generalization guarantee on the clean data distribution\\ndespite being trained using noisy labels. Our generalization analysis relies on\\nthe connection between wide neural network and neural tangent kernel (NTK). The\\ngeneralization bound is independent of the network size, and is comparable to\\nthe bound one can get when there is no label noise. Experimental results verify\\nthe effectiveness of these methods on noisily labeled datasets.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.11468</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.11468</id><submitter>Chris Finlay</submitter><version version=\"v1\"><date>Mon, 27 May 2019 19:40:52 GMT</date><size>116kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 14:12:34 GMT</date><size>530kb</size><source_type>D</source_type></version><title>Scaleable input gradient regularization for adversarial robustness</title><authors>Chris Finlay and Adam M Oberman</authors><categories>stat.ML cs.CR cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we revisit gradient regularization for adversarial robustness\\nwith some new ingredients. First, we derive new per-image theoretical\\nrobustness bounds based on local gradient information. These bounds strongly\\nmotivate input gradient regularization. Second, we implement a scaleable\\nversion of input gradient regularization which avoids double backpropagation:\\nadversarially robust ImageNet models are trained in 33 hours on four consumer\\ngrade GPUs. Finally, we show experimentally and through theoretical\\ncertification that input gradient regularization is competitive with\\nadversarial training. Moreover we demonstrate that gradient regularization does\\nnot lead to gradient obfuscation or gradient masking.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.11498</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.11498</id><submitter>Chu Wang</submitter><version version=\"v1\"><date>Mon, 27 May 2019 20:41:53 GMT</date><size>2686kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 1 Jun 2019 16:31:24 GMT</date><size>2687kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 3 Oct 2019 19:55:42 GMT</date><size>5336kb</size><source_type>D</source_type></version><title>FAN: Focused Attention Networks</title><authors>Chu Wang, Babak Samari, Vladimir Kim, Siddhartha Chaudhuri, Kaleem\\n  Siddiqi</authors><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attention networks show promise for both vision and language tasks, by\\nemphasizing relationships between constituent elements through weighting\\nfunctions. Such elements could be regions in an image output by a region\\nproposal network, or words in a sentence, represented by word embedding. Thus\\nfar the learning of attention weights has been driven solely by the\\nminimization of task specific loss functions. We introduce a method for\\nlearning attention weights to better emphasize informative pair-wise relations\\nbetween entities. The key component is a novel center-mass cross entropy loss,\\nwhich can be applied in conjunction with the task specific ones. We further\\nintroduce a focused attention backbone to learn these attention weights for\\ngeneral tasks. We demonstrate that the focused supervision leads to improved\\nattention distribution across meaningful entities, and that it enhances the\\nrepresentation by aggregating features from them. Our focused attention module\\nleads to state-of-the-art recovery of relations in a relationship proposal task\\nand boosts performance for various vision and language tasks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.11666</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.11666</id><submitter>Wonjae Kim</submitter><version version=\"v1\"><date>Tue, 28 May 2019 08:13:37 GMT</date><size>9417kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 05:50:25 GMT</date><size>5046kb</size><source_type>D</source_type></version><title>Learning Dynamics of Attention: Human Prior for Interpretable Machine\\n  Reasoning</title><authors>Wonjae Kim and Yoonho Lee</authors><categories>stat.ML cs.CV cs.LG</categories><comments>20 pages, 18 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Without relevant human priors, neural networks may learn uninterpretable\\nfeatures. We propose Dynamics of Attention for Focus Transition (DAFT) as a\\nhuman prior for machine reasoning. DAFT is a novel method that regularizes\\nattention-based reasoning by modelling it as a continuous dynamical system\\nusing neural ordinary differential equations. As a proof of concept, we augment\\na state-of-the-art visual reasoning model with DAFT. Our experiments reveal\\nthat applying DAFT yields similar performance to the original model while using\\nfewer reasoning steps, showing that it implicitly learns to skip unnecessary\\nsteps. We also propose a new metric, Total Length of Transition (TLT), which\\nrepresents the effective reasoning step size by quantifying how much a given\\nmodel\\'s focus drifts while reasoning about a question. We show that adding DAFT\\nresults in lower TLT, demonstrating that our method indeed obeys the human\\nprior towards shorter reasoning paths in addition to producing more\\ninterpretable attention maps.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.11742</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.11742</id><submitter>Congzheng Song</submitter><version version=\"v1\"><date>Tue, 28 May 2019 11:16:02 GMT</date><size>35kb</size></version><version version=\"v2\"><date>Fri, 27 Sep 2019 20:01:43 GMT</date><size>48kb</size></version><title>Overlearning Reveals Sensitive Attributes</title><authors>Congzheng Song, Vitaly Shmatikov</authors><categories>cs.LG cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  `Overlearning\\' means that a model trained for a seemingly simple objective\\nimplicitly learns to recognize attributes that are (1) statistically\\nuncorrelated with the objective, and (2) sensitive from a privacy or bias\\nperspective. For example, a binary gender classifier of facial images also\\nlearns to recognize races\\\\textemdash even races that are not represented in the\\ntraining data\\\\textemdash and identities.\\n  We demonstrate overlearning in several image-analysis and NLP models and\\nanalyze its harmful consequences. First, inference-time internal\\nrepresentations of an overlearned model reveal sensitive attributes of the\\ninput, breaking privacy protections such as model partitioning. Second, an\\noverlearned model can be `re-purposed\\' for a different, uncorrelated task.\\n  Overlearning may be inherent to some tasks. We show that techniques for\\ncensoring unwanted properties from representations either fail, or degrade the\\nmodel\\'s performance on both the original and unintended tasks. This is a\\nchallenge for regulations that aim to prevent models from learning or using\\ncertain attributes.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.11814</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.11814</id><submitter>FatemehSadat Mireshghallah</submitter><version version=\"v1\"><date>Sun, 26 May 2019 19:59:34 GMT</date><size>1381kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 16:20:44 GMT</date><size>483kb</size><source_type>D</source_type></version><title>Shredder: Learning Noise Distributions to Protect Inference Privacy</title><authors>Fatemehsadat Mireshghallah, Mohammadkazem Taram, Prakash Ramrakhyani,\\n  Dean Tullsen, Hadi Esmaeilzadeh</authors><categories>cs.CR cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sheer amount of computation in deep neural networks has pushed their\\nexecution to the cloud. This de facto cloud-hosted inference, however, raises\\nserious privacy concerns as private data is communicated and stored in remote\\nservers. The data could be mishandled by cloud providers, used for unsolicited\\nanalytics, or simply compromised through network and system security\\nvulnerability. To that end, this paper devises SHREDDER that reduces the\\ninformation content of the communicated data without diminishing the cloud\\'s\\nability to maintain acceptably high accuracy. To that end, SHREDDER learns two\\nsets of noise distributions whose samples, named multiplicative and additive\\nnoise tensors, are applied to the communicated data while maintaining the\\ninference accuracy. The key idea is that SHREDDER learns these noise\\ndistributions offline without altering the topology or the weights of the\\npre-trained network. SHREDDER repeatedly learns sample noise tensors from the\\ndistributions by casting the tensors as a set of trainable parameters while\\nkeeping the weights constant. Since the key idea is learning the noise, we are\\nable to devise a loss function that strikes a balance between accuracy and\\ninformation degradation. To this end, we use self-supervision to train the\\nnoise tensors to achieve an intermediate representation of the data that\\ncontains less private information. Experimentation with real-world deep neural\\nnetworks shows that, compared to the original execution, SHREDDER reduces the\\nmutual information between the input and the communicated data by 66.90%, and\\nyields a misclassification rate of 94.5% over private labels, significantly\\nreducing adversary\\'s ability to infer private data, while sacrificing only\\n1.74% loss in accuracy without any knowledge about the private labels.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.12090</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.12090</id><submitter>Geoffrey Roeder</submitter><version version=\"v1\"><date>Tue, 28 May 2019 21:06:50 GMT</date><size>36352kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 16:51:00 GMT</date><size>36353kb</size><source_type>D</source_type></version><title>Efficient Amortised Bayesian Inference for Hierarchical and Nonlinear\\n  Dynamical Systems</title><authors>Geoffrey Roeder, Paul K. Grant, Andrew Phillips, Neil Dalchau, and\\n  Edward Meeds</authors><categories>stat.ML cs.LG</categories><comments>Published in &quot;Proceedings of Machine Learning Research, Volume 97:\\n  International Conference on Machine Learning, 9-15 June 2019, Long Beach,\\n  California, USA&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a flexible, scalable Bayesian inference framework for nonlinear\\ndynamical systems characterised by distinct and hierarchical variability at the\\nindividual, group, and population levels. Our model class is a generalisation\\nof nonlinear mixed-effects (NLME) dynamical systems, the statistical workhorse\\nfor many experimental sciences. We cast parameter inference as stochastic\\noptimisation of an end-to-end differentiable, block-conditional variational\\nautoencoder. We specify the dynamics of the data-generating process as an\\nordinary differential equation (ODE) such that both the ODE and its solver are\\nfully differentiable. This model class is highly flexible: the ODE right-hand\\nsides can be a mixture of user-prescribed or &quot;white-box&quot; sub-components and\\nneural network or &quot;black-box&quot; sub-components. Using stochastic optimisation,\\nour amortised inference algorithm could seamlessly scale up to massive data\\ncollection pipelines (common in labs with robotic automation). Finally, our\\nframework supports interpretability with respect to the underlying dynamics, as\\nwell as predictive generalization to unseen combinations of group components\\n(also called &quot;zero-shot&quot; learning). We empirically validate our method by\\npredicting the dynamic behaviour of bacteria that were genetically engineered\\nto function as biosensors. Our implementation of the framework, the dataset,\\nand all code to reproduce the experimental results is available at\\nhttps://www.github.com/Microsoft/vi-hds .\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.12150</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.12150</id><submitter>Sreelekha Guggilam</submitter><version version=\"v1\"><date>Wed, 29 May 2019 00:51:03 GMT</date><size>654kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 21:21:43 GMT</date><size>0kb</size><source_type>I</source_type></version><title>Bayesian Anomaly Detection Using Extreme Value Theory</title><authors>Sreelekha Guggilam and S. M. Arshad Zaidi and Varun Chandola and Abani\\n  Patra</authors><categories>stat.ML cs.LG stat.ME</categories><comments>7 pages, 7 figures, The paper has been withdrawn due to major\\n  modification in the automation model</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data-driven anomaly detection methods typically build a model for the normal\\nbehavior of the target system, and score each data instance with respect to\\nthis model. A threshold is invariably needed to identify data instances with\\nhigh (or low) scores as anomalies. This presents a practical limitation on the\\napplicability of such methods, since most methods are sensitive to the choice\\nof the threshold, and it is challenging to set optimal thresholds. We present a\\nprobabilistic framework to explicitly model the normal and anomalous behaviors\\nand probabilistically reason about the data. An extreme value theory based\\nformulation is proposed to model the anomalous behavior as the extremes of the\\nnormal behavior. As a specific instantiation, a joint non-parametric clustering\\nand anomaly detection algorithm is proposed that models the normal behavior as\\na Dirichlet Process Mixture Model.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.12198</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.12198</id><submitter>Jiangjie Chen</submitter><version version=\"v1\"><date>Wed, 29 May 2019 03:32:38 GMT</date><size>304kb</size><source_type>D</source_type></version><title>Ensuring Readability and Data-fidelity using Head-modifier Templates in\\n  Deep Type Description Generation</title><authors>Jiangjie Chen, Ao Wang, Haiyun Jiang, Suo Feng, Chenguang Li and\\n  Yanghua Xiao</authors><categories>cs.CL</categories><comments>ACL 2019</comments><journal-ref>Proceedings of the 57th Annual Meeting of the Association for\\n  Computational Linguistics. 2019</journal-ref><doi>10.18653/v1/P19-1196</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A type description is a succinct noun compound which helps human and machines\\nto quickly grasp the informative and distinctive information of an entity.\\nEntities in most knowledge graphs (KGs) still lack such descriptions, thus\\ncalling for automatic methods to supplement such information. However, existing\\ngenerative methods either overlook the grammatical structure or make factual\\nmistakes in generated texts. To solve these problems, we propose a\\nhead-modifier template-based method to ensure the readability and data fidelity\\nof generated type descriptions. We also propose a new dataset and two automatic\\nmetrics for this task. Experiments show that our method improves substantially\\ncompared with baselines and achieves state-of-the-art performance on both\\ndatasets.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.12204</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.12204</id><submitter>Hyunwook Kang</submitter><version version=\"v1\"><date>Wed, 29 May 2019 04:02:41 GMT</date><size>412kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 1 Jun 2019 16:51:06 GMT</date><size>644kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 30 Sep 2019 18:44:13 GMT</date><size>666kb</size><source_type>D</source_type></version><title>Learning scalable and transferable multi-robot/machine sequential\\n  assignment planning via graph embedding</title><authors>Hyunwook Kang, Aydar Mynbay, James R. Morrison, Jinkyoo Park</authors><categories>cs.LG cs.AI cs.MA cs.RO stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Can the success of reinforcement learning methods for simple combinatorial\\noptimization problems be extended to multi-robot sequential assignment\\nplanning? In addition to the challenge of achieving near-optimal performance in\\nlarge problems, transferability to an unseen number of robots and tasks is\\nanother key challenge for real-world applications. In this paper, we suggest a\\nmethod that achieves the first success in both challenges for robot/machine\\nscheduling problems.\\n  Our method comprises of three components. First, we show a robot scheduling\\nproblem can be expressed as a random probabilistic graphical model (PGM). We\\ndevelop a mean-field inference method for random PGM and use it for Q-function\\ninference. Second, we show that transferability can be achieved by carefully\\ndesigning two-step sequential encoding of problem state. Third, we resolve the\\ncomputational scalability issue of fitted Q-iteration by suggesting a heuristic\\nauction-based Q-iteration fitting method enabled by transferability we\\nachieved.\\n  We apply our method to discrete-time, discrete space problems (Multi-Robot\\nReward Collection (MRRC)) and scalably achieve 97% optimality with\\ntransferability. This optimality is maintained under stochastic contexts. By\\nextending our method to continuous time, continuous space formulation, we claim\\nto be the first learning-based method with scalable performance among\\nmulti-machine scheduling problems; our method scalability achieves comparable\\nperformance to popular metaheuristics in Identical parallel machine scheduling\\n(IPMS) problems.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.12317</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.12317</id><submitter>Joakim And\\\\\\'en</submitter><version version=\"v1\"><date>Wed, 29 May 2019 10:40:55 GMT</date><size>5760kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 15:31:52 GMT</date><size>5760kb</size><source_type>D</source_type></version><title>Factorization of the translation kernel for fast rigid image alignment</title><authors>Aaditya Rangan, Marina Spivak, Joakim And\\\\\\'en, Alex Barnett</authors><categories>math.NA cs.NA</categories><comments>30 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important component of many image alignment methods is the calculation of\\ninner products (correlations) between an image of $n\\\\times n$ pixels and\\nanother image translated by some shift and rotated by some angle. For robust\\nalignment of an image pair, the number of considered shifts and angles is\\ntypically high, thus the inner product calculation becomes a bottleneck.\\nExisting methods, based on fast Fourier transforms (FFTs), compute all such\\ninner products with computational complexity $\\\\mathcal{O}(n^3 \\\\log n)$ per\\nimage pair, which is reduced to $\\\\mathcal{O}(N n^2)$ if only $N$ distinct\\nshifts are needed. We propose to use a factorization of the translation kernel\\n(FTK), an optimal interpolation method which represents images in a\\nFourier--Bessel basis and uses a rank-$H$ approximation of the translation\\nkernel via an operator singular value decomposition (SVD). Its complexity is\\n$\\\\mathcal{O}(Hn(n + N))$ per image pair. We prove that $H = \\\\mathcal{O}((W +\\n\\\\log(1/\\\\epsilon))^2)$, where $2W$ is the magnitude of the maximum desired shift\\nin pixels and $\\\\epsilon$ is the desired accuracy. For fixed $W$ this leads to\\nan acceleration when $N$ is large, such as when sub-pixel shift grids are\\nconsidered. Finally, we present numerical results in an electron cryomicroscopy\\napplication showing speedup factors of $3$-$10$ with respect to the state of\\nthe art.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.12541</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.12541</id><submitter>Susan Stepney</submitter><version version=\"v1\"><date>Wed, 29 May 2019 15:42:24 GMT</date><size>859kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 16:44:18 GMT</date><size>475kb</size><source_type>D</source_type></version><title>MetaChem: An Algebraic Framework for Artificial Chemistries</title><authors>Penelope Faulkner Rainford, Angelika Sebald, Susan Stepney</authors><categories>cs.ET</categories><comments>38 pages, 19 figures; minor revisions plus reformatted pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce MetaChem, a language for representing and implementing\\nArtificial Chemistries. We motivate the need for modularisation and\\nstandardisation in representation of artificial chemistries. We describe a\\nmathematical formalism for Static Graph MetaChem, a static graph based system.\\nMetaChem supports different levels of description, and has a formal\\ndescription; we illustrate these using StringCatChem, a toy artificial\\nchemistry. We describe two existing Artificial Chemistries -- Jordan Algebra\\nAChem and Swarm Chemistries -- in MetaChem, and demonstrate how they can be\\ncombined in several different configurations by using a MetaChem environmental\\nlink. MetaChem provides a route to standardisation, reuse, and composition of\\nArtificial Chemistries and their tools.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.12614</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.12614</id><submitter>Irina Higgins</submitter><version version=\"v1\"><date>Wed, 29 May 2019 17:56:58 GMT</date><size>1882kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 11:20:04 GMT</date><size>4650kb</size><source_type>D</source_type></version><title>Unsupervised Model Selection for Variational Disentangled Representation\\n  Learning</title><authors>Sunny Duan, Loic Matthey, Andre Saraiva, Nicholas Watters, Christopher\\n  P. Burgess, Alexander Lerchner, Irina Higgins</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Disentangled representations have recently been shown to improve fairness,\\ndata efficiency and generalisation in simple supervised and reinforcement\\nlearning tasks. To extend the benefits of disentangled representations to more\\ncomplex domains and practical applications, it is important to enable\\nhyperparameter tuning and model selection of existing unsupervised approaches\\nwithout requiring access to ground truth attribute labels, which are not\\navailable for most datasets. This paper addresses this problem by introducing a\\nsimple yet robust and reliable method for unsupervised disentangled model\\nselection. We show that our approach performs comparably to the existing\\nsupervised alternatives across 5400 models from six state of the art\\nunsupervised disentangled representation learning model classes. Furthermore,\\nwe show that the ranking produced by our approach correlates well with the\\nfinal task performance on two different domains.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.12724</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.12724</id><submitter>Henry Li</submitter><version version=\"v1\"><date>Wed, 29 May 2019 21:06:09 GMT</date><size>4662kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 14 Jun 2019 20:29:51 GMT</date><size>4666kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 3 Oct 2019 00:13:22 GMT</date><size>7680kb</size><source_type>D</source_type></version><title>Variational Diffusion Autoencoders with Random Walk Sampling</title><authors>Henry Li, Ofir Lindenbaum, Xiuyuan Cheng, Alexander Cloninger</authors><categories>cs.LG stat.ML</categories><comments>18 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variational inference (VI) methods and especially variational autoencoders\\n(VAEs) specify scalable generative models that enjoy an intuitive connection to\\nmanifold learning --- with many default priors the posterior/likelihood pair\\n$q(z|x)$/$p(x|z)$ can be viewed as an approximate homeomorphism (and its\\ninverse) between the data manifold and a latent Euclidean space. However, these\\napproximations are well-documented to become degenerate in training. Unless the\\nsubjective prior is carefully chosen, the topologies of the prior and data\\ndistributions often will not match. Conversely, diffusion maps (DM)\\nautomatically $\\\\textit{infer}$ the data topology and enjoy a rigorous\\nconnection to manifold learning, but do not scale easily or provide the inverse\\nhomeomorphism. In this paper, we propose $\\\\textbf{a)}$ a principled measure for\\nrecognizing the mismatch between data and latent distributions and\\n$\\\\textbf{b)}$ a method that combines the advantages of variational inference\\nand diffusion maps to learn a homeomorphic generative model. The measure, the$\\n\\\\textit{locally bi-Lipschitz property}$, is a sufficient condition for a\\nhomeomorphism and easy to compute and interpret. The method, the\\n$\\\\textit{variational diffusion autoencoder}$ (VDAE), is a novel generative\\nalgorithm that first infers the topology of the data distribution, then models\\na diffusion random walk over the data. To achieve efficient computation in\\nVDAEs, we use stochastic versions of both variational inference and manifold\\nlearning optimization. We prove approximation theoretic results for the\\ndimension dependence of VDAEs, and that locally isotropic sampling in the\\nlatent space results in a random walk over the reconstructed manifold. Finally,\\nwe demonstrate our method on various real and synthetic datasets, and show that\\nit exhibits performance superior to other generative models.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.13183</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.13183</id><submitter>Minjie Xu</submitter><version version=\"v1\"><date>Thu, 30 May 2019 17:09:39 GMT</date><size>1836kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 18:14:52 GMT</date><size>1834kb</size><source_type>D</source_type></version><title>Understanding Goal-Oriented Active Learning via Influence Functions</title><authors>Minjie Xu and Gary Kazantsev</authors><categories>cs.LG stat.ML</categories><comments>Accepted to the &quot;ML with Guarantees&quot; workshop at NeurIPS 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active learning (AL) concerns itself with learning a model from as few\\nlabelled data as possible through actively and iteratively querying an oracle\\nwith selected unlabelled samples. In this paper, we focus on analyzing a\\npopular type of AL in which the utility of a sample is measured by a specified\\ngoal achieved by the retrained model after accounting for the sample\\'s marginal\\ninfluence. Such AL strategies attract a lot of attention thanks to their\\nintuitive motivations, yet they also suffer from impractically high\\ncomputational costs due to their need for many iterations of model retraining.\\nWith the help of influence functions, we present an effective approximation\\nthat bypasses model retraining altogether, and propose a general efficient\\nimplementation that makes such AL strategies applicable in practice, both in\\nthe serial and the more challenging batch-mode setting. Additionally, we\\npresent both theoretical and empirical findings which call into question a few\\ncommon practices and beliefs about such AL strategies.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.13196</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.13196</id><submitter>Peter Bubenik</submitter><version version=\"v1\"><date>Thu, 30 May 2019 17:36:58 GMT</date><size>934kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 12 Jun 2019 13:49:29 GMT</date><size>992kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 19 Sep 2019 16:04:05 GMT</date><size>960kb</size><source_type>D</source_type></version><title>Persistent homology detects curvature</title><authors>Peter Bubenik, Michael Hull, Dhruv Patel, and Benjamin Whittle</authors><categories>cs.CG cs.LG math.AT stat.ML</categories><comments>22 pages, corrections thanks to anonymous referees</comments><msc-class>55N99</msc-class><journal-ref>Inverse Problems, 2019</journal-ref><doi>10.1088/1361-6420/ab4ac0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In topological data analysis, persistent homology is used to study the &quot;shape\\nof data&quot;. Persistent homology computations are completely characterized by a\\nset of intervals called a bar code. It is often said that the long intervals\\nrepresent the &quot;topological signal&quot; and the short intervals represent &quot;noise&quot;.\\nWe give evidence to dispute this thesis, showing that the short intervals\\nencode geometric information. Specifically, we prove that persistent homology\\ndetects the curvature of disks from which points have been sampled. We describe\\na general computational framework for solving inverse problems using the\\naverage persistence landscape, a continuous mapping from metric spaces with a\\nprobability measure to a Hilbert space. In the present application, the average\\npersistence landscapes of points sampled from disks of constant curvature\\nresults in a path in this Hilbert space which may be learned using standard\\ntools from statistical and machine learning.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.13209</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.13209</id><submitter>Michael S. Ryoo</submitter><version version=\"v1\"><date>Thu, 30 May 2019 17:51:03 GMT</date><size>2843kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 17:54:13 GMT</date><size>3191kb</size><source_type>D</source_type></version><title>AssembleNet: Searching for Multi-Stream Neural Connectivity in Video\\n  Architectures</title><authors>Michael S. Ryoo, AJ Piergiovanni, Mingxing Tan, Anelia Angelova</authors><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning to represent videos is a very challenging task both algorithmically\\nand computationally. Standard video CNN architectures have been designed by\\ndirectly extending architectures devised for image understanding to include the\\ntime dimension, using modules such as 3D convolutions, or by using two-stream\\ndesign to capture both appearance and motion in videos. We interpret a video\\nCNN as a collection of multi-stream convolutional blocks connected to each\\nother, and propose the approach of automatically finding neural architectures\\nwith better connectivity and spatio-temporal interactions for video\\nunderstanding. This is done by evolving a population of overly-connected\\narchitectures guided by connection weight learning. Architectures combining\\nrepresentations that abstract different input types (i.e., RGB and optical\\nflow) at multiple temporal resolutions are searched for, allowing different\\ntypes or sources of information to interact with each other. Our method,\\nreferred to as AssembleNet, outperforms prior approaches on public video\\ndatasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and\\n34.27% accuracy on Moments-in-Time.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.13211</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.13211</id><submitter>Keyulu Xu</submitter><version version=\"v1\"><date>Thu, 30 May 2019 17:53:30 GMT</date><size>107kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 31 May 2019 21:50:31 GMT</date><size>107kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sun, 29 Sep 2019 20:42:29 GMT</date><size>386kb</size><source_type>D</source_type></version><title>What Can Neural Networks Reason About?</title><authors>Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S. Du, Ken-ichi\\n  Kawarabayashi, Stefanie Jegelka</authors><categories>cs.LG cs.AI cs.CV cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural networks have successfully been applied to solving reasoning tasks,\\nranging from learning simple concepts like &quot;close to&quot;, to intricate questions\\nwhose reasoning procedures resemble algorithms. Empirically, not all network\\nstructures work equally well for reasoning. For example, Graph Neural Networks\\nhave achieved impressive empirical results, while less structured neural\\nnetworks may fail to learn to reason. Theoretically, there is currently limited\\nunderstanding of the interplay between reasoning tasks and network learning. In\\nthis paper, we develop a framework to characterize which tasks a neural network\\ncan learn well, by studying how well its structure aligns with the algorithmic\\nstructure of the relevant reasoning procedure. This suggests that Graph Neural\\nNetworks can learn dynamic programming, a powerful algorithmic strategy that\\nsolves a broad class of reasoning problems, such as relational question\\nanswering, sorting, intuitive physics, and shortest paths. Our perspective also\\nimplies strategies to design neural architectures for complex reasoning. On\\nseveral abstract reasoning tasks, we see empirically that our theory aligns\\nwell with practice.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.13359</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.13359</id><submitter>Fahad AlGhamdi</submitter><version version=\"v1\"><date>Fri, 31 May 2019 00:08:27 GMT</date><size>39kb</size><source_type>D</source_type></version><title>Leveraging Pretrained Word Embeddings for Part-of-Speech Tagging of Code\\n  Switching Data</title><authors>Fahad AlGhamdi and Mona Diab</authors><categories>cs.CL</categories><doi>10.18653/v1/W19-1410</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linguistic Code Switching (CS) is a phenomenon that occurs when multilingual\\nspeakers alternate between two or more languages/dialects within a single\\nconversation. Processing CS data is especially challenging in intra-sentential\\ndata given state-of-the-art monolingual NLP technologies since such\\ntechnologies are geared toward the processing of one language at a time. In\\nthis paper, we address the problem of Part-of-Speech tagging (POS) in the\\ncontext of linguistic code switching (CS). We explore leveraging multiple\\nneural network architectures to measure the impact of different pre-trained\\nembeddings methods on POS tagging CS data. We investigate the landscape in four\\nCS language pairs, Spanish-English, Hindi-English, Modern Standard Arabic-\\nEgyptian Arabic dialect (MSA-EGY), and Modern Standard Arabic- Levantine Arabic\\ndialect (MSA-LEV). Our results show that multilingual embedding (e.g., MSA-EGY\\nand MSA-LEV) helps closely related languages (EGY/LEV) but adds noise to the\\nlanguages that are distant (SPA/HIN). Finally, we show that our proposed models\\noutperform state-of-the-art CS taggers for MSA-EGY language pair.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.13547</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.13547</id><submitter>Benjamin Gravell</submitter><version version=\"v1\"><date>Tue, 28 May 2019 21:22:24 GMT</date><size>527kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 16:21:35 GMT</date><size>534kb</size><source_type>D</source_type></version><title>Learning robust control for LQR systems with multiplicative noise via\\n  policy gradient</title><authors>Benjamin Gravell, Peyman Mohajerin Esfahani, Tyler Summers</authors><categories>cs.LG cs.SY math.DS math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The linear quadratic regulator (LQR) problem has reemerged as an important\\ntheoretical benchmark for reinforcement learning-based control of complex\\ndynamical systems with continuous state and action spaces. In contrast with\\nnearly all recent work in this area, we consider multiplicative noise models,\\nwhich are increasingly relevant because they explicitly incorporate inherent\\nuncertainty and variation in the system dynamics and thereby improve robustness\\nproperties of the controller. Robustness is a critical and poorly understood\\nissue in reinforcement learning; existing methods which do not account for\\nuncertainty can converge to fragile policies or fail to converge at all.\\nAdditionally, intentional injection of multiplicative noise into learning\\nalgorithms can enhance robustness of policies, as observed in ad hoc work on\\ndomain randomization. Although policy gradient algorithms require optimization\\nof a non-convex cost function, we show that the multiplicative noise LQR cost\\nhas a special property called gradient domination, which is exploited to prove\\nglobal convergence of policy gradient algorithms to the globally optimum\\ncontrol policy with polynomial dependence on problem parameters. Results are\\nprovided both in the model-known and model-unknown settings where samples of\\nsystem trajectories are used to estimate policy gradients.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1905.13654</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1905.13654</id><submitter>Soufiane Hayou</submitter><version version=\"v1\"><date>Fri, 31 May 2019 14:53:59 GMT</date><size>637kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 7 Jun 2019 16:59:15 GMT</date><size>637kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 2 Oct 2019 07:47:17 GMT</date><size>652kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Thu, 3 Oct 2019 07:29:52 GMT</date><size>652kb</size><source_type>D</source_type></version><title>Mean-field Behaviour of Neural Tangent Kernel for Deep Neural Networks</title><authors>Soufiane Hayou, Arnaud Doucet, Judith Rousseau</authors><categories>stat.ML cs.LG</categories><comments>23 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work by Jacot et al. (2018) has showed that training a neural network\\nof any kind with gradient descent in parameter space is equivalent to kernel\\ngradient descent in function space with respect to the Neural Tangent Kernel\\n(NTK). Lee et al. (2019) built on this result to show that the output of a\\nneural network trained using full batch gradient descent can be approximated by\\na linear model for wide networks. In parallel, a recent line of studies\\n(Schoenholz et al. (2017), Hayou et al. (2019)) suggested that a special\\ninitialization known as the Edge of Chaos leads to good performance. In this\\npaper, we bridge the gap between these two concepts and show the impact of the\\ninitialization and the activation function on the NTK as the network depth\\nbecomes large. We provide experiments illustrating our theoretical results.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.00170</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.00170</id><submitter>Herilalaina Rakotoarison</submitter><version version=\"v1\"><date>Sat, 1 Jun 2019 07:19:18 GMT</date><size>1513kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 20:42:05 GMT</date><size>1513kb</size><source_type>D</source_type></version><title>Automated Machine Learning with Monte-Carlo Tree Search</title><authors>Herilalaina Rakotoarison, Marc Schoenauer, Mich\\\\`ele Sebag</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The AutoML task consists of selecting the proper algorithm in a machine\\nlearning portfolio, and its hyperparameter values, in order to deliver the best\\nperformance on the dataset at hand. Mosaic, a Monte-Carlo tree search (MCTS)\\nbased approach, is presented to handle the AutoML hybrid structural and\\nparametric expensive black-box optimization problem. Extensive empirical\\nstudies are conducted to independently assess and compare: i) the optimization\\nprocesses based on Bayesian optimization or MCTS; ii) its warm-start\\ninitialization; iii) the ensembling of the solutions gathered along the search.\\nMosaic is assessed on the OpenML 100 benchmark and the Scikit-learn portfolio,\\nwith statistically significant gains over Auto-Sklearn, winner of former\\ninternational AutoML challenges.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.00380</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.00380</id><submitter>Xiaolei Fang</submitter><version version=\"v1\"><date>Sun, 2 Jun 2019 10:09:20 GMT</date><size>7kb</size></version><version version=\"v2\"><date>Fri, 9 Aug 2019 03:52:48 GMT</date><size>10kb</size></version><version version=\"v3\"><date>Sat, 28 Sep 2019 11:26:24 GMT</date><size>11kb</size></version><title>New MDS Euclidean Self-orthogonal Codes</title><authors>Xiaolei Fang, Meiqing Liu, Jinquan Luo</authors><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a criterion of MDS Euclidean self-orthogonal codes is\\npresented. New MDS Euclidean self-dual codes and self-orthogonal codes are\\nconstructed via this criterion. In particular, among our constructions, for\\nlarge square $q$, about $\\\\frac{1}{8}\\\\cdot q$ new MDS Euclidean (almost)\\nself-dual codes over $\\\\F_q$ can be produced. Moreover, we can construct about\\n$\\\\frac{1}{4}\\\\cdot q$ new MDS Euclidean self-orthogonal codes with different\\neven lengths $n$ with dimension $\\\\frac{n}{2}-1$.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.00411</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.00411</id><submitter>Serhad Sarica</submitter><version version=\"v1\"><date>Sun, 2 Jun 2019 14:11:37 GMT</date><size>1258kb</size></version><version version=\"v2\"><date>Tue, 4 Jun 2019 07:45:57 GMT</date><size>1260kb</size></version><version version=\"v3\"><date>Wed, 7 Aug 2019 06:49:27 GMT</date><size>1566kb</size></version><version version=\"v4\"><date>Fri, 4 Oct 2019 17:28:20 GMT</date><size>1977kb</size></version><title>TechNet: Technology Semantic Network Based on Patent Data</title><authors>Serhad Sarica, Jianxi Luo and Kristin L. Wood</authors><categories>cs.IR cs.CL</categories><comments>Expert Systems With Applications</comments><doi>10.1016/j.eswa.2019.112995</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing developments in general semantic networks, knowledge graphs and\\nontology databases have motivated us to build a large-scale comprehensive\\nsemantic network of technology-related data for engineering knowledge\\ndiscovery, technology search and retrieval, and artificial intelligence for\\nengineering design and innovation. Specially, we constructed a technology\\nsemantic network (TechNet) that covers the elemental concepts in all domains of\\ntechnology and their semantic associations by mining the complete U.S. patent\\ndatabase from 1976. To derive the TechNet, natural language processing\\ntechniques were utilized to extract terms from massive patent texts and recent\\nword embedding algorithms were employed to vectorize such terms and establish\\ntheir semantic relationships. We report and evaluate the TechNet for retrieving\\nterms and their pairwise relevance that is meaningful from a technology and\\nengineering design perspective. The TechNet may serve as an infrastructure to\\nsupport a wide range of applications, e.g., technical text summaries, search\\nquery predictions, relational knowledge discovery, and design ideation support,\\nin the context of engineering and technology, and complement or enrich existing\\nsemantic databases. To enable such applications, the TechNet is made public via\\nan online interface and APIs for public users to retrieve technology-related\\nterms and their relevancies.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.00454</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.00454</id><submitter>Saeed Khaki</submitter><version version=\"v1\"><date>Sun, 2 Jun 2019 17:25:01 GMT</date><size>4494kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 13 Jun 2019 01:19:44 GMT</date><size>4606kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 5 Aug 2019 21:17:59 GMT</date><size>6721kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Tue, 1 Oct 2019 19:39:36 GMT</date><size>6749kb</size><source_type>D</source_type></version><title>Classification of Crop Tolerance to Heat and Drought: A Deep\\n  Convolutional Neural Networks Approach</title><authors>Saeed Khaki, Zahra Khalilzadeh and Lizhi Wang</authors><categories>cs.LG q-bio.QM stat.AP stat.ML</categories><comments>15 pages, 13 figures. Won the Best Paper Award of the Second\\n  International Workshop on Machine Learning for Cyber-Agricultural Systems\\n  (Ames, IA, USA). One of the winning solutions to the 2019 INFORMS Syngenta\\n  Crop Challenge. Presented at 2019 INFORMS Conference on Business Analytics\\n  and Operations Research (Austin, TX, USA)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Environmental stresses such as drought and heat can cause substantial yield\\nloss in agriculture. As such, hybrid crops that are tolerant to drought and\\nheat stress would produce more consistent yields compared to the hybrids that\\nare not tolerant to these stresses. In the 2019 Syngenta Crop Challenge,\\nSyngenta released several large datasets that recorded the yield performances\\nof 2,452 corn hybrids planted in 1,560 locations between 2008 and 2017 and\\nasked participants to classify the corn hybrids as either tolerant or\\nsusceptible to drought stress, heat stress, and combined drought and heat\\nstress. However, no data was provided that classified any set of hybrids as\\ntolerant or susceptible to any type of stress. In this paper, we present an\\nunsupervised approach to solving this problem, which was recognized as one of\\nthe winners in the 2019 Syngenta Crop Challenge. Our results labeled 121\\nhybrids as drought tolerant, 193 as heat tolerant, and 29 as tolerant to both\\nstresses.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.00562</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.00562</id><submitter>Qianru Sun</submitter><version version=\"v1\"><date>Mon, 3 Jun 2019 04:09:32 GMT</date><size>4478kb</size></version><version version=\"v2\"><date>Sun, 29 Sep 2019 04:43:05 GMT</date><size>5035kb</size></version><title>Learning to Self-Train for Semi-Supervised Few-Shot Classification</title><authors>Xinzhe Li, Qianru Sun, Yaoyao Liu, Shibao Zheng, Qin Zhou, Tat-Seng\\n  Chua, and Bernt Schiele</authors><categories>cs.CV cs.LG stat.ML</categories><comments>33rd Conference on Neural Information Processing Systems (NeurIPS\\n  2019), Vancouver, Canada</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Few-shot classification (FSC) is challenging due to the scarcity of labeled\\ntraining data (e.g. only one labeled data point per class). Meta-learning has\\nshown to achieve promising results by learning to initialize a classification\\nmodel for FSC. In this paper we propose a novel semi-supervised meta-learning\\nmethod called learning to self-train (LST) that leverages unlabeled data and\\nspecifically meta-learns how to cherry-pick and label such unsupervised data to\\nfurther improve performance. To this end, we train the LST model through a\\nlarge number of semi-supervised few-shot tasks. On each task, we train a\\nfew-shot model to predict pseudo labels for unlabeled data, and then iterate\\nthe self-training steps on labeled and pseudo-labeled data with each step\\nfollowed by fine-tuning. We additionally learn a soft weighting network (SWN)\\nto optimize the self-training weights of pseudo labels so that better ones can\\ncontribute more to gradient descent optimization. We evaluate our LST method on\\ntwo ImageNet benchmarks for semi-supervised few-shot classification and achieve\\nlarge improvements over the state-of-the-art method. Code is at\\nhttps://github.com/xinzheli1217/learning-to-self-train.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.00579</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.00579</id><submitter>Johanes Effendi</submitter><version version=\"v1\"><date>Mon, 3 Jun 2019 05:25:42 GMT</date><size>281kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 05:09:27 GMT</date><size>280kb</size><source_type>D</source_type></version><title>From Speech Chain to Multimodal Chain: Leveraging Cross-modal Data\\n  Augmentation for Semi-supervised Learning</title><authors>Johanes Effendi, Andros Tjandra, Sakriani Sakti, Satoshi Nakamura</authors><categories>cs.CL cs.SD eess.AS</categories><comments>Accepted in IEEE ASRU 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previously, a machine speech chain, which is based on sequence-to-sequence\\ndeep learning, was proposed to mimic speech perception and production behavior.\\nSuch chains separately processed listening and speaking by automatic speech\\nrecognition (ASR) and text-to-speech synthesis (TTS) and simultaneously enabled\\nthem to teach each other in semi-supervised learning when they received\\nunpaired data. Unfortunately, this speech chain study is limited to speech and\\ntextual modalities. In fact, natural communication is actually multimodal and\\ninvolves both auditory and visual sensory systems. Although the said speech\\nchain reduces the requirement of having a full amount of paired data, in this\\ncase we still need a large amount of unpaired data. In this research, we take a\\nfurther step and construct a multimodal chain and design a closely knit chain\\narchitecture that combines ASR, TTS, image captioning, and image production\\nmodels into a single framework. The framework allows the training of each\\ncomponent without requiring a large number of parallel multimodal data. Our\\nexperimental results also show that an ASR can be further trained without\\nspeech and text data and cross-modal data augmentation remains possible through\\nour proposed chain, which improves the ASR performance.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.00582</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.00582</id><submitter>Chaobing Song</submitter><version version=\"v1\"><date>Mon, 3 Jun 2019 05:27:59 GMT</date><size>523kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 19:19:52 GMT</date><size>561kb</size><source_type>D</source_type></version><title>Unified Acceleration of High-Order Algorithms under H\\\\&quot;{o}lder\\n  Continuity and Uniform Convexity</title><authors>Chaobing Song, Yong Jiang and Yi Ma</authors><categories>math.OC cs.AI cs.LG</categories><comments>39 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, through a very intuitive vanilla proximal method perspective,\\nwe derive accelerated high-order optimization algorithms for minimizing a\\nconvex function that has H\\\\&quot;{o}lder continuous derivatives. In this general\\nconvex setting, we propose a concise unified acceleration framework (UAF),\\nwhich reconciles the two different high-order acceleration approaches, one by\\nNesterov and Baes [29, 3, 33] and one by Monteiro and Svaiter [25]. As result,\\nthe UAF unifies the high-order acceleration instances [29, 3, 33, 15, 16, 25,\\n19, 6, 14] of the two approaches by only two problem-related parameters and two\\nadditional parameters for framework design. Furthermore, the UAF (and its\\nanalysis) is the first approach to make high-order methods applicable for\\nhigh-order smoothness conditions with respect to non-Euclidean norms. If the\\nfunction is further uniformly convex, we propose a general restart scheme for\\nthe UAF. The iteration complexities of instances of both the UAF and the\\nrestarted UAF match existing lower bounds in most important cases [2, 16]. For\\npractical implementation, we introduce a new and effective heuristic that\\nsignificantly simplifies the binary search procedure required by the framework.\\nWe use experiments to verify the effectiveness of the heuristic and demonstrate\\nclear and consistent advantages of high-order acceleration methods over\\nfirst-order ones, in terms of run-time complexity. Finally, the UAF is proposed\\ndirectly in the general composite convex setting, thus show that the existing\\nhigh-order algorithms [29, 3, 33, 16, 6, 14] can be naturally extended to the\\ngeneral composite convex setting.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.00744</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.00744</id><submitter>Denis Yarats</submitter><version version=\"v1\"><date>Mon, 3 Jun 2019 12:28:50 GMT</date><size>3614kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 12 Jun 2019 08:46:09 GMT</date><size>3614kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 20 Aug 2019 20:53:30 GMT</date><size>3614kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Tue, 27 Aug 2019 19:24:48 GMT</date><size>3614kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Wed, 2 Oct 2019 16:10:21 GMT</date><size>3618kb</size><source_type>D</source_type></version><title>Hierarchical Decision Making by Generating and Following Natural\\n  Language Instructions</title><authors>Hengyuan Hu, Denis Yarats, Qucheng Gong, Yuandong Tian, Mike Lewis</authors><categories>cs.AI cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore using latent natural language instructions as an expressive and\\ncompositional representation of complex actions for hierarchical decision\\nmaking. Rather than directly selecting micro-actions, our agent first generates\\na latent plan in natural language, which is then executed by a separate model.\\nWe introduce a challenging real-time strategy game environment in which the\\nactions of a large number of units must be coordinated across long time scales.\\nWe gather a dataset of 76 thousand pairs of instructions and executions from\\nhuman play, and train instructor and executor models. Experiments show that\\nmodels using natural language as a latent variable significantly outperform\\nmodels that directly imitate human actions. The compositional structure of\\nlanguage proves crucial to its effectiveness for action representation. We also\\nrelease our code, models and data.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.00884</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.00884</id><submitter>Haoye Dong</submitter><version version=\"v1\"><date>Mon, 3 Jun 2019 15:43:33 GMT</date><size>15883kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 16:47:46 GMT</date><size>15883kb</size><source_type>D</source_type></version><title>Fashion Editing with Adversarial Parsing Learning</title><authors>Haoye Dong, Xiaodan Liang, Yixuan Zhang, Xujie Zhang, Zhenyu Xie,\\n  Bowen Wu, Ziqi Zhang, Xiaohui Shen, Jian Yin</authors><categories>cs.CV eess.IV</categories><comments>22 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interactive fashion image manipulation, which enables users to edit images\\nwith sketches and color strokes, is an interesting research problem with great\\napplication value. Existing works often treat it as a general inpainting task\\nand do not fully leverage the semantic structural information in fashion\\nimages. Moreover, they directly utilize conventional convolution and\\nnormalization layers to restore the incomplete image, which tends to wash away\\nthe sketch and color information. In this paper, we propose a novel Fashion\\nEditing Generative Adversarial Network (FE-GAN), which is capable of\\nmanipulating fashion images by free-form sketches and sparse color strokes.\\nFE-GAN consists of two modules: 1) a free-form parsing network that learns to\\ncontrol the human parsing generation by manipulating sketch and color; 2) a\\nparsing-aware inpainting network that renders detailed textures with semantic\\nguidance from the human parsing map. A new attention normalization layer is\\nfurther applied at multiple scales in the decoder of the inpainting network to\\nenhance the quality of the synthesized image. Extensive experiments on\\nhigh-resolution fashion image datasets demonstrate that the proposed method\\nsignificantly outperforms the state-of-the-art methods on image manipulation.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.01094</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.01094</id><submitter>Cecilia Jarne</submitter><version version=\"v1\"><date>Mon, 3 Jun 2019 21:56:48 GMT</date><size>7098kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 13 Sep 2019 16:23:44 GMT</date><size>7180kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 1 Oct 2019 14:18:27 GMT</date><size>7194kb</size><source_type>D</source_type></version><title>A detailed study of recurrent neural networks used to model tasks in the\\n  cerebral cortex</title><authors>C. Jarne and R. Laje</authors><categories>q-bio.NC cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent Neural Networks or RNN are frequently used to model different\\naspects of brain regions. We studied the properties of RNN trained to perform\\ntemporal and flow control tasks with temporal stimuli. We present the results\\nregarding three aspects: inner configuration sets, memory capacity with the\\nscale and immunity to induced damage on trained networks. Our results allow us\\nto quantify different aspects of these physical models, which are normally used\\nas black boxes and must be understood previous to modeling the biological\\nresponse of cerebral cortex.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.01139</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.01139</id><submitter>Daniel Hsu</submitter><version version=\"v1\"><date>Tue, 4 Jun 2019 00:22:10 GMT</date><size>609kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 13:10:42 GMT</date><size>784kb</size><source_type>D</source_type></version><title>On the number of variables to use in principal component regression</title><authors>Ji Xu and Daniel Hsu</authors><categories>math.ST cs.LG stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study least squares linear regression over $N$ uncorrelated Gaussian\\nfeatures that are selected in order of decreasing variance. When the number of\\nselected features $p$ is at most the sample size $n$, the estimator under\\nconsideration coincides with the principal component regression estimator; when\\n$p&gt;n$, the estimator is the least $\\\\ell_2$ norm solution over the selected\\nfeatures. We give an average-case analysis of the out-of-sample prediction\\nerror as $p,n,N \\\\to \\\\infty$ with $p/N \\\\to \\\\alpha$ and $n/N \\\\to \\\\beta$, for some\\nconstants $\\\\alpha \\\\in [0,1]$ and $\\\\beta \\\\in (0,1)$. In this average-case\\nsetting, the prediction error exhibits a &quot;double descent&quot; shape as a function\\nof $p$. We also establish conditions under which the minimum risk is achieved\\nin the interpolating ($p&gt;n$) regime.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.01211</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.01211</id><submitter>Jean-Philip Piquemal</submitter><version version=\"v1\"><date>Tue, 4 Jun 2019 06:01:37 GMT</date><size>234kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 8 Aug 2019 10:18:58 GMT</date><size>246kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sat, 14 Sep 2019 07:52:09 GMT</date><size>246kb</size><source_type>D</source_type></version><title>Raising the Performance of the Tinker-HP Molecular Modeling Package\\n  [Article v1.0]</title><authors>Luc-Henri Jolly, Alejandro Duran, Louis Lagard\\\\`ere, Jay W. Ponder,\\n  Pengyu Ren, Jean-Philip Piquemal</authors><categories>cs.MS physics.chem-ph physics.comp-ph</categories><doi>10.33011/livecoms.1.2.10409</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This living paper reviews the present High Performance Computing (HPC)\\ncapabilities of the Tinker-HP molecular modeling package. We focus here on the\\nreference, double precision, massively parallel molecular dynamics engine\\npresent in Tinker-HP and dedicated to perform large scale simulations. We show\\nhow it can be adapted to recent Intel Central Processing Unit (CPU) petascale\\narchitectures. First, we discuss the new set of Intel Advanced Vector\\nExtensions 512 (Intel AVX-512) instructions present in recent Intel processors\\n(e.g., the Intel Xeon Scalable and Intel Xeon Phi 2nd generation processors)\\nallowing for larger vectorization enhancements. These instructions constitute\\nthe central source of potential computational gains when using the latest\\nprocessors, justifying important vectorization efforts for developers. We then\\nbriefly review the organization of the Tinker-HP code and identify the\\ncomputational hotspots which require Intel AVX-512 optimization and we propose\\na general and optimal strategy to vectorize those particular parts of the code.\\nWe intended to present our optimization strategy in a pedagogical way so it\\ncould benefit to other researchers and students interested in gaining\\nperformances in their own software. Finally we present the performance\\nenhancements obtained compared to the unoptimized code both sequentially and at\\nthe scaling limit in parallel for classical non-polarizable (CHARMM) and\\npolarizable force fields (AMOEBA). This paper never ceases to be updated as we\\naccumulate new data on the associated Github repository between new versions of\\nthis living paper.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.01799</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.01799</id><submitter>Pooja Gupta</submitter><version version=\"v1\"><date>Wed, 5 Jun 2019 03:13:32 GMT</date><size>414kb</size></version><title>A Decentralized IoT Data Marketplace</title><authors>P. Gupta, S. Kanhere, R. Jurdak</authors><categories>cs.NI</categories><comments>6 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an architecture for dynamic decentralized marketplace for\\ntrading of Internet of Things data. To this end, we introduce a 3-tier\\nframework which consists of provider, consumer and broker. The framework is\\nrealized using multiple trustless broker which matches and selects potential\\ndata provider based on the consumers requirements. Rather than using a\\ncentralized server to manage the contract between provider and consumer, the\\nframework leverages smart contract-based agreement for automatically enforcing\\nthe terms of the contract to the involved parties.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.01861</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.01861</id><submitter>Wataru Kawai</submitter><version version=\"v1\"><date>Wed, 5 Jun 2019 07:33:06 GMT</date><size>1156kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 12:07:53 GMT</date><size>3919kb</size><source_type>D</source_type></version><title>Scalable Generative Models for Graphs with Graph Attention Mechanism</title><authors>Wataru Kawai, Yusuke Mukuta, Tatsuya Harada</authors><categories>cs.LG stat.ML</categories><comments>22 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphs are ubiquitous real-world data structures, and generative models that\\napproximate distributions over graphs and derive new samples from them have\\nsignificant importance. Among the known challenges in graph generation tasks,\\nscalability handling of large graphs and datasets is one of the most important\\nfor practical applications. Recently, an increasing number of graph generative\\nmodels have been proposed and have demonstrated impressive results. However,\\nscalability is still an unresolved problem due to the complex generation\\nprocess or difficulty in training parallelization. In this paper, we first\\ndefine scalability from three different perspectives: number of nodes, data,\\nand node/edge labels. Then, we propose GRAM, a generative model for graphs that\\nis scalable in all three contexts, especially in training. We aim to achieve\\nscalability by employing a novel graph attention mechanism, formulating the\\nlikelihood of graphs in a simple and general manner. Also, we apply two\\ntechniques to reduce computational complexity. Furthermore, we construct a\\nunified and non-domain-specific evaluation metric in node/edge-labeled graph\\ngeneration tasks by combining a graph kernel and Maximum Mean Discrepancy. Our\\nexperiments on synthetic and real-world graphs demonstrated the scalability of\\nour models and their superior performance compared with baseline methods.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.01891</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.01891</id><submitter>Florian Dubost</submitter><version version=\"v1\"><date>Wed, 5 Jun 2019 09:08:38 GMT</date><size>1341kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 14 Jun 2019 15:44:55 GMT</date><size>1340kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 2 Oct 2019 21:17:34 GMT</date><size>1518kb</size><source_type>D</source_type></version><title>Weakly Supervised Object Detection with 2D and 3D Regression Neural\\n  Networks</title><authors>Florian Dubost, Hieab Adams, Pinar Yilmaz, Gerda Bortsova, Gijs van\\n  Tulder, M. Arfan Ikram, Wiro Niessen, Meike Vernooij, Marleen de Bruijne</authors><categories>cs.CV</categories><comments>New formatting. A few changes in introduction, discussion and\\n  conclusion</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding automatically multiple lesions in large images is a common problem in\\nmedical image analysis. Solving this problem can be challenging if, during\\noptimization, the automated method cannot access information about the location\\nof the lesions nor is given single examples of the lesions. We propose a new\\nweakly supervised detection method using neural networks, that computes\\nattention maps revealing the locations of brain lesions. These attention maps\\nare computed using the last feature maps of a segmentation network optimized\\nonly with global image-level labels. The proposed method can generate attention\\nmaps at full input resolution without need for interpolation during\\npreprocessing, which allows small lesions to appear in attention maps. For\\ncomparison, we modify state-of-the-art methods to compute attention maps for\\nweakly supervised object detection, by using a global regression objective\\ninstead of the more conventional classification objective. This regression\\nobjective optimizes the number of occurrences of the target object in an image,\\ne.g. the number of brain lesions in a scan, or the number of digits in an\\nimage. We study the behavior of the proposed method in MNIST-based detection\\ndatasets, and evaluate it for the challenging detection of enlarged\\nperivascular spaces - a type of brain lesion - in a dataset of 2202 3D scans\\nwith point-wise annotations in the center of all lesions in four brain regions.\\nIn the brain dataset, the weakly supervised detection methods come close to the\\nhuman intrarater agreement in each region. The proposed method reaches the best\\narea under the curve in two out of four regions, and has the lowest number of\\nfalse positive detections in all regions, while its average sensitivity over\\nall regions is similar to that of the other best methods. The proposed method\\ncan facilitate epidemiological and clinical studies of enlarged perivascular\\nspaces.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.02249</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.02249</id><submitter>Dmitry Kopitkov</submitter><version version=\"v1\"><date>Wed, 5 Jun 2019 18:58:44 GMT</date><size>3262kb</size><source_type>D</source_type></version><title>General Purpose Incremental Covariance Update and Efficient Belief Space\\n  Planning via Factor-Graph Propagation Action Tree</title><authors>Dmitry Kopitkov and Vadim Indelman</authors><categories>cs.RO stat.ML</categories><doi>10.1177/0278364919875199</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fast covariance calculation is required both for SLAM (e.g.~in order to solve\\ndata association) and for evaluating the information-theoretic term for\\ndifferent candidate actions in belief space planning (BSP). In this paper we\\nmake two primary contributions. First, we develop a novel general-purpose\\nincremental covariance update technique, which efficiently recovers specific\\ncovariance entries after any change in the inference problem, such as\\nintroduction of new observations/variables or re-linearization of the state\\nvector. Our approach is shown to recover them faster than other\\nstate-of-the-art methods. Second, we present a computationally efficient\\napproach for BSP in high-dimensional state spaces, leveraging our incremental\\ncovariance update method. State of the art BSP approaches perform belief\\npropagation for each candidate action and then evaluate an objective function\\nthat typically includes an information-theoretic term, such as entropy or\\ninformation gain. Yet, candidate actions often have similar parts (e.g. common\\ntrajectory parts), which are however evaluated separately for each candidate.\\nMoreover, calculating the information-theoretic term involves a costly\\ndeterminant computation of the entire information (covariance) matrix which is\\nO(n^3) with n being dimension of the state or costly Schur complement\\noperations if only marginal posterior covariance of certain variables is of\\ninterest. Our approach, rAMDL-Tree, extends our previous BSP method rAMDL, by\\nexploiting incremental covariance calculation and performing calculation re-use\\nbetween common parts of non-myopic candidate actions, such that these parts are\\nevaluated only once, in contrast to existing approaches.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.02562</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.02562</id><submitter>Osama Haq</submitter><version version=\"v1\"><date>Thu, 6 Jun 2019 13:10:27 GMT</date><size>2466kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 25 Sep 2019 02:30:37 GMT</date><size>1607kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 26 Sep 2019 23:34:35 GMT</date><size>1607kb</size><source_type>D</source_type></version><title>Judicious QoS using Cloud Overlays</title><authors>Osama Haq (1), Cody Doucette (2), John W. Byers (2), and Fahad R.\\n  Dogar (1) ((1) Tufts University, (2) Boston University)</authors><categories>cs.NI</categories><comments>Compared to the previous version, we have made a number of changes,\\n  including new experiments on RIPE ATLAS testbed to evaluate the feasibility\\n  of our services, discussion on end-to-end working of the system, and several\\n  other changes in writing to clarify ambiguities in design or positioning of\\n  the work. arXiv admin note: substantial text overlap with arXiv:1812.10835</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the long-standing problem of providing network QoS to\\napplications, and propose the concept of judicious QoS -- combining the\\ncheaper, best effort IP service with the cloud, which offers a highly reliable\\ninfrastructure and the ability to add in-network services, albeit at higher\\ncost. Our proposed J-QoS framework offers a range of reliability services with\\ndifferent cost vs. delay trade-offs, including: i) a forwarding service that\\nforwards packets over the cloud overlay, ii) a caching service, which stores\\npackets inside the cloud and allows them to be pulled in case of packet loss or\\ndisruption on the Internet, and iii) a novel coding service that provides the\\nleast expensive packet recovery option by combining packets of multiple\\napplication streams and sending a small number of coded packets across the more\\nexpensive cloud paths. We demonstrate the feasibility of these services using\\nmeasurements from RIPE Atlas and a live deployment on PlanetLab. We also\\nconsider case studies on how J-QoS works with services up and down the network\\nstack, including Skype video conferencing, TCP-based web transfers, and\\ncellular access networks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.02795</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.02795</id><submitter>Yan Zhang</submitter><version version=\"v1\"><date>Thu, 6 Jun 2019 20:16:40 GMT</date><size>224kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 22:43:55 GMT</date><size>246kb</size><source_type>D</source_type></version><title>FSPool: Learning Set Representations with Featurewise Sort Pooling</title><authors>Yan Zhang and Jonathon Hare and Adam Pr\\\\&quot;ugel-Bennett</authors><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional set prediction models can struggle with simple datasets due to an\\nissue we call the responsibility problem. We introduce a pooling method for\\nsets of feature vectors based on sorting features across elements of the set.\\nThis can be used to construct a permutation-equivariant auto-encoder that\\navoids this responsibility problem. On a toy dataset of polygons and a set\\nversion of MNIST, we show that such an auto-encoder produces considerably\\nbetter reconstructions and representations. Replacing the pooling function in\\nexisting set encoders with FSPool improves accuracy and convergence speed on a\\nvariety of datasets.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.02864</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.02864</id><submitter>Nicholas Ralph Mr</submitter><version version=\"v1\"><date>Fri, 7 Jun 2019 02:20:49 GMT</date><size>11342kb</size><source_type>D</source_type></version><title>Radio Galaxy Zoo: Unsupervised Clustering of Convolutionally\\n  Auto-encoded Radio-astronomical Images</title><authors>Nicholas O. Ralph, Ray P. Norris, Gu Fang, Laurence A. F. Park,\\n  Timothy J. Galvin, Matthew J. Alger, Heinz Andernach, Chris Lintott, Lawrence\\n  Rudnick, Stanislav Shabala, and O. Ivy Wong</authors><categories>astro-ph.IM cs.LG</categories><comments>Accepted in Publications of the Astronomical Society of the Pacific,\\n  special issue on Machine Intelligence in Astronomy and Astrophysics. 23\\n  pages, 8 full-page colour figures</comments><doi>10.1088/1538-3873/ab213d</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper demonstrates a novel and efficient unsupervised clustering method\\nwith the combination of a Self-Organising Map (SOM) and a convolutional\\nautoencoder. The rapidly increasing volume of radio-astronomical data has\\nincreased demand for machine learning methods as solutions to classification\\nand outlier detection. Major astronomical discoveries are unplanned and found\\nin the unexpected, making unsupervised machine learning highly desirable by\\noperating without assumptions and labelled training data. Our approach shows\\nSOM training time is drastically reduced and high-level features can be\\nclustered by training on auto-encoded feature vectors instead of raw images.\\nOur results demonstrate this method is capable of accurately separating\\noutliers on a SOM with neighbourhood similarity and K-means clustering of\\nradio-astronomical features complexity. We present this method as a powerful\\nnew approach to data exploration by providing a detailed understanding of the\\nmorphology and relationships of Radio Galaxy Zoo (RGZ) dataset image features\\nwhich can be applied to new radio survey data.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.02944</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.02944</id><submitter>Han-Jia Ye</submitter><version version=\"v1\"><date>Fri, 7 Jun 2019 08:07:05 GMT</date><size>5521kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 06:27:48 GMT</date><size>6092kb</size><source_type>D</source_type></version><title>Learning Classifier Synthesis for Generalized Few-Shot Learning</title><authors>Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, Fei Sha</authors><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object recognition in real-world requires handling long-tailed or even\\nopen-ended data. An ideal visual system needs to reliably recognize the\\npopulated visual concepts and meanwhile efficiently learn about emerging new\\ncategories with a few training instances. Class-balanced many-shot learning and\\nfew-shot learning tackle one side of this problem, via either learning strong\\nclassifiers for populated categories or learning to learn few-shot classifiers\\nfor the tail classes. In this paper, we investigate the problem of generalized\\nfew-shot learning (GFSL) -- a model during the deployment is required to not\\nonly learn about &quot;tail&quot; categories with few shots, but simultaneously classify\\nthe &quot;head&quot; and &quot;tail&quot; categories. We propose the Classifier Synthesis Learning\\n(CASTLE), a learning framework that learns how to synthesize calibrated\\nfew-shot classifiers in addition to the multi-class classifiers of &quot;head&quot;\\nclasses, leveraging a shared neural dictionary. CASTLE sheds light upon the\\ninductive GFSL through optimizing one clean and effective GFSL learning\\nobjective. It demonstrates superior performances than existing GFSL algorithms\\nand strong baselines on MiniImageNet and TieredImageNet data sets. More\\ninterestingly, it outperforms previous state-of-the-art methods when evaluated\\non standard few-shot learning.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.02975</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.02975</id><submitter>Eduardo Fonseca</submitter><version version=\"v1\"><date>Fri, 7 Jun 2019 09:09:56 GMT</date><size>550kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 14 Jul 2019 23:05:39 GMT</date><size>356kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sun, 29 Sep 2019 17:39:18 GMT</date><size>366kb</size><source_type>D</source_type></version><title>Audio tagging with noisy labels and minimal supervision</title><authors>Eduardo Fonseca, Manoj Plakal, Frederic Font, Daniel P. W. Ellis and\\n  Xavier Serra</authors><categories>cs.SD cs.LG eess.AS stat.ML</categories><comments>DCASE2019 Workshop</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper introduces Task 2 of the DCASE2019 Challenge, titled &quot;Audio\\ntagging with noisy labels and minimal supervision&quot;. This task was hosted on the\\nKaggle platform as &quot;Freesound Audio Tagging 2019&quot;. The task evaluates systems\\nfor multi-label audio tagging using a large set of noisy-labeled data, and a\\nmuch smaller set of manually-labeled data, under a large vocabulary setting of\\n80 everyday sound classes. In addition, the proposed dataset poses an acoustic\\nmismatch problem between the noisy train set and the test set due to the fact\\nthat they come from different web audio sources. This can correspond to a\\nrealistic scenario given by the difficulty in gathering large amounts of\\nmanually labeled data. We present the task setup, the FSDKaggle2019 dataset\\nprepared for this scientific evaluation, and a baseline system consisting of a\\nconvolutional neural network. All these resources are freely available.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.03113</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.03113</id><submitter>Paul Burkhardt</submitter><version version=\"v1\"><date>Fri, 7 Jun 2019 14:06:23 GMT</date><size>38kb</size></version><version version=\"v2\"><date>Thu, 3 Oct 2019 14:43:59 GMT</date><size>52kb</size></version><title>Optimal algebraic Breadth-First Search for sparse graphs</title><authors>Paul Burkhardt</authors><categories>cs.DS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been a rise in the popularity of algebraic methods for graph\\nalgorithms given the development of the GraphBLAS library and other sparse\\nmatrix methods. An exemplar for these approaches is Breadth-First Search (BFS).\\nThe algebraic BFS algorithm is simply a recursion of matrix-vector\\nmultiplications with the $n \\\\times n$ adjacency matrix, but the many redundant\\noperations over nonzeros ultimately lead to suboptimal performance. Therefore\\nan optimal algebraic BFS should be of keen interest especially if it is easily\\nintegrated with existing matrix methods.\\n  Current methods, notably in the GraphBLAS, use a Sparse Matrix Sparse Vector\\n(SpMSpV) multiplication in which the input vector is kept in a sparse\\nrepresentation in each step of the BFS. But simply applying SpMSpV in BFS does\\nnot lead to optimal runtime. Each nonzero in the vector must be masked in\\nsubsequent steps. This has been an area of recent research in GraphBLAS and\\nother libraries. While in theory these masking methods are asymptotically\\noptimal on sparse graphs, many add work that leads to suboptimal runtime. We\\ngive a new optimal, algebraic BFS for sparse graphs that is also a constant\\nfactor faster than theoretically optimal SpMSpV methods, closing a gap in the\\nliterature.\\n  Our method multiplies progressively smaller submatrices of the adjacency\\nmatrix at each step, taking $O(m)$ algebraic operations on a sparse graph of\\n$O(m)$ edges as opposed to $O(mn)$ operations of other sparse matrix\\napproaches. Thus for sparse graphs it matches the bounds of the best-known\\nsequential algorithm and on a Parallel Random Access Machine (PRAM) it is\\nwork-optimal. Compared to a leading GraphBLAS library our method achieves up to\\n24x faster sequential time and for parallel computation it can be 17x faster on\\nlarge graphs and 12x faster on large-diameter graphs.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.03299</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.03299</id><submitter>Zhiheng Kang</submitter><version version=\"v1\"><date>Fri, 7 Jun 2019 19:06:24 GMT</date><size>5980kb</size></version><version version=\"v2\"><date>Mon, 30 Sep 2019 04:04:47 GMT</date><size>5341kb</size></version><title>PyramNet: Point Cloud Pyramid Attention Network and Graph Embedding\\n  Module for Classification and Segmentation</title><authors>Kang Zhiheng and Li Ning</authors><categories>cs.CV cs.GR cs.RO</categories><comments>Accepted for presentation at ICONIP2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the tide of artificial intelligence, we try to apply deep learning to\\nunderstand 3D data. Point cloud is an important 3D data structure, which can\\naccurately and directly reflect the real world. In this paper, we propose a\\nsimple and effective network, which is named PyramNet, suites for point cloud\\nobject classification and semantic segmentation in 3D scene. We design two new\\noperators: Graph Embedding Module(GEM) and Pyramid Attention Network(PAN).\\nSpecifically, GEM projects point cloud onto the graph and practices the\\ncovariance matrix to explore the relationship between points, so as to improve\\nthe local feature expression ability of the model. PAN assigns some strong\\nsemantic features to each point to retain fine geometric features as much as\\npossible. Furthermore, we provide extensive evaluation and analysis for the\\neffectiveness of PyramNet. Empirically, we evaluate our model on ModelNet40,\\nShapeNet and S3DIS.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.03563</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.03563</id><submitter>Jingkang Wang</submitter><version version=\"v1\"><date>Sun, 9 Jun 2019 04:32:13 GMT</date><size>1893kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 15:49:55 GMT</date><size>2000kb</size><source_type>D</source_type></version><title>Towards A Unified Min-Max Framework for Adversarial Exploration and\\n  Robustness</title><authors>Jingkang Wang, Tianyun Zhang, Sijia Liu, Pin-Yu Chen, Jiacen Xu, Makan\\n  Fardad, Bo Li</authors><categories>cs.LG cs.CR cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The worst-case training principle that minimizes the maximal adversarial\\nloss, also known as adversarial training (AT), has shown to be a\\nstate-of-the-art approach for enhancing adversarial robustness against\\nnorm-ball bounded input perturbations. Nonetheless, min-max optimization beyond\\nthe purpose of AT has not been rigorously explored in the research of\\nadversarial attack and defense. In particular, given a set of risk sources\\n(domains), minimizing the maximal loss induced from the domain set can be\\nreformulated as a general min-max problem that is different from AT. Examples\\nof this general formulation include attacking model ensembles, devising\\nuniversal perturbation under multiple inputs or data transformations, and\\ngeneralized AT over different types of attack models. We show that these\\nproblems can be solved under a unified and theoretically principled min-max\\noptimization framework. We also show that the self-adjusted domain weights\\nlearned from our method provides a means to explain the difficulty level of\\nattack and defense over multiple domains. Extensive experiments show that our\\napproach leads to substantial performance improvement over the conventional\\naveraging strategy.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.03750</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.03750</id><submitter>Yao Ma</submitter><version version=\"v1\"><date>Mon, 10 Jun 2019 01:00:07 GMT</date><size>235kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 21:58:52 GMT</date><size>252kb</size><source_type>D</source_type></version><title>Attacking Graph Convolutional Networks via Rewiring</title><authors>Yao Ma, Suhang Wang, Tyler Derr, Lingfei Wu and Jiliang Tang</authors><categories>cs.LG cs.CR stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph Neural Networks (GNNs) have boosted the performance of many graph\\nrelated tasks such as node classification and graph classification. Recent\\nresearches show that graph neural networks are vulnerable to adversarial\\nattacks, which deliberately add carefully created unnoticeable perturbation to\\nthe graph structure. The perturbation is usually created by adding/deleting a\\nfew edges, which might be noticeable even when the number of edges modified is\\nsmall. In this paper, we propose a graph rewiring operation which affects the\\ngraph in a less noticeable way compared to adding/deleting edges. We then use\\nreinforcement learning to learn the attack strategy based on the proposed\\nrewiring operation. Experiments on real world graphs demonstrate the\\neffectiveness of the proposed framework. To understand the proposed framework,\\nwe further analyze how its generated perturbation to the graph structure\\naffects the output of the target model.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.03752</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.03752</id><submitter>S. S. Ravi</submitter><version version=\"v1\"><date>Mon, 10 Jun 2019 01:04:42 GMT</date><size>24kb</size></version><version version=\"v2\"><date>Wed, 2 Oct 2019 15:55:50 GMT</date><size>25kb</size></version><title>Symmetry Properties of Nested Canalyzing Functions</title><authors>Daniel J. Rosenkrantz, Madhav V. Marathe, S. S. Ravi and Richard E.\\n  Stearns</authors><categories>cs.DM cs.DS</categories><comments>17 pages</comments><msc-class>68R99 (primary), 68W01 (secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many researchers have studied symmetry properties of various Boolean\\nfunctions. A class of Boolean functions, called nested canalyzing functions\\n(NCFs), has been used to model certain biological phenomena. We identify some\\ninteresting relationships between NCFs, symmetric Boolean functions and a\\ngeneralization of symmetric Boolean functions, which we call $r$-symmetric\\nfunctions (where $r$ is the symmetry level). Using a normalized representation\\nfor NCFs, we develop a characterization of when two variables of an NCF are\\nsymmetric. Using this characterization, we show that the symmetry level of an\\nNCF $f$ can be easily computed given a standard representation of $f$. We also\\npresent an algorithm for testing whether a given $r$-symmetric function is an\\nNCF. Further, we show that for any NCF $f$ with $n$ variables, the notion of\\nstrong asymmetry considered in the literature is equivalent to the property\\nthat $f$ is $n$-symmetric. We use this result to derive a closed form\\nexpression for the number of $n$-variable Boolean functions that are NCFs and\\nstrongly asymmetric. We also identify all the Boolean functions that are NCFs\\nand symmetric.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.03757</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.03757</id><submitter>Ruth Ikwu Dr</submitter><version version=\"v1\"><date>Mon, 10 Jun 2019 01:32:32 GMT</date><size>371kb</size></version><version version=\"v2\"><date>Wed, 2 Oct 2019 17:05:15 GMT</date><size>141kb</size></version><title>Identifying Data And Information Streams In Cyberspace: A\\n  Multi-Dimensional Perspective</title><authors>Ruth Ikwu</authors><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyberspace has gradually replaced the physical reality, its role evolving\\nfrom a simple enabler of daily live processes to a necessity for modern\\nexistence. As a result of this convergence of physical and virtual realities,\\nfor all processes being critically dependent on networked communications,\\ninformation representative of our physical, logical and social thoughts is\\nconstantly being generated in cyberspace. The interconnection and integration\\nof links between our physical and virtual realities creates a new hyperspace as\\na source of data and information. Additionally, significant studies in cyber\\nanalysis have predominantly revolved around a single linear analysis of\\ninformation from a single source of evidence (The Network). These studies are\\nlimited in their ability to understand the dynamics of relationships across the\\nmultiple dimensions of cyberspace. This paper introduces a multi-dimensional\\nperspective for data identification in cyberspace. It provides critical\\ndiscussions for identifying entangled relationships amongst entities across\\ncyberspace.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.03764</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.03764</id><submitter>Adam Harley</submitter><version version=\"v1\"><date>Mon, 10 Jun 2019 01:53:42 GMT</date><size>6907kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 24 Jun 2019 02:02:58 GMT</date><size>6923kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 10 Jul 2019 23:02:29 GMT</date><size>6479kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Mon, 30 Sep 2019 18:52:19 GMT</date><size>8915kb</size><source_type>D</source_type></version><title>Visual Representation Learning with 3D View-Contrastive Inverse Graphics\\n  Networks</title><authors>Adam W. Harley and Fangyu Li and Shrinidhi K. Lakshmikanth and Xian\\n  Zhou and Hsiao-Yu Fish Tung and Katerina Fragkiadaki</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predictive coding theories suggest that the brain learns by predicting\\nobservations at various levels of abstraction. One of the most basic prediction\\ntasks is view prediction: how would a given scene look from an alternative\\nviewpoint? Humans excel at this task. Our ability to imagine and fill in\\nmissing visual information is tightly coupled with perception: we feel as if we\\nsee the world in 3 dimensions, while in fact, information from only the front\\nsurface of the world hits our (2D) retinas. This paper explores the connection\\nbetween view-predictive representation learning and its role in the development\\nof 3D visual recognition. We propose inverse graphics networks, which take as\\ninput 2.5D video streams captured by a moving camera, and map to stable 3D\\nfeature maps of the scene, by disentangling the scene content from the motion\\nof the camera. The model can also project its 3D feature maps to novel\\nviewpoints, to predict and match against target views. We propose contrastive\\nprediction losses that can handle stochasticity of the visual input and can\\nscale view-predictive learning to more photorealistic scenes than those\\nconsidered in previous works. We show that the proposed model learns 3D visual\\nrepresentations useful for (1) semi-supervised learning of 3D object detectors,\\nand (2) unsupervised learning of 3D moving object detectors, by estimating\\nmotion of the inferred 3D feature maps in videos of dynamic scenes. To the best\\nof our knowledge, this is the first work that empirically shows view prediction\\nto be a useful and scalable self-supervised task beneficial to 3D object\\ndetection.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.03840</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.03840</id><submitter>Nicolas Robinson-Garcia</submitter><version version=\"v1\"><date>Mon, 10 Jun 2019 08:44:40 GMT</date><size>893kb</size></version><version version=\"v2\"><date>Tue, 8 Oct 2019 11:22:42 GMT</date><size>829kb</size></version><title>Indicators of Open Access for universities</title><authors>Nicolas Robinson-Garcia, Rodrigo Costas and Thed N. van Leeuwen</authors><categories>cs.DL</categories><comments>Paper accepted for oral presentation at the ISSI 2019 Conference held\\n  in Rome 2-5 September, 2019. Corrected figure 2 and renumbered some figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a first attempt to analyse Open Access integration at the\\ninstitutional level. For this, we combine information from Unpaywall and the\\nLeiden Ranking to offer basic OA indicators for universities. We calculate the\\noverall number of Open Access publications for 930 universities worldwide. OA\\nindicators are also disaggregated by green, gold and hybrid Open Access. We\\nthen explore differences between and within countries and offer a general\\nranking of universities based on the proportion of their output which is openly\\naccessible.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.04349</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.04349</id><submitter>Jack Parker-Holder</submitter><version version=\"v1\"><date>Tue, 11 Jun 2019 02:06:51 GMT</date><size>5600kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 19 Jun 2019 14:57:54 GMT</date><size>5600kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sun, 29 Sep 2019 16:02:32 GMT</date><size>7565kb</size><source_type>D</source_type></version><title>Behavior-Guided Reinforcement Learning</title><authors>Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, Anna Choromanska,\\n  Krzysztof Choromanski, Michael I. Jordan</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new approach for comparing reinforcement learning policies,\\nusing Wasserstein distances (WDs) in a newly defined latent behavioral space.\\nWe show that by utilizing the dual formulation of the WD, we can learn score\\nfunctions over trajectories that can be in turn used to lead policy\\noptimization towards (or away from) (un)desired behaviors. Combined with\\nsmoothed WDs, the dual formulation allows us to devise efficient algorithms\\nthat take stochastic gradient descent steps through WD regularizers. We\\nincorporate these regularizers into two novel on-policy algorithms,\\nBehavior-Guided Policy Gradient and Behavior-Guided Evolution Strategies, which\\nwe demonstrate can outperform existing methods in a variety of challenging\\nenvironments. We also provide an open source demo.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.04426</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.04426</id><submitter>Andreas Guillot</submitter><version version=\"v1\"><date>Tue, 11 Jun 2019 08:04:39 GMT</date><size>1336kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 08:54:36 GMT</date><size>1321kb</size><source_type>D</source_type></version><title>Chocolatine: Outage Detection for Internet Background Radiation</title><authors>Andreas Guillot, Romain Fontugne, Philipp Winter, Pascal Merindol,\\n  Alistair King, Alberto Dainotti, Cristel Pelsser</authors><categories>cs.NI</categories><comments>TMA 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet is a complex ecosystem composed of thousands of Autonomous\\nSystems (ASs) operated by independent organizations; each AS having a very\\nlimited view outside its own network. These complexities and limitations impede\\nnetwork operators to finely pinpoint the causes of service degradation or\\ndisruption when the problem lies outside of their network. In this paper, we\\npresent Chocolatine, a solution to detect remote connectivity loss using\\nInternet Background Radiation (IBR) through a simple and efficient method. IBR\\nis unidirectional unsolicited Internet traffic, which is easily observed by\\nmonitoring unused address space. IBR features two remarkable properties: it is\\noriginated worldwide, across diverse ASs, and it is incessant. We show that the\\nnumber of IP addresses observed from an AS or a geographical area follows a\\nperiodic pattern. Then, using Seasonal ARIMA to statistically model IBR data,\\nwe predict the number of IPs for the next time window. Significant deviations\\nfrom these predictions indicate an outage. We evaluated Chocolatine using data\\nfrom the UCSD Network Telescope, operated by CAIDA, with a set of documented\\noutages. Our experiments show that the proposed methodology achieves a good\\ntrade-off between true-positive rate (90%) and false-positive rate (2%) and\\nlargely outperforms CAIDA\\'s own IBR-based detection method. Furthermore,\\nperforming a comparison against other methods, i.e., with BGP monitoring and\\nactive probing, we observe that Chocolatine shares a large common set of\\noutages with them in addition to many specific outages that would otherwise go\\nundetected.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.04536</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.04536</id><submitter>Armand Boschin</submitter><version version=\"v1\"><date>Tue, 11 Jun 2019 12:47:59 GMT</date><size>182kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 2 Jul 2019 18:08:38 GMT</date><size>184kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 4 Oct 2019 13:27:17 GMT</date><size>228kb</size><source_type>D</source_type></version><title>WikiDataSets: Standardized sub-graphs from Wikidata</title><authors>Armand Boschin, Thomas Bonald</authors><categories>cs.LG cs.AI cs.SI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developing new ideas and algorithms in the fields of graph processing and\\nrelational learning requires public datasets. While Wikidata is the largest\\nopen source knowledge graph, involving more than fifty million entities, it is\\nlarger than needed in many cases and even too large to be processed easily.\\nStill, it is a goldmine of relevant facts and relations. Using this knowledge\\ngraph is time consuming and prone to task specific tuning which can affect\\nreproducibility of results. Providing a unified framework to extract\\ntopic-specific subgraphs solves this problem and allows researchers to evaluate\\nalgorithms on common datasets. This paper presents various topic-specific\\nsubgraphs of Wikidata along with the generic Python code used to extract them.\\nThese datasets can help develop new methods of knowledge graph processing and\\nrelational learning.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.04657</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.04657</id><submitter>Katrin Mang</submitter><version version=\"v1\"><date>Tue, 11 Jun 2019 15:36:46 GMT</date><size>1461kb</size><source_type>D</source_type></version><title>Mesh adaptivity for quasi-static phase-field fractures based on a\\n  residual-type a posteriori error estimator</title><authors>Katrin Mang, Mirjam Walloth, Thomas Wick, Winnifried Wollner</authors><categories>math.NA cs.NA</categories><comments>This is the preprint version of an accepted article to be published\\n  in the GAMM-Mitteilungen 2019.\\n  https://onlinelibrary.wiley.com/journal/15222608</comments><doi>10.1002/gamm.202000003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider adaptive mesh refinement for a monolithic\\nphase-field description for fractures in brittle materials. Our approach is\\nbased on an a posteriori error estimator for the phase-field variational\\ninequality realizing the fracture irreversibility constraint. The key goal is\\nthe development of a reliable and efficient residual-type error estimator for\\nthe phase-field fracture model in each time-step. Based on this error\\nestimator, error indicators for local mesh adaptivity are extracted. The\\nproposed estimator is based on a technique known for singularly perturbed\\nequations in combination with estimators for variational inequalities. These\\ntheoretical developments are used to formulate an adaptive mesh refinement\\nalgorithm. For the numerical solution, the fracture irreversibility is imposed\\nusing a Lagrange multiplier. The resulting saddle-point system has three\\nunknowns: displacements, phase-field, and a Lagrange multiplier for the crack\\nirreversibility. Several numerical experiments demonstrate our theoretical\\nfindings with the newly developed estimators and the corresponding refinement\\nstrategy.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.05132</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.05132</id><submitter>Nicolas Liochon</submitter><version version=\"v1\"><date>Wed, 12 Jun 2019 13:38:25 GMT</date><size>2035kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 18:18:13 GMT</date><size>72kb</size></version><title>Handel: Practical Multi-Signature Aggregation for Large Byzantine\\n  Committees</title><authors>Olivier B\\\\\\'egassat, Blazej Kolad, Nicolas Gailly, Nicolas Liochon</authors><categories>cs.DC cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Handel, a Byzantine-tolerant aggregation protocol that allows for\\nthe quick aggregation of cryptographic signatures over a WAN. Handel has\\npolylogarithmic time, communication and processing complexity. We implemented\\nHandel as an open source Go library with a flexible design to support any\\nassociative and commutative aggregation function. We tested Handel on 2000 AWS\\ninstances running two nodes per instance and located in 10 AWS regions. The\\n4000 signatures are aggregated in less than 900 milliseconds with an average\\nper-node communication cost of 56KB.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.05274</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.05274</id><submitter>Benjamin Eysenbach</submitter><version version=\"v1\"><date>Wed, 12 Jun 2019 17:57:02 GMT</date><size>2181kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 13:17:24 GMT</date><size>1979kb</size><source_type>D</source_type></version><title>Efficient Exploration via State Marginal Matching</title><authors>Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric Xing, Sergey\\n  Levine, Ruslan Salakhutdinov</authors><categories>cs.LG cs.AI cs.RO stat.ML</categories><comments>Videos and code:\\n  https://sites.google.com/view/state-marginal-matching</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reinforcement learning agents need to explore their unknown environments to\\nsolve the tasks given to them. The Bayes optimal solution to exploration is\\nintractable for complex environments, and while several exploration methods\\nhave been proposed as approximations, it remains unclear what underlying\\nobjective is being optimized by existing exploration methods, or how they can\\nbe altered to incorporate prior knowledge about the task. Moreover, it is\\nunclear how to acquire a single exploration strategy that will be useful for\\nsolving multiple downstream tasks. We address these shortcomings by learning a\\nsingle exploration policy that can quickly solve a suite of downstream tasks in\\na multi-task setting, amortizing the cost of learning to explore. We recast\\nexploration as a problem of State Marginal Matching (SMM), where we aim to\\nlearn a policy for which the state marginal distribution matches a given target\\nstate distribution, which can incorporate prior knowledge about the task. We\\noptimize the objective by reducing it to a two-player, zero-sum game between a\\nstate density model and a parametric policy. Our theoretical analysis of this\\napproach suggests that prior exploration methods do not learn a policy that\\ndoes distribution matching, but acquire a replay buffer that performs\\ndistribution matching, an observation that potentially explains prior methods\\'\\nsuccess in single-task settings. On both simulated and real-world tasks, we\\ndemonstrate that our algorithm explores faster and adapts more quickly than\\nprior methods.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.05374</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.05374</id><submitter>Sarah Bechtle</submitter><version version=\"v1\"><date>Wed, 12 Jun 2019 20:55:18 GMT</date><size>398kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 01:44:23 GMT</date><size>492kb</size><source_type>D</source_type></version><title>Meta-Learning via Learned Loss</title><authors>Sarah Bechtle, Artem Molchanov, Yevgen Chebotar, Edward Grefenstette,\\n  Ludovic Righetti, Gaurav Sukhatme, Franziska Meier</authors><categories>cs.LG cs.AI cs.RO stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a meta-learning method for learning parametric loss functions that\\ncan generalize across different tasks and model architectures. We develop a\\npipeline for training such loss functions, targeted at maximizing the\\nperformance of model learning with them. We observe that the loss landscape\\nproduced by our learned losses significantly improves upon the original\\ntask-specific losses in both supervised and reinforcement learning tasks.\\nFurthermore, we show that our meta-learning framework is flexible enough to\\nincorporate additional information at meta-train time. This information shapes\\nthe learned loss function such that the environment does not need to provide\\nthis information during meta-test time\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.05458</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.05458</id><submitter>Arijit Ghosh</submitter><version version=\"v1\"><date>Thu, 13 Jun 2019 02:24:36 GMT</date><size>426kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 13:30:54 GMT</date><size>429kb</size><source_type>D</source_type></version><title>Structural Parameterization for Graph Deletion Problems over Data\\n  Streams</title><authors>Arijit Bishnu and Arijit Ghosh and Sudeshna Kolay and Gopinath Mishra\\n  and Saket Saurabh</authors><categories>cs.CC cs.DS math.CO</categories><comments>Title and introduction changed to better reflect the content of the\\n  paper; 27 pages; 7 figures</comments><msc-class>68Q05</msc-class><acm-class>F.2.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The study of parameterized streaming complexity on graph problems was\\ninitiated by Fafianie et al. (MFCS\\'14) and Chitnis et al. (SODA\\'15 and\\nSODA\\'16). Simply put, the main goal is to design streaming algorithms for\\nparameterized problems such that $O\\\\left(f(k)\\\\log^{O(1)}n\\\\right)$ space is\\nenough, where $f$ is an arbitrary computable function depending only on the\\nparameter $k$. However, in the past few years, very few positive results have\\nbeen established. Most of the graph problems that do have streaming algorithms\\nof the above nature are ones where localized checking is required, like Vertex\\nCover or Maximum Matching parameterized by the size $k$ of the solution we are\\nseeking. Many important parameterized problems that form the backbone of\\ntraditional parameterized complexity are known to require $\\\\Omega(n)$ bits for\\nany streaming algorithm; e.g., Feedback Vertex Set, Even/Odd Cycle Transversal,\\nTriangle Deletion or the more general ${\\\\cal F}$-Subgraph Deletion when\\nparameterized by solution size $k$.\\n  Our main conceptual contribution is to overcome the obstacles to efficient\\nparameterized streaming algorithms by utilizing the power of parameterization.\\nTo the best of our knowledge, this is the first work in parameterized streaming\\ncomplexity that considers structural parameters instead of the solution size as\\na parameter. We focus on the vertex cover size $K$ as the parameter for the\\nparameterized graph deletion problems we consider. At the same time, most of\\nthe previous work in parameterized streaming complexity was restricted to the\\nEA (edge arrival) or DEA (dynamic edge arrival) models. In this work, we\\nconsider the above mentioned graph deletion problems in the four most\\nwell-studied streaming models, i.e., the EA, DEA, VA (vertex arrival) and AL\\n(adjacency list) models.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.05591</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.05591</id><submitter>Mark Kozdoba</submitter><version version=\"v1\"><date>Thu, 13 Jun 2019 10:36:43 GMT</date><size>282kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 15:32:07 GMT</date><size>765kb</size><source_type>D</source_type></version><title>Variance Estimation For Dynamic Regression via Spectrum Thresholding</title><authors>Mark Kozdoba and Edward Moroshko and Shie Mannor and Koby Crammer</authors><categories>cs.LG cs.SY eess.SY stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the dynamic linear regression problem, where the predictor vector\\nmay vary with time. This problem can be modelled as a linear dynamical system,\\nwhere the parameters that need to be learned are the variance of both the\\nprocess noise and the observation noise. The classical approach to learning the\\nvariance is via the maximum likelihood estimator -- a non-convex optimization\\nproblem prone to local minima and with no finite sample complexity bounds. In\\nthis paper we study the global system operator: the operator that maps the\\nnoises vectors to the output. In particular, we obtain estimates on its\\nspectrum, and as a result derive the {\\\\em first known} variance estimators with\\nsample complexity guarantees for dynamic regression problems. We evaluate the\\napproach on synthetic and real-world benchmarks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.05743</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.05743</id><submitter>Chen Sun</submitter><version version=\"v1\"><date>Thu, 13 Jun 2019 15:03:52 GMT</date><size>1930kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 27 Sep 2019 21:59:59 GMT</date><size>108kb</size><source_type>D</source_type></version><title>Learning Video Representations using Contrastive Bidirectional\\n  Transformer</title><authors>Chen Sun, Fabien Baradel, Kevin Murphy, Cordelia Schmid</authors><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a self-supervised learning approach for video features\\nthat results in significantly improved performance on downstream tasks (such as\\nvideo classification, captioning and segmentation) compared to existing\\nmethods. Our method extends the BERT model for text sequences to the case of\\nsequences of real-valued feature vectors, by replacing the softmax loss with\\nnoise contrastive estimation (NCE). We also show how to learn representations\\nfrom sequences of visual features and sequences of words derived from ASR\\n(automatic speech recognition), and show that such cross-modal training (when\\npossible) helps even more.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.05774</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.05774</id><submitter>Alexios Balatsoukas-Stimming</submitter><version version=\"v1\"><date>Thu, 13 Jun 2019 16:02:52 GMT</date><size>65kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 14:44:59 GMT</date><size>65kb</size><source_type>D</source_type></version><title>Deep Unfolding for Communications Systems: A Survey and Some New\\n  Directions</title><authors>Alexios Balatsoukas-Stimming and Christoph Studer</authors><categories>eess.SP cs.IT math.IT</categories><comments>IEEE Workshop on Signal Processing Systems (SiPS) 2019, special\\n  session on &quot;Practical Machine-Learning-Aided Communications Systems.&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep unfolding is a method of growing popularity that fuses iterative\\noptimization algorithms with tools from neural networks to efficiently solve a\\nrange of tasks in machine learning, signal and image processing, and\\ncommunication systems. This survey summarizes the principle of deep unfolding\\nand discusses its recent use for communication systems with focus on detection\\nand precoding in multi-antenna (MIMO) wireless systems and belief propagation\\ndecoding of error-correcting codes. To showcase the efficacy and generality of\\ndeep unfolding, we describe a range of other tasks relevant to communication\\nsystems that can be solved using this emerging paradigm. We conclude the survey\\nby outlining a list of open research problems and future research directions.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.05827</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.05827</id><submitter>Blake Woodworth</submitter><version version=\"v1\"><date>Thu, 13 Jun 2019 17:16:12 GMT</date><size>367kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 17:07:19 GMT</date><size>450kb</size><source_type>D</source_type></version><title>Kernel and Rich Regimes in Overparametrized Models</title><authors>Blake Woodworth, Suriya Gunasekar, Pedro Savarese, Edward Moroshko,\\n  Itay Golan, Jason Lee, Daniel Soudry, Nathan Srebro</authors><categories>cs.LG stat.ML</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recent line of work studies overparametrized neural networks in the &quot;kernel\\nregime,&quot; i.e. when the network behaves during training as a kernelized linear\\npredictor, and thus training with gradient descent has the effect of finding\\nthe minimum RKHS norm solution. This stands in contrast to other studies which\\ndemonstrate how gradient descent on overparametrized multilayer networks can\\ninduce rich implicit biases that are not RKHS norms. Building on an observation\\nby Chizat and Bach, we show how the scale of the initialization controls the\\ntransition between the &quot;kernel&quot; (aka lazy) and &quot;rich&quot; (aka active) regimes and\\naffects generalization properties in multilayer homogeneous models. We provide\\na complete and detailed analysis for a simple two-layer model that already\\nexhibits an interesting and meaningful transition between the kernel and rich\\nregimes, and we demonstrate the transition for more complex matrix\\nfactorization models and multilayer non-linear networks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.05846</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.05846</id><submitter>Marc Serra-Garcia</submitter><version version=\"v1\"><date>Thu, 13 Jun 2019 17:47:49 GMT</date><size>1242kb</size><source_type>D</source_type></version><title>Turing complete mechanical processor via automated nonlinear system\\n  design</title><authors>Marc Serra-Garcia</authors><categories>cs.ET</categories><journal-ref>Phys. Rev. E 100, 042202 (2019)</journal-ref><doi>10.1103/PhysRevE.100.042202</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nanomechanical computers promise a greatly improved energetic efficiency\\ncompared to their electrical counterparts. However, progress towards this goal\\nis hindered by a lack of modular components, such as logic gates or\\ntransistors, and systematic design strategies. This article describes a\\nuniversal logic gate implemented as a nonlinear mass-spring-damper model,\\nfollowed by an automated method to translate computations, expressed as source\\ncode of arbitrary complexity, into combinations of this basic building block.\\nThe proposed approach is validated numerically in two steps: First, a set of\\ndiscrete models are generated from code. The models implement computations with\\nincreasing complexity, starting by a simple adder and ending in a 8-bit Turing\\ncomplete mechanical processor. Then, the models are forward integrated to\\ndemonstrate their computing performance. The processor is validated by\\nexecuting the Erathostenes\\' sieve algorithm to mechanically compute prime\\nnumbers.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.06412</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.06412</id><submitter>Dominik Seitz</submitter><version version=\"v1\"><date>Fri, 31 May 2019 15:40:26 GMT</date><size>37kb</size></version><version version=\"v2\"><date>Wed, 2 Oct 2019 14:30:54 GMT</date><size>1343kb</size><source_type>D</source_type></version><title>Value Functions for Depth-Limited Solving in Imperfect-Information Games\\n  beyond Poker</title><authors>Dominik Seitz, Vojt\\\\v{e}ch Kova\\\\v{r}\\\\\\'ik, Viliam Lis\\\\\\'y, Jan Rudolf,\\n  Shuo Sun, Karel Ha</authors><categories>cs.AI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Depth-limited look-ahead search is an essential tool for agents playing\\nperfect-information games. In imperfect information games, the lack of a clear\\ndefinition of a value of a state makes designing theoretically sound\\ndepth-limited solving algorithms substantially more difficult. Furthermore,\\nmost results in this direction only consider the domain of poker. We propose a\\ndomain and algorithm independent definition of a value function in general\\nextensive-form games, formally analyze its uniqueness, structure, and compact\\nrepresentations. In an empirical study, we show that neural networks can be\\neasily trained to approximate value functions in three substantially different\\ndomains. Furthermore, we analyze the influence of the precision of the value\\nfunction on the quality of the strategies produced by the depth-limited\\nequilibrium solving algorithm using it.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.06512</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.06512</id><submitter>Nicolo Pollini</submitter><version version=\"v1\"><date>Sat, 15 Jun 2019 09:53:05 GMT</date><size>630kb</size><source_type>D</source_type></version><title>Mixed projection- and density-based topology optimization with\\n  applications to structural assemblies</title><authors>Nicol\\\\`o Pollini, Oded Amir</authors><categories>cs.CE</categories><journal-ref>Structural and Multidisciplinary Optimization 2019</journal-ref><doi>10.1007/s00158-019-02390-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a mixed projection- and density-based topology\\noptimization approach. The aim is to combine the benefits of both\\nparametrizations: the explicit geometric representation provides specific\\ncontrols on certain design regions while the implicit density representation\\nprovides the ultimate design freedom elsewhere. This approach is particularly\\nsuited for structural assemblies, where the optimization of the structural\\ntopology is coupled with the optimization of the shape of the interface between\\nthe sub-components in a unified formulation. The interface between the\\nassemblies is defined by a segmented profile made of linear geometric entities.\\nThe geometric coordinates of the nodes connecting the profile segments are used\\nas shape variables in the problem, together with density variables as in\\nconventional topology optimization. The variable profile is used to locally\\nimpose specific geometric constraints or to project particular material\\nproperties. Examples of the properties considered herein are a local volume\\nconstraint, a local maximum length scale control, a variable Young\\'s modulus\\nfor the distributed solid material, and spatially variable minimum and maximum\\nlength scale. The resulting optimization approach is general and various\\ngeometric entities can be used. The potential for complex design manipulations\\nis demonstrated through several numerical examples.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.07009</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.07009</id><submitter>Miguel Sepulcre</submitter><version version=\"v1\"><date>Mon, 17 Jun 2019 13:02:45 GMT</date><size>451kb</size></version><version version=\"v2\"><date>Fri, 4 Oct 2019 08:14:31 GMT</date><size>455kb</size></version><title>Power and Packet Rate Control for Vehicular Networks in\\n  Multi-Application Scenarios</title><authors>Miguel Sepulcre, Javier Gozalvez, M. Carmen Lucas-Esta\\\\~n</authors><categories>cs.NI</categories><journal-ref>IEEE Transactions on Vehicular Technology, Volume 68, Issue 9, pp.\\n  9029 - 9037, September 2019</journal-ref><doi>10.1109/TVT.2019.2922539</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular networks require vehicles to periodically transmit 1-hop broadcast\\npackets in order to detect other vehicles in their local neighborhood. Many\\nvehicular applications depend on the correct reception of these packets that\\nare transmitted on a common control channel. Vehicles will actually be required\\nto simultaneously execute multiple applications. The transmission of the\\nbroadcast packets should hence be configured to satisfy the requirements of all\\napplications while controlling the channel load. This can be challenging when\\nvehicles simultaneously run multiple applications, and each application has\\ndifferent requirements that vary with the vehicular context (e.g. speed and\\ndensity). In this context, this paper proposes and evaluates different\\ntechniques to dynamically adapt the rate and power of 1-hop broadcast packets\\nper vehicle in multi-application scenarios. The proposed techniques are\\ndesigned to satisfy the requirements of multiple simultaneous applications and\\nreduce the channel load. The evaluation shows that the proposed techniques\\nsignificantly decrease the channel load, and can better satisfy the\\nrequirements of multiple applications compared to existing approaches, in\\nparticular the Message Handler specified in the SAE J2735 DSRC Message Set\\nDictionary.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.07207</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.07207</id><submitter>Qiaoyun Wu</submitter><version version=\"v1\"><date>Mon, 17 Jun 2019 18:14:03 GMT</date><size>3739kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 15:23:16 GMT</date><size>4967kb</size><source_type>D</source_type></version><title>NeoNav: Improving the Generalization of Visual Navigation via Generating\\n  Next Expected Observations</title><authors>Qiaoyun Wu, Dinesh Manocha, Jun Wang and Kai Xu</authors><categories>cs.RO cs.CV cs.LG</categories><comments>added experiments, 7 content pages, 4 supplementary pages,\\n  Corresponding author: Kai Xu (kevin.kai.xu@gmail.com)</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  We propose improving the cross-target and cross-scene generalization of\\nvisual navigation through learning an agent that is guided by conceiving the\\nnext observations it expects to see. This is achieved by learning a variational\\nBayesian model, called NeoNav, which generates the next expected observations\\n(NEO) conditioned on the current observations of the agent and the target view.\\nOur generative model is learned through optimizing a variational objective\\nencompassing two key designs. First, the latent distribution is conditioned on\\ncurrent observations and the target view, leading to a model-based,\\ntarget-driven navigation. Second, the latent space is modeled with a Mixture of\\nGaussians conditioned on the current observation and the next best action. Our\\nuse of mixture-of-posteriors prior effectively alleviates the issue of\\nover-regularized latent space, thus significantly boosting the model\\ngeneralization for new targets and in novel scenes. Moreover, the NEO\\ngeneration models the forward dynamics of agent-environment interaction, which\\nimproves the quality of approximate inference and hence benefits data\\nefficiency. We have conducted extensive evaluations on both real-world and\\nsynthetic benchmarks, and show that our model consistently outperforms the\\nstate-of-the-art models in terms of success rate, data efficiency, and\\ngeneralization.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.07282</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.07282</id><submitter>Alex Lu</submitter><version version=\"v1\"><date>Mon, 17 Jun 2019 21:35:00 GMT</date><size>175kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 15:05:54 GMT</date><size>177kb</size><source_type>D</source_type></version><title>The Cells Out of Sample (COOS) dataset and benchmarks for measuring\\n  out-of-sample generalization of image classifiers</title><authors>Alex X. Lu, Amy X. Lu, Wiebke Schormann, Marzyeh Ghassemi, David W.\\n  Andrews, Alan M. Moses</authors><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding if classifiers generalize to out-of-sample datasets is a\\ncentral problem in machine learning. Microscopy images provide a standardized\\nway to measure the generalization capacity of image classifiers, as we can\\nimage the same classes of objects under increasingly divergent, but controlled\\nfactors of variation. We created a public dataset of 132,209 images of mouse\\ncells, COOS-7 (Cells Out Of Sample 7-Class). COOS-7 provides a classification\\nsetting where four test datasets have increasing degrees of covariate shift:\\nsome images are random subsets of the training data, while others are from\\nexperiments reproduced months later and imaged by different instruments. We\\nbenchmarked a range of classification models using different representations,\\nincluding transferred neural network features, end-to-end classification with a\\nsupervised deep CNN, and features from a self-supervised CNN. While most\\nclassifiers perform well on test datasets similar to the training dataset, all\\nclassifiers failed to generalize their performance to datasets with greater\\ncovariate shifts. These baselines highlight the challenges of covariate shifts\\nin image data, and establish metrics for improving the generalization capacity\\nof image classifiers.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.07414</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.07414</id><submitter>Hieu-Thi Luong</submitter><version version=\"v1\"><date>Tue, 18 Jun 2019 07:24:45 GMT</date><size>2088kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 02:36:55 GMT</date><size>2299kb</size><source_type>D</source_type></version><title>A Unified Speaker Adaptation Method for Speech Synthesis using\\n  Transcribed and Untranscribed Speech with Backpropagation</title><authors>Hieu-Thi Luong and Junichi Yamagishi</authors><categories>eess.AS cs.CL cs.LG cs.SD</categories><comments>14 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By representing speaker characteristic as a single fixed-length vector\\nextracted solely from speech, we can train a neural multi-speaker speech\\nsynthesis model by conditioning the model on those vectors. This model can also\\nbe adapted to unseen speakers regardless of whether the transcript of\\nadaptation data is available or not. However, this setup restricts the speaker\\ncomponent to just a single bias vector, which in turn limits the performance of\\nadaptation process. In this study, we propose a novel speech synthesis model,\\nwhich can be adapted to unseen speakers by fine-tuning part of or all of the\\nnetwork using either transcribed or untranscribed speech. Our methodology\\nessentially consists of two steps: first, we split the conventional acoustic\\nmodel into a speaker-independent (SI) linguistic encoder and a speaker-adaptive\\n(SA) acoustic decoder; second, we train an auxiliary acoustic encoder that can\\nbe used as a substitute for the linguistic encoder whenever linguistic features\\nare unobtainable. The results of objective and subjective evaluations show that\\nadaptation using either transcribed or untranscribed speech with our\\nmethodology achieved a reasonable level of performance with an extremely\\nlimited amount of data and greatly improved performance with more data.\\nSurprisingly, adaptation with untranscribed speech surpassed the transcribed\\ncounterpart in the subjective test, which reveals the limitations of the\\nconventional acoustic model and hints at potential directions for improvements.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.07528</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.07528</id><submitter>Kevin Alexander Laube</submitter><version version=\"v1\"><date>Tue, 18 Jun 2019 12:32:30 GMT</date><size>110kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 06:59:00 GMT</date><size>221kb</size></version><title>Prune and Replace NAS</title><authors>Kevin Alexander Laube and Andreas Zell</authors><categories>cs.LG stat.ML</categories><comments>9 pages, 3 figures, 3 tables reworked, accepted at the ICMLA 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While recent NAS algorithms are thousands of times faster than the pioneering\\nworks, it is often overlooked that they use fewer candidate operations,\\nresulting in a significantly smaller search space. We present PR-DARTS, a NAS\\nalgorithm that discovers strong network configurations in a much larger search\\nspace and a single day. A small candidate operation pool is used, from which\\ncandidates are progressively pruned and replaced with better performing ones.\\nExperiments on CIFAR-10 and CIFAR-100 achieve 2.51% and 15.53% test error,\\nrespectively, despite searching in a space where each cell has 150 times as\\nmany possible configurations than in the DARTS baseline. Code is available at\\nhttps://github.com/cogsys-tuebingen/prdarts\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.07610</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.07610</id><submitter>Jeremy Barnes</submitter><version version=\"v1\"><date>Tue, 18 Jun 2019 14:31:58 GMT</date><size>138kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 09:17:24 GMT</date><size>360kb</size><source_type>D</source_type></version><title>Improving Sentiment Analysis with Multi-task Learning of Negation</title><authors>Jeremy Barnes, Erik Velldal, Lilja {\\\\O}vrelid</authors><categories>cs.CL</categories><comments>Under submission for Journal of Natural Language Engineering special\\n  issue on Negation. 30 pages with references</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Sentiment analysis is directly affected by compositional phenomena in\\nlanguage that act on the prior polarity of the words and phrases found in the\\ntext. Negation is the most prevalent of these phenomena and in order to\\ncorrectly predict sentiment, a classifier must be able to identify negation and\\ndisentangle the effect that its scope has on the final polarity of a text. This\\npaper proposes a multi-task approach to explicitly incorporate information\\nabout negation in sentiment analysis, which we show outperforms learning\\nnegation implicitly in a data-driven manner. We describe our approach, a\\ncascading neural architecture with selective sharing of LSTM layers, and show\\nthat explicitly training the model with negation as an auxiliary task helps\\nimprove the main task of sentiment analysis. The effect is demonstrated across\\nseveral different standard English-language data sets for both tasks and we\\nanalyze several aspects of our system related to its performance, varying types\\nand amounts of input data and different multi-task setups.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.07902</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.07902</id><submitter>Han Zhao</submitter><version version=\"v1\"><date>Wed, 19 Jun 2019 04:00:38 GMT</date><size>45kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 05:08:11 GMT</date><size>85kb</size><source_type>D</source_type></version><title>Adversarial Privacy Preservation under Attribute Inference Attack</title><authors>Han Zhao, Jianfeng Chi, Yuan Tian, Geoffrey J. Gordon</authors><categories>cs.LG cs.CR stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the prevalence of machine learning services, crowdsourced data\\ncontaining sensitive information poses substantial privacy challenges. Existing\\nwork focusing on protecting against membership inference attacks under the\\nrigorous framework of differential privacy are vulnerable to attribute\\ninference attacks. In light of the current gap between theory and practice, we\\ndevelop a novel theoretical framework for privacy-preservation under the attack\\nof attribute inference. Under our framework, we propose a minimax optimization\\nformulation to protect the given attribute and analyze its privacy guarantees\\nagainst arbitrary adversaries. On the other hand, it is clear that privacy\\nconstraint may cripple utility when the protected attribute is correlated with\\nthe target variable. To this end, we also prove an information-theoretic lower\\nbound to precisely characterize the fundamental trade-off between utility and\\nprivacy. Empirically, we extensively conduct experiments to corroborate our\\nprivacy guarantee and validate the inherent trade-offs in different privacy\\npreservation algorithms. Our experimental results indicate that the adversarial\\nrepresentation learning approaches achieve the best trade-off in terms of\\nprivacy preservation and utility maximization.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.07947</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.07947</id><submitter>Kun Zhan</submitter><version version=\"v1\"><date>Wed, 19 Jun 2019 07:19:06 GMT</date><size>3764kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 28 Jun 2019 07:54:52 GMT</date><size>3764kb</size><source_type>D</source_type></version><title>Generative approach to unsupervised deep local learning</title><authors>Changlu Chen, Chaoxi Niu, Xia Zhan, Kun Zhan</authors><categories>cs.CV</categories><journal-ref>Journal of Electronic Imaging, 2019</journal-ref><doi>10.1117/1.JEI.28.4.043005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most existing feature learning methods optimize inflexible handcrafted\\nfeatures and the affinity matrix is constructed by shallow linear embedding\\nmethods. Different from these conventional methods, we pretrain a generative\\nneural network by stacking convolutional autoencoders to learn the latent data\\nrepresentation and then construct an affinity graph with them as a prior. Based\\non the pretrained model and the constructed graph, we add a self-expressive\\nlayer to complete the generative model and then fine-tune it with a new loss\\nfunction, including the reconstruction loss and a deliberately defined\\nlocality-preserving loss. The locality-preserving loss designed by the\\nconstructed affinity graph serves as prior to preserve the local structure\\nduring the fine-tuning stage, which in turn improves the quality of feature\\nrepresentation effectively. Furthermore, the self-expressive layer between the\\nencoder and decoder is based on the assumption that each latent feature is a\\nlinear combination of other latent features, so the weighted combination\\ncoefficients of the self-expressive layer are used to construct a new refined\\naffinity graph for representing the data structure. We conduct experiments on\\nfour datasets to demonstrate the superiority of the representation ability of\\nour proposed model over the state-of-the-art methods.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.08479</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.08479</id><submitter>Antoine Maillard</submitter><version version=\"v1\"><date>Thu, 20 Jun 2019 07:45:10 GMT</date><size>63kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 12:33:23 GMT</date><size>68kb</size><source_type>D</source_type></version><title>High-temperature Expansions and Message Passing Algorithms</title><authors>Antoine Maillard, Laura Foini, Alejandro Lage Castellanos, Florent\\n  Krzakala, Marc M\\\\\\'ezard and Lenka Zdeborov\\\\\\'a</authors><categories>cond-mat.dis-nn cond-mat.stat-mech cs.IT math.IT math.PR</categories><comments>59 pages, updated version matching the version published in J. Stat.\\n  Mech</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Improved mean-field technics are a central theme of statistical physics\\nmethods applied to inference and learning. We revisit here some of these\\nmethods using high-temperature expansions for disordered systems initiated by\\nPlefka, Georges and Yedidia. We derive the Gibbs free entropy and the\\nsubsequent self-consistent equations for a generic class of statistical models\\nwith correlated matrices and show in particular that many classical\\napproximation schemes, such as adaptive TAP, Expectation-Consistency, or the\\napproximations behind the Vector Approximate Message Passing algorithm all rely\\non the same assumptions, that are also at the heart of high-temperature\\nexpansions. We focus on the case of rotationally invariant random coupling\\nmatrices in the `high-dimensional\\' limit in which the number of samples and the\\ndimension are both large, but with a fixed ratio. This encapsulates many widely\\nstudied models, such as Restricted Boltzmann Machines or Generalized Linear\\nModels with correlated data matrices. In this general setting, we show that all\\nthe approximation schemes described before are equivalent, and we conjecture\\nthat they are exact in the thermodynamic limit in the replica symmetric phases.\\nWe achieve this conclusion by resummation of the infinite perturbation series,\\nwhich generalizes a seminal result of Parisi and Potters. A rigorous derivation\\nof this conjecture is an interesting mathematical challenge. On the way to\\nthese conclusions, we uncover several diagrammatical results in connection with\\nfree probability and random matrix theory, that are interesting independently\\nof the rest of our work.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.08635</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.08635</id><submitter>Angelica I. Aviles-Rivero</submitter><version version=\"v1\"><date>Thu, 20 Jun 2019 14:04:39 GMT</date><size>1300kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 6 Oct 2019 20:27:11 GMT</date><size>1303kb</size><source_type>D</source_type></version><title>Beyond Supervised Classification: Extreme Minimal Supervision with the\\n  Graph 1-Laplacian</title><authors>Angelica I. Aviles-Rivero, Nicolas Papadakis, Ruoteng Li, Samar M\\n  Alsaleh, Robby T Tan, Carola-Bibiane Schonlieb</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the task of classifying when an extremely reduced amount of\\nlabelled data is available. This problem is of a great interest, in several\\nreal-world problems, as obtaining large amounts of labelled data is expensive\\nand time consuming. We present a novel semi-supervised framework for\\nmulti-class classification that is based on the non-smooth $\\\\ell_1$ norm of a\\nnormalised Dirichlet energy based on the graph Laplacian. Our transductive\\nframework is framed under a novel functional with carefully selected class\\npriors - that enforces a sufficiently smooth solution that strengthens the\\nintrinsic relation between the labelled and unlabelled data. We demonstrate\\nthrough extensive experimental results on large datasets CIFAR-10 and\\nChestX-ray14, that our method outperforms classic methods and readily competes\\nwith recent deep-learning approaches.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.08731</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.08731</id><submitter>Sandro Stucki</submitter><version version=\"v1\"><date>Thu, 20 Jun 2019 16:23:44 GMT</date><size>68kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 24 Sep 2019 13:41:38 GMT</date><size>73kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 4 Oct 2019 16:32:08 GMT</date><size>73kb</size><source_type>D</source_type></version><title>Gray-box Monitoring of Hyperproperties (Extended Version)</title><authors>Sandro Stucki, C\\\\\\'esar S\\\\\\'anchez, Gerardo Schneider and Borzoo\\n  Bonakdarpour</authors><categories>cs.LO cs.CR cs.FL</categories><comments>This is an extended version of a paper presented at the 23rd\\n  International Symposium on Formal Methods (FM \\'19). This version contains\\n  full proofs, a description of the proof-of-concept monitor for DDM, and\\n  experimental results that were not included in the original publication</comments><journal-ref>ter Beek M., McIver A., Oliveira J. (eds), Formal Methods - The\\n  Next 30 Years. FM 2019. Lecture Notes in Computer Science, vol 11800.\\n  Springer, Cham</journal-ref><doi>10.1007/978-3-030-30942-8_25</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many important system properties, particularly in security and privacy,\\ncannot be verified statically. Therefore, runtime verification is an appealing\\nalternative. Logics for hyperproperties, such as HyperLTL, support a rich set\\nof such properties. We first show that black-box monitoring of HyperLTL is in\\ngeneral unfeasible, and suggest a gray-box approach. Gray-box monitoring\\nimplies performing analysis of the system at run-time, which brings new\\nlimitations to monitorabiliy (the feasibility of solving the monitoring\\nproblem). Thus, as another contribution of this paper we refine the classic\\nnotions of monitorability, both for trace properties and hyperproperties,\\ntaking into account the computability of the monitor. We then apply our\\napproach to monitor a privacy hyperproperty called distributed data minimality,\\nexpressed as a HyperLTL property, by using an SMT-based static verifier at\\nruntime.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.08937</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.08937</id><submitter>Qiang Hao</submitter><version version=\"v1\"><date>Fri, 21 Jun 2019 04:02:12 GMT</date><size>265kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 25 Jun 2019 04:25:54 GMT</date><size>265kb</size><source_type>D</source_type></version><title>Investigating the Essential of Meaningful Automated Formative Feedback\\n  for Programming Assignments</title><authors>Qiang Hao, Jack P Wilson, Camille Ottaway, Naitra Iriumi, Kai Arakawa,\\n  David H Smith IV</authors><categories>cs.HC</categories><journal-ref>2019 IEEE Symposium on Visual Languages and Human-Centric\\n  Computing (VL/HCC)</journal-ref><doi>10.1109/VLHCC.2019.8818922</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study investigated the essential of meaningful automated feedback for\\nprogramming assignments. Three different types of feedback were tested,\\nincluding (a) What\\'s wrong - what test cases were testing and which failed, (b)\\nGap - comparisons between expected and actual outputs, and (c) Hint - hints on\\nhow to fix problems if test cases failed. 46 students taking a CS2 participated\\nin this study. They were divided into three groups, and the feedback\\nconfigurations for each group were different: (1) Group One - What\\'s wrong, (2)\\nGroup Two - What\\'s wrong + Gap, (3) Group Three - What\\'s wrong + Gap + Hint.\\nThis study found that simply knowing what failed did not help students\\nsufficiently, and might stimulate system gaming behavior. Hints were not found\\nto be impactful on student performance or their usage of automated feedback.\\nBased on the findings, this study provides practical guidance on the design of\\nautomated feedback.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.09548</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.09548</id><submitter>Phuong-Duy Nguyen</submitter><version version=\"v1\"><date>Sun, 23 Jun 2019 03:40:17 GMT</date><size>115kb</size></version><title>Computation Offloading and Resource Allocation for Backhaul Limited\\n  Cooperative MEC Systems</title><authors>Phuong-Duy Nguyen and Vu Nguyen Ha and Long Bao Le</authors><categories>cs.DC cs.NI cs.SY eess.SY</categories><journal-ref>2019 IEEE 90th Vehicular Technology Conference: VTC2019-Fall</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we jointly optimize computation offloading and resource\\nallocation to minimize the weighted sum of energy consumption of all mobile\\nusers in a backhaul limited cooperative MEC system with multiple fog servers.\\nConsidering the partial offloading strategy and TDMA transmission at each base\\nstation, the underlying optimization problem with constraints on maximum task\\nlatency and limited computation resource at mobile users and fog servers is\\nnon-convex. We propose to convexify the problem exploiting the relationship\\namong some optimization variables from which an optimal algorithm is proposed\\nto solve the resulting problem. We then present numerical results to\\ndemonstrate the significant gains of our proposed design compared to\\nconventional designs without exploiting cooperation among fog servers and a\\ngreedy algorithm.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.09682</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.09682</id><submitter>Sandra Siby</submitter><version version=\"v1\"><date>Mon, 24 Jun 2019 01:05:29 GMT</date><size>1584kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 25 Jun 2019 05:32:54 GMT</date><size>1568kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sun, 6 Oct 2019 21:37:29 GMT</date><size>2026kb</size><source_type>D</source_type></version><title>Encrypted DNS --&gt; Privacy? A Traffic Analysis Perspective</title><authors>Sandra Siby, Marc Juarez, Claudia Diaz, Narseo Vallina-Rodriguez and\\n  Carmela Troncoso</authors><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtually every connection to an Internet service is preceded by a DNS lookup\\nwhich is performed without any traffic-level protection, thus enabling\\nmanipulation, redirection, surveillance, and censorship. To address these\\nissues, large organizations such as Google and Cloudflare are deploying\\nrecently standardized protocols that encrypt DNS traffic between end users and\\nrecursive resolvers such as DNS-over-TLS (DoT) and DNS-over-HTTPS (DoH). In\\nthis paper, we examine whether encrypting DNS traffic can protect users from\\ntraffic analysis-based monitoring and censoring. We propose a novel feature set\\nto perform the attacks, as those used to attack HTTPS or Tor traffic are not\\nsuitable for DNS\\' characteristics. We show that traffic analysis enables the\\nidentification of domains with high accuracy in closed and open world settings,\\nusing 124 times less data than attacks on HTTPS flows. We find that factors\\nsuch as location, resolver, platform, or client do mitigate the attacks\\nperformance but they are far from completely stopping them. Our results\\nindicate that DNS-based censorship is still possible on encrypted DNS traffic.\\nIn fact, we demonstrate that the standardized padding schemes are not\\neffective. Yet, Tor -- which does not effectively mitigate traffic analysis\\nattacks on web traffic -- is a good defense against DoH traffic analysis.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.09880</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.09880</id><submitter>Federico Fusco</submitter><version version=\"v1\"><date>Mon, 24 Jun 2019 12:26:13 GMT</date><size>321kb</size></version><version version=\"v2\"><date>Fri, 19 Jul 2019 10:50:31 GMT</date><size>52kb</size></version><version version=\"v3\"><date>Tue, 1 Oct 2019 12:32:43 GMT</date><size>290kb</size></version><title>Online Revenue Maximization for Server Pricing</title><authors>Shant Boodaghians, Federico Fusco, Stefano Leonardi, Yishay Mansour,\\n  Ruta Mehta</authors><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient and truthful mechanisms to price resources on remote\\nservers/machines has been the subject of much work in recent years due to the\\nimportance of the cloud market. This paper considers revenue maximization in\\nthe online stochastic setting with non-preemptive jobs and a unit capacity\\nserver. One agent/job arrives at every time step, with parameters drawn from an\\nunderlying unknown distribution.\\n  We design a posted-price mechanism which can be efficiently computed, and is\\nrevenue-optimal in expectation and in retrospect, up to additive error. The\\nprices are posted prior to learning the agent\\'s type, and the computed pricing\\nscheme is deterministic, depending only on the length of the allotted time\\ninterval and on the earliest time the server is available. If the distribution\\nof agent\\'s type is only learned from observing the jobs that are executed, we\\nprove that a polynomial number of samples is sufficient to obtain a\\nnear-optimal truthful pricing strategy.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.09955</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.09955</id><submitter>Piotr Koniusz</submitter><version version=\"v1\"><date>Mon, 24 Jun 2019 13:45:21 GMT</date><size>4618kb</size><source_type>D</source_type></version><title>A Comparative Review of Recent Kinect-based Action Recognition\\n  Algorithms</title><authors>Lei Wang and Du Q. Huynh and Piotr Koniusz</authors><categories>cs.CV</categories><comments>Accepted by the IEEE Transactions on Image Processing</comments><doi>10.1109/TIP.2019.2925285</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video-based human action recognition is currently one of the most active\\nresearch areas in computer vision. Various research studies indicate that the\\nperformance of action recognition is highly dependent on the type of features\\nbeing extracted and how the actions are represented. Since the release of the\\nKinect camera, a large number of Kinect-based human action recognition\\ntechniques have been proposed in the literature. However, there still does not\\nexist a thorough comparison of these Kinect-based techniques under the grouping\\nof feature types, such as handcrafted versus deep learning features and\\ndepth-based versus skeleton-based features. In this paper, we analyze and\\ncompare ten recent Kinect-based algorithms for both cross-subject action\\nrecognition and cross-view action recognition using six benchmark datasets. In\\naddition, we have implemented and improved some of these techniques and\\nincluded their variants in the comparison. Our experiments show that the\\nmajority of methods perform better on cross-subject action recognition than\\ncross-view action recognition, that skeleton-based features are more robust for\\ncross-view recognition than depth-based features, and that deep learning\\nfeatures are suitable for large datasets.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.10040</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.10040</id><submitter>Nestor Nahuel Deniz</submitter><version version=\"v1\"><date>Mon, 24 Jun 2019 16:04:51 GMT</date><size>34kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 16:41:07 GMT</date><size>643kb</size><source_type>D</source_type></version><title>Adaptive polytopic estimation for nonlinear systems under bounded\\n  disturbances using moving horizon</title><authors>Nestor N. Deniz, Marina H. Murillo, Guido Sanchez, Leonardo L.\\n  Giovanini</authors><categories>eess.SY cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces an adaptive polytopic estimator design for nonlinear\\nsystems under bounded disturbances combining moving horizon and dual estimation\\ntechniques. It extends the moving horizon estimation results for LTI systems to\\npolytopic LPV systems. The design and necessary conditions to guarantee the\\nrobust stability and convergence to the true state and parameters for the case\\nof bounded disturbances and convergence to the true system and state are given\\nfor the vanishing disturbances.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.10189</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.10189</id><submitter>Joel Lehman</submitter><version version=\"v1\"><date>Mon, 24 Jun 2019 19:26:10 GMT</date><size>58kb</size></version><version version=\"v2\"><date>Fri, 4 Oct 2019 17:49:07 GMT</date><size>79kb</size><source_type>D</source_type></version><title>Evolutionary Computation and AI Safety: Research Problems Impeding\\n  Routine and Safe Real-world Application of Evolution</title><authors>Joel Lehman</authors><categories>cs.NE cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent developments in artificial intelligence and machine learning have\\nspurred interest in the growing field of AI safety, which studies how to\\nprevent human-harming accidents when deploying AI systems. This paper thus\\nexplores the intersection of AI safety with evolutionary computation, to show\\nhow safety issues arise in evolutionary computation and how understanding from\\nevolutionary computational and biological evolution can inform the broader\\nstudy of AI safety.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.10462</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.10462</id><submitter>Long Yang</submitter><version version=\"v1\"><date>Tue, 25 Jun 2019 11:36:18 GMT</date><size>1458kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 27 Sep 2019 18:12:00 GMT</date><size>1247kb</size><source_type>D</source_type></version><title>Policy Optimization with Stochastic Mirror Descent</title><authors>Long Yang, Gang Zheng, Haotian Zhang, Yu Zhang, Qian Zheng, Jun Wen,\\n  Gang Pan</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Improving sample efficiency has been a longstanding goal in reinforcement\\nlearning. In this paper, we propose the $\\\\mathtt{VRMPO}$: a sample efficient\\npolicy gradient method with stochastic mirror descent. A novel variance reduced\\npolicy gradient estimator is the key of $\\\\mathtt{VRMPO}$ to improve sample\\nefficiency. Our $\\\\mathtt{VRMPO}$ needs only $\\\\mathcal{O}(\\\\epsilon^{-3})$ sample\\ntrajectories to achieve an $\\\\epsilon$-approximate first-order stationary point,\\nwhich matches the best-known sample complexity. We conduct extensive\\nexperiments to show our algorithm outperforms state-of-the-art policy gradient\\nmethods in various settings.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.10657</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.10657</id><submitter>Marina Murillo</submitter><version version=\"v1\"><date>Tue, 25 Jun 2019 16:53:41 GMT</date><size>175kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 11:38:10 GMT</date><size>0kb</size><source_type>I</source_type></version><title>A Receding Horizon Framework for Autonomy in Unmanned Vehicles</title><authors>Marina Murillo and Guido S\\\\\\'anchez and Lucas Genzelis and Nahuel Deniz\\n  and Leonardo Giovanini</authors><categories>eess.SY cs.SY</categories><comments>This article was rejected for publication in Optimal Control,\\n  Applications and Methods</comments><journal-ref>Under review at Optimal Control, Applications and Methods, 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we present a unified framework based on receding horizon\\ntechniques that can be used to design the three tasks (guidance, navigation and\\npath-planning) which are involved in the autonomy of unmanned vehicles. This\\ntasks are solved using model predictive control and moving horizon estimation\\ntechniques, which allows us to include physical and dynamical constraints at\\nthe design stage, thus leading to optimal and feasible results. In order to\\ndemonstrate the capabilities of the proposed framework, we have used Gazebo\\nsimulator in order to drive a Jackal unmanned ground vehicle (UGV) along a\\ndesired path computed by the path-planning task. The results we have obtained\\nare successful as the estimation and guidance errors are small and the Jackal\\nUGV is able to follow the desired path satisfactorily and it is also capable to\\navoid the obstacles which are in its way.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.10969</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.10969</id><submitter>Mirco Sch\\\\&quot;onfeld</submitter><version version=\"v1\"><date>Wed, 26 Jun 2019 10:57:34 GMT</date><size>207kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 12:30:38 GMT</date><size>207kb</size><source_type>D</source_type></version><title>The UN Security Council debates 1995-2017</title><authors>Mirco Sch\\\\&quot;onfeld, Steffen Eckhard, Ronny Patz, Hilde van Meegdenburg</authors><categories>cs.DL</categories><comments>The UN Security Council Debates corpus is available online at\\n  https://doi.org/10.7910/DVN/KGVSYH</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new dataset containing 65,393 speeches held in the\\npublic meetings of the UN Security Council (UNSC) between 1995 and 2017. The\\ndataset is based on publicly available meeting transcripts with the S/PV\\ndocument symbol and includes the full substance of individual speeches as well\\nas automatically extracted and manually corrected metadata on the speaker, the\\nposition of the speech in the sequence of speeches of a meeting, and the date\\nof the speech. After contextualizing the dataset in recent research on the\\nUNSC, the paper presents descriptive statistics on UNSC meetings and speeches\\nthat characterize the period covered by the dataset. Data highlight the\\nextensive presence of the UN bureaucracy in UNSC meetings as well as an\\nemerging trend towards more lengthy open UNSC debates. These open debates cover\\nkey issues that have emerged only during the period that is covered by the\\ndataset, for example the debates relating to Women, Peace and Security or\\nClimate-related Disasters.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.11084</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.11084</id><submitter>W. Steven Gray</submitter><version version=\"v1\"><date>Wed, 26 Jun 2019 13:24:56 GMT</date><size>420kb</size></version><version version=\"v2\"><date>Tue, 1 Oct 2019 18:54:50 GMT</date><size>420kb</size></version><title>Combining Learning and Model Based Control via Discrete-Time Chen-Fliess\\n  Series</title><authors>W. Steven Gray, G. S. Venkatesh, Luis A. Duffaut Espinosa</authors><categories>eess.SY cs.SY</categories><comments>Fixed typos, discussed future work, updated references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A learning control system is presented suitable for control affine nonlinear\\nplants based on discrete-time Chen-Fliess series and capable of incorporating\\nknowledge of a given physical model. The underlying noncommutative algebraic\\nand combinatorial structures needed to realize the multivariable case are also\\ndescribed. The method is demonstrated using a two-input, two-output\\nLotka-Volterra system.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.11525</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.11525</id><submitter>Marc Chaumont</submitter><version version=\"v1\"><date>Thu, 27 Jun 2019 09:55:05 GMT</date><size>745kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 13:51:44 GMT</date><size>579kb</size><source_type>D</source_type></version><title>Pooled Steganalysis in JPEG: how to deal with the spreading strategy?</title><authors>Ahmad Zakaria, Marc Chaumont and G\\\\\\'erard Subsol</authors><categories>cs.CR cs.MM</categories><comments>Ahmad Zakaria, Marc Chaumont, Gerard Subsol, &quot; Pooled Steganalysis in\\n  JPEG: how to deal with the spreading strategy? &quot;, WIFS\\'2019, IEEE\\n  International Workshop on Information Forensics and Security, December 9-12,\\n  2019, Delft, The Netherlands, 6 pages, Acceptance rate = 30%</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In image pooled steganalysis, a steganalyst, Eve, aims to detect if a set of\\nimages sent by a steganographer, Alice, to a receiver, Bob, contains a hidden\\nmessage. We can reasonably assess that the steganalyst does not know the\\nstrategy used to spread the payload across images. To the best of our\\nknowledge, in this case, the most appropriate solution for pooled steganalysis\\nis to use a Single-Image Detector (SID) to estimate/quantify if an image is\\ncover or stego, and to average the scores obtained on the set of images.\\n  In such a scenario, where Eve does not know the spreading strategies, we\\nexperimentally show that if Eve can discriminate among few well-known spreading\\nstrategies, she can improve her steganalysis performances compared to a simple\\naveraging or maximum pooled approach. Our discriminative approach allows\\nobtaining steganalysis efficiencies comparable to those obtained by a\\nclairvoyant, Eve, who knows the Alice spreading strategy. Another interesting\\nobservation is that DeLS spreading strategy behaves really better than all the\\nother spreading strategies.\\n  Those observations results in the experimentation with six different\\nspreading strategies made on Jpeg images with J-UNIWARD, a state-of-the-art\\nSingle-Image-Detector, and a discriminative architecture that is invariant to\\nthe individual payload in each image, invariant to the size of the analyzed set\\nof images, and build on a binary detector (for the pooling) that is able to\\ndeal with various spreading strategies.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.11796</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.11796</id><submitter>Yedid Hoshen</submitter><version version=\"v1\"><date>Thu, 27 Jun 2019 16:58:26 GMT</date><size>1375kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 17:28:03 GMT</date><size>2903kb</size><source_type>D</source_type></version><title>Demystifying Inter-Class Disentanglement</title><authors>Aviv Gabbay and Yedid Hoshen</authors><categories>cs.LG cs.CV stat.ML</categories><comments>Project page: http://www.vision.huji.ac.il/lord</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning to disentangle the hidden factors of variations within a set of\\nobservations is a key task for artificial intelligence. We present a unified\\nformulation for class and content disentanglement and use it to illustrate the\\nlimitations of current methods. We therefore introduce \\\\textsc{LORD}, a novel\\nmethod based on Latent Optimization for Representation Disentanglement. We find\\nthat latent optimization, along with an asymmetric noise regularization, is\\nsuperior to amortized inference for achieving disentangled representations. In\\nextensive experiments, our method is shown to achieve better disentanglement\\nperformance than both adversarial and non-adversarial methods that use the same\\nlevel of supervision. We further introduce a clustering-based approach for\\nextending our method for settings that exhibit in-class variation with\\npromising results on the task of domain translation.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.11893</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.11893</id><submitter>Ahmed Elfakharany</submitter><version version=\"v1\"><date>Mon, 10 Jun 2019 06:55:14 GMT</date><size>501kb</size></version><title>HalalNet: A Deep Neural Network that Classifies the Halalness\\n  Slaughtered Chicken from their Images</title><authors>A. Elfakharany, R. Yusof, N. Ismail, R. Arfa, M. Yunus</authors><categories>cs.CV eess.IV</categories><comments>Submitted in the International Conference on Artificial Intelligence\\n  and Robotics for Industrial Applications, AIR2018</comments><journal-ref>International Journal of Integrated Engineering, Vol. 11, no. 4,\\n  Sept. 2019,\\n  https://publisher.uthm.edu.my/ojs/index.php/ijie/article/view/4194</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Halal requirement in food is important for millions of Muslims worldwide\\nespecially for meat and chicken products, insuring that slaughter houses adhere\\nto this requirement is a challenging task to do manually. In this paper a\\nmethod is proposed that uses a camera that takes images of slaughtered chicken\\non the conveyor in a slaughter house, the images are then analyzed by a deep\\nneural network to classify if the image is of a halal slaughtered chicken or\\nnot. However, traditional deep learning models require large amounts of data to\\ntrain on, which in this case these amounts of data were challenging to collect\\nespecially the images of non-halal slaughtered chicken, hence this paper shows\\nhow the use of one shot learning [1] and transfer learning [2] can reach high\\naccuracy on the few amounts of data that were available. The architecture used\\nis based on the Siamese neural networks architecture which ranks the similarity\\nbetween two inputs [3] while using the Xception network [4] as the twin\\nnetworks. We call it HalalNet. This work was done as part of SYCUT (syriah\\ncompliant slaughtering system) which is a monitoring system that monitors the\\nhalalness of the slaughtered chicken in a slaughter house. The data used to\\ntrain and validate HalalNet was collected from the Azain slaughtering site\\n(Semenyih, Selangor, Malaysia) containing images of both halal and non-halal\\nslaughtered chicken.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.11994</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.11994</id><submitter>Chaoyang He</submitter><version version=\"v1\"><date>Thu, 27 Jun 2019 23:34:45 GMT</date><size>419kb</size></version><version version=\"v2\"><date>Sat, 28 Sep 2019 20:25:05 GMT</date><size>1860kb</size></version><title>Bipartite Graph Neural Networks for Efficient Node Representation\\n  Learning</title><authors>Chaoyang He, Tian Xie, Yu Rong, Wenbing Huang, Yanfang Li, Junzhou\\n  Huang, Xiang Ren, Cyrus Shahabi</authors><categories>cs.SI cs.LG stat.ML</categories><comments>AAAI 2020</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Existing Graph Neural Networks (GNNs) mainly focus on general structures,\\nwhile the specific architecture on bipartite graphs---a crucial practical data\\nform that consists of two distinct domains of nodes---is seldom studied. In\\nthis paper, we propose Bipartite Graph Neural Network (BGNN), a novel model\\nthat is domain-consistent, unsupervised, and efficient. At its core, BGNN\\nutilizes the proposed Inter-domain Message Passing (IDMP) for message\\naggregation and Intra-domain Alignment (IDA) towards information fusion over\\ndomains, both of which are trained without requiring any supervision. Moreover,\\nwe formulate a multi-layer BGNN in a cascaded manner to enable multi-hop\\nrelation modeling while enjoying promising efficiency in training. Extensive\\nexperiments on several datasets of varying scales verify the effectiveness of\\nBGNN compared to other counterparts. Particularly for the experiment on a\\nlarge-scale bipartite graph dataset, the scalability of our BGNN is validated\\nin terms of fast training speed and low memory cost.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1906.12255</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1906.12255</id><submitter>Steven Wise</submitter><version version=\"v1\"><date>Fri, 28 Jun 2019 14:59:53 GMT</date><size>9308kb</size><source_type>D</source_type></version><title>An Energy Stable BDF2 Fourier Pseudo-Spectral Numerical Scheme for the\\n  Square Phase Field Crystal Equation</title><authors>Kelong Cheng, Cheng Wang, Steven M. Wise</authors><categories>math.NA cs.NA nlin.PS</categories><msc-class>65M12 (primary), 35K30, 35K55, 65K10, 65M70 (secondary)</msc-class><doi>10.4208/cicp.2019.js60.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose and analyze an energy stable numerical scheme for\\nthe square phase field crystal (SPFC) equation, a gradient flow modeling\\ncrystal dynamics at the atomic scale in space but on diffusive scales in time.\\nIn particular, a modification of the free energy potential to the standard\\nphase field crystal model leads to a composition of the 4-Laplacian and the\\nregular Laplacian operators. To overcome the difficulties associated with this\\nhighly nonlinear operator, we design numerical algorithms based on the\\nstructures of the individual energy terms. A Fourier pseudo-spectral\\napproximation is taken in space, in such a way that the energy structure is\\nrespected, and summation-by-parts formulae enable us to study the discrete\\nenergy stability for such a high-order spatial discretization. In the temporal\\napproximation, a second order BDF stencil is applied, combined with an\\nappropriate extrapolation for the concave diffusion term(s). A second order\\nartificial Douglas-Dupont-type regularization term is added to ensure energy\\nstability, and a careful analysis leads to the artificial linear diffusion\\ncoming at an order lower that that of surface diffusion term. Such a choice\\nleads to reduced numerical dissipation. At a theoretical level, the unique\\nsolvability, energy stability are established, and an optimal rate convergence\\nanalysis is derived in the $\\\\ell^\\\\infty (0,T; \\\\ell^2) \\\\cap \\\\ell^2 (0,T; H_N^3)$\\nnorm. In the numerical implementation, the preconditioned steepest descent\\n(PSD) iteration is applied to solve for the composition of the highly nonlinear\\n4-Laplacian term and the standard Laplacian term, and a geometric convergence\\nis assured for such an iteration. Finally, a few numerical experiments are\\npresented, which confirm the robustness and accuracy of the proposed scheme.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.00208</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.00208</id><submitter>Ziyin Liu</submitter><version version=\"v1\"><date>Sat, 29 Jun 2019 14:04:36 GMT</date><size>2516kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 08:57:03 GMT</date><size>2473kb</size><source_type>D</source_type></version><title>Deep Gamblers: Learning to Abstain with Portfolio Theory</title><authors>Liu Ziyin, Zhikang Wang, Paul Pu Liang, Ruslan Salakhutdinov,\\n  Louis-Philippe Morency, Masahito Ueda</authors><categories>cs.LG stat.ML</categories><comments>Camera-Ready version for NeurIPS2019. Link to our code updated</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We deal with the \\\\textit{selective classification} problem\\n(supervised-learning problem with a rejection option), where we want to achieve\\nthe best performance at a certain level of coverage of the data. We transform\\nthe original $m$-class classification problem to $(m+1)$-class where the\\n$(m+1)$-th class represents the model abstaining from making a prediction due\\nto disconfidence. Inspired by portfolio theory, we propose a loss function for\\nthe selective classification problem based on the doubling rate of gambling.\\nMinimizing this loss function corresponds naturally to maximizing the return of\\na \\\\textit{horse race}, where a player aims to balance between betting on an\\noutcome (making a prediction) when confident and reserving one\\'s winnings\\n(abstaining) when not confident. This loss function allows us to train neural\\nnetworks and characterize the disconfidence of prediction in an end-to-end\\nfashion. In comparison with previous methods, our method requires almost no\\nmodification to the model inference algorithm or model architecture.\\nExperiments show that our method can identify uncertainty in data points, and\\nachieves strong results on SVHN and CIFAR10 at various coverages of the data.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.00301</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.00301</id><submitter>Xinping Xu</submitter><version version=\"v1\"><date>Sun, 30 Jun 2019 00:41:39 GMT</date><size>264kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 6 Oct 2019 15:28:25 GMT</date><size>958kb</size><source_type>D</source_type></version><title>Strategic Learning Approach for Deploying UAV-provided Wireless Services</title><authors>Xinping Xu, Lingjie Duan and Minming Li</authors><categories>cs.GT</categories><comments>14 pages, 7 figures, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unmanned Aerial Vehicle (UAV) have emerged as a promising technique to\\nrapidly provide wireless services to a group of mobile users simultaneously.\\nThe paper aims to address a challenging issue that each user is selfish and may\\nmisreport his location or preference for changing the optimal UAV location to\\nbe close to himself. Using algorithmic game theory, we study how to determine\\nthe final location of a UAV in the 3D space, by ensuring all selfish users\\'\\ntruthfulness in reporting their locations for learning purpose. To minimize the\\nsocial service cost in this UAV placement game, we design strategyproof\\nmechanisms with the approximation ratios, when comparing to the social optimum.\\nWe also study the obnoxious UAV placement game to maximally keep their social\\nutility, where each incumbent user may misreport his location to keep the UAV\\naway from him. Moreover, we present the dual-preference UAV placement game by\\nconsidering the coexistence of the two groups of users above, where users can\\nmisreport both their locations and preference types (favorable or obnoxious)\\ntowards the UAV. Finally, we extend the three games above to include multiple\\nUAVs and design strategyproof mechanisms with provable approximation ratios.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.00431</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.00431</id><submitter>Rosanna Grassi</submitter><version version=\"v1\"><date>Sun, 30 Jun 2019 19:02:22 GMT</date><size>2675kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 08:01:01 GMT</date><size>2678kb</size><source_type>D</source_type></version><title>Influence measures in subnetworks using vertex centrality</title><authors>Roy Cerqueti and Gian Paolo Clemente and Rosanna Grassi</authors><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work deals with the issue of assessing the influence of a node in the\\nentire network and in the subnetwork to which it belongs as well, adapting the\\nclassical idea of vertex centrality. We provide a general definition of\\nrelative vertex centrality measure with respect to the classical one, referred\\nto the whole network. Specifically, we give a decomposition of the relative\\ncentrality measure by including also the relative influence of the single node\\nwith respect to a given subgraph containing it. The proposed measure of\\nrelative centrality is tested in the empirical networks generated by collecting\\nassets of the $S\\\\&amp;P$ 100, focusing on two specific centrality indices:\\nbetweenness and eigenvector centrality. The analysis is performed in a time\\nperspective, capturing the assets influence, with respect to the\\ncharacteristics of the analysed measures, in both the entire network and the\\nspecific sectors to which the assets belong.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.00533</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.00533</id><submitter>Travis Dick</submitter><version version=\"v1\"><date>Mon, 1 Jul 2019 04:08:40 GMT</date><size>205kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 16 Jul 2019 18:36:24 GMT</date><size>126kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 2 Oct 2019 20:32:34 GMT</date><size>166kb</size><source_type>D</source_type></version><title>Learning to Link</title><authors>Maria-Florina Balcan, Travis Dick, Manuel Lang</authors><categories>cs.LG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering is an important part of many modern data analysis pipelines,\\nincluding network analysis and data retrieval. There are many different\\nclustering algorithms developed by various communities, and it is often not\\nclear which algorithm will give the best performance on a specific clustering\\ntask. Similarly, we often have multiple ways to measure distances between data\\npoints, and the best clustering performance might require a non-trivial\\ncombination of those metrics. In this work, we study data-driven algorithm\\nselection and metric learning for clustering problems, where the goal is to\\nsimultaneously learn the best algorithm and metric for a specific application.\\nThe family of clustering algorithms we consider is parameterized linkage based\\nprocedures that includes single and complete linkage. The family of distance\\nfunctions we learn over are convex combinations of base distance functions. We\\ndesign efficient learning algorithms which receive samples from an\\napplication-specific distribution over clustering instances and simultaneously\\nlearn both a near-optimal distance and clustering algorithm from these classes.\\nWe also carry out a comprehensive empirical evaluation of our techniques\\nshowing that they can lead to significantly improved clustering performance.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.00784</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.00784</id><submitter>Charles Pillet</submitter><version version=\"v1\"><date>Mon, 1 Jul 2019 13:52:11 GMT</date><size>123kb</size></version><version version=\"v2\"><date>Mon, 7 Oct 2019 09:33:46 GMT</date><size>139kb</size></version><title>On list decoding of 5G-NR polar codes</title><authors>Charles Pillet, Valerio Bioglio, Carlo Condo</authors><categories>cs.IT math.IT</categories><comments>Submitted for WCNC 2020</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 5G standardization process of the 3GPP included polar codes concatenated\\nwith distributed cyclic redundancy check (CRC) as a channel coding scheme for\\ndownlink control information. Whereas CRC bits allow to improve the performance\\nof successive cancellation list (SCL) decoders by improving distance\\nproperties, distributed CRC bits allow for path pruning and decoding\\nearly-termination. In this letter, we show how to take advantage of the\\ndistributed CRC to improve SCL decoding, analizing various schemes having\\ndifferent early-termination and error correction properties. Simulation results\\ncompare the decoding schemes, showing different tradeoffs between\\nerror-correction performance and early-termination with different decoder\\nparameters.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.00872</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.00872</id><submitter>Marcin Wrochna</submitter><version version=\"v1\"><date>Mon, 1 Jul 2019 15:35:21 GMT</date><size>38kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 5 Jul 2019 20:35:21 GMT</date><size>40kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 4 Oct 2019 17:14:22 GMT</date><size>40kb</size><source_type>D</source_type></version><title>Improved hardness for H-colourings of G-colourable graphs</title><authors>Marcin Wrochna, Stanislav \\\\v{Z}ivn\\\\\\'y</authors><categories>cs.CC cs.DM math.AT</categories><comments>Mention improvement in Proposition 2.5. SODA 2020</comments><msc-class>05C15, 05C60, 57M15, 18B30, 05C25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present new results on approximate colourings of graphs and, more\\ngenerally, approximate H-colourings and promise constraint satisfaction\\nproblems.\\n  First, we show NP-hardness of colouring $k$-colourable graphs with\\n$\\\\binom{k}{\\\\lfloor k/2\\\\rfloor}-1$ colours for every $k\\\\geq 4$. This improves\\nthe result of Bul\\\\\\'in, Krokhin, and Opr\\\\v{s}al [STOC\\'19], who gave NP-hardness\\nof colouring $k$-colourable graphs with $2k-1$ colours for $k\\\\geq 3$, and the\\nresult of Huang [APPROX-RANDOM\\'13], who gave NP-hardness of colouring\\n$k$-colourable graphs with $2^{k^{1/3}}$ colours for sufficiently large $k$.\\nThus, for $k\\\\geq 4$, we improve from known linear/sub-exponential gaps to\\nexponential gaps.\\n  Second, we show that the topology of the box complex of H alone determines\\nwhether H-colouring of G-colourable graphs is NP-hard for all (non-bipartite,\\nH-colourable) G. This formalises the topological intuition behind the result of\\nKrokhin and Opr\\\\v{s}al [FOCS\\'19] that 3-colouring of G-colourable graphs is\\nNP-hard for all (3-colourable, non-bipartite) G. We use this technique to\\nestablish NP-hardness of H-colouring of G-colourable graphs for H that include\\nbut go beyond $K_3$, including square-free graphs and circular cliques (leaving\\n$K_4$ and larger cliques open).\\n  Underlying all of our proofs is a very general observation that adjoint\\nfunctors give reductions between promise constraint satisfaction problems.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.00900</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.00900</id><submitter>Antonio Toral</submitter><version version=\"v1\"><date>Mon, 1 Jul 2019 16:16:39 GMT</date><size>40kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 12:37:40 GMT</date><size>40kb</size><source_type>D</source_type></version><title>Post-editese: an Exacerbated Translationese</title><authors>Antonio Toral</authors><categories>cs.CL</categories><comments>Accepted at the 17th Machine Translation Summit. v2: two references\\n  added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Post-editing (PE) machine translation (MT) is widely used for dissemination\\nbecause it leads to higher productivity than human translation from scratch\\n(HT). In addition, PE translations are found to be of equal or better quality\\nthan HTs. However, most such studies measure quality solely as the number of\\nerrors. We conduct a set of computational analyses in which we compare PE\\nagainst HT on three different datasets that cover five translation directions\\nwith measures that address different translation universals and laws of\\ntranslation: simplification, normalisation and interference. We find out that\\nPEs are simpler and more normalised and have a higher degree of interference\\nfrom the source language than HTs.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.00941</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.00941</id><submitter>Yi Liu</submitter><version version=\"v1\"><date>Mon, 1 Jul 2019 17:28:08 GMT</date><size>7059kb</size></version><version version=\"v2\"><date>Tue, 2 Jul 2019 02:42:32 GMT</date><size>7058kb</size></version><version version=\"v3\"><date>Mon, 30 Sep 2019 16:13:19 GMT</date><size>3952kb</size><source_type>D</source_type></version><title>Global Pixel Transformers for Virtual Staining of Microscopy Images</title><authors>Yi Liu, Hao Yuan, Zhengyang Wang, Shuiwang Ji</authors><categories>eess.IV cs.LG stat.ML</categories><comments>10 pages, 6 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visualizing the details of different cellular structures is of great\\nimportance to elucidate cellular functions. However, it is challenging to\\nobtain high quality images of different structures directly due to complex\\ncellular environments. Fluorescence staining is a popular technique to label\\ndifferent structures but has several drawbacks. In particular, label staining\\nis time consuming and may affect cell morphology, and simultaneous labels are\\ninherently limited. This raises the need of building computational models to\\nlearn relationships between unlabeled microscopy images and labeled\\nfluorescence images, and to infer fluorescence labels of other microscopy\\nimages excluding the physical staining process. We propose to develop a novel\\ndeep model for virtual staining of unlabeled microscopy images. We first\\npropose a novel network layer, known as the global pixel transformer layer,\\nthat fuses global information from inputs effectively. The proposed global\\npixel transformer layer can generate outputs with arbitrary dimensions, and can\\nbe employed for all the regular, down-sampling, and up-sampling operators. We\\nthen incorporate our proposed global pixel transformer layers and dense blocks\\nto build an U-Net like network. We believe such a design can promote feature\\nreusing between layers. In addition, we propose a multi-scale input strategy to\\nencourage networks to capture features at different scales. We conduct\\nevaluations across various fluorescence image prediction tasks to demonstrate\\nthe effectiveness of our approach. Both quantitative and qualitative results\\nshow that our method outperforms the state-of-the-art approach significantly.\\nIt is also shown that our proposed global pixel transformer layer is useful to\\nimprove the fluorescence image prediction results.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.00957</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.00957</id><submitter>Seonghoon Woo</submitter><version version=\"v1\"><date>Mon, 1 Jul 2019 17:51:55 GMT</date><size>712kb</size></version><version version=\"v2\"><date>Mon, 30 Sep 2019 15:49:46 GMT</date><size>656kb</size></version><title>Magnetic skyrmion artificial synapse for neuromorphic computing</title><authors>Kyung Mee Song, Jae-Seung Jeong, Biao Pan, Xichao Zhang, Jing Xia, Sun\\n  Kyung Cha, Tae-Eon Park, Kwangsu Kim, Simone Finizio, Joerg Raabe, Joonyeon\\n  Chang, Yan Zhou, Weisheng Zhao, Wang Kang, Hyunsu Ju, Seonghoon Woo</authors><categories>physics.app-ph cs.ET</categories><comments>11 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the experimental discovery of magnetic skyrmions achieved one decade\\nago, there have been significant efforts to bring the virtual particles into\\nall-electrical fully functional devices, inspired by their fascinating physical\\nand topological properties suitable for future low-power electronics. Here, we\\nexperimentally demonstrate such a device: electrically-operating skyrmion-based\\nartificial synaptic device designed for neuromorphic computing. We present that\\ncontrolled current-induced creation, motion, detection and deletion of\\nskyrmions in ferrimagnetic multilayers can be harnessed in a single device at\\nroom temperature to imitate the behaviors of biological synapses. Using\\nsimulations, we demonstrate that such skyrmion-based synapses could be used to\\nperform neuromorphic pattern-recognition computing using handwritten\\nrecognition data set, reaching to the accuracy of ~89 percents, comparable to\\nthe software-based training accuracy of ~94 percents. Chip-level simulation\\nthen highlights the potential of skyrmion synapse compared to existing\\ntechnologies. Our findings experimentally illustrate the basic concepts of\\nskyrmion-based fully functional electronic devices while providing a new\\nbuilding block in the emerging field of spintronics-based bio-inspired\\ncomputing.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.01023</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.01023</id><submitter>Nader Asadi</submitter><version version=\"v1\"><date>Mon, 1 Jul 2019 19:21:22 GMT</date><size>647kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 17:43:49 GMT</date><size>879kb</size><source_type>D</source_type></version><title>Diminishing the Effect of Adversarial Perturbations via Refining Feature\\n  Representation</title><authors>Nader Asadi, AmirMohammad Sarfi, Mehrdad Hosseinzadeh, Sahba Tahsini,\\n  Mahdi Eftekhari</authors><categories>cs.CV cs.LG</categories><comments>Accepted at NeuralIPS 2019 workshop on Safety and Robustness in\\n  Decision Making</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks are highly vulnerable to adversarial examples, which\\nimposes severe security issues for these state-of-the-art models. Many defense\\nmethods have been proposed to mitigate this problem. However, a lot of them\\ndepend on modification or additional training of the target model. In this\\nwork, we analytically investigate each layer\\'s representation of non-perturbed\\nand perturbed images and show the effect of perturbations on each of these\\nrepresentations. Accordingly, a method based on whitening coloring transform is\\nproposed in order to diminish the misrepresentation of any desirable layer\\ncaused by adversaries. Our method can be applied to any layer of any arbitrary\\nmodel without the need of any modification or additional training. Due to the\\nfact that the full whitening of the layer\\'s representation is not easily\\ndifferentiable, our proposed method is superbly robust against white-box\\nattacks. Furthermore, we demonstrate the strength of our method against some\\nstate-of-the-art black-box attacks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.01095</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.01095</id><submitter>Tae Jong Choi</submitter><version version=\"v1\"><date>Mon, 1 Jul 2019 23:13:43 GMT</date><size>3294kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 21:38:08 GMT</date><size>2250kb</size><source_type>D</source_type></version><title>ACM-DE: Adaptive p-best Cauchy Mutation with linear failure threshold\\n  reduction for Differential Evolution in numerical optimization</title><authors>Tae Jong Choi, Julian Togelius, Yun-Gyung Cheong</authors><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new Cauchy mutation for improving the convergence of differential evolution\\n(DE) is proposed in this paper. DE is an efficient evolutionary algorithm for\\noptimizing multidimensional real-valued functions, which has been successfully\\napplied to various real-world problems. To improve convergence a Cauchy\\nmutation-based DE variant called modified DE was proposed, but it has serious\\nlimitations of 1) controlling the balance between exploration and exploitation;\\n2) adjusting the algorithm to a given problem; 3) having less reliable\\nperformance on multimodal problems. In this paper, we propose a new adaptive\\nCauchy mutation-based DE variant called ACM-DE (Adaptive Cauchy Mutation\\nDifferential Evolution), which removes all of these limitations. Specifically,\\ntwo popular parameter controls are employed for the exploration and\\nexploitation scheme and robust performance. Also, a less greedy approach is\\nemployed, which uses any of the top p% individuals in the phase of the Cauchy\\nmutation. Experimental results on a set of 58 benchmark problems show that\\nACM-DE is capable of finding more accurate solutions than modified DE,\\nespecially for multimodal problems. In addition, we applied ACM to two\\nstate-of-the-art DE variants, and similar to the previous results, ACM based\\nvariants exhibit significantly improved performance.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.01139</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.01139</id><submitter>Jehanzeb Chaudhry</submitter><version version=\"v1\"><date>Tue, 2 Jul 2019 03:06:52 GMT</date><size>130kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 14 Jul 2019 22:12:00 GMT</date><size>143kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 24 Jul 2019 00:08:22 GMT</date><size>131kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Tue, 8 Oct 2019 14:55:03 GMT</date><size>291kb</size><source_type>D</source_type></version><title>A posteriori error analysis for Schwarz overlapping domain decomposition\\n  methods</title><authors>Jehanzeb Chaudhry, Don Estep and Simon Tavener</authors><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain decomposition methods are widely used for the numerical solution of\\npartial differential equations on high performance computers. We develop an\\nadjoint-based a posteriori error analysis for both multiplicative and additive\\noverlapping Schwarz domain decomposition methods. The numerical error in a\\nuser-specified functional of the solution (quantity of interest) is decomposed\\ninto contributions that arise as a result of the finite iteration between the\\nsubdomains and from the spatial discretization. The spatial discretization\\ncontribution is further decomposed into contributions arising from each\\nsubdomain. This decomposition of the numerical error is used to construct a two\\nstage solution strategy that efficiently reduces the error in the quantity of\\ninterest by adjusting the relative contributions to the error.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.01216</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.01216</id><submitter>Moshe Kravchik</submitter><version version=\"v1\"><date>Tue, 2 Jul 2019 07:58:42 GMT</date><size>1655kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 16:55:00 GMT</date><size>3538kb</size><source_type>D</source_type></version><title>Efficient Cyber Attacks Detection in Industrial Control Systems Using\\n  Lightweight Neural Networks and PCA</title><authors>Moshe Kravchik, Asaf Shabtai</authors><categories>cs.CR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Industrial control systems (ICSs) are widely used and vital to industry and\\nsociety. Their failure can have severe impact on both economics and human life.\\nHence, these systems have become an attractive target for attacks, both\\nphysical and cyber. A number of attack detection methods have been proposed,\\nhowever they are characterized by a low detection rate, a substantial false\\npositive rate, or are system specific. In this paper, we study an attack\\ndetection method based on simple and lightweight neural networks, namely, 1D\\nconvolutions and autoencoders. We apply these networks to both the time and\\nfrequency domains of the collected data and discuss pros and cons of each\\napproach. We evaluate the suggested method on three popular public datasets and\\nachieve detection rates matching or exceeding previously published detection\\nresults, while featuring small footprint, short training and detection times,\\nand generality. We also demonstrate the effectiveness of PCA, which, given\\nproper data preprocessing and feature selection, can provide high attack\\ndetection scores in many settings. Finally, we study the proposed method\\'s\\nrobustness against adversarial attacks, that exploit inherent blind spots of\\nneural networks to evade detection while achieving their intended physical\\neffect. Our results show that the proposed method is robust to such evasion\\nattacks: in order to evade detection, the attacker is forced to sacrifice the\\ndesired physical impact on the system. This finding suggests that neural\\nnetworks trained under the constraints of the laws of physics can be trusted\\nmore than networks trained under more flexible conditions.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.02189</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.02189</id><submitter>Xiang Li</submitter><version version=\"v1\"><date>Thu, 4 Jul 2019 02:04:56 GMT</date><size>887kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 15:07:31 GMT</date><size>934kb</size><source_type>D</source_type></version><title>On the Convergence of FedAvg on Non-IID Data</title><authors>Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, Zhihua Zhang</authors><categories>stat.ML cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Federated learning makes a large amount of edge computing devices jointly\\nlearn a model without data sharing. As a leading algorithm in this setting,\\nFederated Averaging (\\\\texttt{FedAvg}) runs Stochastic Gradient Descent (SGD) in\\nparallel on a small subset of the total devices and averages the sequences only\\nonce in a while. Despite its simplicity, it lacks theoretical guarantees under\\nrealistic settings. In this paper, we analyze the convergence of\\n\\\\texttt{FedAvg} on non-iid data and establish a convergence rate of\\n$\\\\mathcal{O}(\\\\frac{1}{T})$ for strongly convex and smooth problems, where $T$\\nis the iteration number of SGDs. Importantly, our bound demonstrates a\\ntrade-off between communication-efficiency and convergence rate. As user\\ndevices may be disconnected from the server, we relax the assumption of full\\ndevice participation to partial device participation and study different\\naveraging schemes; low device participation rate can be achieved without\\nseverely slowing down the learning. Our results indicate that heterogeneity of\\ndata slows down the convergence, which matches empirical observations.\\nFurthermore, we provide a necessary condition for \\\\texttt{FedAvg}\\'s convergence\\non non-iid data: the learning rate $\\\\eta$ must decay, even if full-gradient is\\nused; otherwise, the solution will be $\\\\Omega (\\\\eta)$ away from the optimal.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.02482</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.02482</id><submitter>Hangjin Liu</submitter><version version=\"v1\"><date>Thu, 4 Jul 2019 16:35:30 GMT</date><size>203kb</size></version><version version=\"v2\"><date>Mon, 30 Sep 2019 22:35:29 GMT</date><size>265kb</size></version><title>Nonlinear Function Estimation with Empirical Bayes and Approximate\\n  Message Passing</title><authors>Hangjin Liu, You (Joe) Zhou, Ahmad Beirami, and Dror Baron</authors><categories>cs.IT math.IT</categories><comments>in Proc. of the 57th Annual Allerton Conference on Communication,\\n  Control, and Computing (8 pages, 2 figures)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonlinear function estimation is core to modern machine learning\\napplications. In this paper, to perform nonlinear function estimation, we\\nreduce a nonlinear inverse problem to a linear one using a polynomial kernel\\nexpansion. These kernels increase the feature set, and may result in poorly\\nconditioned matrices. Nonetheless, we show several examples where the matrix in\\nour linear inverse problem contains only mild linear correlations among\\ncolumns. The coefficients vector is modeled within a Bayesian setting for which\\napproximate message passing (AMP), an algorithmic framework for signal\\nreconstruction, offers Bayes-optimal signal reconstruction quality. While the\\nBayesian setting limits the scope of our work, it is a first step toward\\nestimation of real world nonlinear functions. The coefficients vector is\\nestimated using two AMP-based approaches, a Bayesian one and empirical Bayes.\\nNumerical results confirm that our AMP-based approaches learn the function\\nbetter than LASSO, offering markedly lower error in predicting test data.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.02565</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.02565</id><submitter>Giovanni Colavizza</submitter><version version=\"v1\"><date>Thu, 4 Jul 2019 19:14:54 GMT</date><size>446kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 23:07:32 GMT</date><size>1195kb</size><source_type>D</source_type></version><title>The citation advantage of linking publications to research data</title><authors>Giovanni Colavizza, Iain Hrynaszkiewicz, Isla Staden, Kirstie\\n  Whitaker, Barbara McGillivray</authors><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efforts to make research results open and reproducible are increasingly\\nreflected by journal policies encouraging or mandating authors to provide data\\navailability statements. As a consequence of this, there has been a strong\\nuptake of data availability statements in recent literature. Nevertheless, it\\nis still unclear what proportion of these statements actually contain\\nwell-formed links to data, for example via a URL or permanent identifier, and\\nif there is an added value in providing such links. We consider $531,889$\\njournal articles published by PLOS and BMC, develop an automatic system for\\nlabelling their data availability statements according to four categories based\\non their content and the type of data availability they display, and finally\\nanalyze the citation advantage of different statement categories via\\nregression. We find that, following mandated publisher policies, data\\navailability statements have become common by now, yet statements containing a\\nlink to a repository are still just a fraction of the total. We also find that\\narticles with these statements, in particular, can have up to 25.36\\\\% higher\\ncitation impact on average: an encouraging result for all publishers and\\nauthors who make the effort of sharing their data. All our data and code are\\nmade available in order to reproduce and extend our results.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.03289</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.03289</id><submitter>Le Liang</submitter><version version=\"v1\"><date>Sun, 7 Jul 2019 13:41:13 GMT</date><size>1537kb</size></version><version version=\"v2\"><date>Tue, 1 Oct 2019 17:55:23 GMT</date><size>2892kb</size></version><title>Deep Learning based Wireless Resource Allocation with Application to\\n  Vehicular Networks</title><authors>Le Liang, Hao Ye, Guanding Yu, Geoffrey Ye Li</authors><categories>cs.IT cs.LG math.IT</categories><comments>14 pages; 8 figures; 3 tables; submitted to Proceedings of IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been a long-held belief that judicious resource allocation is critical\\nto mitigating interference, improving network efficiency, and ultimately\\noptimizing wireless communication performance. The traditional wisdom is to\\nexplicitly formulate resource allocation as an optimization problem and then\\nexploit mathematical programming to solve the problem to a certain level of\\noptimality. Nonetheless, as wireless networks become increasingly diverse and\\ncomplex, e.g., in the high-mobility vehicular networks, the current design\\nmethodologies face significant challenges and thus call for rethinking of the\\ntraditional design philosophy. Meanwhile, deep learning, with many success\\nstories in various disciplines, represents a promising alternative due to its\\nremarkable power to leverage data for problem solving. In this paper, we\\ndiscuss the key motivations and roadblocks of using deep learning for wireless\\nresource allocation with application to vehicular networks. We review major\\nrecent studies that mobilize the deep learning philosophy in wireless resource\\nallocation and achieve impressive results. We first discuss deep learning\\nassisted optimization for resource allocation. We then highlight the deep\\nreinforcement learning approach to address resource allocation problems that\\nare difficult to handle in the traditional optimization framework. We also\\nidentify some research directions that deserve further investigation.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.03613</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.03613</id><submitter>Yuxiang Yang</submitter><version version=\"v1\"><date>Mon, 8 Jul 2019 13:43:06 GMT</date><size>5498kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 6 Oct 2019 18:56:57 GMT</date><size>5737kb</size><source_type>D</source_type></version><title>Data Efficient Reinforcement Learning for Legged Robots</title><authors>Yuxiang Yang, Ken Caluwaerts, Atil Iscen, Tingnan Zhang, Jie Tan,\\n  Vikas Sindhwani</authors><categories>cs.LG cs.AI cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a model-based framework for robot locomotion that achieves walking\\nbased on only 4.5 minutes (45,000 control steps) of data collected on a\\nquadruped robot. To accurately model the robot\\'s dynamics over a long horizon,\\nwe introduce a loss function that tracks the model\\'s prediction over multiple\\ntimesteps. We adapt model predictive control to account for planning latency,\\nwhich allows the learned model to be used for real time control. Additionally,\\nto ensure safe exploration during model learning, we embed prior knowledge of\\nleg trajectories into the action space. The resulting system achieves fast and\\nrobust locomotion. Unlike model-free methods, which optimize for a particular\\ntask, our planner can use the same learned dynamics for various tasks, simply\\nby changing the reward function. To the best of our knowledge, our approach is\\nmore than an order of magnitude more sample efficient than current model-free\\nmethods.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.03643</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.03643</id><submitter>Martin Lackner</submitter><version version=\"v1\"><date>Mon, 8 Jul 2019 14:22:36 GMT</date><size>30kb</size></version><version version=\"v2\"><date>Mon, 30 Sep 2019 08:58:34 GMT</date><size>31kb</size></version><title>A Mathematical Analysis of an Election System Proposed by Gottlob Frege</title><authors>Paul Harrenstein and Marie-Louise Lackner and Martin Lackner</authors><categories>cs.GT econ.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a mathematical analysis of an election system proposed by the\\neminent logician Gottlob Frege (1848--1925). His proposal was written\\npresumably in 1918, was (re)discovered around the turn of the millennium, and\\npublished for the first time in the original German in 2000. A remarkable\\nfeature of Frege\\'s proposal is its concern for the representation of minorities\\nand its sensitivity to past election results. Frege\\'s proposal is based on some\\nhighly original and relevant ideas; his core idea is that the votes of\\nunelected candidates are carried over to the next election. All candidates thus\\naccumulate votes over time and eventually each candidate is elected at some\\npoint. We provide a mathematical formulation of Frege\\'s election system and\\ninvestigate how well it achieves its aim of a fair representation of all\\npolitical opinions in a community over time. We can prove that this goal is\\nfulfilled remarkably well. However, we also show that, in other aspects, it\\nfalls short of Frege\\'s high ambition that no voter\\'s vote be lost. We propose a\\nslight modification of his voting rule, the modified Frege method, that\\nremedies these shortcomings. We analyse both methods from the perspective of\\nmodern social choice and apportionment theory, and can show that they are novel\\ncontributions with noteworthy proportionality properties over time.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.03792</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.03792</id><submitter>Marc Lelarge</submitter><version version=\"v1\"><date>Mon, 8 Jul 2019 18:08:05 GMT</date><size>184kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 21:26:23 GMT</date><size>185kb</size><source_type>D</source_type></version><title>Asymptotic Bayes risk for Gaussian mixture in a semi-supervised setting</title><authors>Marc Lelarge and Leo Miolane</authors><categories>cs.LG math.ST stat.ML stat.TH</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semi-supervised learning (SSL) uses unlabeled data for training and has been\\nshown to greatly improve performance when compared to a supervised approach on\\nthe labeled data available. This claim depends both on the amount of labeled\\ndata available and on the algorithm used.\\n  In this paper, we compute analytically the gap between the best\\nfully-supervised approach using only labeled data and the best semi-supervised\\napproach using both labeled and unlabeled data. We quantify the best possible\\nincrease in performance obtained thanks to the unlabeled data, i.e. we compute\\nthe accuracy increase due to the information contained in the unlabeled data.\\nOur work deals with a simple high-dimensional Gaussian mixture model for the\\ndata in a Bayesian setting. Our rigorous analysis builds on recent theoretical\\nbreakthroughs in high-dimensional inference and a large body of mathematical\\ntools from statistical physics initially developed for spin glasses.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.04018</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.04018</id><submitter>Ben Mussay</submitter><version version=\"v1\"><date>Tue, 9 Jul 2019 07:11:39 GMT</date><size>909kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 10:40:25 GMT</date><size>2320kb</size><source_type>D</source_type></version><title>Data-Independent Neural Pruning via Coresets</title><authors>Ben Mussay, Margarita Osadchy, Vladimir Braverman, Samson Zhou, Dan\\n  Feldman</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous work showed empirically that large neural networks can be\\nsignificantly reduced in size while preserving their accuracy. Model\\ncompression became a central research topic, as it is crucial for deployment of\\nneural networks on devices with limited computational and memory resources. The\\nmajority of the compression methods are based on heuristics and offer no\\nworst-case guarantees on the trade-off between the compression rate and the\\napproximation error for an arbitrarily new sample.\\n  We propose the first efficient, data-independent neural pruning algorithm\\nwith a provable trade-off between its compression rate and the approximation\\nerror for any future test sample. Our method is based on the coreset framework,\\nwhich finds a small weighted subset of points that provably approximates the\\noriginal inputs. Specifically, we approximate the output of a layer of neurons\\nby a coreset of neurons in the previous layer and discard the rest. We apply\\nthis framework in a layer-by-layer fashion from the top to the bottom. Unlike\\nprevious works, our coreset is data independent, meaning that it provably\\nguarantees the accuracy of the function for any input $x\\\\in \\\\mathbb{R}^d$,\\nincluding an adversarial one. We demonstrate the effectiveness of our method on\\npopular network architectures. In particular, our coresets yield 90\\\\%\\ncompression of the LeNet-300-100 architecture on MNIST while improving the\\naccuracy.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.04135</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.04135</id><submitter>James Wexler</submitter><version version=\"v1\"><date>Tue, 9 Jul 2019 13:16:24 GMT</date><size>2851kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 17:00:59 GMT</date><size>2853kb</size><source_type>D</source_type></version><title>The What-If Tool: Interactive Probing of Machine Learning Models</title><authors>James Wexler, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg,\\n  Fernanda Viegas, Jimbo Wilson</authors><categories>cs.LG stat.ML</categories><comments>IEEE VIS (VAST) 2019</comments><acm-class>H.5.2</acm-class><doi>10.1109/TVCG.2019.2934619</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key challenge in developing and deploying Machine Learning (ML) systems is\\nunderstanding their performance across a wide range of inputs. To address this\\nchallenge, we created the What-If Tool, an open-source application that allows\\npractitioners to probe, visualize, and analyze ML systems, with minimal coding.\\nThe What-If Tool lets practitioners test performance in hypothetical\\nsituations, analyze the importance of different data features, and visualize\\nmodel behavior across multiple models and subsets of input data. It also lets\\npractitioners measure systems according to multiple ML fairness metrics. We\\ndescribe the design of the tool, and report on real-life usage at different\\norganizations.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.04202</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.04202</id><submitter>Masashi Okada Dr</submitter><version version=\"v1\"><date>Mon, 8 Jul 2019 01:54:08 GMT</date><size>4047kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 01:03:08 GMT</date><size>2153kb</size><source_type>D</source_type></version><title>Variational Inference MPC for Bayesian Model-based Reinforcement\\n  Learning</title><authors>Masashi Okada, Tadahiro Taniguchi</authors><categories>cs.LG cs.SY eess.SY stat.ML</categories><comments>Accepted to CoRL2019. Camera-ready ver</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent studies on model-based reinforcement learning (MBRL), incorporating\\nuncertainty in forward dynamics is a state-of-the-art strategy to enhance\\nlearning performance, making MBRLs competitive to cutting-edge model free\\nmethods, especially in simulated robotics tasks. Probabilistic ensembles with\\ntrajectory sampling (PETS) is a leading type of MBRL, which employs Bayesian\\ninference to dynamics modeling and model predictive control (MPC) with\\nstochastic optimization via the cross entropy method (CEM). In this paper, we\\npropose a novel extension to the uncertainty-aware MBRL. Our main contributions\\nare twofold: Firstly, we introduce a variational inference MPC, which\\nreformulates various stochastic methods, including CEM, in a Bayesian fashion.\\nSecondly, we propose a novel instance of the framework, called probabilistic\\naction ensembles with trajectory sampling (PaETS). As a result, our Bayesian\\nMBRL can involve multimodal uncertainties both in dynamics and optimal\\ntrajectories. In comparison to PETS, our method consistently improves\\nasymptotic performance on several challenging locomotion tasks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.04229</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.04229</id><submitter>Thomas Roy</submitter><version version=\"v1\"><date>Fri, 5 Jul 2019 18:21:55 GMT</date><size>1146kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 11:34:20 GMT</date><size>1177kb</size><source_type>D</source_type></version><title>A constrained pressure-temperature residual (CPTR) method for\\n  non-isothermal multiphase flow in porous media</title><authors>Thomas Roy, Tom B J\\\\&quot;onsth\\\\&quot;ovel, Christopher Lemon, Andrew J Wathen</authors><categories>math.NA cs.NA physics.flu-dyn</categories><comments>29 pages, 2 figures. Sources/sinks description in arXiv:1902.00095</comments><msc-class>65F08, 65F10, 76S05, 65M08, 65M60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For both isothermal and thermal petroleum reservoir simulation, the\\nConstrained Pressure Residual (CPR) method is the industry-standard\\npreconditioner. This method is a two-stage process involving the solution of a\\nrestricted pressure system. While initially designed for the isothermal case,\\nCPR is also the standard for thermal cases. However, its treatment of the\\nenergy conservation equation does not incorporate heat diffusion, which is\\noften dominant in thermal cases. In this paper, we present an extension of CPR:\\nthe Constrained Pressure-Temperature Residual (CPTR) method, where a restricted\\npressure-temperature system is solved in the first stage. In previous work, we\\nintroduced a block preconditioner with an efficient Schur complement\\napproximation for a pressure-temperature system. Here, we extend this method\\nfor multiphase flow as the first stage of CPTR. The algorithmic performance of\\ndifferent two-stage preconditioners is evaluated for reservoir simulation test\\ncases.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.04242</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.04242</id><submitter>Pierre Baudot</submitter><version version=\"v1\"><date>Sat, 6 Jul 2019 15:04:08 GMT</date><size>4428kb</size><source_type>D</source_type></version><title>Topological Information Data Analysis</title><authors>Pierre Baudot and Monica Tapia and Daniel Bennequin and Jean-Marc\\n  Goaillard</authors><categories>stat.OT cs.IT math.IT q-bio.NC</categories><doi>10.3390/e21090869</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents methods that quantify the structure of statistical\\ninteractions within a given data set, and was first used in \\\\cite{Tapia2018}.\\nIt establishes new results on the k-multivariate mutual-informations (I_k)\\ninspired by the topological formulation of Information introduced in. In\\nparticular we show that the vanishing of all I_k for 2\\\\leq k \\\\leq n of n random\\nvariables is equivalent to their statistical independence. Pursuing the work of\\nHu Kuo Ting and Te Sun Han, we show that information functions provide\\nco-ordinates for binary variables, and that they are analytically independent\\non the probability simplex for any set of finite variables. The maximal\\npositive I_k identifies the variables that co-vary the most in the population,\\nwhereas the minimal negative I_k identifies synergistic clusters and the\\nvariables that differentiate-segregate the most the population. Finite data\\nsize effects and estimation biases severely constrain the effective computation\\nof the information topology on data, and we provide simple statistical tests\\nfor the undersampling bias and the k-dependences following. We give an example\\nof application of these methods to genetic expression and unsupervised\\ncell-type classification. The methods unravel biologically relevant subtypes,\\nwith a sample size of 41 genes and with few errors. It establishes generic\\nbasic methods to quantify the epigenetic information storage and a unified\\nepigenetic unsupervised learning formalism. We propose that higher-order\\nstatistical interactions and non identically distributed variables are\\nconstitutive characteristics of biological systems that should be estimated in\\norder to unravel their significant statistical structure and diversity. The\\ntopological information data analysis presented here allows to precisely\\nestimate this higher-order structure characteristic of biological systems.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.04274</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.04274</id><submitter>Xue Chen</submitter><version version=\"v1\"><date>Tue, 9 Jul 2019 16:24:06 GMT</date><size>47kb</size></version><version version=\"v2\"><date>Fri, 4 Oct 2019 22:06:07 GMT</date><size>210kb</size></version><title>Reconstruction under outliers for Fourier-sparse functions</title><authors>Xue Chen and Anindya De</authors><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning an unknown $f$ with a sparse Fourier\\nspectrum in the presence of outlier noise. In particular, the algorithm has\\naccess to a noisy oracle for (an unknown) $f$ such that (i) the Fourier\\nspectrum of $f$ is $k$-sparse; (ii) at any query point $x$, the oracle returns\\n$y$ such that with probability $1-\\\\rho$, $|y-f(x)| \\\\le \\\\epsilon$. However, with\\nprobability $\\\\rho$, the error $y-f(x)$ can be arbitrarily large.\\n  We study Fourier sparse functions over both the discrete cube $\\\\{0,1\\\\}^n$ and\\nthe torus $[0,1)$ and for both these domains, we design efficient algorithms\\nwhich can tolerate any $\\\\rho&lt;1/2$ fraction of outliers. We note that the\\nanalogous problem for low-degree polynomials has recently been studied in\\nseveral works~[AK03, GZ16, KKP17] and similar algorithmic guarantees are known\\nin that setting.\\n  While our main results pertain to the case where the location of the\\noutliers, i.e., $x$ such that $|y-f(x)|&gt;\\\\epsilon$ is randomly distributed, we\\nalso study the case where the outliers are adversarially located. In\\nparticular, we show that over the torus, assuming that the Fourier transform\\nsatisfies a certain \\\\emph{granularity} condition, there is a sample efficient\\nalgorithm to tolerate $\\\\rho =\\\\Omega(1)$ fraction of outliers and further, that\\nthis is not possible without such a granularity condition. Finally, while not\\nthe principal thrust, our techniques also allow us non-trivially improve on\\nlearning low-degree functions $f$ on the hypercube in the presence of\\nadversarial outlier noise.\\n  Our techniques combine a diverse array of tools from compressive sensing,\\nsparse Fourier transform, chaining arguments and complex analysis.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.04371</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.04371</id><submitter>Kenji Kawaguchi</submitter><version version=\"v1\"><date>Tue, 9 Jul 2019 19:09:51 GMT</date><size>2980kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 20:01:12 GMT</date><size>2871kb</size><source_type>D</source_type></version><title>Ordered SGD: A New Stochastic Optimization Framework for Empirical Risk\\n  Minimization</title><authors>Kenji Kawaguchi, Haihao Lu</authors><categories>stat.ML cs.LG math.OC</categories><comments>code available at: https://github.com/k9k2/qSGD</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new stochastic optimization framework for empirical risk\\nminimization problems such as those that arise in machine learning. The\\ntraditional approaches, such as (mini-batch) stochastic gradient descent (SGD),\\nutilize an unbiased gradient estimator of the empirical average loss. In\\ncontrast, we develop a computationally efficient method to construct a gradient\\nestimator that is purposely biased toward those observations with higher\\ncurrent losses. On the theory side, we show that the proposed method minimizes\\na new ordered modification of the empirical average loss, and is guaranteed to\\nconverge at a sublinear rate to a global optimum for convex loss and to a\\ncritical point for weakly convex (non-convex) loss. Furthermore, we prove a new\\ngeneralization bound for the proposed algorithm. On the empirical side, the\\nnumerical experiments show that our proposed method consistently improves the\\ntest errors compared with the standard mini-batch SGD in various models\\nincluding SVM, logistic regression, and deep learning problems.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.04543</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.04543</id><submitter>Rishabh Agarwal</submitter><version version=\"v1\"><date>Wed, 10 Jul 2019 07:23:27 GMT</date><size>14061kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 16:35:52 GMT</date><size>18428kb</size><source_type>D</source_type></version><title>Striving for Simplicity in Off-Policy Deep Reinforcement Learning</title><authors>Rishabh Agarwal, Dale Schuurmans, Mohammad Norouzi</authors><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper advocates the use of offline (batch) reinforcement learning (RL)\\nto help (1) isolate the contributions of exploitation vs. exploration in\\noff-policy deep RL, (2) improve reproducibility of deep RL research, and (3)\\nfacilitate the design of simpler deep RL algorithms. We propose an offline RL\\nbenchmark on Atari 2600 games comprising all of the replay data of a DQN agent.\\nUsing this benchmark, we demonstrate that recent off-policy deep RL algorithms,\\neven when trained solely on logged DQN data, can outperform online DQN. We\\npresent Random Ensemble Mixture (REM), a simple Q-learning algorithm that\\nenforces optimal Bellman consistency on random convex combinations of multiple\\nQ-value estimates. The REM algorithm outperforms more complex RL agents such as\\nC51 and QR-DQN on the offline Atari benchmark and performs comparably in the\\nonline setting.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.04629</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.04629</id><submitter>Thijs Laarhoven</submitter><version version=\"v1\"><date>Wed, 10 Jul 2019 11:38:33 GMT</date><size>544kb</size><source_type>D</source_type></version><title>Evolutionary techniques in lattice sieving algorithms</title><authors>Thijs Laarhoven</authors><categories>cs.DS cs.NE</categories><comments>9 pages, 2 figures</comments><journal-ref>11th International Conference on Evolutionary Computation Theory\\n  and Applications (ECTA), pp. 31-39, 2019</journal-ref><doi>10.5220/0007968800310039</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lattice-based cryptography has recently emerged as a prominent candidate for\\nsecure communication in the quantum age. Its security relies on the hardness of\\ncertain lattice problems, and the inability of known lattice algorithms, such\\nas lattice sieving, to solve these problems efficiently. In this paper we\\ninvestigate the similarities between lattice sieving and evolutionary\\nalgorithms, how various improvements to lattice sieving can be viewed as\\napplications of known techniques from evolutionary computation, and how other\\nevolutionary techniques can benefit lattice sieving in practice.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.04655</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.04655</id><submitter>Antoine Deleforge</submitter><version version=\"v1\"><date>Wed, 3 Jul 2019 09:04:50 GMT</date><size>2393kb</size><source_type>D</source_type></version><title>Audio-Based Search and Rescue with a Drone: Highlights from the IEEE\\n  Signal Processing Cup 2019 Student Competition</title><authors>Antoine Deleforge, Diego Di Carlo, Martin Strauss, Romain Serizel,\\n  Lucio Marcenaro</authors><categories>eess.SP cs.SD eess.AS</categories><proxy>ccsd</proxy><journal-ref>IEEE Signal Processing Magazine, Institute of Electrical and\\n  Electronics Engineers, In press</journal-ref><doi>10.1109/MSP.2019.2924687</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unmanned aerial vehicles (UAV), commonly referred to as drones, have raised\\nincreasing interest in recent years. Search and rescue scenarios where humans\\nin emergency situations need to be quickly found in areas difficult to access\\nconstitute an important field of application for this technology. While\\nresearch efforts have mostly focused on developing video-based solutions for\\nthis task \\\\cite{lopez2017cvemergency}, UAV-embedded audio-based localization\\nhas received relatively less attention. Though, UAVs equipped with a microphone\\narray could be of critical help to localize people in emergency situations, in\\nparticular when video sensors are limited by a lack of visual feedback due to\\nbad lighting conditions or obstacles limiting the field of view. This motivated\\nthe topic of the 6th edition of the IEEE Signal Processing Cup (SP Cup): a\\nUAV-embedded sound source localization challenge for search and rescue. In this\\narticle, we share an overview of the IEEE SP Cup experience including the\\ncompetition tasks, participating teams, technical approaches and statistics.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.04780</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.04780</id><submitter>Noah Constant</submitter><version version=\"v1\"><date>Wed, 10 Jul 2019 15:16:36 GMT</date><size>26kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 03:14:32 GMT</date><size>96kb</size><source_type>D</source_type></version><title>ReQA: An Evaluation for End-to-End Answer Retrieval Models</title><authors>Amin Ahmad, Noah Constant, Yinfei Yang, Daniel Cer</authors><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Popular QA benchmarks like SQuAD have driven progress on the task of\\nidentifying answer spans within a specific passage, with models now surpassing\\nhuman performance. However, retrieving relevant answers from a huge corpus of\\ndocuments is still a challenging problem, and places different requirements on\\nthe model architecture. There is growing interest in developing scalable answer\\nretrieval models trained end-to-end, bypassing the typical document retrieval\\nstep. In this paper, we introduce Retrieval Question-Answering (ReQA), a\\nbenchmark for evaluating large-scale sentence-level answer retrieval models. We\\nestablish baselines using both neural encoding models as well as classical\\ninformation retrieval techniques. We release our evaluation code to encourage\\nfurther work on this challenging task.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.04931</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.04931</id><submitter>Hanqing Zeng</submitter><version version=\"v1\"><date>Wed, 10 Jul 2019 21:11:13 GMT</date><size>57kb</size></version><version version=\"v2\"><date>Sun, 29 Sep 2019 08:36:31 GMT</date><size>184kb</size><source_type>D</source_type></version><title>GraphSAINT: Graph Sampling Based Inductive Learning Method</title><authors>Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan,\\n  Viktor Prasanna</authors><categories>cs.LG stat.ML</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph Convolutional Networks (GCNs) are powerful models for learning\\nrepresentations of attributed graphs. To scale GCNs to large graphs,\\nstate-of-the-art methods use various layer sampling techniques to alleviate the\\n&quot;neighbor explosion&quot; problem during minibatch training. We propose GraphSAINT,\\na graph sampling based inductive learning method that improves training\\nefficiency and accuracy in a fundamentally different way. By changing\\nperspective, GraphSAINT constructs minibatches by sampling the training graph,\\nrather than the nodes or edges across GCN layers. Each iteration, a complete\\nGCN is built from the properly sampled subgraph. Thus, we ensure fixed number\\nof well-connected nodes in all layers. We further propose normalization\\ntechnique to eliminate bias, and sampling algorithms for variance reduction.\\nImportantly, we can decouple the sampling from the forward and backward\\npropagation, and extend GraphSAINT with many architecture variants (e.g., graph\\nattention, jumping connection). GraphSAINT demonstrates superior performance in\\nboth accuracy and training time on five large graphs, and achieves new\\nstate-of-the-art F1 scores for PPI (0.995) and Reddit (0.970).\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.05012</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.05012</id><submitter>Antonio Ginart</submitter><version version=\"v1\"><date>Thu, 11 Jul 2019 06:19:51 GMT</date><size>1160kb</size><source_type>D</source_type></version><title>Making AI Forget You: Data Deletion in Machine Learning</title><authors>Antonio Ginart, Melody Y. Guan, Gregory Valiant, James Zou</authors><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Intense recent discussions have focused on how to provide individuals with\\ncontrol over when their data can and cannot be used -- the EU\\'s Right To Be\\nForgotten regulation is an example of this effort. In this paper we initiate a\\nframework studying what to do when it is no longer permissible to deploy models\\nderivative from specific user data. In particular, we formulate the problem of\\nhow to efficiently delete individual data points from trained machine learning\\nmodels. For many standard ML models, the only way to completely remove an\\nindividual\\'s data is to retrain the whole model from scratch on the remaining\\ndata, which is often not computationally practical. We investigate algorithmic\\nprinciples that enable efficient data deletion in ML. For the specific setting\\nof k-means clustering, we propose two provably deletion efficient algorithms\\nwhich achieve an average of over 100X improvement in deletion efficiency across\\n6 datasets, while producing clusters of comparable statistical quality to a\\ncanonical k-means++ baseline.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.05073</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.05073</id><submitter>Tobias Rapp</submitter><version version=\"v1\"><date>Thu, 11 Jul 2019 09:29:39 GMT</date><size>9020kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 09:36:52 GMT</date><size>9020kb</size><source_type>D</source_type></version><title>Void-and-Cluster Sampling of Large Scattered Data and Trajectories</title><authors>Tobias Rapp, Christoph Peters, Carsten Dachsbacher</authors><categories>cs.GR</categories><comments>To appear in IEEE Transactions on Visualization and Computer Graphics\\n  as a special issue from the proceedings of VIS 2019</comments><doi>10.1109/TVCG.2019.2934335</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a data reduction technique for scattered data based on statistical\\nsampling. Our void-and-cluster sampling technique finds a representative subset\\nthat is optimally distributed in the spatial domain with respect to the blue\\nnoise property. In addition, it can adapt to a given density function, which we\\nuse to sample regions of high complexity in the multivariate value domain more\\ndensely. Moreover, our sampling technique implicitly defines an ordering on the\\nsamples that enables progressive data loading and a continuous level-of-detail\\nrepresentation. We extend our technique to sample time-dependent trajectories,\\nfor example pathlines in a time interval, using an efficient and iterative\\napproach. Furthermore, we introduce a local and continuous error measure to\\nquantify how well a set of samples represents the original dataset. We apply\\nthis error measure during sampling to guide the number of samples that are\\ntaken. Finally, we use this error measure and other quantities to evaluate the\\nquality, performance, and scalability of our algorithm.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.05096</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.05096</id><submitter>Rafael Pereira Pires</submitter><version version=\"v1\"><date>Thu, 11 Jul 2019 10:44:31 GMT</date><size>645kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 12:34:19 GMT</date><size>645kb</size><source_type>D</source_type></version><title>Supply chain malware targets SGX: Take care of what you sign</title><authors>Andrei Mogage, Rafael Pires, Vlad Cr\\\\u{a}ciun, Emanuel Onica and\\n  Pascal Felber</authors><categories>cs.CR</categories><comments>European Commission Project: EBSIS - Event Based Systems in Ia\\\\c{s}i\\n  (H2020-692178)</comments><journal-ref>2019 IEEE 38th Symposium on Reliable Distributed Systems (SRDS),\\n  Lyon, France, 2019</journal-ref><doi>10.1109/SRDS.2019.00016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Malware attacks represent a significant part of today\\'s security threats.\\nSoftware guard extensions (SGX) are a set of hardware instructions introduced\\nby Intel in their recent lines of processors that are intended to provide a\\nsecure execution environment for user-developed applications. To our knowledge,\\nthere was no serious attempt yet to overcome the SGX protection by leveraging\\nthe software supply chain infrastructure, such as weaknesses in the\\ndevelopment, build or signing servers. While SGX protection does not\\nspecifically take into consideration such threats, we show in the current paper\\nthat a simple malware attack exploiting a separation between the build and\\nsigning processes can have a serious damaging impact, practically nullifying\\nthe SGX integrity protection measures. Finally, we also suggest some possible\\nmitigations against the attack.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.05127</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.05127</id><submitter>Weiming Zhi</submitter><version version=\"v1\"><date>Thu, 11 Jul 2019 11:56:44 GMT</date><size>9119kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 13:09:59 GMT</date><size>6915kb</size><source_type>D</source_type></version><title>Kernel Trajectory Maps for Multi-Modal Probabilistic Motion Prediction</title><authors>Weiming Zhi, Lionel Ott, Fabio Ramos</authors><categories>cs.RO cs.LG</categories><comments>To appear in Conference on Robot Learning 2019</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Understanding the dynamics of an environment, such as the movement of humans\\nand vehicles, is crucial for agents to achieve long-term autonomy in urban\\nenvironments. This requires the development of methods to capture the\\nmulti-modal and probabilistic nature of motion patterns. We present Kernel\\nTrajectory Maps (KTM) to capture the trajectories of movement in an\\nenvironment. KTMs leverage the expressiveness of kernels from non-parametric\\nmodelling by projecting input trajectories onto a set of representative\\ntrajectories, to condition on a sequence of observed waypoint coordinates, and\\npredict a multi-modal distribution over possible future trajectories. The\\noutput is a mixture of continuous stochastic processes, where each realisation\\nis a continuous functional trajectory, which can be queried at arbitrarily fine\\ntime steps.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.05991</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.05991</id><submitter>Yusuke Kawamoto</submitter><version version=\"v1\"><date>Sat, 13 Jul 2019 01:13:40 GMT</date><size>340kb</size></version><version version=\"v2\"><date>Mon, 30 Sep 2019 14:32:11 GMT</date><size>411kb</size></version><title>Local Distribution Obfuscation via Probability Coupling</title><authors>Yusuke Kawamoto and Takao Murakami</authors><categories>cs.CR cs.DB cs.IT cs.LG math.IT</categories><comments>Full version of Allerton 2019 paper (This paper extends some part of\\n  the unpublished v3 of arXiv:1812.00939, while v4 of arXiv:1812.00939 extends\\n  the other part and is published in ESORICS\\'19.)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a general model for the local obfuscation of probability\\ndistributions by probabilistic perturbation, e.g., by adding differentially\\nprivate noise, and investigate its theoretical properties. Specifically, we\\nrelax a notion of distribution privacy (DistP) by generalizing it to\\ndivergence, and propose local obfuscation mechanisms that provide divergence\\ndistribution privacy. To provide f-divergence distribution privacy, we prove\\nthat probabilistic perturbation noise should be added proportionally to the\\nEarth mover\\'s distance between the probability distributions that we want to\\nmake indistinguishable. Furthermore, we introduce a local obfuscation\\nmechanism, which we call a coupling mechanism, that provides divergence\\ndistribution privacy while optimizing the utility of obfuscated data by using\\nexact/approximate auxiliary information on the input distributions we want to\\nprotect.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.06258</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.06258</id><submitter>Jose Ortiz-Bejar</submitter><version version=\"v1\"><date>Sun, 14 Jul 2019 18:35:58 GMT</date><size>467kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 19:57:28 GMT</date><size>631kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 3 Oct 2019 02:03:56 GMT</date><size>631kb</size><source_type>D</source_type></version><title>Improving classification performance by feature space transformations\\n  and model selection</title><authors>Jose Ortiz-Bejar, Eric S. Tellez and Mario Graff</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Improving the performance of classifiers is the realm of feature mapping,\\nprototype selection, and kernel function transformations; these techniques aim\\nfor reducing the complexity, and also, improving the accuracy of models. In\\nparticular, our objective is to combine them to transform data\\'s shape into\\nanother more convenient distribution; such that some simple algorithms, such as\\nNa\\\\&quot;ive Bayes or k-Nearest Neighbors, can produce competitive classifiers. In\\nthis paper, we introduce a family of classifiers based on feature mapping and\\nkernel functions, orchestrated by a model selection scheme that excels in\\nperformance. We provide an extensive experimental comparison of our methods\\nwith sixteen popular classifiers on more than thirty benchmarks supporting our\\nclaims. In addition to their competitive performance, our statistical tests\\nalso found that our methods are different among them, supporting our claim of a\\ncompelling family of classifiers.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.06388</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.06388</id><submitter>Behrooz Razeghi</submitter><version version=\"v1\"><date>Mon, 15 Jul 2019 09:34:49 GMT</date><size>1672kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 19:15:55 GMT</date><size>1672kb</size><source_type>D</source_type></version><title>Single-Component Privacy Guarantees in Helper Data Systems and Sparse\\n  Coding with Ambiguation</title><authors>Behrooz Razeghi, Taras Stanko, Boris \\\\v{S}kori\\\\\\'c, Slava\\n  Voloshynovskiy</authors><categories>cs.IT cs.CR cs.IR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the privacy of two approaches to (biometric) template\\nprotection: Helper Data Systems and Sparse Ternary Coding with Ambiguization.\\nIn particular, we focus on a privacy property that is often overlooked, namely\\nhow much leakage exists about one specific binary property of one component of\\nthe feature vector. This property is e.g. the sign or an indicator that a\\nthreshold is exceeded.\\n  We provide evidence that both approaches are able to protect such sensitive\\nbinary variables, and discuss how system parameters need to be set.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.06466</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.06466</id><submitter>Rafael Pereira Pires</submitter><version version=\"v1\"><date>Mon, 15 Jul 2019 12:34:52 GMT</date><size>408kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 12:26:51 GMT</date><size>408kb</size><source_type>D</source_type></version><title>Anonymous and confidential file sharing over untrusted clouds</title><authors>Stefan Contiu, S\\\\\\'ebastien Vaucher, Rafael Pires, Marcelo Pasin,\\n  Pascal Felber and Laurent R\\\\\\'eveill\\\\`ere</authors><categories>cs.CR cs.DC</categories><journal-ref>2019 IEEE 38th Symposium on Reliable Distributed Systems (SRDS),\\n  Lyon, France, 2019</journal-ref><doi>10.1109/SRDS.2019.00013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using public cloud services for storing and sharing confidential data\\nrequires end users to cryptographically protect both the data and the access to\\nthe data. In some cases, the identity of end users needs to remain confidential\\nagainst the cloud provider and fellow users accessing the data. As such, the\\nunderlying cryptographic access control mechanism needs to ensure the anonymity\\nof both data producers and consumers. We introduce A-SKY, a cryptographic\\naccess control extension capable of providing confidentiality and anonymity\\nguarantees, all while efficiently scaling to large organizations. A-SKY\\nleverages trusted execution environments (TEEs) to address the impracticality\\nof anonymous broadcast encryption (ANOBE) schemes, achieving faster execution\\ntimes and shorter ciphertexts. The innovative design of A-SKY limits the usage\\nof the TEE to the narrow set of data producing operations, and thus optimizes\\nthe dominant data consumption actions by not requiring a TEE. Furthermore, we\\npropose a scalable implementation for A-SKY leveraging micro-services that\\npreserves strong security guarantees while being able to efficiently manage\\nrealistic large user bases. Results highlight that the A-SKY cryptographic\\nscheme is 3 orders of magnitude better than state of the art ANOBE, and an\\nend-to-end system encapsulating A-SKY can elastically scale to support groups\\nof 10 000 users while maintaining processing costs below 1 second.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.06479</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.06479</id><submitter>Zhenyu Zhang</submitter><version version=\"v1\"><date>Mon, 15 Jul 2019 12:56:38 GMT</date><size>2435kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 8 Sep 2019 01:06:28 GMT</date><size>2377kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 30 Sep 2019 07:45:09 GMT</date><size>2377kb</size><source_type>D</source_type></version><title>Proximal Policy Optimization with Mixed Distributed Training</title><authors>Zhenyu Zhang, Xiangfeng Luo, Tong Liu, Shaorong Xie, Jianshu Wang, Wei\\n  Wang, Yang Li and Yan Peng</authors><categories>cs.LG cs.AI</categories><comments>ICTAI 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Instability and slowness are two main problems in deep reinforcement\\nlearning. Even if proximal policy optimization (PPO) is the state of the art,\\nit still suffers from these two problems. We introduce an improved algorithm\\nbased on proximal policy optimization, mixed distributed proximal policy\\noptimization (MDPPO), and show that it can accelerate and stabilize the\\ntraining process. In our algorithm, multiple different policies train\\nsimultaneously and each of them controls several identical agents that interact\\nwith environments. Actions are sampled by each policy separately as usual, but\\nthe trajectories for the training process are collected from all agents,\\ninstead of only one policy. We find that if we choose some auxiliary\\ntrajectories elaborately to train policies, the algorithm will be more stable\\nand quicker to converge especially in the environments with sparse rewards.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.06627</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.06627</id><submitter>Babak Ehteshami Bejnordi</submitter><version version=\"v1\"><date>Mon, 15 Jul 2019 17:58:04 GMT</date><size>5417kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 6 Oct 2019 12:13:32 GMT</date><size>2949kb</size><source_type>D</source_type></version><title>Batch-Shaping for Learning Conditional Channel Gated Networks</title><authors>Babak Ehteshami Bejnordi, Tijmen Blankevoort and Max Welling</authors><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method that trains large capacity neural networks with\\nsignificantly improved accuracy and lower dynamic computational cost. We\\nachieve this by gating the deep-learning architecture on a fine-grained-level.\\nIndividual convolutional maps are turned on/off conditionally on features in\\nthe network. To achieve this, we introduce a new residual block architecture\\nthat gates convolutional channels in a fine-grained manner. We also introduce a\\ngenerally applicable tool $batch$-$shaping$ that matches the marginal aggregate\\nposteriors of features in a neural network to a pre-specified prior\\ndistribution. We use this novel technique to force gates to be more conditional\\non the data. We present results on CIFAR-10 and ImageNet datasets for image\\nclassification, and Cityscapes for semantic segmentation. Our results show that\\nour method can slim down large architectures conditionally, such that the\\naverage computational cost on the data is on par with a smaller architecture,\\nbut with higher accuracy. In particular, on ImageNet, our ResNet50 and ResNet34\\ngated networks obtain 74.60% and 72.55% top-1 accuracy compared to the 69.76%\\naccuracy of the baseline ResNet18 model, for similar complexity. We also show\\nthat the resulting networks automatically learn to use more features for\\ndifficult examples and fewer features for simple examples.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.06722</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.06722</id><submitter>Guang Yang</submitter><version version=\"v1\"><date>Mon, 15 Jul 2019 20:03:53 GMT</date><size>5677kb</size><source_type>D</source_type></version><title>Sampling-based Motion Planning via Control Barrier Functions</title><authors>Guang Yang, Bee Vang, Zachary Serlin, Calin Belta, Roberto Tron</authors><categories>cs.RO</categories><doi>10.1145/3365265.3365282</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robot motion planning is central to real-world autonomous applications, such\\nas self-driving cars, persistence surveillance, and robotic arm manipulation.\\nOne challenge in motion planning is generating control signals for nonlinear\\nsystems that result in obstacle free paths through dynamic environments. In\\nthis paper, we propose Control Barrier Function guided Rapidly-exploring Random\\nTrees (CBF-RRT), a sampling-based motion planning algorithm for continuous-time\\nnonlinear systems in dynamic environments. The algorithm focuses on two\\nobjectives: efficiently generating feasible controls that steer the system\\ntoward a goal region, and handling environments with dynamical obstacles in\\ncontinuous time. We formulate the control synthesis problem as a Quadratic\\nProgram (QP) that enforces Control Barrier Function (CBF) constraints to\\nachieve obstacle avoidance. Additionally, CBF-RRT does not require nearest\\nneighbor or collision checks when sampling, which greatly reduce the run-time\\noverhead when compared to standard RRT variants.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.06731</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.06731</id><submitter>William Kretschmer</submitter><version version=\"v1\"><date>Mon, 15 Jul 2019 20:20:40 GMT</date><size>12kb</size></version><version version=\"v2\"><date>Mon, 7 Oct 2019 19:33:23 GMT</date><size>13kb</size></version><title>Lower Bounding the AND-OR Tree via Symmetrization</title><authors>William Kretschmer</authors><categories>cs.CC quant-ph</categories><comments>11 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a simple, nearly tight lower bound on the approximate degree of the\\ntwo-level $\\\\mathsf{AND}$-$\\\\mathsf{OR}$ tree using symmetrization arguments.\\nSpecifically, we show that $\\\\widetilde{\\\\mathrm{deg}}(\\\\mathsf{AND}_m \\\\circ\\n\\\\mathsf{OR}_n) = \\\\widetilde{\\\\Omega}(\\\\sqrt{mn})$. To our knowledge, this is the\\nfirst proof of this fact that relies on symmetrization exclusively; most other\\nproofs involve the more complicated formulation of approximate degree as a\\nlinear program [BT13, She13, BDBGK18]. Our proof also demonstrates the power of\\na symmetrization technique involving Laurent polynomials (polynomials with\\nnegative exponents) that was previously introduced by Aaronson, Kothari,\\nKretschmer, and Thaler [AKKT19].\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.06845</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.06845</id><submitter>Gabriel Loaiza-Ganem</submitter><version version=\"v1\"><date>Tue, 16 Jul 2019 05:11:46 GMT</date><size>804kb</size><source_type>AD</source_type></version><version version=\"v2\"><date>Tue, 23 Jul 2019 04:45:07 GMT</date><size>783kb</size><source_type>AD</source_type></version><version version=\"v3\"><date>Fri, 27 Sep 2019 19:59:42 GMT</date><size>874kb</size><source_type>AD</source_type></version><version version=\"v4\"><date>Wed, 2 Oct 2019 00:11:02 GMT</date><size>874kb</size><source_type>AD</source_type></version><title>The continuous Bernoulli: fixing a pervasive error in variational\\n  autoencoders</title><authors>Gabriel Loaiza-Ganem, John P. Cunningham</authors><categories>stat.ML cs.LG</categories><comments>Accepted at NeurIPS 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variational autoencoders (VAE) have quickly become a central tool in machine\\nlearning, applicable to a broad range of data types and latent variable models.\\nBy far the most common first step, taken by seminal papers and by core software\\nlibraries alike, is to model MNIST data using a deep network parameterizing a\\nBernoulli likelihood. This practice contains what appears to be and what is\\noften set aside as a minor inconvenience: the pixel data is [0,1] valued, not\\n{0,1} as supported by the Bernoulli likelihood. Here we show that, far from\\nbeing a triviality or nuisance that is convenient to ignore, this error has\\nprofound importance to VAE, both qualitative and quantitative. We introduce and\\nfully characterize a new [0,1]-supported, single parameter distribution: the\\ncontinuous Bernoulli, which patches this pervasive bug in VAE. This\\ndistribution is not nitpicking; it produces meaningful performance improvements\\nacross a range of metrics and datasets, including sharper image samples, and\\nsuggests a broader class of performant VAE.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.06901</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.06901</id><submitter>Pankaj Malhotra</submitter><version version=\"v1\"><date>Tue, 16 Jul 2019 09:10:50 GMT</date><size>377kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 06:25:39 GMT</date><size>377kb</size><source_type>D</source_type></version><title>Meta-Learning for Black-box Optimization</title><authors>Vishnu TV, Pankaj Malhotra, Jyoti Narwariya, Lovekesh Vig, Gautam\\n  Shroff</authors><categories>cs.LG stat.ML</categories><comments>Accepted at ECML-PKDD 2019 Research Track</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, neural networks trained as optimizers under the &quot;learning to learn&quot;\\nor meta-learning framework have been shown to be effective for a broad range of\\noptimization tasks including derivative-free black-box function optimization.\\nRecurrent neural networks (RNNs) trained to optimize a diverse set of synthetic\\nnon-convex differentiable functions via gradient descent have been effective at\\noptimizing derivative-free black-box functions. In this work, we propose\\nRNN-Opt: an approach for learning RNN-based optimizers for optimizing\\nreal-parameter single-objective continuous functions under limited budget\\nconstraints. Existing approaches utilize an observed improvement based\\nmeta-learning loss function for training such models. We propose training\\nRNN-Opt by using synthetic non-convex functions with known (approximate)\\noptimal values by directly using discounted regret as our meta-learning loss\\nfunction. We hypothesize that a regret-based loss function mimics typical\\ntesting scenarios, and would therefore lead to better optimizers compared to\\noptimizers trained only to propose queries that improve over previous queries.\\nFurther, RNN-Opt incorporates simple yet effective enhancements during training\\nand inference procedures to deal with the following practical challenges: i)\\nUnknown range of possible values for the black-box function to be optimized,\\nand ii) Practical and domain-knowledge based constraints on the input\\nparameters. We demonstrate the efficacy of RNN-Opt in comparison to existing\\nmethods on several synthetic as well as standard benchmark black-box functions\\nalong with an anonymized industrial constrained optimization problem.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.06944</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.06944</id><submitter>Bla\\\\v{z} \\\\v{S}krlj</submitter><version version=\"v1\"><date>Tue, 16 Jul 2019 11:33:04 GMT</date><size>907kb</size><source_type>D</source_type></version><title>Language comparison via network topology</title><authors>Bla\\\\v{z} \\\\v{S}krlj and Senja Pollak</authors><categories>cs.CL</categories><doi>10.1007/978-3-030-31372-2_10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling relations between languages can offer understanding of language\\ncharacteristics and uncover similarities and differences between languages.\\nAutomated methods applied to large textual corpora can be seen as opportunities\\nfor novel statistical studies of language development over time, as well as for\\nimproving cross-lingual natural language processing techniques. In this work,\\nwe first propose how to represent textual data as a directed, weighted network\\nby the text2net algorithm. We next explore how various fast,\\nnetwork-topological metrics, such as network community structure, can be used\\nfor cross-lingual comparisons. In our experiments, we employ eight different\\nnetwork topology metrics, and empirically showcase on a parallel corpus, how\\nthe methods can be used for modeling the relations between nine selected\\nlanguages. We demonstrate that the proposed method scales to large corpora\\nconsisting of hundreds of thousands of aligned sentences on an of-the-shelf\\nlaptop. We observe that on the one hand properties such as communities, capture\\nsome of the known differences between the languages, while others can be seen\\nas novel opportunities for linguistic studies.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.07296</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.07296</id><submitter>Ross Maciejewski</submitter><version version=\"v1\"><date>Wed, 17 Jul 2019 00:50:37 GMT</date><size>8137kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 20 Jul 2019 01:12:30 GMT</date><size>8137kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 1 Oct 2019 20:34:56 GMT</date><size>8137kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Thu, 3 Oct 2019 19:38:48 GMT</date><size>8138kb</size><source_type>D</source_type></version><title>Explaining Vulnerabilities to Adversarial Machine Learning through\\n  Visual Analytics</title><authors>Yuxin Ma, Tiankai Xie, Jundong Li, Ross Maciejewski</authors><categories>cs.HC cs.CR cs.LG</categories><comments>IEEE VAST (Transactions on Visualization and Computer Graphics), 2019</comments><doi>10.1109/TVCG.2019.2934631</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning models are currently being deployed in a variety of\\nreal-world applications where model predictions are used to make decisions\\nabout healthcare, bank loans, and numerous other critical tasks. As the\\ndeployment of artificial intelligence technologies becomes ubiquitous, it is\\nunsurprising that adversaries have begun developing methods to manipulate\\nmachine learning models to their advantage. While the visual analytics\\ncommunity has developed methods for opening the black box of machine learning\\nmodels, little work has focused on helping the user understand their model\\nvulnerabilities in the context of adversarial attacks. In this paper, we\\npresent a visual analytics framework for explaining and exploring model\\nvulnerabilities to adversarial attacks. Our framework employs a multi-faceted\\nvisualization scheme designed to support the analysis of data poisoning attacks\\nfrom the perspective of models, data instances, features, and local structures.\\nWe demonstrate our framework through two case studies on binary classifiers and\\nillustrate model vulnerabilities with respect to varying attack strategies.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.07487</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.07487</id><submitter>Alessandro Erba</submitter><version version=\"v1\"><date>Wed, 17 Jul 2019 12:54:48 GMT</date><size>412kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 12:29:27 GMT</date><size>410kb</size><source_type>D</source_type></version><title>Real-time Evasion Attacks with Physical Constraints on Deep\\n  Learning-based Anomaly Detectors in Industrial Control Systems</title><authors>Alessandro Erba, Riccardo Taormina, Stefano Galelli, Marcello\\n  Pogliani, Michele Carminati, Stefano Zanero, Nils Ole Tippenhauer</authors><categories>cs.CR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a number of deep learning-based anomaly detection algorithms were\\nproposed to detect attacks in dynamic industrial control systems. The detectors\\noperate on measured sensor data, leveraging physical process models learned a\\npriori. Evading detection by such systems is challenging, as an attacker needs\\nto manipulate a constrained number of sensor readings in real-time with\\nrealistic perturbations according to the current state of the system. In this\\nwork, we propose a number of evasion attacks (with different assumptions on the\\nattacker\\'s knowledge), and compare the attacks\\' cost and efficiency against\\nreplay attacks. In particular, we show that a replay attack on a subset of\\nsensor values can be detected easily as it violates physical constraints. In\\ncontrast, our proposed attacks leverage manipulated sensor readings that\\nobserve learned physical constraints of the system. Our proposed white box\\nattacker uses an optimization approach with a detection oracle, while our black\\nbox attacker uses an autoencoder (or a convolutional neural network) to\\ntranslate anomalous data into normal data. Our proposed approaches are\\nimplemented and evaluated on two different datasets pertaining to the domain of\\nwater distribution networks. We then demonstrated the efficacy of the real-time\\nattack on a realistic testbed. Results show that the accuracy of the detection\\nalgorithms can be significantly reduced through real-time adversarial actions:\\nfor the BATADAL dataset, the attacker can reduce the detection accuracy from\\n0.6 to 0.14. In addition, we discuss and implement an Availability attack, in\\nwhich the attacker introduces detection events with minimal changes of the\\nreported data, in order to reduce confidence in the detector.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.07606</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.07606</id><submitter>Ecenaz Erdemir</submitter><version version=\"v1\"><date>Wed, 17 Jul 2019 16:07:46 GMT</date><size>262kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 19:37:31 GMT</date><size>161kb</size><source_type>D</source_type></version><title>Privacy-Aware Location Sharing with Deep Reinforcement Learning</title><authors>Ecenaz Erdemir, Pier Luigi Dragotti and Deniz Gunduz</authors><categories>cs.IT cs.CR math.IT</categories><comments>Accepted to IEEE Workshop on Information Forensics and Security (WIFS\\n  2019)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Location-based services (LBSs) have become widely popular. Despite their\\nutility, these services raise concerns for privacy since they require sharing\\nlocation information with untrusted third parties. In this work, we study\\nprivacy-utility trade-off in location sharing mechanisms. Existing approaches\\nare mainly focused on privacy of sharing a single location or myopic location\\ntrace privacy; neither of them taking into account the temporal correlations\\nbetween the past and current locations. Although these methods preserve the\\nprivacy for the current time, they may leak significant amount of information\\nat the trace level as the adversary can exploit temporal correlations in a\\ntrace. We propose an information theoretically optimal privacy preserving\\nlocation release mechanism that takes temporal correlations into account. We\\nmeasure the privacy leakage by the mutual information between the user\\'s true\\nand released location traces. To tackle the history-dependent mutual\\ninformation minimization, we reformulate the problem as a Markov decision\\nprocess (MDP), and solve it using asynchronous actor-critic deep reinforcement\\nlearning (RL).\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.07841</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.07841</id><submitter>Wanchun Liu</submitter><version version=\"v1\"><date>Thu, 18 Jul 2019 02:11:10 GMT</date><size>659kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 6 Oct 2019 08:27:54 GMT</date><size>1958kb</size><source_type>D</source_type></version><title>Optimal Downlink-Uplink Scheduling of Wireless Networked Control for\\n  Industrial IoT</title><authors>Kang Huang and Wanchun Liu and Yonghui Li, Branka Vucetic and Andrey\\n  Savkin</authors><categories>cs.IT cs.SY eess.SY math.IT</categories><comments>This work has been submitted to the IEEE for possible publication.\\n  Copyright may be transferred without notice, after which this version may no\\n  longer be accessible</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a wireless networked control system (WNCS) consisting of\\na dynamic system to be controlled (i.e., a plant), a sensor, an actuator and a\\nremote controller for mission-critical Industrial Internet of Things (IIoT)\\napplications. A WNCS has two types of wireless transmissions, i.e., the\\nsensor\\'s measurement transmission to the controller and the controller\\'s\\ncommand transmission to the actuator. In this work, we consider a practical\\nhalf-duplex controller, which introduces a novel transmission-scheduling\\nproblem for WNCSs. A frequent scheduling of sensor\\'s transmission results in a\\nbetter estimation of plant states at the controller and thus a higher quality\\nof control command, but it leads to a less frequent/timely control of the\\nplant. Therefore, considering the overall control performance of the plant in\\nterms of its average cost function, there exists a fundamental tradeoff between\\nthe sensor\\'s and the controller\\'s transmissions. We formulate a new problem to\\noptimize the transmission-scheduling policy for minimizing the long-term\\naverage cost function. We derive the necessary and sufficient condition of the\\nexistence of a stationary and deterministic optimal policy that results in a\\nbounded average cost in terms of the transmission reliabilities of the\\nsensor-to-controller and controller-to-actuator channels. Also, we derive an\\neasy-to-compute suboptimal policy, which notably reduces the average cost of\\nthe plant compared to a naive alternative-scheduling policy.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.07890</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.07890</id><submitter>Julia Schulte</submitter><version version=\"v1\"><date>Thu, 18 Jul 2019 06:29:31 GMT</date><size>586kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 08:21:15 GMT</date><size>601kb</size><source_type>D</source_type></version><title>A feasibility study of deep neural networks for the recognition of\\n  banknotes regarding central bank requirements</title><authors>Julia Schulte, Daniel Staps, Alexander Lampe</authors><categories>cs.CV cs.LG eess.IV stat.ML</categories><comments>6 pages, 4 figures</comments><acm-class>I.7.5; I.5.1; I.2.6; G.1.6; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper contains a feasibility study of deep neural networks for the\\nclassification of Euro banknotes with respect to requirements of central banks\\non the ATM and high speed sorting industry. Instead of concentrating on the\\naccuracy for a large number of classes as in the famous ImageNet Challenge we\\nfocus thus on conditions with few classes and the requirement of rejection of\\nimages belonging clearly to neither of the trained classes (i.e. classification\\nin a so-called 0-class). These special requirements are part of frameworks\\ndefined by central banks as the European Central Bank and are met by current\\nATMs and high speed sorting machines. We also consider training and\\nclassification time on state of the art GPU hardware. The study concentrates on\\nthe banknote recognition whereas banknote class dependent authenticity and\\nfitness checks are a topic of its own which is not considered in this work.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.08175</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.08175</id><submitter>Terrance DeVries</submitter><version version=\"v1\"><date>Thu, 11 Jul 2019 17:41:57 GMT</date><size>7394kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 27 Sep 2019 21:02:23 GMT</date><size>15270kb</size><source_type>D</source_type></version><title>On the Evaluation of Conditional GANs</title><authors>Terrance DeVries, Adriana Romero, Luis Pineda, Graham W. Taylor,\\n  Michal Drozdzal</authors><categories>cs.CV cs.LG eess.IV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conditional Generative Adversarial Networks (cGANs) are finding increasingly\\nwidespread use in many application domains. Despite outstanding progress,\\nquantitative evaluation of such models often involves multiple distinct metrics\\nto assess different desirable properties, such as image quality, conditional\\nconsistency, and intra-conditioning diversity. In this setting, model\\nbenchmarking becomes a challenge, as each metric may indicate a different\\n&quot;best&quot; model. In this paper, we propose the Frechet Joint Distance (FJD), which\\nis defined as the Frechet distance between joint distributions of images and\\nconditioning, allowing it to implicitly capture the aforementioned properties\\nin a single metric. We conduct proof-of-concept experiments on a controllable\\nsynthetic dataset, which consistently highlight the benefits of FJD when\\ncompared to currently established metrics. Moreover, we use the newly\\nintroduced metric to compare existing cGAN-based models for a variety of\\nconditioning modalities (e.g. class labels, object masks, bounding boxes,\\nimages, and text captions). We show that FJD can be used as a promising single\\nmetric for model benchmarking.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.08199</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.08199</id><submitter>Daniel Angelov</submitter><version version=\"v1\"><date>Thu, 18 Jul 2019 14:28:46 GMT</date><size>4051kb</size></version><version version=\"v2\"><date>Tue, 1 Oct 2019 15:31:18 GMT</date><size>4203kb</size></version><title>Composing Diverse Policies for Temporally Extended Tasks</title><authors>Daniel Angelov, Yordan Hristov, Michael Burke, Subramanian Ramamoorthy</authors><categories>cs.RO cs.LG</categories><comments>arXiv admin note: substantial text overlap with arXiv:1906.10099</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robot control policies for temporally extended and sequenced tasks are often\\ncharacterized by discontinuous switches between different local dynamics. These\\nchange-points are often exploited in hierarchical motion planning to build\\napproximate models and to facilitate the design of local, region-specific\\ncontrollers. However, it becomes combinatorially challenging to implement such\\na pipeline for complex temporally extended tasks, especially when the\\nsub-controllers work on different information streams, time scales and action\\nspaces. In this paper, we introduce a method that can compose diverse policies\\ncomprising motion planning trajectories, dynamic motion primitives and neural\\nnetwork controllers. We introduce a global goal scoring estimator that uses\\nlocal, per-motion primitive dynamics models and corresponding activation\\nstate-space sets to sequence diverse policies in a locally optimal fashion. We\\nuse expert demonstrations to convert what is typically viewed as a\\ngradient-based learning process into a planning process without explicitly\\nspecifying pre- and post-conditions. We first illustrate the proposed framework\\nusing an MDP benchmark to showcase robustness to action and model dynamics\\nmismatch, and then with a particularly complex physical gear assembly task,\\nsolved on a PR2 robot. We show that the proposed approach successfully\\ndiscovers the optimal sequence of controllers and solves both tasks\\nefficiently.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.08475</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.08475</id><submitter>Bernhard Bermeitinger</submitter><version version=\"v1\"><date>Fri, 19 Jul 2019 11:56:09 GMT</date><size>103kb</size></version><title>Representational Capacity of Deep Neural Networks -- A Computing Study</title><authors>Bernhard Bermeitinger, Tomas Hrycej, Siegfried Handschuh</authors><categories>cs.LG stat.ML</categories><journal-ref>2019 11th International Conference on Knowledge Discovery and\\n  Information Retrieval (KDIR)</journal-ref><doi>10.5220/0008364305320538</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is some theoretical evidence that deep neural networks with multiple\\nhidden layers have a potential for more efficient representation of\\nmultidimensional mappings than shallow networks with a single hidden layer. The\\nquestion is whether it is possible to exploit this theoretical advantage for\\nfinding such representations with help of numerical training methods. Tests\\nusing prototypical problems with a known mean square minimum did not confirm\\nthis hypothesis. Minima found with the help of deep networks have always been\\nworse than those found using shallow networks. This does not directly\\ncontradict the theoretical findings---it is possible that the superior\\nrepresentational capacity of deep networks is genuine while finding the mean\\nsquare minimum of such deep networks is a substantially harder problem than\\nwith shallow ones.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.08553</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.08553</id><submitter>Andreas Walch</submitter><version version=\"v1\"><date>Fri, 19 Jul 2019 15:49:57 GMT</date><size>3004kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 09:45:36 GMT</date><size>3008kb</size><source_type>D</source_type></version><title>LightGuider: Guiding Interactive Lighting Design using Suggestions,\\n  Provenance, and Quality Visualization</title><authors>Andreas Walch, Michael Schw\\\\&quot;arzler, Christian Luksch, Elmar Eisemann,\\n  Theresia Gschwandtner</authors><categories>cs.GR</categories><doi>10.1109/TVCG.2019.2934658</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LightGuider is a novel guidance-based approach to interactive lighting\\ndesign, which typically consists of interleaved 3D modeling operations and\\nlight transport simulations. Rather than having designers use a trial-and-error\\napproach to match their illumination constraints and aesthetic goals,\\nLightGuider supports the process by simulating potential next modeling steps\\nthat can deliver the most significant improvements. LightGuider takes\\npredefined quality criteria and the current focus of the designer into account\\nto visualize suggestions for lighting-design improvements via a specialized\\nprovenance tree. This provenance tree integrates snapshot visualizations of how\\nwell a design meets the given quality criteria weighted by the designer\\'s\\npreferences. This integration facilitates the analysis of quality improvements\\nover the course of a modeling workflow as well as the comparison of alternative\\ndesign solutions. We evaluate our approach with three lighting designers to\\nillustrate its usefulness.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.08796</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.08796</id><submitter>Leo Yu-Ho Lo</submitter><version version=\"v1\"><date>Sat, 20 Jul 2019 11:11:38 GMT</date><size>62kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 08:40:59 GMT</date><size>115kb</size><source_type>D</source_type></version><title>Learning Vis Tools: Teaching Data Visualization Tutorials</title><authors>Leo Yu-Ho Lo, Yao Ming, and Huamin Qu</authors><categories>cs.HC</categories><comments>5 pages, 1 figure, IEEE VIS 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Teaching and advocating data visualization are among the most important\\nactivities in the visualization community. With growing interest in data\\nanalysis from business and science professionals, data visualization courses\\nattract students across different disciplines. However, comprehensive\\nvisualization training requires students to have a certain level of proficiency\\nin programming, a requirement that imposes challenges on both teachers and\\nstudents. With recent developments in visualization tools, we have managed to\\novercome these obstacles by teaching a wide range of visualization and\\nsupporting tools. Starting with GUI-based visualization tools and data analysis\\nwith Python, students put visualization knowledge into practice with increasing\\namounts of programming. At the end of the course, students can design and\\nimplement visualizations with D3 and other programming-based visualization\\ntools. Throughout the course, we continuously collect student feedback and\\nrefine the teaching materials. This paper documents our teaching methods and\\nconsiderations when designing the teaching materials.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.08951</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.08951</id><submitter>Yang Li</submitter><version version=\"v1\"><date>Sun, 21 Jul 2019 10:16:52 GMT</date><size>2458kb</size></version><title>Dynamic State Estimation of Synchronous Machines Using Robust Cubature\\n  Kalman Filter Against Complex Measurement Noise Statistics</title><authors>Yang Li, Jing Li, Liang Chen, Junjian Qi, Guoqing Li</authors><categories>eess.SY cs.SY eess.SP</categories><comments>Accepted by Transactions of China Electrotechnical Society, in\\n  Chinese</comments><journal-ref>Transactions of China Electrotechnical Society 34 (2019) 3651-3660</journal-ref><doi>10.19595/j.cnki.1000-6753.tces.181150</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cubature Kalman Filter (CKF) has good performance when handling nonlinear\\ndynamic state estimations. However, it cannot work well in non-Gaussian noise\\nand bad data environment due to the lack of auto-adaptive ability to measure\\nnoise statistics on line. In order to address the problem of behavioral decline\\nand divergence when measure noise statistics deviate prior noise statistics, a\\nnew robust CKF (RCKF) algorithm is developed by combining the Huber\\'s\\nM-estimation theory with the classical CKF, and thereby it is proposed to\\ncoping with the dynamic state estimation of synchronous generators in this\\nstudy. The simulation results on the IEEE-9 bus system and New England\\n16-machine-68-bus system demonstrate that the estimation accuracy and\\nconvergence of the proposed RCKF are superior to those of the classical CKF\\nunder complex measurement noise environments including different measurement\\nnoises and bad data, and that the RCKF is capable of effectively eliminating\\nthe impact of bad data on the estimation effects.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.09204</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.09204</id><submitter>Gabriel Michau Dr.</submitter><version version=\"v1\"><date>Mon, 22 Jul 2019 09:49:50 GMT</date><size>317kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 07:24:00 GMT</date><size>495kb</size><source_type>D</source_type></version><title>Domain Adaptation for One-Class Classification: Monitoring the Health of\\n  Critical Systems Under Limited Information</title><authors>Gabriel Michau and Olga Fink</authors><categories>stat.ML cs.LG stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The failure of a complex and safety critical industrial asset can have\\nextremely high consequences. Close monitoring for early detection of abnormal\\nsystem conditions is therefore required. Data-driven solutions to this problem\\nhave been limited for two reasons: First, safety critical assets are designed\\nand maintained to be highly reliable and faults are rare. Fault detection can\\nthus not be solved with supervised learning. Second, complex industrial systems\\nusually have long lifetime during which they face very different operating\\nconditions. In the early life of the system, the collected data is probably not\\nrepresentative of future operating conditions, making it challenging to train a\\nrobust model.\\n  In this paper, we propose a methodology to monitor the systems in their early\\nlife. To do so, we enhance the training dataset with other units from a fleet,\\nfor which longer observations are available. Since each unit has its own\\nspecificity, we propose to extract features made independent of their origin by\\nthree unsupervised feature alignment techniques. First, using a variational\\nencoder, we impose a shared probabilistic encoder/decoder for both units.\\nSecond, we introduce a new loss designed to conserve inter-point spacial\\nrelationships between the input and the learned features. Last, we propose to\\ntrain in an adversarial manner a discriminator on the origin of the features.\\nOnce aligned, the features are fed to a one-class classifier to monitor the\\nhealth of the system. By exploring the different combinations of the proposed\\nalignment strategies, and by testing them on a real case study, a fleet\\ncomposed of 112 power plants operated in different geographical locations and\\nunder very different operating regimes, we demonstrate that this alignment is\\nnecessary and beneficial.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.09495</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.09495</id><submitter>Lin Meng</submitter><version version=\"v1\"><date>Mon, 22 Jul 2019 18:01:04 GMT</date><size>1028kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 03:55:50 GMT</date><size>1065kb</size><source_type>D</source_type></version><title>IsoNN: Isomorphic Neural Network for Graph Representation Learning and\\n  Classification</title><authors>Lin Meng and Jiawei Zhang</authors><categories>cs.LG stat.ML</categories><comments>14 pages, 6 figures, submitted to ICLR 2020</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning models have achieved huge success in numerous fields, such as\\ncomputer vision and natural language processing. However, unlike such fields,\\nit is hard to apply traditional deep learning models on the graph data due to\\nthe \\'node-orderless\\' property. Normally, adjacency matrices will cast an\\nartificial and random node-order on the graphs, which renders the performance\\nof deep models on graph classification tasks extremely erratic, and the\\nrepresentations learned by such models lack clear interpretability. To\\neliminate the unnecessary node-order constraint, we propose a novel model named\\nIsomorphic Neural Network (IsoNN), which learns the graph representation by\\nextracting its isomorphic features via the graph matching between input graph\\nand templates. IsoNN has two main components: graph isomorphic feature\\nextraction component and classification component. The graph isomorphic feature\\nextraction component utilizes a set of subgraph templates as the kernel\\nvariables to learn the possible subgraph patterns existing in the input graph\\nand then computes the isomorphic features. A set of permutation matrices is\\nused in the component to break the node-order brought by the matrix\\nrepresentation. Three fully-connected layers are used as the classification\\ncomponent in IsoNN. Extensive experiments are conducted on benchmark datasets,\\nthe experimental results can demonstrate the effectiveness of ISONN, especially\\ncompared with both classic and state-of-the-art graph classification methods.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.09636</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.09636</id><submitter>Woojay Jeon</submitter><version version=\"v1\"><date>Mon, 22 Jul 2019 23:53:41 GMT</date><size>212kb</size></version><version version=\"v2\"><date>Sat, 5 Oct 2019 00:43:25 GMT</date><size>212kb</size></version><title>On Modeling ASR Word Confidence</title><authors>Woojay Jeon, Maxwell Jordan, Mahesh Krishnamoorthy</authors><categories>cs.CL cs.SD eess.AS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new method for computing ASR word confidences that effectively\\nmitigates ASR errors for diverse downstream applications, improves the word\\nerror rate of the 1-best result, and allows better comparison of scores across\\ndifferent models. We propose 1) a new method for modeling word confidence using\\na Heterogeneous Word Confusion Network (HWCN) that addresses some key flaws in\\nconventional Word Confusion Networks, and 2) a new score calibration method for\\nfacilitating direct comparison of scores from different models. Using a\\nbidirectional lattice recurrent neural network to compute the confidence scores\\nof each word in the HWCN, we show that the word sequence with the best overall\\nconfidence is more accurate than the default 1-best result of the recognizer,\\nand that the calibration method greatly improves the reliability of recognizer\\ncombination.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.09949</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.09949</id><submitter>Peng Cheng</submitter><version version=\"v1\"><date>Sun, 21 Jul 2019 10:54:21 GMT</date><size>2614kb</size><source_type>D</source_type></version><title>A Learning-Based Two-Stage Spectrum Sharing Strategy with Multiple\\n  Primary Transmit Power Levels</title><authors>Rui Zhang, Peng Cheng, Zhuo Chen, Yonghui Li, Branka Vucetic</authors><categories>eess.SP cs.IT cs.LG math.IT stat.ML</categories><comments>46 pages, 10 figures, accepted by IEEE Transactions on Signal\\n  Processing 2019</comments><doi>10.1109/TSP.2019.2932866</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-parameter cognition in a cognitive radio network (CRN) provides a more\\nthorough understanding of the radio environments, and could potentially lead to\\nfar more intelligent and efficient spectrum usage for a secondary user. In this\\npaper, we investigate the multi-parameter cognition problem for a CRN where the\\nprimary transmitter (PT) radiates multiple transmit power levels, and propose a\\nlearning-based two-stage spectrum sharing strategy. We first propose a\\ndata-driven/machine learning based multi-level spectrum sensing scheme,\\nincluding the spectrum learning (Stage I) and prediction (the first part in\\nStage II). This fully blind sensing scheme does not require any prior knowledge\\nof the PT power characteristics. Then, based on a novel normalized power level\\nalignment metric, we propose two prediction-transmission structures, namely\\nperiodic and non-periodic, for spectrum access (the second part in Stage II),\\nwhich enable the secondary transmitter (ST) to closely follow the PT power\\nlevel variation. The periodic structure features a fixed prediction interval,\\nwhile the non-periodic one dynamically determines the interval with a proposed\\nreinforcement learning algorithm to further improve the alignment metric.\\nFinally, we extend the prediction-transmission structure to an online scenario,\\nwhere the number of PT power levels might change as a consequence of PT\\nadapting to the environment fluctuation or quality of service variation. The\\nsimulation results demonstrate the effectiveness of the proposed strategy in\\nvarious scenarios.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.10046</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.10046</id><submitter>Naftali Cohen</submitter><version version=\"v1\"><date>Tue, 23 Jul 2019 17:58:10 GMT</date><size>2393kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 14:02:59 GMT</date><size>1380kb</size><source_type>D</source_type></version><title>Trading via Image Classification</title><authors>Naftali Cohen, Tucker Balch, and Manuela Veloso</authors><categories>cs.CV q-fin.CP q-fin.TR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The art of systematic financial trading evolved with an array of approaches,\\nranging from simple strategies to complex algorithms all relying, primary, on\\naspects of time-series analysis. Recently, after visiting the trading floor of\\na leading financial institution, we noticed that traders always execute their\\ntrade orders while observing images of financial time-series on their screens.\\nIn this work, we built upon the success in image recognition and examine the\\nvalue in transforming the traditional time-series analysis to that of image\\nclassification. We create a large sample of financial time-series images\\nencoded as candlestick (Box and Whisker) charts and label the samples following\\nthree algebraically-defined binary trade strategies. Using the images, we train\\nover a dozen machine-learning classification models and find that the\\nalgorithms are very efficient in recovering the complicated, multiscale\\nlabel-generating rules when the data is represented visually. We suggest that\\nthe transformation of continuous numeric time-series classification problem to\\na vision problem is useful for recovering signals typical of technical\\nanalysis.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.10147</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.10147</id><submitter>Mineto Tsukada</submitter><version version=\"v1\"><date>Tue, 23 Jul 2019 21:40:46 GMT</date><size>1426kb</size></version><version version=\"v2\"><date>Tue, 27 Aug 2019 12:23:16 GMT</date><size>922kb</size></version><version version=\"v3\"><date>Wed, 28 Aug 2019 17:25:50 GMT</date><size>927kb</size></version><version version=\"v4\"><date>Wed, 2 Oct 2019 15:42:33 GMT</date><size>2073kb</size></version><title>A Neural Network-Based On-device Learning Anomaly Detector for Edge\\n  Devices</title><authors>Mineto Tsukada, Masaaki Kondo, Hiroki Matsutani</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semi-supervised anomaly detection is an approach to identify anomalies by\\nmodeling the distribution of normal data. Nowadays, backpropagation neural\\nnetworks (i.e., BP-NNs) based approaches have been drawing attention because of\\ntheir high generalization performance for a wide range of real-world data. In a\\ntypical application, such BP-NN-based models are iteratively optimized in\\nserver machines with a large amount of data gathered from edge devices.\\nHowever, there are two issues in this framework: (1) BP-NNs\\' iterative\\noptimization approach often takes too long a time to follow time-series changes\\nof the distribution of normal data (i.e., concept drift), and (2) data\\ntransfers between the server machines and the edge devices have a risk to cause\\ndata breaches. To address these issues, we propose an ON-device sequential\\nLearning semi-supervised Anomaly Detector called ONLAD and its FPGA-based IP\\ncore called ONLAD Core so that various kinds of resource-limited edge devices\\ncan use our approach. Experimental results show that ONLAD has favorable\\nanomaly detection capability especially in an environment which simulates\\nconcept drift. Evaluations of ONLAD Core confirm that it can perform training\\nand prediction computations faster than BP-NN-based software implementations by\\nx1.95 - x4.51 and x2.29 - x4.73, respectively. We also demonstrate that our\\non-board implementation which integrates ONLAD Core works at x6.3 $\\\\sim$ x25.4\\nlower power consumption while training computations are continuously executed.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.10282</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.10282</id><submitter>Dibyasundar Das</submitter><version version=\"v1\"><date>Wed, 24 Jul 2019 07:52:57 GMT</date><size>1326kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 26 Jul 2019 08:37:30 GMT</date><size>1326kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 29 Jul 2019 12:48:12 GMT</date><size>1326kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Mon, 7 Oct 2019 08:37:20 GMT</date><size>2624kb</size><source_type>D</source_type></version><title>Backward-Forward Algorithm: An Improvement towards Extreme Learning\\n  Machine</title><authors>Dibyasundar Das, Deepak Ranjan Nayak, Ratnakar Dash, Banshidhar Majhi</authors><categories>cs.LG cs.CV</categories><comments>12 Pages, 11 figures, to be submitted to journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The extreme learning machine needs a large number of hidden nodes to\\ngeneralize a single hidden layer neural network for a given training data-set.\\nThe need for more number of hidden nodes suggests that the neural-network is\\nmemorizing rather than generalizing the model. Hence, a supervised learning\\nmethod is described here that uses Moore-Penrose approximation to determine\\nboth input-weight and output-weight in two epochs, namely, backward-pass and\\nforward-pass. The proposed technique has an advantage over the back-propagation\\nmethod in terms of iterations required and is superior to the extreme learning\\nmachine in terms of the number of hidden units necessary for generalization.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.10823</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.10823</id><submitter>Isay Katsman</submitter><version version=\"v1\"><date>Tue, 23 Jul 2019 23:37:15 GMT</date><size>351kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 6 Oct 2019 00:45:51 GMT</date><size>923kb</size><source_type>D</source_type></version><title>Enhancing Adversarial Example Transferability with an Intermediate Level\\n  Attack</title><authors>Qian Huang, Isay Katsman, Horace He, Zeqi Gu, Serge Belongie, Ser-Nam\\n  Lim</authors><categories>cs.LG cs.CR cs.CV stat.ML</categories><comments>ICCV 2019 camera-ready. arXiv admin note: text overlap with\\n  arXiv:1811.08458</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural networks are vulnerable to adversarial examples, malicious inputs\\ncrafted to fool trained models. Adversarial examples often exhibit black-box\\ntransfer, meaning that adversarial examples for one model can fool another\\nmodel. However, adversarial examples are typically overfit to exploit the\\nparticular architecture and feature representation of a source model, resulting\\nin sub-optimal black-box transfer attacks to other target models. We introduce\\nthe Intermediate Level Attack (ILA), which attempts to fine-tune an existing\\nadversarial example for greater black-box transferability by increasing its\\nperturbation on a pre-specified layer of the source model, improving upon\\nstate-of-the-art methods. We show that we can select a layer of the source\\nmodel to perturb without any knowledge of the target models while achieving\\nhigh transferability. Additionally, we provide some explanatory insights\\nregarding our method and the effect of optimizing for adversarial examples\\nusing intermediate feature maps.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.10917</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.10917</id><submitter>Chee Siang Ang</submitter><version version=\"v1\"><date>Thu, 25 Jul 2019 09:22:29 GMT</date><size>2601kb</size></version><title>An EMG-based Eating Behaviour Monitoring System with Haptic Feedback to\\n  Promote Mindful Eating</title><authors>Ben Nicholls, Chee Siang Ang, Eiman Kanjo, Panote Siriaraya, Woon-Hong\\n  Yeo, Athanasios Tsanas</authors><categories>cs.HC</categories><comments>v1: 13 + 12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mindless eating, or the lack of awareness of the food we are consuming, has\\nbeen linked to health problems attributed to unhealthy eating behaviour,\\nincluding obesity. Traditional approaches used to moderate eating behaviour\\noften rely on inaccurate self-logging, manual observations or bulky equipment.\\nOverall, there is a need for an intelligent and lightweight system which can\\nautomatically monitor eating behaviour and provide feedback. In this paper, we\\ninvestigate: i) the development of an automated system for detecting eating\\nbehaviour using wearable Electromyography (EMG) sensors, and ii) the\\napplication of such a system in combination with real time wristband haptic\\nfeedback to facilitate mindful eating. Data collected from 16 participants were\\nused to develop an algorithm for detecting chewing and swallowing. We extracted\\n18 features from EMG and presented those features to different classifiers. We\\ndemonstrated that eating behaviour can be automatically assessed accurately\\nusing the EMG-extracted features and a Support Vector Machine (SVM):\\nF1-Score=0.94 for chewing classification, and F1-Score=0.86 for swallowing\\nclassification. Based on this algorithm, we developed a system to enable\\nparticipants to self-moderate their chewing behaviour using haptic feedback. An\\nexperiment study was carried out with 20 additional participants showing that\\nparticipants exhibited a lower rate of chewing when haptic feedback delivered\\nin forms of wristband vibration was used compared to a baseline and non-haptic\\ncondition (F (2,38)=58.243, p&lt;0.001). These findings may have major\\nimplications for research in eating behaviour, providing key new insights into\\nthe impacts of automatic chewing detection and haptic feedback systems on\\nmoderating eating behaviour with the aim to improve health outcomes.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.10982</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.10982</id><submitter>Zeju Li</submitter><version version=\"v1\"><date>Thu, 25 Jul 2019 11:47:12 GMT</date><size>1589kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 13:22:58 GMT</date><size>1589kb</size><source_type>D</source_type></version><title>Overfitting of neural nets under class imbalance: Analysis and\\n  improvements for segmentation</title><authors>Zeju Li and Konstantinos Kamnitsas and Ben Glocker</authors><categories>cs.LG cs.CV stat.ML</categories><comments>Accepted at MICCAI 2019; typo corrected in Table 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Overfitting in deep learning has been the focus of a number of recent works,\\nyet its exact impact on the behavior of neural networks is not well understood.\\nThis study analyzes overfitting by examining how the distribution of logits\\nalters in relation to how much the model overfits. Specifically, we find that\\nwhen training with few data samples, the distribution of logit activations when\\nprocessing unseen test samples of an under-represented class tends to shift\\ntowards and even across the decision boundary, while the over-represented class\\nseems unaffected. In image segmentation, foreground samples are often heavily\\nunder-represented. We observe that sensitivity of the model drops as a result\\nof overfitting, while precision remains mostly stable. Based on our analysis,\\nwe derive asymmetric modifications of existing loss functions and regularizers\\nincluding a large margin loss, focal loss, adversarial training and mixup,\\nwhich specifically aim at reducing the shift observed when embedding unseen\\nsamples of the under-represented class. We study the case of binary\\nsegmentation of brain tumor core and show that our proposed simple\\nmodifications lead to significantly improved segmentation performance over the\\nsymmetric variants.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.11019</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.11019</id><submitter>Siddharth Barman</submitter><version version=\"v1\"><date>Thu, 25 Jul 2019 13:04:34 GMT</date><size>197kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 14:16:35 GMT</date><size>199kb</size><source_type>D</source_type></version><title>Fair and Efficient Cake Division with Connected Pieces</title><authors>Eshwar Ram Arunachaleswaran, Siddharth Barman, Rachitesh Kumar, and\\n  Nidhi Rathi</authors><categories>cs.GT</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classic cake-cutting problem provides a model for addressing fair and\\nefficient allocation of a divisible, heterogeneous resource (metaphorically,\\nthe cake) among agents with distinct preferences. Focusing on a standard\\nformulation of cake cutting, in which each agent must receive a contiguous\\npiece of the cake, this work establishes algorithmic and hardness results for\\nmultiple fairness/efficiency measures.\\n  First, we consider the well-studied notion of envy-freeness and develop an\\nefficient algorithm that finds a cake division (with connected pieces) wherein\\nthe envy is multiplicatively within a factor of $3 + o(1)$. The same algorithm\\nin fact achieves an approximation ratio of $3 + o(1)$ for the problem of\\nfinding cake divisions with as large a Nash social welfare (NSW) as possible.\\nNSW is another standard measure of fairness and this work also establishes a\\nconnection between envy-freeness and NSW: approximately envy-free cake\\ndivisions (with connected pieces) always have near-optimal Nash social welfare.\\nFurthermore, we develop an approximation algorithm for maximizing the\\n$\\\\rho$-mean welfare--this unifying objective, with different values of $\\\\rho$,\\ninterpolates between notions of fairness (NSW) and efficiency (average social\\nwelfare). Finally, we complement these algorithmic results by proving that\\nmaximizing NSW (and, in general, the $\\\\rho$-mean welfare) is APX-hard in the\\ncake-division context.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.11357</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.11357</id><submitter>Gen Li</submitter><version version=\"v1\"><date>Fri, 26 Jul 2019 01:50:31 GMT</date><size>3402kb</size></version><version version=\"v2\"><date>Tue, 1 Oct 2019 01:29:58 GMT</date><size>3402kb</size></version><title>DABNet: Depth-wise Asymmetric Bottleneck for Real-time Semantic\\n  Segmentation</title><authors>Gen Li, Inyoung Yun, Jonghyun Kim, Joongkyu Kim</authors><categories>cs.CV</categories><comments>Accepted to BMVC 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a pixel-level prediction task, semantic segmentation needs large\\ncomputational cost with enormous parameters to obtain high performance.\\nRecently, due to the increasing demand for autonomous systems and robots, it is\\nsignificant to make a tradeoff between accuracy and inference speed. In this\\npaper, we propose a novel Depthwise Asymmetric Bottleneck (DAB) module to\\naddress this dilemma, which efficiently adopts depth-wise asymmetric\\nconvolution and dilated convolution to build a bottleneck structure. Based on\\nthe DAB module, we design a Depth-wise Asymmetric Bottleneck Network (DABNet)\\nespecially for real-time semantic segmentation, which creates sufficient\\nreceptive field and densely utilizes the contextual information. Experiments on\\nCityscapes and CamVid datasets demonstrate that the proposed DABNet achieves a\\nbalance between speed and precision. Specifically, without any pretrained model\\nand postprocessing, it achieves 70.1% Mean IoU on the Cityscapes test dataset\\nwith only 0.76 million parameters and a speed of 104 FPS on a single GTX 1080Ti\\ncard.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.11368</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.11368</id><submitter>Jeremy Cook</submitter><version version=\"v1\"><date>Tue, 9 Jul 2019 17:05:26 GMT</date><size>11kb</size></version><version version=\"v2\"><date>Sun, 6 Oct 2019 19:49:06 GMT</date><size>12kb</size></version><title>On the relationships between Z-, C-, and H-local unitaries</title><authors>Jeremy Cook</authors><categories>quant-ph cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum walk algorithms can speed up search of physical regions of space in\\nboth the discrete-time [arXiv:quant-ph/0402107] and continuous-time setting\\n[arXiv:quant-ph/0306054], where the physical region of space being searched is\\nmodeled as a connected graph. In such a model, Aaronson and Ambainis\\n[arXiv:quant-ph/0303041] provide three different criteria for a unitary matrix\\nto act locally with respect to a graph, called $Z$-local, $C$-local, and\\n$H$-local unitaries, and left the open question of relating these three\\nlocality criteria. Using a correspondence between continuous- and discrete-time\\nquantum walks by Childs [arXiv:0810.0312], we provide a way to approximate\\n$N\\\\times N$ $H$-local unitaries with error $\\\\delta$ using\\n$O(1/\\\\sqrt{\\\\delta},\\\\sqrt{N})$ $C$-local unitaries, where the comma denotes the\\nmaximum of the two terms.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.11651</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.11651</id><submitter>Mohsen Rakhshandehroo</submitter><version version=\"v1\"><date>Tue, 23 Jul 2019 18:11:05 GMT</date><size>2663kb</size></version><version version=\"v2\"><date>Tue, 20 Aug 2019 15:15:11 GMT</date><size>2663kb</size></version><version version=\"v3\"><date>Sun, 29 Sep 2019 19:21:14 GMT</date><size>2665kb</size></version><title>Time Series Analysis of Electricity Price and Demand to Find\\n  Cyber-attacks using Stationary Analysis</title><authors>Mohsen Rakhshandehroo, Mohammad Rajabdorri</authors><categories>eess.SP cs.CR cs.DB cs.SY eess.SY stat.ML</categories><comments>9pages, 13 figs, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With developing of computation tools in the last years, data analysis methods\\nto find insightful information are becoming more common among industries and\\nresearchers. This paper is the first part of the times series analysis of New\\nEngland electricity price and demand to find anomaly in the data. In this paper\\ntime-series stationary criteria to prepare data for further times-series\\nrelated analysis is investigated. Three main analysis are conducted in this\\npaper, including moving average, moving standard deviation and augmented\\nDickey-Fuller test. The data used in this paper is New England big data from 9\\ndifferent operational zones. For each zone, 4 different variables including\\nday-ahead (DA) electricity demand, price and real-time (RT) electricity demand\\nprice are considered.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.12013</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.12013</id><submitter>Thomas Vandal</submitter><version version=\"v1\"><date>Sun, 28 Jul 2019 04:44:50 GMT</date><size>8393kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 27 Sep 2019 22:47:30 GMT</date><size>7748kb</size><source_type>D</source_type></version><title>Optical Flow for Intermediate Frame Interpolation of Multispectral\\n  Geostationary Satellite Data</title><authors>Thomas Vandal and Ramakrishna Nemani</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applications of satellite data in areas such as weather tracking and\\nmodeling, ecosystem monitoring, wildfire detection, and landcover change are\\nheavily dependent on the trade-offs related to the spatial, spectral and\\ntemporal resolutions of the observations. For instance, geostationary weather\\ntracking satellites are designed to take hemispherical snapshots many times\\nthroughout the day but sensor hardware limits data collection. In this work we\\ntackle this limitation by developing a method for temporal upsampling of\\nmulti-spectral satellite imagery using optical flow video interpolation deep\\nconvolutional neural networks. The presented model, extends Super SloMo (SSM)\\nfrom single optical flow estimates to multichannel where flows are computed per\\nwavelength band. We apply this technique on up to 8 multispectral bands of\\nGOES-R/Advanced Baseline Imager mesoscale dataset to temporally enhance full\\ndisk hemispheric snapshots from 15 minutes to 1 minute. Through extensive\\nexperimentation with a multi-terabyte dataset, we show SSM greatly outperforms\\nthe linear interpolation baseline and that multichannel optical flows improves\\nperformance on GOES/ABI. Furthermore, we discuss challenges and open questions\\nrelated to temporal interpolation of multispectral geostationary satellite\\nimagery.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.12155</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.12155</id><submitter>Adrien Le Coent</submitter><version version=\"v1\"><date>Thu, 25 Jul 2019 09:07:36 GMT</date><size>677kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 12:33:32 GMT</date><size>677kb</size><source_type>D</source_type></version><title>Guaranteed optimal reachability control of reaction-diffusion equations\\n  using one-sided Lipschitz constants and model reduction</title><authors>Adrien Le Co\\\\&quot;ent, Laurent Fribourg (LSV, ENS Paris Saclay, CNRS)</authors><categories>math.OC cs.SY eess.SY</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that, for any spatially discretized system of reaction-diffusion, the\\napproximate solution given by the explicit Euler time-discretization scheme\\nconverges to the exact time-continuous solution, provided that diffusion\\ncoefficient be sufficiently large. By ``sufficiently large\\'\\', we mean that the\\ndiffusion coefficient value makes the one-sided Lipschitz constant of the\\nreaction-diffusion system negative. We apply this result to solve a finite\\nhorizon control problem for a 1D reaction-diffusion example. We also explain\\nhow to perform model reduction in order to improve the efficiency of the\\nmethod.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.12201</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.12201</id><submitter>Dong Sun</submitter><version version=\"v1\"><date>Mon, 29 Jul 2019 03:53:07 GMT</date><size>8964kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 6 Aug 2019 07:50:09 GMT</date><size>8963kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sat, 5 Oct 2019 05:23:51 GMT</date><size>8964kb</size><source_type>D</source_type></version><title>PlanningVis: A Visual Analytics Approach to Production Planning in Smart\\n  Factories</title><authors>Dong Sun, Renfei Huang, Yuanzhe Chen, Yong Wang, Jia Zeng, Mingxuan\\n  Yuan, Ting-Chuen Pong, and Huamin Qu</authors><categories>cs.HC</categories><doi>10.1109/TVCG.2019.2934275</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Production planning in the manufacturing industry is crucial for fully\\nutilizing factory resources (e.g., machines, raw materials and workers) and\\nreducing costs. With the advent of industry 4.0, plenty of data recording the\\nstatus of factory resources have been collected and further involved in\\nproduction planning, which brings an unprecedented opportunity to understand,\\nevaluate and adjust complex production plans through a data-driven approach.\\nHowever, developing a systematic analytics approach for production planning is\\nchallenging due to the large volume of production data, the complex dependency\\nbetween products, and unexpected changes in the market and the plant. Previous\\nstudies only provide summarized results and fail to show details for\\ncomparative analysis of production plans. Besides, the rapid adjustment to the\\nplan in the case of an unanticipated incident is also not supported. In this\\npaper, we propose PlanningVis, a visual analytics system to support the\\nexploration and comparison of production plans with three levels of details: a\\nplan overview presenting the overall difference between plans, a product view\\nvisualizing various properties of individual products, and a production detail\\nview displaying the product dependency and the daily production details in\\nrelated factories. By integrating an automatic planning algorithm with\\ninteractive visual explorations, PlanningVis can facilitate the efficient\\noptimization of daily production planning as well as support a quick response\\nto unanticipated incidents in manufacturing. Two case studies with real-world\\ndata and carefully designed interviews with domain experts demonstrate the\\neffectiveness and usability of PlanningVis.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.12392</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.12392</id><submitter>Felix Leibfried</submitter><version version=\"v1\"><date>Fri, 26 Jul 2019 16:34:21 GMT</date><size>578kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 30 Jul 2019 12:13:31 GMT</date><size>578kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 9 Sep 2019 14:24:58 GMT</date><size>633kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Tue, 8 Oct 2019 15:25:58 GMT</date><size>633kb</size><source_type>D</source_type></version><title>A Unified Bellman Optimality Principle Combining Reward Maximization and\\n  Empowerment</title><authors>Felix Leibfried and Sergio Pascual-Diaz and Jordi Grau-Moya</authors><categories>cs.LG cs.AI stat.ML</categories><comments>Proceedings of the 33rd Conference on Neural Information Processing\\n  Systems (NeurIPS), Vancouver, Canada, 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Empowerment is an information-theoretic method that can be used to\\nintrinsically motivate learning agents. It attempts to maximize an agent\\'s\\ncontrol over the environment by encouraging visiting states with a large number\\nof reachable next states. Empowered learning has been shown to lead to complex\\nbehaviors, without requiring an explicit reward signal. In this paper, we\\ninvestigate the use of empowerment in the presence of an extrinsic reward\\nsignal. We hypothesize that empowerment can guide reinforcement learning (RL)\\nagents to find good early behavioral solutions by encouraging highly empowered\\nstates. We propose a unified Bellman optimality principle for empowered reward\\nmaximization. Our empowered reward maximization approach generalizes both\\nBellman\\'s optimality principle as well as recent information-theoretical\\nextensions to it. We prove uniqueness of the empowered values and show\\nconvergence to the optimal solution. We then apply this idea to develop\\noff-policy actor-critic RL algorithms which we validate in high-dimensional\\ncontinuous robotics domains (MuJoCo). Our methods demonstrate improved initial\\nand competitive final performance compared to model-free state-of-the-art\\ntechniques.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.12859</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.12859</id><submitter>Onur Tasar</submitter><version version=\"v1\"><date>Tue, 30 Jul 2019 12:30:32 GMT</date><size>8552kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 19:56:56 GMT</date><size>8820kb</size><source_type>D</source_type></version><title>ColorMapGAN: Unsupervised Domain Adaptation for Semantic Segmentation\\n  Using Color Mapping Generative Adversarial Networks</title><authors>Onur Tasar, S L Happy, Yuliya Tarabalka, Pierre Alliez</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the various reasons such as atmospheric effects and differences in\\nacquisition, it is often the case that there exists a large difference between\\nspectral bands of satellite images collected from different geographic\\nlocations. The large shift between spectral distributions of training and test\\ndata causes the current state of the art supervised learning approaches to\\noutput unsatisfactory maps. We present a novel semantic segmentation framework\\nthat is robust to such shift. The key component of the proposed framework is\\nColor Mapping Generative Adversarial Networks (ColorMapGAN), which can generate\\nfake training images that are semantically exactly the same as training images,\\nbut whose spectral distribution is similar to the distribution of the test\\nimages. We then use the fake images and the ground-truth for the training\\nimages to fine-tune the already trained classifier. Contrary to the existing\\nGenerative Adversarial Networks (GANs), the generator in ColorMapGAN does not\\nhave any convolutional or pooling layers. It learns to transform the colors of\\nthe training data to the colors of the test data by performing only one\\nelement-wise matrix multiplication and one matrix addition operations. Thanks\\nto the architecturally simple but powerful design of ColorMapGAN, the proposed\\nframework outperforms the existing approaches with a large margin in terms of\\nboth accuracy and computational complexity.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.12887</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.12887</id><submitter>Fabian B. Fuchs Mr</submitter><version version=\"v1\"><date>Fri, 12 Jul 2019 22:40:13 GMT</date><size>9110kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 9 Aug 2019 17:17:49 GMT</date><size>9120kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 30 Sep 2019 15:44:01 GMT</date><size>7363kb</size><source_type>D</source_type></version><title>End-to-end Recurrent Multi-Object Tracking and Trajectory Prediction\\n  with Relational Reasoning</title><authors>Fabian B. Fuchs, Adam R. Kosiorek, Li Sun, Oiwi Parker Jones, Ingmar\\n  Posner</authors><categories>cs.CV cs.AI cs.LG cs.RO stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relational reasoning - the ability to model interactions and relations\\nbetween objects - is valuable for robust multi-object tracking and pivotal for\\ntrajectory prediction. In this paper, we propose MOHART, a class-agnostic,\\nend-to-end multi-object tracking and trajectory prediction algorithm, which\\nexplicitly accounts for permutation invariance in its relational reasoning. We\\nexplore a number of permutation invariant architectures and show that\\nmulti-headed self-attention outperforms the provided baselines and better\\naccounts for complex physical interactions in a challenging toy experiment. We\\nshow on three real-world tracking datasets that adding relational reasoning\\ncapabilities in this way increases the tracking and trajectory prediction\\nperformance, particularly in the presence of ego-motion, occlusions, crowded\\nscenes, and faulty sensor inputs. To the best of our knowledge, MOHART is the\\nfirst fully end-to-end multi-object tracking from vision approach applied to\\nreal-world data reported in the literature.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.12898</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.12898</id><submitter>Yang Hu</submitter><version version=\"v1\"><date>Fri, 19 Jul 2019 21:20:49 GMT</date><size>2487kb</size></version><version version=\"v2\"><date>Tue, 8 Oct 2019 09:21:58 GMT</date><size>2489kb</size></version><title>A Multi-Scale Mapping Approach Based on a Deep Learning CNN Model for\\n  Reconstructing High-Resolution Urban DEMs</title><authors>Ling Jiang, Yang Hu, Xilin Xia, Qiuhua Liang, Andrea Soltoggio</authors><categories>cs.CV cs.LG eess.IV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The shortage of high-resolution urban digital elevation model (DEM) datasets\\nhas been a challenge for modelling urban flood and managing its risk. A\\nsolution is to develop effective approaches to reconstruct high-resolution DEMs\\nfrom their low-resolution equivalents that are more widely available. However,\\nthe current high-resolution DEM reconstruction approaches mainly focus on\\nnatural topography. Few attempts have been made for urban topography which is\\ntypically an integration of complex man-made and natural features. This study\\nproposes a novel multi-scale mapping approach based on convolutional neural\\nnetwork (CNN) to deal with the complex characteristics of urban topography and\\nreconstruct high-resolution urban DEMs. The proposed multi-scale CNN model is\\nfirstly trained using urban DEMs that contain topographic features at different\\nresolutions, and then used to reconstruct the urban DEM at a specified (high)\\nresolution from a low-resolution equivalent. A two-level accuracy assessment\\napproach is also designed to evaluate the performance of the proposed urban DEM\\nreconstruction method, in terms of numerical accuracy and morphological\\naccuracy. The proposed DEM reconstruction approach is applied to a 121 km2\\nurbanized area in London, UK. Compared with other commonly used methods, the\\ncurrent CNN based approach produces superior results, providing a\\ncost-effective innovative method to acquire high-resolution DEMs in other\\ndata-scarce environments.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.12945</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.12945</id><submitter>Tao Sun</submitter><version version=\"v1\"><date>Sat, 27 Jul 2019 02:50:45 GMT</date><size>3050kb</size><source_type>D</source_type></version><title>Inertial nonconvex alternating minimizations for the image deblurring</title><authors>Tao Sun, Roberto Barrio, Marcos Rodriguez, Hao Jiang</authors><categories>math.OC cs.CV cs.MM eess.IV</categories><comments>Transactions on Image Processing</comments><doi>10.1109/TIP.2019.2924339</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In image processing, Total Variation (TV) regularization models are commonly\\nused to recover blurred images. One of the most efficient and popular methods\\nto solve the convex TV problem is the Alternating Direction Method of\\nMultipliers (ADMM) algorithm, recently extended using the inertial proximal\\npoint method. Although all the classical studies focus on only a convex\\nformulation, recent articles are paying increasing attention to the nonconvex\\nmethodology due to its good numerical performance and properties. In this\\npaper, we propose to extend the classical formulation with a novel nonconvex\\nAlternating Direction Method of Multipliers with the Inertial technique\\n(IADMM). Under certain assumptions on the parameters, we prove the convergence\\nof the algorithm with the help of the Kurdyka-{\\\\L}ojasiewicz property. We also\\npresent numerical simulations on classical TV image reconstruction problems to\\nillustrate the efficiency of the new algorithm and its behavior compared with\\nthe well established ADMM method.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.13030</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.13030</id><submitter>Mathieu Dugr\\\\\\'e</submitter><version version=\"v1\"><date>Tue, 30 Jul 2019 15:42:32 GMT</date><size>2828kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 31 Jul 2019 19:38:32 GMT</date><size>2828kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sat, 5 Oct 2019 20:24:36 GMT</date><size>2837kb</size><source_type>D</source_type></version><title>A performance comparison of Dask and Apache Spark for data-intensive\\n  neuroimaging pipelines</title><authors>Mathieu Dugr\\\\\\'e, Val\\\\\\'erie Hayot-Sasson, Tristan Glatard</authors><categories>cs.DC cs.PF</categories><comments>10 pages, 15 figures, 1 tables. To appear in the proceeding of the\\n  14th WORKS Workshop on Topics in Workflows in Support of Large-Scale Science,\\n  17 November 2019, Denver, CO, USA</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In the past few years, neuroimaging has entered the Big Data era due to the\\njoint increase in image resolution, data sharing, and study sizes. However, no\\nparticular Big Data engines have emerged in this field, and several\\nalternatives remain available. We compare two popular Big Data engines with\\nPython APIs, Apache Spark and Dask, for their runtime performance in processing\\nneuroimaging pipelines. Our evaluation uses two synthetic pipelines processing\\nthe 81GB BigBrain image, and a real pipeline processing anatomical data from\\nmore than 1,000 subjects. We benchmark these pipelines using various\\ncombinations of task durations, data sizes, and numbers of workers, deployed on\\nan 8-node (8 cores ea.) compute cluster in Compute Canada\\'s Arbutus cloud. We\\nevaluate PySpark\\'s RDD API against Dask\\'s Bag, Delayed and Futures. Results\\nshow that despite slight differences between Spark and Dask, both engines\\nperform comparably. However, Dask pipelines risk being limited by Python\\'s GIL\\ndepending on task type and cluster configuration. In all cases, the major\\nlimiting factor was data transfer. While either engine is suitable for\\nneuroimaging pipelines, more effort needs to be placed in reducing data\\ntransfer time.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.13052</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.13052</id><submitter>Martin Engelcke</submitter><version version=\"v1\"><date>Tue, 30 Jul 2019 16:22:39 GMT</date><size>505kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 20:19:08 GMT</date><size>1905kb</size><source_type>D</source_type></version><title>GENESIS: Generative Scene Inference and Sampling with Object-Centric\\n  Latent Representations</title><authors>Martin Engelcke, Adam R. Kosiorek, Oiwi Parker Jones, Ingmar Posner</authors><categories>cs.LG cs.CV cs.NE cs.RO stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative latent-variable models are emerging as promising tools in robotics\\nand reinforcement learning. Yet, even though tasks in these domains typically\\ninvolve distinct objects, most state-of-the-art generative models do not\\nexplicitly capture the compositional nature of visual scenes. Two recent\\nexceptions, MONet and IODINE, decompose scenes into objects in an unsupervised\\nfashion. Their underlying generative processes, however, do not account for\\ncomponent interactions. Hence, neither of them allows for principled sampling\\nof novel scenes. Here we present GENESIS, the first object-centric generative\\nmodel of 3D visual scenes capable of both decomposing and generating scenes by\\ncapturing relationships between scene components. GENESIS parameterises a\\nspatial GMM over images which is decoded from a set of object-centric latent\\nvariables that are either inferred sequentially in an amortised fashion or\\nsampled from an autoregressive prior. We train GENESIS on several publicly\\navailable datasets and evaluate its performance on scene generation,\\ndecomposition, and semi-supervised learning.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.13095</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.13095</id><submitter>Fabio Sanchez PhD</submitter><version version=\"v1\"><date>Tue, 30 Jul 2019 17:27:39 GMT</date><size>415kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 15:28:37 GMT</date><size>416kb</size><source_type>D</source_type></version><title>Climate-driven statistical models as effective predictors of local\\n  dengue incidence in Costa Rica: A Generalized Additive Model and Random\\n  Forest approach</title><authors>Paola V\\\\\\'asquez, Antonio Lor\\\\\\'ia, Fabio Sanchez, Luis A. Barboza</authors><categories>stat.ML cs.LG q-bio.PE q-bio.QM</categories><comments>17 pages, 3 figures</comments><msc-class>62J02, 62M20, 92D30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Climate has been an important factor in shaping the distribution and\\nincidence of dengue cases in tropical and subtropical countries. In Costa Rica,\\na tropical country with distinctive micro-climates, dengue has been endemic\\nsince its introduction in 1993, inflicting substantial economic, social, and\\npublic health repercussions. Using the number of dengue reported cases and\\nclimate data from 2007-2017, we fitted a prediction model applying a\\nGeneralized Additive Model (GAM) and Random Forest (RF) approach, which allowed\\nus to retrospectively predict the relative risk of dengue in five\\nclimatological diverse municipalities around the country.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.13115</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.13115</id><submitter>Tom\\\\\\'a\\\\v{s} Masopust</submitter><version version=\"v1\"><date>Mon, 29 Jul 2019 10:50:26 GMT</date><size>519kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 18:03:19 GMT</date><size>411kb</size><source_type>D</source_type></version><title>Partially Ordered Automata and Piecewise Testability</title><authors>Tom\\\\\\'a\\\\v{s} Masopust and Markus Kr\\\\&quot;otzsch</authors><categories>cs.LO cs.FL</categories><comments>This paper is a revised and full version of our recent work partially\\n  presented at MFCS 2016 (10.4230/LIPIcs.MFCS.2016.67) and SOFSEM 2018\\n  (preprint arxiv:1704.07860). The PSpace-hardness proof of deciding\\n  universality for ptNFAs is a nontrivial extension of our proof for rpoNFAs\\n  (preprint arxiv:1609.03460)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partially ordered automata are automata where the transition relation induces\\na partial order on states. The expressive power of partially ordered automata\\nis closely related to the expressivity of fragments of first-order logic on\\nwords or, equivalently, to the language classes of the levels of the\\nStraubing-Th\\\\\\'{e}rien hierarchy. Several fragments (levels) have been\\nintensively investigated and possess several names. For instance, the fragment\\nof first-order formulae with a single existential block of quantifiers in the\\nprenex normal form is known as piecewise testable languages or J-trivial\\nlanguages. These languages are characterized by confluent partially ordered\\nDFAs or by complete, confluent, and self-loop-deterministic partially ordered\\nNFAs (ptNFAs for short). In this paper, we study the complexity of basic\\nquestions for several types of partially ordered automata; namely, the\\nquestions of inclusion, equivalence, and (k-)piecewise testability. The\\nlower-bound complexity boils down to the complexity of universality. The\\nuniversality problem asks whether a system recognizes all words over its\\nalphabet. For ptNFAs, the complexity of universality decreases if the alphabet\\nis fixed, but it is open if the alphabet may grow with the number of states. We\\nshow that deciding universality for general ptNFAs is as hard as for general\\nNFAs. Our proof is a novel and nontrivial extension of our recent construction\\nfor self-loop-deterministic partially ordered NFAs, a model strictly more\\nexpressive than ptNFAs. We provide the whole complexity picture for considered\\npartially ordered automata and for the problems of inclusion, equivalence, and\\n(k-)piecewise testability.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.13314</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.13314</id><submitter>Mosab Khayat</submitter><version version=\"v1\"><date>Wed, 31 Jul 2019 05:47:20 GMT</date><size>5209kb</size></version><version version=\"v2\"><date>Mon, 7 Oct 2019 23:52:59 GMT</date><size>5209kb</size></version><title>The Validity, Generalizability and Feasibility of Summative Evaluation\\n  Methods in Visual Analytics</title><authors>Mosab Khayat, Morteza Karimzadeh, David S. Ebert, Arif Ghafoor</authors><categories>cs.HC</categories><comments>IEEE VIS (VAST) 2019</comments><doi>10.1109/TVCG.2019.2934264</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many evaluation methods have been used to assess the usefulness of Visual\\nAnalytics (VA) solutions. These methods stem from a variety of origins with\\ndifferent assumptions and goals, which cause confusion about their proofing\\ncapabilities. Moreover, the lack of discussion about the evaluation processes\\nmay limit our potential to develop new evaluation methods specialized for VA.\\nIn this paper, we present an analysis of evaluation methods that have been used\\nto summatively evaluate VA solutions. We provide a survey and taxonomy of the\\nevaluation methods that have appeared in the VAST literature in the past two\\nyears. We then analyze these methods in terms of validity and generalizability\\nof their findings, as well as the feasibility of using them. We propose a new\\nmetric called summative quality to compare evaluation methods according to\\ntheir ability to prove usefulness, and make recommendations for selecting\\nevaluation methods based on their summative quality in the VA domain.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.13319</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.13319</id><submitter>Mosab Khayat</submitter><version version=\"v1\"><date>Wed, 31 Jul 2019 06:05:55 GMT</date><size>2506kb</size></version><version version=\"v2\"><date>Mon, 7 Oct 2019 23:53:55 GMT</date><size>2507kb</size></version><title>VASSL: A Visual Analytics Toolkit for Social Spambot Labeling</title><authors>Mosab Khayat, Morteza Karimzadeh, Jieqiong Zhao, David S. Ebert</authors><categories>cs.HC cs.SI</categories><comments>IEEE VIS (VAST) 2019</comments><doi>10.1109/TVCG.2019.2934266</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media platforms such as Twitter are filled with social spambots.\\nDetecting these malicious accounts is essential, yet challenging, as they\\ncontinually evolve and evade traditional detection techniques. In this work, we\\npropose VASSL, a visual analytics system that assists in the process of\\ndetecting and labeling spambots. Our tool enhances the performance and\\nscalability of manual labeling by providing multiple connected views and\\nutilizing dimensionality reduction, sentiment analysis and topic modeling\\ntechniques, which offer new insights that enable the identification of\\nspambots. The system allows users to select and analyze groups of accounts in\\nan interactive manner, which enables the detection of spambots that may not be\\nidentified when examined individually. We conducted a user study to objectively\\nevaluate the performance of VASSL users, as well as capturing subjective\\nopinions about the usefulness and the ease of use of the tool.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.13324</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.13324</id><submitter>David Sarrut</submitter><version version=\"v1\"><date>Wed, 31 Jul 2019 06:27:04 GMT</date><size>605kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 10:58:13 GMT</date><size>5621kb</size><source_type>D</source_type></version><title>Generative Adversarial Networks (GAN) for compact beam source modelling\\n  in Monte Carlo simulations</title><authors>David Sarrut and Nils Krah and Jean-Michel L\\\\\\'etang</authors><categories>physics.med-ph cs.LG</categories><comments>Phys Med Biol (Accepted Manuscript online 30 August 2019)</comments><doi>10.1088/1361-6560/ab3fc1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method is proposed and evaluated to model large and inconvenient phase\\nspace files used in Monte Carlo simulations by a compact Generative Adversarial\\nNetwork (GAN). The GAN is trained based on a phase space dataset to create a\\nneural network, called Generator (G), allowing G to mimic the multidimensional\\ndata distribution of the phase space. At the end of the training process, G is\\nstored with about 0.5 million weights, around 10 MB, instead of few GB of the\\ninitial file. Particles are then generated with G to replace the phase space\\ndataset.\\n  This concept is applied to beam models from linear accelerators (linacs) and\\nfrom brachytherapy seed models. Simulations using particles from the reference\\nphase space on one hand and those generated by the GAN on the other hand were\\ncompared. 3D distributions of deposited energy obtained from source\\ndistributions generated by the GAN were close to the reference ones, with less\\nthan 1% of voxel-by-voxel relative difference. Sharp parts such as the\\nbrachytherapy emission lines in the energy spectra were not perfectly modeled\\nby the GAN. Detailed statistical properties and limitations of the\\nGAN-generated particles still require further investigation, but the proposed\\nexploratory approach is already promising and paves the way for a wide range of\\napplications.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.13538</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.13538</id><submitter>Zhutian Chen</submitter><version version=\"v1\"><date>Wed, 31 Jul 2019 14:51:53 GMT</date><size>5114kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 11:45:48 GMT</date><size>6784kb</size><source_type>D</source_type></version><title>LassoNet: Deep Lasso-Selection of 3D Point Clouds</title><authors>Zhutian Chen and Wei Zeng and Zhiguang Yang and Lingyun Yu and\\n  Chi-Wing Fu and Huamin Qu</authors><categories>cs.HC cs.GR</categories><comments>10 pages</comments><journal-ref>TVCG2019</journal-ref><doi>10.1109/TVCG.2019.2934332</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Selection is a fundamental task in exploratory analysis and visualization of\\n3D point clouds. Prior researches on selection methods were developed mainly\\nbased on heuristics such as local point density, thus limiting their\\napplicability in general data. Specific challenges root in the great\\nvariabilities implied by point clouds (e.g., dense vs. sparse), viewpoint\\n(e.g., occluded vs. non-occluded), and lasso (e.g., small vs. large). In this\\nwork, we introduce LassoNet, a new deep neural network for lasso selection of\\n3D point clouds, attempting to learn a latent mapping from viewpoint and lasso\\nto point cloud regions. To achieve this, we couple user-target points with\\nviewpoint and lasso information through 3D coordinate transform and naive\\nselection, and improve the method scalability via an intention filtering and\\nfarthest point sampling. A hierarchical network is trained using a dataset with\\nover 30K lasso-selection records on two different point cloud data. We conduct\\na formal user study to compare LassoNet with two state-of-the-art\\nlasso-selection methods. The evaluations confirm that our approach improves the\\nselection effectiveness and efficiency across different combinations of 3D\\npoint clouds, viewpoints, and lasso selections. Project Website:\\nhttps://lassonet.github.io\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.13550</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.13550</id><submitter>Zhutian Chen</submitter><version version=\"v1\"><date>Wed, 31 Jul 2019 15:20:33 GMT</date><size>8784kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 12:12:49 GMT</date><size>8792kb</size><source_type>D</source_type></version><title>Towards Automated Infographic Design: Deep Learning-based\\n  Auto-Extraction of Extensible Timeline</title><authors>Zhutian Chen, Yun Wang, Qianwen Wang, Yong Wang, and Huamin Qu</authors><categories>cs.HC cs.GR</categories><comments>10 pages, Automated Infographic Design, Deep Learning-based Approach,\\n  Timeline Infographics, Multi-task Model</comments><journal-ref>TVCG2019</journal-ref><doi>10.1109/TVCG.2019.2934810</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designers need to consider not only perceptual effectiveness but also visual\\nstyles when creating an infographic. This process can be difficult and time\\nconsuming for professional designers, not to mention non-expert users, leading\\nto the demand for automated infographics design. As a first step, we focus on\\ntimeline infographics, which have been widely used for centuries. We contribute\\nan end-to-end approach that automatically extracts an extensible timeline\\ntemplate from a bitmap image. Our approach adopts a deconstruction and\\nreconstruction paradigm. At the deconstruction stage, we propose a multi-task\\ndeep neural network that simultaneously parses two kinds of information from a\\nbitmap timeline: 1) the global information, i.e., the representation, scale,\\nlayout, and orientation of the timeline, and 2) the local information, i.e.,\\nthe location, category, and pixels of each visual element on the timeline. At\\nthe reconstruction stage, we propose a pipeline with three techniques, i.e.,\\nNon-Maximum Merging, Redundancy Recover, and DL GrabCut, to extract an\\nextensible template from the infographic, by utilizing the deconstruction\\nresults. To evaluate the effectiveness of our approach, we synthesize a\\ntimeline dataset (4296 images) and collect a real-world timeline dataset (393\\nimages) from the Internet. We first report quantitative evaluation results of\\nour approach over the two datasets. Then, we present examples of automatically\\nextracted templates and timelines automatically generated based on these\\ntemplates to qualitatively demonstrate the performance. The results confirm\\nthat our approach can effectively extract extensible templates from real-world\\ntimeline infographics.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.13601</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.13601</id><submitter>Jieqiong Zhao</submitter><version version=\"v1\"><date>Wed, 31 Jul 2019 17:06:47 GMT</date><size>3043kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 1 Aug 2019 00:34:32 GMT</date><size>3043kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 7 Oct 2019 18:53:15 GMT</date><size>3241kb</size><source_type>D</source_type></version><title>MetricsVis: A Visual Analytics System for Evaluating Employee\\n  Performance in Public Safety Agencies</title><authors>Jieqiong Zhao, Morteza Karimzadeh, Luke S. Snyder, Chittayong\\n  Surakitbanharn, Zhenyu Cheryl Qian, David S. Ebert</authors><categories>cs.HC</categories><comments>To appear in 2019 IEEE Transactions on Visualization and Computer\\n  Graphics</comments><doi>10.1109/TVCG.2019.2934603</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluating employee performance in organizations with varying workloads and\\ntasks is challenging. Specifically, it is important to understand how\\nquantitative measurements of employee achievements relate to supervisor\\nexpectations, what the main drivers of good performance are, and how to combine\\nthese complex and flexible performance evaluation metrics into an accurate\\nportrayal of organizational performance in order to identify shortcomings and\\nimprove overall productivity. To facilitate this process, we summarize common\\norganizational performance analyses into four visual exploration task\\ncategories. Additionally, we develop MetricsVis, a visual analytics system\\ncomposed of multiple coordinated views to support the dynamic evaluation and\\ncomparison of individual, team, and organizational performance in public safety\\norganizations. MetricsVis provides four primary visual components to expedite\\nperformance evaluation: (1) a priority adjustment view to support direct\\nmanipulation on evaluation metrics; (2) a reorderable performance matrix to\\ndemonstrate the details of individual employees; (3) a group performance view\\nthat highlights aggregate performance and individual contributions for each\\ngroup; and (4) a projection view illustrating employees with similar\\nspecialties to facilitate shift assignments and training. We demonstrate the\\nusability of our framework with two case studies from medium-sized law\\nenforcement agencies and highlight its broader applicability to other domains.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1907.13627</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1907.13627</id><submitter>Yordan Hristov</submitter><version version=\"v1\"><date>Wed, 31 Jul 2019 17:52:25 GMT</date><size>1810kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 6 Oct 2019 14:53:22 GMT</date><size>1918kb</size><source_type>D</source_type></version><title>Disentangled Relational Representations for Explaining and Learning from\\n  Demonstration</title><authors>Yordan Hristov, Daniel Angelov, Michael Burke, Alex Lascarides,\\n  Subramanian Ramamoorthy</authors><categories>cs.RO cs.LG</categories><comments>15 pages, 12 figures, accepted at the Conference on Robot Learning\\n  (CoRL) 2019, Osaka, Japan</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning from demonstration is an effective method for human users to\\ninstruct desired robot behaviour. However, for most non-trivial tasks of\\npractical interest, efficient learning from demonstration depends crucially on\\ninductive bias in the chosen structure for rewards/costs and policies. We\\naddress the case where this inductive bias comes from an exchange with a human\\nuser. We propose a method in which a learning agent utilizes the information\\nbottleneck layer of a high-parameter variational neural model, with auxiliary\\nloss terms, in order to ground abstract concepts such as spatial relations. The\\nconcepts are referred to in natural language instructions and are manifested in\\nthe high-dimensional sensory input stream the agent receives from the world. We\\nevaluate the properties of the latent space of the learned model in a\\nphotorealistic synthetic environment and particularly focus on examining its\\nusability for downstream tasks. Additionally, through a series of controlled\\ntable-top manipulation experiments, we demonstrate that the learned manifold\\ncan be used to ground demonstrations as symbolic plans, which can then be\\nexecuted on a PR2 robot.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.00087</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.00087</id><submitter>Thilo Spinner</submitter><version version=\"v1\"><date>Mon, 29 Jul 2019 15:04:59 GMT</date><size>7440kb</size><source_type>AD</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 12:54:46 GMT</date><size>7442kb</size><source_type>AD</source_type></version><title>explAIner: A Visual Analytics Framework for Interactive and Explainable\\n  Machine Learning</title><authors>Thilo Spinner, Udo Schlegel, Hanna Sch\\\\&quot;afer and Mennatallah El-Assady</authors><categories>cs.HC cs.AI cs.LG</categories><comments>9 pages paper, 2 pages references, 5 pages supplementary material\\n  (ancillary files)</comments><journal-ref>IEEE Transactions on Visualization and Computer Graphics (2019)</journal-ref><doi>10.1109/TVCG.2019.2934629</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a framework for interactive and explainable machine learning that\\nenables users to (1) understand machine learning models; (2) diagnose model\\nlimitations using different explainable AI methods; as well as (3) refine and\\noptimize the models. Our framework combines an iterative XAI pipeline with\\neight global monitoring and steering mechanisms, including quality monitoring,\\nprovenance tracking, model comparison, and trust building. To operationalize\\nthe framework, we present explAIner, a visual analytics system for interactive\\nand explainable machine learning that instantiates all phases of the suggested\\npipeline within the commonly used TensorBoard environment. We performed a\\nuser-study with nine participants across different expertise levels to examine\\ntheir perception of our workflow and to collect suggestions to fill the gap\\nbetween our system and framework. The evaluation confirms that our tightly\\nintegrated system leads to an informed machine learning process while\\ndisclosing opportunities for further extensions.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.00093</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.00093</id><submitter>Jingmei Hu</submitter><version version=\"v1\"><date>Wed, 31 Jul 2019 20:50:04 GMT</date><size>158kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 5 Oct 2019 00:57:50 GMT</date><size>154kb</size><source_type>D</source_type></version><title>Aquarium: Cassiopea and Alewife Languages</title><authors>David A. Holland and Jingmei Hu and Ming Kawaguchi and Eric Lu and\\n  Stephen Chong and Margo I. Seltzer</authors><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This technical report describes two of the domain specific languages used in\\nthe Aquarium kernel code synthesis project. It presents the language cores in\\nterms of abstract syntax. Cassiopea is a machine description language for\\ndescribing the semantics of processor instruction sets. Alewife is a\\nspecification language that can be used to write machine independent\\nspecifications for assembly-level instruction blocks. An Alewife specification\\ncan be used to verify and synthesize code for any machine described in\\nCassiopea, given a machine-specific translation for abstractions used in the\\nspecification. This article does not include an introduction to either the\\nAquarium system or the use of the languages. In addition to this version of the\\narticle being a draft, the Aquarium project and the languages are work in\\nprogress. This article cannot currently be considered either final or complete.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.00220</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.00220</id><submitter>Laurent Lessard</submitter><version version=\"v1\"><date>Thu, 1 Aug 2019 05:48:42 GMT</date><size>6120kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 19:26:40 GMT</date><size>6120kb</size><source_type>D</source_type></version><title>Estimating Color-Concept Associations from Image Statistics</title><authors>Ragini Rathore, Zachary Leggon, Laurent Lessard, Karen B. Schloss</authors><categories>cs.HC</categories><comments>IEEE VIS InfoVis 2019 ACM 2012 CSS: 1) Human-centered computing,\\n  Human computer interaction (HCI), Empirical studies in HCI 2) Human-centered\\n  computing, Human computer interaction (HCI), HCI design and evaluation\\n  methods, Laboratory experiments 3) Human-centered computing, Visualization,\\n  Empirical studies in visualization</comments><acm-class>J.4</acm-class><doi>10.1109/TVCG.2019.2934536</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To interpret the meanings of colors in visualizations of categorical\\ninformation, people must determine how distinct colors correspond to different\\nconcepts. This process is easier when assignments between colors and concepts\\nin visualizations match people\\'s expectations, making color palettes\\nsemantically interpretable. Efforts have been underway to optimize color\\npalette design for semantic interpretablity, but this requires having good\\nestimates of human color-concept associations. Obtaining these data from humans\\nis costly, which motivates the need for automated methods. We developed and\\nevaluated a new method for automatically estimating color-concept associations\\nin a way that strongly correlates with human ratings. Building on prior studies\\nusing Google Images, our approach operates directly on Google Image search\\nresults without the need for humans in the loop. Specifically, we evaluated\\nseveral methods for extracting raw pixel content of the images in order to best\\nestimate color-concept associations obtained from human ratings. The most\\neffective method extracted colors using a combination of cylindrical sectors\\nand color categories in color space. We demonstrate that our approach can\\naccurately estimate average human color-concept associations for different\\nfruits using only a small set of images. The approach also generalizes\\nmoderately well to more complicated recycling-related concepts of objects that\\ncan appear in any color.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.00221</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.00221</id><submitter>I Ansary Sainul</submitter><version version=\"v1\"><date>Thu, 1 Aug 2019 05:51:57 GMT</date><size>1174kb</size></version><version version=\"v2\"><date>Mon, 30 Sep 2019 17:50:03 GMT</date><size>1184kb</size></version><title>Automatic pre-grasps generation for unknown 3D objects</title><authors>IA Sainul, Sankha Deb, AK Deb</authors><categories>cs.RO cs.CG</categories><comments>6 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the problem of automating the pre-grasps generation for novel\\n3d objects has been discussed. The objects represented as cloud of 3D points\\nare split into parts and organized in a tree structure, where parts are\\napproximated by simple box primitives. Applying grasping only on the individual\\nobject parts may miss a good grasp which involves a combination of parts. The\\nproblem has been addressed by traversing the decomposition tree and checking\\neach node of the tree for possible pre-grasps against a set of conditions.\\nFurther, a face mask has been introduced to encode the free and blocked faces\\nof the box primitives. Pre-grasps are generated only for the free faces.\\nFinally, the proposed method implemented on a set twenty-four household objects\\nand toys, where a grasp planner based on object slicing method has been used to\\ncompute the contact-level grasp plan.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.00375</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.00375</id><submitter>Dingquan Li</submitter><version version=\"v1\"><date>Thu, 1 Aug 2019 13:08:04 GMT</date><size>838kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 2 Aug 2019 07:16:53 GMT</date><size>838kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sat, 5 Oct 2019 14:31:25 GMT</date><size>838kb</size><source_type>D</source_type></version><title>Quality Assessment of In-the-Wild Videos</title><authors>Dingquan Li, Tingting Jiang and Ming Jiang</authors><categories>cs.MM cs.CV eess.IV</categories><comments>9 pages, 7 figures, 4 tables. ACM Multimedia 2019 camera ready. -&gt;\\n  Update alignment formatting of Table 1</comments><doi>10.1145/3343031.3351028</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quality assessment of in-the-wild videos is a challenging problem because of\\nthe absence of reference videos and shooting distortions. Knowledge of the\\nhuman visual system can help establish methods for objective quality assessment\\nof in-the-wild videos. In this work, we show two eminent effects of the human\\nvisual system, namely, content-dependency and temporal-memory effects, could be\\nused for this purpose. We propose an objective no-reference video quality\\nassessment method by integrating both effects into a deep neural network. For\\ncontent-dependency, we extract features from a pre-trained image classification\\nneural network for its inherent content-aware property. For temporal-memory\\neffects, long-term dependencies, especially the temporal hysteresis, are\\nintegrated into the network with a gated recurrent unit and a\\nsubjectively-inspired temporal pooling layer. To validate the performance of\\nour method, experiments are conducted on three publicly available in-the-wild\\nvideo quality assessment databases: KoNViD-1k, CVD2014, and LIVE-Qualcomm,\\nrespectively. Experimental results demonstrate that our proposed method\\noutperforms five state-of-the-art methods by a large margin, specifically,\\n12.39%, 15.71%, 15.45%, and 18.09% overall performance improvements over the\\nsecond-best method VBLIINDS, in terms of SROCC, KROCC, PLCC and RMSE,\\nrespectively. Moreover, the ablation study verifies the crucial role of both\\nthe content-aware features and the modeling of temporal-memory effects. The\\nPyTorch implementation of our method is released at\\nhttps://github.com/lidq92/VSFA.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.00403</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.00403</id><submitter>Yating Wei</submitter><version version=\"v1\"><date>Thu, 1 Aug 2019 13:53:36 GMT</date><size>2306kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 03:03:57 GMT</date><size>2306kb</size><source_type>D</source_type></version><title>Evaluating Perceptual Bias During Geometric Scaling of Scatterplots</title><authors>Yating Wei, Honghui Mei, Ying Zhao, Shuyue Zhou, Bingru Lin, Haojing\\n  Jiang and Wei Chen</authors><categories>cs.HC</categories><doi>10.1109/TVCG.2019.2934208</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scatterplots are frequently scaled to fit display areas in multi-view and\\nmulti-device data analysis environments. A common method used for scaling is to\\nenlarge or shrink the entire scatterplot together with the inside points\\nsynchronously and proportionally. This process is called geometric scaling.\\nHowever, geometric scaling of scatterplots may cause a perceptual bias, that\\nis, the perceived and physical values of visual features may be dissociated\\nwith respect to geometric scaling. For example, if a scatterplot is projected\\nfrom a laptop to a large projector screen, then observers may feel that the\\nscatterplot shown on the projector has fewer points than that viewed on the\\nlaptop. This paper presents an evaluation study on the perceptual bias of\\nvisual features in scatterplots caused by geometric scaling. The study focuses\\non three fundamental visual features (i.e., numerosity, correlation, and\\ncluster separation) and three hypotheses that are formulated on the basis of\\nour experience. We carefully design three controlled experiments by using\\nwell-prepared synthetic data and recruit participants to complete the\\nexperiments on the basis of their subjective experience. With a detailed\\nanalysis of the experimental results, we obtain a set of instructive findings.\\nFirst, geometric scaling causes a bias that has a linear relationship with the\\nscale ratio. Second, no significant difference exists between the biases\\nmeasured from normally and uniformly distributed scatterplots. Third, changing\\nthe point radius can correct the bias to a certain extent. These findings can\\nbe used to inspire the design decisions of scatterplots in various scenarios.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.00574</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.00574</id><submitter>Tamir Bendory</submitter><version version=\"v1\"><date>Thu, 1 Aug 2019 18:43:06 GMT</date><size>5292kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 21:19:59 GMT</date><size>5351kb</size><source_type>D</source_type></version><title>Single-particle cryo-electron microscopy: Mathematical theory,\\n  computational challenges, and opportunities</title><authors>Tamir Bendory, Alberto Bartesaghi, and Amit Singer</authors><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, an abundance of new molecular structures have been\\nelucidated using cryo-electron microscopy (cryo-EM), largely due to advances in\\nhardware technology and data processing techniques. Owing to these new exciting\\ndevelopments, cryo-EM was selected by Nature Methods as Method of the Year\\n2015, and the Nobel Prize in Chemistry 2017 was awarded to three pioneers in\\nthe field.\\n  The main goal of this article is to introduce the challenging and exciting\\ncomputational tasks involved in reconstructing 3-D molecular structures by\\ncryo-EM. Determining molecular structures requires a wide range of\\ncomputational tools in a variety of fields, including signal processing,\\nestimation and detection theory, high-dimensional statistics, convex and\\nnon-convex optimization, spectral algorithms, dimensionality reduction, and\\nmachine learning. The tools from these fields must be adapted to work under\\nexceptionally challenging conditions, including extreme noise levels, the\\npresence of missing data, and massively large datasets as large as several\\nTerabytes.\\n  In addition, we present two statistical models: multi-reference alignment and\\nmulti-target detection, that abstract away much of the intricacies of cryo-EM,\\nwhile retaining some of its essential features. Based on these abstractions, we\\ndiscuss some recent intriguing results in the mathematical theory of cryo-EM,\\nand delineate relations with group theory, invariant theory, and information\\ntheory.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.00681</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.00681</id><submitter>Bowen Yu</submitter><version version=\"v1\"><date>Fri, 2 Aug 2019 02:19:21 GMT</date><size>1375kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 6 Oct 2019 18:30:35 GMT</date><size>1375kb</size><source_type>D</source_type></version><title>FlowSense: A Natural Language Interface for Visual Data Exploration\\n  within a Dataflow System</title><authors>Bowen Yu, Claudio T. Silva</authors><categories>cs.HC cs.LG</categories><comments>Published in IEEE Transactions on Visualization and Computer Graphics</comments><doi>10.1109/TVCG.2019.2934668</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dataflow visualization systems enable flexible visual data exploration by\\nallowing the user to construct a dataflow diagram that composes query and\\nvisualization modules to specify system functionality. However learning\\ndataflow diagram usage presents overhead that often discourages the user. In\\nthis work we design FlowSense, a natural language interface for dataflow\\nvisualization systems that utilizes state-of-the-art natural language\\nprocessing techniques to assist dataflow diagram construction. FlowSense\\nemploys a semantic parser with special utterance tagging and special utterance\\nplaceholders to generalize to different datasets and dataflow diagrams. It\\nexplicitly presents recognized dataset and diagram special utterances to the\\nuser for dataflow context awareness. With FlowSense the user can expand and\\nadjust dataflow diagrams more conveniently via plain English. We apply\\nFlowSense to the VisFlow subset-flow visualization system to enhance its\\nusability. We evaluate FlowSense by one case study with domain experts on a\\nreal-world data analysis problem and a formal user study.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.00813</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.00813</id><submitter>Mario Kapl</submitter><version version=\"v1\"><date>Fri, 2 Aug 2019 11:45:42 GMT</date><size>3794kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 11:17:36 GMT</date><size>3796kb</size><source_type>D</source_type></version><title>Isogeometric collocation on planar multi-patch domains</title><authors>Mario Kapl and Vito Vitrih</authors><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an isogeometric framework based on collocation to construct a\\n$C^2$-smooth approximation of the solution of the Poisson\\'s equation over\\nplanar bilinearly parameterized multi-patch domains. The construction of the\\nused globally $C^2$-smooth discretization space for the partial differential\\nequation is simple and works uniformly for all possible multi-patch\\nconfigurations. The basis of the $C^2$-smooth space can be described as the\\nspan of three different types of locally supported functions corresponding to\\nthe single patches, edges and vertices of the multi-patch domain. For the\\nselection of the collocation points, which is important for the stability and\\nconvergence of the collocation problem, two different choices are numerically\\ninvestigated. The first approach employs the tensor-product Greville abscissae\\nas collocation points, and shows for the multi-patch case the same convergence\\nbehavior as for the one-patch case [2], which is suboptimal in particular for\\nodd spline degree. The second approach generalizes the concept of\\nsuperconvergent points from the one-patch case (cf. [1, 15, 32]) to the\\nmulti-patch case. Again, these points possess better convergence properties\\nthan Greville abscissae in case of odd spline degree.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.00914</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.00914</id><submitter>Selim Engin</submitter><version version=\"v1\"><date>Fri, 2 Aug 2019 15:35:29 GMT</date><size>577kb</size><source_type>D</source_type></version><title>Asynchronous Network Formation in Unknown Unbounded Environments</title><authors>Selim Engin and Volkan Isler</authors><categories>cs.RO</categories><doi>10.1109/ICRA.2019.8794031</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the Online Network Formation Problem (ONFP) for a\\nmobile multi-robot system. Consider a group of robots with a bounded\\ncommunication range operating in a large open area. One of the robots has a\\npiece of information which has to be propagated to all other robots. What\\nstrategy should the robots pursue to disseminate the information to the rest of\\nthe robots as quickly as possible? The initial locations of the robots are\\nunknown to each other, therefore the problem must be solved in an online\\nfashion.\\n  For this problem, we present an algorithm whose competitive ratio is $O(H\\n\\\\cdot \\\\max\\\\{M,\\\\sqrt{M H}\\\\})$ for arbitrary robot deployments, where $M$ is the\\nlargest edge length in the Euclidean minimum spanning tree on the initial robot\\nconfiguration and $H$ is the height of the tree. We also study the case when\\nthe robot initial positions are chosen uniformly at random and improve the\\nratio to $O(M)$. Finally, we present simulation results to validate the\\nperformance in larger scales and demonstrate our algorithm using three robots\\nin a field experiment.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.00963</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.00963</id><submitter>Mathukumalli Vidyasagar</submitter><version version=\"v1\"><date>Fri, 2 Aug 2019 17:32:56 GMT</date><size>36kb</size></version><version version=\"v2\"><date>Tue, 8 Oct 2019 16:56:17 GMT</date><size>23kb</size></version><title>Deterministic Completion of Rectangular Matrices Using Ramanujan\\n  Bigraphs -- I: Error Bounds and Exact Recovery</title><authors>Shantanu Prasad Burnwal and Mathukumalli Vidyasagar</authors><categories>stat.ML cs.LG</categories><comments>The original submission 1908.00963 has been split into two parts. The\\n  replacement submission is Part-1 of the revised version. Part-2 can also be\\n  found on arXiv</comments><msc-class>68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the matrix completion problem: Suppose $X \\\\in {\\\\mathbb\\nR}^{n_r \\\\times n_c}$ is unknown except for an upper bound $r$ on its rank. By\\nmeasuring a small number $m \\\\ll n_r n_c$ of the elements of $X$, is it possible\\nto recover $X$ exactly, or at least, to construct a reasonable approximation of\\n$X$? At present there are two approaches to choosing the sample set, namely\\nprobabilistic and deterministic. Probabilistic methods can guarantee the exact\\nrecovery of the unknown matrix, but only with high probability. At present\\nthere are very few deterministic methods, and they mostly apply only to square\\nmatrices. The focus in the present paper is on deterministic methods that work\\nfor rectangular as well as square matrices, and where possible, can guarantee\\nexact recovery of the unknown matrix. We achieve this by choosing the elements\\nto be sampled as the edge set of an asymmetric Ramanujan graph or Ramanujan\\nbigraph. For such a measurement matrix, we (i) derive bounds on the error\\nbetween a scaled version of the sampled matrix and unknown matrix; (ii) derive\\nbounds on the recovery error when max norm minimization is used, and (iii)\\npresent suitable conditions under which the unknown matrix can be recovered\\nexactly via nuclear norm minimization. In the process we streamline some\\nexisting proofs and improve upon them, and also make the results applicable to\\nrectangular matrices. This raises two questions: (i) How can Ramanujan bigraphs\\nbe constructed? (ii) How close are the sufficient conditions derived in this\\npaper to being necessary? Both questions are studied in a companion paper.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.01441</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.01441</id><submitter>Kazuo Misue</submitter><version version=\"v1\"><date>Mon, 5 Aug 2019 01:56:23 GMT</date><size>614kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 6 Aug 2019 05:06:19 GMT</date><size>614kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 12 Aug 2019 12:16:00 GMT</date><size>614kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Thu, 22 Aug 2019 05:28:23 GMT</date><size>614kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Wed, 2 Oct 2019 02:04:42 GMT</date><size>614kb</size><source_type>D</source_type></version><title>Graph Drawing with Morphing Partial Edges</title><authors>Kazuo Misue and Katsuya Akasaka</authors><categories>cs.DS cs.CG</categories><comments>Appears in the Proceedings of the 27th International Symposium on\\n  Graph Drawing and Network Visualization (GD 2019)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A partial edge drawing (PED) of a graph is a variation of a node-link\\ndiagram. PED draws a link, which is a partial visual representation of an edge,\\nand reduces visual clutter of the node-link diagram. However, more time is\\nrequired to read a PED to infer undrawn parts. The authors propose a morphing\\nedge drawing (MED), which is a PED that changes with time. In MED, links morph\\nbetween partial and complete drawings; thus, a reduced load for estimation of\\nundrawn parts in a PED is expected. Herein, a formalization of MED is shown\\nbased on a formalization of PED. Then, requirements for the scheduling of\\nmorphing are specified. The requirements inhibit morphing from crossing and\\nshorten the overall time for morphing the edges. Moreover, an algorithm for a\\nscheduling method implemented by the authors is illustrated and the\\neffectiveness of PED from a reading time viewpoint is shown through an\\nexperimental evaluation.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.01526</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.01526</id><submitter>Alessandro Di Stefano</submitter><version version=\"v1\"><date>Mon, 5 Aug 2019 09:13:36 GMT</date><size>333kb</size></version><version version=\"v2\"><date>Thu, 3 Oct 2019 11:14:22 GMT</date><size>365kb</size></version><title>EdgeMORE: Improving Resource Allocation with Multiple Options from\\n  Tenants</title><authors>Andrea Araldo, Alessandro Di Stefano and Antonella Di Stefano</authors><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Under the paradigm of Edge Computing (EC), a Network Operator (NO) deploys\\ncomputational resources at the network edge and let third-party services run on\\ntop of them. Besides the clear advantages for Service Providers (SPs) and final\\nusers thanks to the vicinity of computation nodes, a NO aims to allocate edge\\nresources in order to increase its own utility, including bandwidth saving,\\noperational cost reduction, QoE for its user, etc. However, while the number of\\nthird-party services competing for edge resources is expected to dramatically\\ngrow, the resources deployed cannot increase accordingly due to physical\\nlimitations. Therefore, other strategies are needed to fully exploit the\\npotential of EC, despite its constrains.\\n  To this aim we propose to leverage \\\\emph{service adaptability}, a dimension\\nthat has mainly been neglected so far. Each service can adapt to the amount of\\nresources that the NO has allocated to it, balancing the fraction of service\\ncomputation performed locally at the edge and relying on remote servers, e.g.,\\nin the Cloud, for the rest. We propose \\\\emph{EdgeMORE}, a resource allocation\\nstrategy in which SPs declare the different configurations to which they are\\nable to adapt, specifying the resources needed and the expected utility. The NO\\nthen chooses the most convenient option per each SP, in order to maximize the\\ntotal utility.\\n  We formalize EdgeMORE as a Integer Linear Programming and we propose a\\npolynomial time heuristic. We show via simulation that, by letting SPs declare\\ntheir ability to adapt to different constrained configurations and letting the\\nNO choose between them, EdgeMORE greatly improves EC utility and resource\\nutilization.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.01533</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.01533</id><submitter>Dante Kalise</submitter><version version=\"v1\"><date>Mon, 5 Aug 2019 09:38:49 GMT</date><size>227kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 05:54:54 GMT</date><size>1208kb</size><source_type>D</source_type></version><title>Tensor Decomposition for High-Dimensional Hamilton-Jacobi-Bellman\\n  Equations</title><authors>Sergey Dolgov and Dante Kalise and Karl Kunisch</authors><categories>math.OC cs.NA math.NA</categories><msc-class>49J20, 49LXX, 49MXX, 15A69, 15A23, 65F10, 65N22, 65D15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A tensor decomposition approach for the solution of high-dimensional, fully\\nnonlinear Hamilton-Jacobi-Bellman equations arising in optimal feedback control\\nof nonlinear dynamics is presented. The method combines a tensor train\\napproximation for the value function together with a Newton-like iterative\\nmethod for the solution of the resulting nonlinear system. The tensor\\napproximation leads to a polynomial scaling with respect to the dimension,\\npartially circumventing the curse of dimensionality. An analysis of the\\nlinear-quadratic case is presented. For nonlinear dynamics, the effectiveness\\nof the high-dimensional control synthesis method is assessed in the optimal\\nfeedback stabilization of the Allen-Cahn and Fokker-Planck equations.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.01667</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.01667</id><submitter>Chris Finlay</submitter><version version=\"v1\"><date>Mon, 5 Aug 2019 14:57:01 GMT</date><size>1850kb</size></version><version version=\"v2\"><date>Tue, 8 Oct 2019 17:21:21 GMT</date><size>2799kb</size></version><title>A principled approach for generating adversarial images under non-smooth\\n  dissimilarity metrics</title><authors>Aram-Alexandre Pooladian, Chris Finlay, Tim Hoheisel, Adam Oberman</authors><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks perform well on real world data but are prone to\\nadversarial perturbations: small changes in the input easily lead to\\nmisclassification. In this work, we propose an attack methodology not only for\\ncases where the perturbations are measured by $\\\\ell_p$ norms, but in fact any\\nadversarial dissimilarity metric with a closed proximal form. This includes,\\nbut is not limited to, $\\\\ell_1, \\\\ell_2$, and $\\\\ell_\\\\infty$ perturbations; the\\n$\\\\ell_0$ counting &quot;norm&quot; (i.e. true sparseness); and the total variation\\nseminorm, which is a (non-$\\\\ell_p$) convolutional dissimilarity measuring local\\npixel changes. Our approach is a natural extension of a recent adversarial\\nattack method, and eliminates the differentiability requirement of the metric.\\nWe demonstrate our algorithm, ProxLogBarrier, on the MNIST, CIFAR10, and\\nImageNet-1k datasets. We consider undefended and defended models, and show that\\nour algorithm easily transfers to various datasets. We observe that\\nProxLogBarrier outperforms a host of modern adversarial attacks specialized for\\nthe $\\\\ell_0$ case. Moreover, by altering images in the total variation\\nseminorm, we shed light on a new class of perturbations that exploit\\nneighboring pixel information.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.02182</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.02182</id><submitter>Fabian Isensee</submitter><version version=\"v1\"><date>Tue, 6 Aug 2019 14:28:17 GMT</date><size>249kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 11:03:40 GMT</date><size>287kb</size><source_type>D</source_type></version><title>An attempt at beating the 3D U-Net</title><authors>Fabian Isensee and Klaus H. Maier-Hein</authors><categories>eess.IV cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The U-Net is arguably the most successful segmentation architecture in the\\nmedical domain. Here we apply a 3D U-Net to the 2019 Kidney and Kidney Tumor\\nSegmentation Challenge and attempt to improve upon it by augmenting it with\\nresidual and pre-activation residual blocks. Cross-validation results on the\\ntraining cases suggest only very minor, barely measurable improvements. Due to\\nmarginally higher dice scores, the residual 3D U-Net is chosen for test set\\nprediction. With a Composite Dice score of 91.23 on the test set, our method\\noutperformed all 105 competing teams and won the KiTS2019 challenge by a small\\nmargin.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.02415</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.02415</id><submitter>Amir Behrouzi-Far</submitter><version version=\"v1\"><date>Wed, 7 Aug 2019 01:42:14 GMT</date><size>533kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 19:11:51 GMT</date><size>531kb</size><source_type>D</source_type></version><title>Redundancy Scheduling in Systems with Bi-Modal Job Service Time\\n  Distribution</title><authors>Amir Behrouzi-Far, Emina Soljanin</authors><categories>cs.PF cs.DC cs.IT math.IT</categories><comments>Presented at Allerton 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Queuing systems with redundant requests have drawn great attention because of\\ntheir promise to reduce the job completion time and variability. Despite a\\nlarge body of work on the topic, we are still far from fully understanding the\\nbenefits of redundancy in practice. We here take one step towards practical\\nsystems by studying queuing systems with bi-modal job service time\\ndistribution. Such distributions have been observed in practice, as can be seen\\nin, e.g., Google cluster traces. We develop an analogy to a classical urns and\\nballs problem, and use it to study the queuing time performance of two\\nnon-adaptive classical scheduling policies: random and round-robin. We\\nintroduce new performance indicators in the analogous model, and argue that\\nthey are good predictors of the queuing time in non-adaptive scheduling\\npolicies. We then propose a non-adaptive scheduling policy that is based on\\ncombinatorial designs, and show that it has better performance indicators.\\nSimulations confirm that the proposed scheduling policy, as the performance\\nindicators suggest, reduces the queuing times compared to random and\\nround-robin scheduling.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.02419</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.02419</id><submitter>Kenji Kawaguchi</submitter><version version=\"v1\"><date>Mon, 5 Aug 2019 20:19:39 GMT</date><size>228kb</size></version><version version=\"v2\"><date>Fri, 27 Sep 2019 18:27:14 GMT</date><size>237kb</size></version><title>Gradient Descent Finds Global Minima for Generalizable Deep Neural\\n  Networks of Practical Sizes</title><authors>Kenji Kawaguchi, Jiaoyang Huang</authors><categories>stat.ML cs.LG cs.NE math.OC</categories><comments>Accepted final version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we theoretically prove that gradient descent can find a global\\nminimum for nonlinear deep neural networks of sizes commonly encountered in\\npractice. The theory developed in this paper only requires the practical\\ndegrees of over-parameterization unlike previous theories. Our theory only\\nrequires the number of trainable parameters to increase linearly as the number\\nof training samples increases. This allows the size of the deep neural networks\\nto be consistent with practice and to be several orders of magnitude smaller\\nthan that required by the previous theories. Moreover, we prove that the linear\\nincrease of the size of the network is the optimal rate and that it cannot be\\nimproved, except by a logarithmic factor. Furthermore, deep neural networks\\nwith the trainability guarantee are shown to generalize well to unseen test\\nsamples with a natural dataset but not a random dataset.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.02436</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.02436</id><submitter>Megha Nawhal</submitter><version version=\"v1\"><date>Wed, 7 Aug 2019 04:24:48 GMT</date><size>3905kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 04:34:55 GMT</date><size>8066kb</size><source_type>D</source_type></version><title>Continuous Graph Flow</title><authors>Zhiwei Deng, Megha Nawhal, Lili Meng, Greg Mori</authors><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose Continuous Graph Flow, a generative continuous flow\\nbased method that aims to model complex distributions of graph-structured data.\\nOnce learned, the model can be applied to an arbitrary graph, defining a\\nprobability density over the random variables represented by the graph. It is\\nformulated as an ordinary differential equation system with shared and reusable\\nfunctions that operate over the graphs. This leads to a new type of neural\\ngraph message passing scheme that performs continuous message passing over\\ntime. This class of models offers several advantages: a flexible representation\\nthat can generalize to variable data dimensions; ability to model dependencies\\nin complex data distributions; reversible and memory-efficient; and exact and\\nefficient computation of the likelihood of the data. We demonstrate the\\neffectiveness of our model on a diverse set of generation tasks across\\ndifferent domains: graph generation, image puzzle generation, and layout\\ngeneration from scene graphs. Our proposed model achieves significantly better\\nperformance compared to state-of-the-art models.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.02588</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.02588</id><submitter>Luke Snyder</submitter><version version=\"v1\"><date>Thu, 1 Aug 2019 09:01:19 GMT</date><size>4348kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 19:11:52 GMT</date><size>3676kb</size><source_type>D</source_type></version><title>Interactive Learning for Identifying Relevant Tweets to Support\\n  Real-time Situational Awareness</title><authors>Luke S. Snyder, Yi-Shan Lin, Morteza Karimzadeh, Dan Goldwasser, and\\n  David S. Ebert</authors><categories>cs.SI cs.CL cs.HC cs.LG stat.ML</categories><comments>12 pages, 8 figures, 3 tables, IEEE VIS VAST 2019, TVCG</comments><doi>10.1109/TVCG.2019.2934614</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various domain users are increasingly leveraging real-time social media data\\nto gain rapid situational awareness. However, due to the high noise in the\\ndeluge of data, effectively determining semantically relevant information can\\nbe difficult, further complicated by the changing definition of relevancy by\\neach end user for different events. The majority of existing methods for short\\ntext relevance classification fail to incorporate users\\' knowledge into the\\nclassification process. Existing methods that incorporate interactive user\\nfeedback focus on historical datasets. Therefore, classifiers cannot be\\ninteractively retrained for specific events or user-dependent needs in\\nreal-time. This limits real-time situational awareness, as streaming data that\\nis incorrectly classified cannot be corrected immediately, permitting the\\npossibility for important incoming data to be incorrectly classified as well.\\nWe present a novel interactive learning framework to improve the classification\\nprocess in which the user iteratively corrects the relevancy of tweets in\\nreal-time to train the classification model on-the-fly for immediate predictive\\nimprovements. We computationally evaluate our classification model adapted to\\nlearn at interactive rates. Our results show that our approach outperforms\\nstate-of-the-art machine learning models. In addition, we integrate our\\nframework with the extended Social Media Analytics and Reporting Toolkit\\n(SMART) 2.0 system, allowing the use of our interactive learning framework\\nwithin a visual analytics system tailored for real-time situational awareness.\\nTo demonstrate our framework\\'s effectiveness, we provide domain expert feedback\\nfrom first responders who used the extended SMART 2.0 system.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.02648</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.02648</id><submitter>Seongmin Hwang</submitter><version version=\"v1\"><date>Wed, 7 Aug 2019 14:09:46 GMT</date><size>128kb</size></version><version version=\"v2\"><date>Mon, 7 Oct 2019 10:22:59 GMT</date><size>2562kb</size><source_type>D</source_type></version><title>Attention-aware Linear Depthwise Convolution for Single Image\\n  Super-Resolution</title><authors>Seongmin Hwang, Gwanghuyn Yu, Cheolkon Jung and Jinyoung Kim</authors><categories>eess.IV cs.CV</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work on super-resolution shows that a very deep convolutional neural\\nnetworks (CNN) have obtained remarkable performance. However, as CNN models\\nhave become deeper and wider, the required computational cost is substantially\\nhigher. Meanwhile, the features of intermediate layers are treated equally\\nacross the channel, hence hindering the representational capability of CNNs. In\\nthis paper, we propose Attention-aware Linear Depthwise Network (ALDNet) to\\naddress these problems for single image super-resolution task. Specifically,\\nLinear Depthwise Convolution allows CNN-based SR models to preserve useful\\ninformation used to reconstruct super-resolved image, while reducing\\ncomputational burden. Furthermore, we design an attention-aware module which\\nenhance the representational ability of depthwise convolution layers by making\\nfull use of depthwise filter interdependency. We evaluate the proposed approach\\nusing widely used benchmark datasets and experimental results show it performs\\nsuperior performance to traditional depthwise separable convolution.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.02734</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.02734</id><submitter>Digvijay Boob</submitter><version version=\"v1\"><date>Wed, 7 Aug 2019 17:10:52 GMT</date><size>66kb</size></version><version version=\"v2\"><date>Mon, 30 Sep 2019 17:59:22 GMT</date><size>77kb</size></version><version version=\"v3\"><date>Fri, 4 Oct 2019 16:17:43 GMT</date><size>80kb</size></version><title>Stochastic First-order Methods for Convex and Nonconvex Functional\\n  Constrained Optimization</title><authors>Digvijay Boob, Qi Deng, Guanghui Lan</authors><categories>math.OC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Functional constrained optimization is becoming more and more important in\\nmachine learning and operations research. Such problems have potential\\napplications in risk-averse machine learning, semisupervised learning and\\nrobust optimization among others. In this paper, we first present a novel\\nConstraint Extrapolation (ConEx) method for solving convex functional\\nconstrained problems, which utilizes linear approximations of the constraint\\nfunctions to define the extrapolation (or acceleration) step. We show that this\\nmethod is a unified algorithm that achieves the best-known rate of convergence\\nfor solving different functional constrained convex composite problems,\\nincluding convex or strongly convex, and smooth or nonsmooth problems with\\nstochastic objective and/or stochastic constraints. Many of these rates of\\nconvergence were in fact obtained for the first time in the literature. In\\naddition, ConEx is a single-loop algorithm that does not involve any penalty\\nsubproblems. Contrary to existing dual methods, it does not require the\\nprojection of Lagrangian multipliers into a (possibly unknown) bounded set.\\nSecond, for nonconvex functional constrained problem, we introduce a new\\nproximal point method which transforms the initial nonconvex problem into a\\nsequence of convex functional constrained subproblems. We establish the\\nconvergence and rate of convergence of this algorithm to KKT points under\\ndifferent constraint qualifications. For practical use, we present inexact\\nvariants of this algorithm, in which approximate solutions of the subproblems\\nare computed using the aforementioned ConEx method and establish their\\nassociated rate of convergence. To the best of our knowledge, most of these\\nconvergence and complexity results of the proximal point method for nonconvex\\nproblems also seem to be new in the literature.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.03406</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.03406</id><submitter>Wei Wang</submitter><version version=\"v1\"><date>Fri, 9 Aug 2019 10:50:04 GMT</date><size>202kb</size></version><title>Optimizing spreading dynamics in interconnected networks</title><authors>Liming Pan and Wei Wang and Shimin Cai and Tao Zhou</authors><categories>physics.soc-ph cs.SI</categories><comments>9 pages, 4 figures</comments><doi>10.1063/1.5090902</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Adding edges between layers of interconnected networks is an important way to\\noptimize the spreading dynamics. While previous studies mostly focus on the\\ncase of adding a single edge, the theoretical optimal strategy for adding\\nmultiple edges still need to be studied. In this study, based on the\\nsusceptible-infected-susceptible (SIS) model, we investigate the problem of\\nmaximizing the stationary spreading prevalence in interconnected networks. For\\ntwo isolated networks, we maximize the spreading prevalence near the critical\\npoint by choosing multiple interconnecting edges. We present a theoretical\\nanalysis based on the discrete-time Markov chain approach to derive the\\napproximate optimal strategy. The optimal inter-layer structure predicted by\\nthe strategy maximizes the spreading prevalence, meanwhile minimizes the\\nspreading outbreak threshold for the interconnected network simultaneously.\\nNumerical simulations on synthetic and real-world networks show that near the\\ncritical point, the proposed strategy gives better performance than connecting\\nlarge degree nodes and randomly connecting.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.03442</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.03442</id><submitter>Rafael Caba\\\\~nas</submitter><version version=\"v1\"><date>Fri, 9 Aug 2019 12:55:54 GMT</date><size>3784kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 20 Sep 2019 08:37:56 GMT</date><size>3784kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 2 Oct 2019 10:06:39 GMT</date><size>533kb</size><source_type>D</source_type></version><title>Probabilistic Models with Deep Neural Networks</title><authors>Andr\\\\\\'es R. Masegosa, Rafael Caba\\\\~nas, Helge Langseth, Thomas D.\\n  Nielsen, Antonio Salmer\\\\\\'on</authors><categories>cs.LG math.ST stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in statistical inference have significantly expanded the\\ntoolbox of probabilistic modeling. Historically, probabilistic modeling has\\nbeen constrained to (i) very restricted model classes where exact or\\napproximate probabilistic inference were feasible, and (ii) small or\\nmedium-sized data sets which fit within the main memory of the computer.\\nHowever, developments in variational inference, a general form of approximate\\nprobabilistic inference originated in statistical physics, are allowing\\nprobabilistic modeling to overcome these restrictions: (i) Approximate\\nprobabilistic inference is now possible over a broad class of probabilistic\\nmodels containing a large number of parameters, and (ii) scalable inference\\nmethods based on stochastic gradient descent and distributed computation\\nengines allow to apply probabilistic modeling over massive data sets. One\\nimportant practical consequence of these advances is the possibility to include\\ndeep neural networks within a probabilistic model to capture complex non-linear\\nstochastic relationships between random variables. These advances in\\nconjunction with the release of novel probabilistic modeling toolboxes have\\ngreatly expanded the scope of application of probabilistic models, and allow\\nthese models to take advantage of the recent strides made by the deep learning\\ncommunity. In this paper we review the main concepts, methods and tools needed\\nto use deep neural networks within a probabilistic modeling framework.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.03538</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.03538</id><submitter>Siyuan Feng</submitter><version version=\"v1\"><date>Fri, 9 Aug 2019 16:57:04 GMT</date><size>182kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 09:36:37 GMT</date><size>196kb</size><source_type>D</source_type></version><title>Exploiting Cross-Lingual Speaker and Phonetic Diversity for Unsupervised\\n  Subword Modeling</title><authors>Siyuan Feng, Tan Lee</authors><categories>eess.AS cs.CL</categories><comments>12 pages, 6 figures. Manuscript published in the IEEE/ACM\\n  Transactions on Audio, Speech and Language Processing (Volume: 27 , Issue: 12\\n  , Dec. 2019)</comments><doi>10.1109/TASLP.2019.2937953</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research addresses the problem of acoustic modeling of low-resource\\nlanguages for which transcribed training data is absent. The goal is to learn\\nrobust frame-level feature representations that can be used to identify and\\ndistinguish subword-level speech units. The proposed feature representations\\ncomprise various types of multilingual bottleneck features (BNFs) that are\\nobtained via multi-task learning of deep neural networks (MTL-DNN). One of the\\nkey problems is how to acquire high-quality frame labels for untranscribed\\ntraining data to facilitate supervised DNN training. It is shown that learning\\nof robust BNF representations can be achieved by effectively leveraging\\ntranscribed speech data and well-trained automatic speech recognition (ASR)\\nsystems from one or more out-of-domain (resource-rich) languages. Out-of-domain\\nASR systems can be applied to perform speaker adaptation with untranscribed\\ntraining data of the target language, and to decode the training speech into\\nframe-level labels for DNN training. It is also found that better frame labels\\ncan be generated by considering temporal dependency in speech when performing\\nframe clustering. The proposed methods of feature learning are evaluated on the\\nstandard task of unsupervised subword modeling in Track 1 of the ZeroSpeech\\n2017 Challenge. The best performance achieved by our system is $9.7\\\\%$ in terms\\nof across-speaker triphone minimal-pair ABX error rate, which is comparable to\\nthe best systems reported recently. Lastly, our investigation reveals that the\\ncloseness between target languages and out-of-domain languages and the amount\\nof available training data for individual target languages could have\\nsignificant impact on the goodness of learned features.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.03560</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.03560</id><submitter>Mohamed Akrout</submitter><version version=\"v1\"><date>Fri, 9 Aug 2019 17:59:35 GMT</date><size>1632kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 00:21:17 GMT</date><size>1632kb</size><source_type>D</source_type></version><title>On the Adversarial Robustness of Neural Networks without Weight\\n  Transport</title><authors>Mohamed Akrout</authors><categories>cs.LG cs.CR stat.ML</categories><comments>Accepted for the workshop on Real Neurons &amp; Hidden Units at NeurIPS\\n  2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural networks trained with backpropagation, the standard algorithm of deep\\nlearning which uses weight transport, are easily fooled by existing\\ngradient-based adversarial attacks. This class of attacks are based on certain\\nsmall perturbations of the inputs to make networks misclassify them. We show\\nthat less biologically implausible deep neural networks trained with feedback\\nalignment, which do not use weight transport, can be harder to fool, providing\\nactual robustness. Tested on MNIST, deep neural networks trained without weight\\ntransport (1) have an adversarial accuracy of 98% compared to 0.03% for neural\\nnetworks trained with backpropagation and (2) generate non-transferable\\nadversarial examples. However, this gap decreases on CIFAR-10 but is still\\nsignificant particularly for small perturbation magnitude less than 1/2.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.03630</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.03630</id><submitter>Loris Nanni</submitter><version version=\"v1\"><date>Fri, 9 Aug 2019 21:05:11 GMT</date><size>345kb</size></version><version version=\"v2\"><date>Mon, 23 Sep 2019 22:01:10 GMT</date><size>376kb</size></version><title>Learning morphological operators for skin detection</title><authors>Alessandra Lumini, Loris Nanni, Alice Codogno, Filippo Berno</authors><categories>cs.CV</categories><journal-ref>Journal of Artificial Intelligence and Systems (2019)</journal-ref><doi>10.33969/AIS.2019.11004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we propose a novel post processing approach for skin detectors\\nbased on trained morphological operators. The first step, consisting in skin\\nsegmentation is performed according to an existing skin detection approach is\\nperformed for skin segmentation, then a second step is carried out consisting\\nin the application of a set of morphological operators to refine the resulting\\nmask. Extensive experimental evaluation performed considering two different\\ndetection approaches (one based on deep learning and a handcrafted one) carried\\non 10 different datasets confirms the quality of the proposed method.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.03839</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.03839</id><submitter>Yang Zhao</submitter><version version=\"v1\"><date>Sun, 11 Aug 2019 02:33:38 GMT</date><size>7437kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 10:09:18 GMT</date><size>7334kb</size><source_type>D</source_type></version><title>MobileFAN: Transferring Deep Hidden Representation for Face Alignment</title><authors>Yang Zhao, Yifan Liu, Chunhua Shen, Yongsheng Gao, Shengwu Xiong</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Facial landmark detection is a crucial prerequisite for many face analysis\\napplications. Deep learning-based methods currently dominate the approach of\\naddressing the facial landmark detection. However, such works generally\\nintroduce a large number of parameters, resulting in high memory cost. In this\\npaper, we aim for lightweight as well as effective solutions to facial landmark\\ndetection. To this end, we propose an effective lightweight model, namely\\nMobile Face Alignment Network (MobileFAN), using a simple backbone MobileNetV2\\nas the encoder and three deconvolutional layers as the decoder. The proposed\\nMobileFAN, with only 8% of the model size and lower computational cost,\\nachieves superior or equivalent performance compared with state-of-the-art\\nmodels. Moreover, by transferring the geometric structural information of a\\nface graph from a large complex model to our proposed MobileFAN through\\nfeature-aligned distillation and feature-similarity distillation, the\\nperformance of MobileFAN is further improved in effectiveness and efficiency\\nfor face alignment. Extensive experiment results on three challenging facial\\nlandmark estimation benchmarks including COFW, 300W and WFLW show the\\nsuperiority of our proposed MobileFAN against state-of-the-art methods.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.03973</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.03973</id><submitter>Ping Lu</submitter><version version=\"v1\"><date>Sun, 11 Aug 2019 23:46:05 GMT</date><size>378kb</size></version><version version=\"v2\"><date>Sat, 5 Oct 2019 22:32:06 GMT</date><size>1416kb</size></version><title>Enhanced Seismic Imaging with Predictive Neural Networks for Geophysics</title><authors>Ping Lu, Yanyan Zhang, Jianxiong Chen, Yuan Xiao, George Zhao</authors><categories>eess.IV cs.LG physics.geo-ph stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a predictive neural network architecture that can be utilized to\\nupdate reference velocity models as inputs to the full waveform inversion. Deep\\nlearning models are explored to augment velocity model building workflows\\nduring processing the 3D seismic volume in salt-prone environments.\\nSpecifically, a neural network architecture, with 3D convolutional,\\nde-convolutional layers, and 3D max-pooling, is designed to take standard\\namplitude 3D seismic volumes as an input. Enhanced data augmentations through\\ngenerative adversarial networks and a weighted loss function enable the network\\nto train with few sparsely annotated slices. Batch normalization is also\\napplied for faster convergence. A 3D probability cube for salt bodies and\\ninclusions is generated through ensembles of predictions from multiple models\\nin order to reduce variance. Velocity models inferred from the proposed\\nnetworks provide opportunities for FWI forward models to converge faster with\\nan initial condition closer to the true model. In addition, in each iteration\\nstep, the probability cubes of salt bodies and inclusions inferred from the\\nproposed networks can be used as a regularization term within the FWI forward\\nmodelling, which may result in an improved velocity model estimation while the\\noutput of seismic migration can be utilized as an input of the 3D neural\\nnetwork for subsequent iterations.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.04211</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.04211</id><submitter>Damian Pascual</submitter><version version=\"v1\"><date>Mon, 12 Aug 2019 15:48:34 GMT</date><size>1159kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 13:04:40 GMT</date><size>438kb</size><source_type>D</source_type></version><title>On Identifiability in Transformers</title><authors>Gino Brunner, Yang Liu, Dami\\\\\\'an Pascual, Oliver Richter, Massimiliano\\n  Ciaramita, Roger Wattenhofer</authors><categories>cs.CL cs.LG</categories><comments>Preprint. Work in progress</comments><msc-class>46-04</msc-class><acm-class>I.2.7; I.7.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we contribute towards a deeper understanding of the Transformer\\narchitecture by investigating two of its core components: self-attention and\\ncontextual embeddings. In particular, we study the identifiability of attention\\nweights and token embeddings, and the aggregation of context into hidden\\ntokens. We show that attention weights are not unique and propose effective\\nattention as an alternative for better interpretability. Furthermore, we show\\nthat input tokens retain their identity in the first hidden layers and then\\nprogressively become less identifiable. We also provide evidence for the role\\nof non-linear activations in preserving token identity. Finally, we demonstrate\\nstrong mixing of input information in the generation of contextual embeddings\\nby means of a novel quantification method based on gradient attribution.\\nOverall, we show that self-attention distributions are not directly\\ninterpretable and present tools to further investigate Transformer models.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.04355</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.04355</id><submitter>Divyam Madaan</submitter><version version=\"v1\"><date>Mon, 12 Aug 2019 19:33:58 GMT</date><size>869kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 08:48:00 GMT</date><size>1418kb</size><source_type>D</source_type></version><title>Adversarial Neural Pruning</title><authors>Divyam Madaan, Jinwoo Shin, Sung Ju Hwang</authors><categories>cs.LG cs.CR cs.CV cs.NE stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  It is well known that neural networks are susceptible to adversarial\\nperturbations and are also computationally and memory intensive which makes it\\ndifficult to deploy them in real-world applications where security and\\ncomputation are constrained. In this work, we aim to obtain both robust and\\nsparse networks that are applicable to such scenarios, based on the intuition\\nthat latent features have a varying degree of susceptibility to adversarial\\nperturbations. Specifically, we define vulnerability at the latent feature\\nspace and then propose a Bayesian framework to prioritize features based on\\ntheir contribution to both the original and adversarial loss, to prune\\nvulnerable features and preserve the robust ones. Through quantitative\\nevaluation and qualitative analysis of the perturbation to latent features, we\\nshow that our sparsification method is a defense mechanism against adversarial\\nattacks and the robustness indeed comes from our model\\'s ability to prune\\nvulnerable latent features that are more susceptible to adversarial\\nperturbations.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.04385</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.04385</id><submitter>Zheng Liu</submitter><version version=\"v1\"><date>Tue, 6 Aug 2019 02:23:29 GMT</date><size>1721kb</size></version><version version=\"v2\"><date>Mon, 30 Sep 2019 03:17:40 GMT</date><size>1762kb</size></version><title>OD-GCN: Object Detection by Knowledge Graph with GCN</title><authors>Zheng Liu, Zidong Jiang, Wei Feng</authors><categories>cs.CV cs.AI cs.LG</categories><comments>6 pages</comments><msc-class>68T45</msc-class><acm-class>I.2.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical object detection methods only extract the objects\\' image features\\nvia CNN, lack of utilizing the relationship among objects in the same image. In\\nthis article, we introduce the graph convolutional networks (GCN) into the\\nobject detection field and propose a new framework called OD-GCN (object\\ndetection with graph convolutional network). It utilizes the category\\nrelationship to improve the detection precision. We set up a knowledge graph to\\nreflect the co-exist relationships among objects. GCN plays the role of\\npost-processing to adjust the output of base object detection models, so it is\\na flexible framework that any pre-trained object detection models can be used\\nas the base model. In experiments, we try several popular base detection\\nmodels. OD-GCN always improve mAP by 1-5pp on COCO dataset. In addition,\\nvisualized analysis reveals the benchmark improvement is quite reasonable in\\nhuman\\'s opinion.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.04411</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.04411</id><submitter>Afroza Shirin</submitter><version version=\"v1\"><date>Fri, 9 Aug 2019 06:00:37 GMT</date><size>1625kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 27 Sep 2019 19:13:40 GMT</date><size>2103kb</size><source_type>D</source_type></version><title>Stability Analysis of Reservoir Computers Dynamics via Lyapunov\\n  Functions</title><authors>Afroza Shirin, Isaac S. Klickstein, Francesco Sorrentino</authors><categories>eess.SY cs.SY nlin.AO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Lyapunov design method is used to analyze the nonlinear stability of a\\ngeneric reservoir computer for both the cases of continuous-time and\\ndiscrete-time dynamics. Using this method, for a given nonlinear reservoir\\ncomputer, a radial region of stability around a fixed point is analytically\\ndetermined. We see that the training error of the reservoir computer is lower\\nin the region where the analysis predicts global stability but is also affected\\nby the particular choice of the individual dynamics for the reservoir systems.\\nFor the case that the dynamics is polynomial, it appears to be important for\\nthe polynomial to have nonzero coefficients corresponding to at least one odd\\npower (e.g., linear term) and one even power (e.g., quadratic term).\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.04758</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.04758</id><submitter>Philippe Terrier PhD</submitter><version version=\"v1\"><date>Wed, 24 Jul 2019 09:49:57 GMT</date><size>966kb</size></version><version version=\"v2\"><date>Wed, 2 Oct 2019 13:47:44 GMT</date><size>992kb</size></version><title>Gait recognition via deep learning of the center-of-pressure trajectory</title><authors>Philippe Terrier</authors><categories>q-bio.QM cs.LG q-bio.NC stat.ML</categories><comments>Article submitted for publication. Updated (V2) during the revision\\n  process</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The fact that every human has a distinctive walking style has prompted a\\nproposal to use gait recognition as an identification criterion. Using\\nend-to-end learning, I investigated whether the center-of-pressure trajectory\\nis sufficiently unique to identify a person with a high certainty. Thirty-six\\nadults walked on a treadmill equipped with a force platform that recorded the\\npositions of the center of pressure. The raw two-dimensional signals were\\nsliced into segments of two gait cycles. A set of 20,250 segments from 30\\nsubjects was used to configure and train convolutional neural networks (CNNs).\\nThe best CNN classified a separate set containing 2,250 segments with 99.9%\\noverall accuracy. A second set of 4,500 segments from the six remaining\\nsubjects was then used for transfer learning. Several small subsamples of this\\nset were selected randomly and used for fine tuning. Training with two segments\\nper subject was sufficient to achieve 100% accuracy. The results suggest that\\nevery person produces a unique trajectory of underfoot pressures and that CNNs\\ncan learn the distinctive features of these trajectories. Using transfer\\nlearning, a few strides could be sufficient to learn and identify new gaits.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.04777</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.04777</id><submitter>Xusen Yin</submitter><version version=\"v1\"><date>Tue, 13 Aug 2019 17:48:10 GMT</date><size>202kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 05:13:21 GMT</date><size>256kb</size><source_type>D</source_type></version><title>Learn How to Cook a New Recipe in a New House: Using Map\\n  Familiarization, Curriculum Learning, and Bandit Feedback to Learn Families\\n  of Text-Based Adventure Games</title><authors>Xusen Yin and Jonathan May</authors><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the task of learning to play families of text-based computer\\nadventure games, i.e., fully textual environments with a common theme (e.g.\\ncooking) and goal (e.g. prepare a meal from a recipe) but with different\\nspecifics; new instances of such games are relatively straightforward for\\nhumans to master after a brief exposure to the genre but have been curiously\\ndifficult for computer agents to learn. We find that the deep Q-learning\\nstrategies that have been successfully leveraged for superhuman performance in\\nsingle-instance action video games can be applied to learn families of text\\nvideo games when adopting simple strategies that correlate with human-like\\nlearning behavior. Specifically, we build agents that learn to tackle simple\\nscenarios before more complex ones using curriculum learning, that familiarize\\nthemselves in an unfamiliar environment by navigating before acting, and that\\nexplore uncertain environment more thoroughly using multi-armed bandit decision\\npolicies. We demonstrate improved task completion rates over reasonable\\nbaselines when evaluating on never-before-seen games of that theme.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.05224</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.05224</id><submitter>Ofir Nachum</submitter><version version=\"v1\"><date>Tue, 13 Aug 2019 15:12:02 GMT</date><size>9723kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 21:26:34 GMT</date><size>9723kb</size><source_type>D</source_type></version><title>Multi-Agent Manipulation via Locomotion using Hierarchical Sim2Real</title><authors>Ofir Nachum, Michael Ahn, Hugo Ponte, Shixiang Gu, Vikash Kumar</authors><categories>cs.RO cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Manipulation and locomotion are closely related problems that are often\\nstudied in isolation. In this work, we study the problem of coordinating\\nmultiple mobile agents to exhibit manipulation behaviors using a reinforcement\\nlearning (RL) approach. Our method hinges on the use of hierarchical sim2real\\n-- a simulated environment is used to learn low-level goal-reaching skills,\\nwhich are then used as the action space for a high-level RL controller, also\\ntrained in simulation. The full hierarchical policy is then transferred to the\\nreal world in a zero-shot fashion. The application of domain randomization\\nduring training enables the learned behaviors to generalize to real-world\\nsettings, while the use of hierarchy provides a modular paradigm for learning\\nand transferring increasingly complex behaviors. We evaluate our method on a\\nnumber of real-world tasks, including coordinated object manipulation in a\\nmulti-agent setting. See videos at\\nhttps://sites.google.com/view/manipulation-via-locomotion\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.05445</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.05445</id><submitter>Pedro Matias</submitter><version version=\"v1\"><date>Thu, 15 Aug 2019 07:13:20 GMT</date><size>198kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 02:39:23 GMT</date><size>203kb</size><source_type>D</source_type></version><title>Tracking Paths in Planar Graphs</title><authors>David Eppstein, Michael T. Goodrich, James A. Liu, Pedro Matias</authors><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the NP-complete problem of tracking paths in a graph, first\\nintroduced by Banik et. al. [3]. Given an undirected graph with a source $s$\\nand a destination $t$, find the smallest subset of vertices whose intersection\\nwith any $s-t$ path results in a unique sequence. In this paper, we show that\\nthis problem remains NP-complete when the graph is planar and we give a\\n4-approximation algorithm in this setting. We also show, via Courcelle\\'s\\ntheorem, that it can be solved in linear time for graphs of bounded-clique\\nwidth, when its clique decomposition is given in advance.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.05570</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.05570</id><submitter>Pei Peng</submitter><version version=\"v1\"><date>Thu, 15 Aug 2019 14:58:49 GMT</date><size>960kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 02:39:41 GMT</date><size>2030kb</size><source_type>D</source_type></version><title>Straggling for Covert Message Passing on Complete Graphs</title><authors>Pei Peng, Nikolas Melissaris, Emina Soljanin, Bill Lee, Huafeng Fan,\\n  Anton Maliev</authors><categories>cs.CR</categories><comments>This paper is accepted by &quot;57th Annual Allerton Conference on\\n  Communication, Control, and Computing&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a model for mobile, multi-agent information transfer that\\nincreases the communication covertness through a protocol which also increases\\nthe information transfer delay. Covertness is achieved in the presence of a\\nwarden who has the ability to patrol the communication channels. Furthermore we\\nshow how two forms of redundancy can be used as an effective tool to control\\nthe tradeoff between the covertness and the delay.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.05584</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.05584</id><submitter>Li Yu</submitter><version version=\"v1\"><date>Thu, 15 Aug 2019 15:32:25 GMT</date><size>17kb</size></version><version version=\"v2\"><date>Thu, 5 Sep 2019 13:59:03 GMT</date><size>27kb</size></version><version version=\"v3\"><date>Thu, 3 Oct 2019 17:50:03 GMT</date><size>68kb</size><source_type>D</source_type></version><title>Quantum preprocessing for information-theoretic security in two-party\\n  computation</title><authors>Li Yu</authors><categories>quant-ph cs.CR</categories><comments>19 pages, 2 figures. Added Protocol 2, and revised Scheme 1, among\\n  other minor revisions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In classical two-party computation, a trusted initializer who prepares\\ncertain initial correlations, known as one-time tables, can help make the\\ninputs of both parties information-theoretically secure. We propose some\\nbipartite quantum protocols with possible aborts for approximately generating\\nsuch bipartite classical correlations with varying degrees of privacy, without\\nintroducing a third party. Under some weak requirements for the parties, the\\nsecurity level is nontrivial for use in bipartite computation. We show that the\\nsecurity is usually dependent on the noise level, but not for some party in one\\nof the protocols. The security is ``forced security\\'\\', which implies that the\\nprobability that some useful one-time tables are generated can approach 1 in\\nthe noiseless case under quite weak assumptions about the parties, although the\\nprotocols allow aborts. We show how to use the generated one-time tables to\\nachieve nontrivial information-theoretic security in generic two-party\\nclassical or quantum computation tasks, including (interactive) quantum\\nhomomorphic encryption. Our methods provide check-based implementations of some\\nno-signaling correlations, including the PR-box type, with the help of\\ncommunication which carry no information about the inputs in the generated\\ncorrelations.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.05660</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.05660</id><submitter>Abhishek Panigrahi</submitter><version version=\"v1\"><date>Fri, 16 Aug 2019 16:22:07 GMT</date><size>556kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 14:37:14 GMT</date><size>692kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 2 Oct 2019 09:39:17 GMT</date><size>690kb</size><source_type>D</source_type></version><title>Effect of Activation Functions on the Training of Overparametrized\\n  Neural Nets</title><authors>Abhishek Panigrahi, Abhishek Shetty and Navin Goyal</authors><categories>cs.LG stat.ML</categories><comments>Major update: Several new results, some reorganization and rewriting\\n  of previous results, new references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that overparametrized neural networks trained using\\ngradient-based methods quickly achieve small training error with appropriate\\nhyperparameter settings. Recent papers have proved this statement theoretically\\nfor highly overparametrized networks under reasonable assumptions. These\\nresults either assume that the activation function is ReLU or they crucially\\ndepend on the minimum eigenvalue of a certain Gram matrix depending on the\\ndata, random initialization and the activation function. In the later case,\\nexisting works only prove that this minimum eigenvalue is non-zero and do not\\nprovide quantitative bounds. On the empirical side, a contemporary line of\\ninvestigations has proposed a number of alternative activation functions which\\ntend to perform better than ReLU at least in some settings but no clear\\nunderstanding has emerged. This state of affairs underscores the importance of\\ntheoretically understanding the impact of activation functions on training. In\\nthe present paper, we provide theoretical results about the effect of\\nactivation function on the training of highly overparametrized 2-layer neural\\nnetworks. A crucial property that governs the performance of an activation is\\nwhether or not it is smooth. For non-smooth activations such as ReLU, SELU and\\nELU, all eigenvalues of the associated Gram matrix are large under minimal\\nassumptions on the data. For smooth activations such as tanh, swish and\\npolynomials, the situation is more complex. If the subspace spanned by the data\\nhas small dimension then the minimum eigenvalue of the Gram matrix can be small\\nleading to slow training. But if the dimension is large and the data satisfies\\nanother mild condition, then the eigenvalues are large. If we allow deep\\nnetworks, then the small data dimension is not a limitation provided that the\\ndepth is sufficient. We discuss a number of extensions and applications of\\nthese results.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.06039</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.06039</id><submitter>Yujia Bao</submitter><version version=\"v1\"><date>Fri, 16 Aug 2019 15:46:14 GMT</date><size>1459kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 19:14:03 GMT</date><size>1461kb</size><source_type>D</source_type></version><title>Few-shot Text Classification with Distributional Signatures</title><authors>Yujia Bao, Menghua Wu, Shiyu Chang, Regina Barzilay</authors><categories>cs.CL cs.LG</categories><comments>Work in progress</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explore meta-learning for few-shot text classification.\\nMeta-learning has shown strong performance in computer vision, where low-level\\npatterns are transferable across learning tasks. However, directly applying\\nthis approach to text is challenging-lexical features highly informative for\\none task maybe insignificant for another. Thus, rather than learning solely\\nfrom words, our model also leverages their distributional signatures, which\\nencode pertinent word occurrence patterns. Our model is trained within a\\nmeta-learning framework to map these signatures into attention scores, which\\nare then used to weight the lexical representations of words. We demonstrate\\nthat our model consistently outperforms prototypical networks learned on\\nlexical knowledge (Snell et al., 2017) in both few-shot text classification and\\nrelation classification by a significant margin across six benchmark datasets\\n(19.96% on average in 1-shot classification). Our code is available at\\nhttps://github.com/YujiaBao/Distributional-Signatures.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.06242</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.06242</id><submitter>Shinsaku Sakaue</submitter><version version=\"v1\"><date>Sat, 17 Aug 2019 04:50:57 GMT</date><size>1143kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 20 Aug 2019 01:43:30 GMT</date><size>1144kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 3 Oct 2019 03:13:54 GMT</date><size>1299kb</size><source_type>D</source_type></version><title>Guarantees of Stochastic Greedy Algorithms for Non-monotone Submodular\\n  Maximization with Cardinality Constraint</title><authors>Shinsaku Sakaue</authors><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Submodular maximization with a cardinality constraint can model various\\nproblems, and those problems are often very large in practice. For the case\\nwhere objective functions are monotone, many fast approximation algorithms have\\nbeen developed. The stochastic greedy algorithm (SG) is one such algorithm,\\nwhich is widely used thanks to its simplicity, efficiency, and high empirical\\nperformance. However, its approximation guarantee has been proved only for\\nmonotone objective functions. When it comes to non-monotone objective\\nfunctions, existing approximation algorithms are inefficient relative to the\\nfast algorithms developed for the case of monotone objectives. In this paper,\\nwe prove that SG (with slight modification) can achieve almost\\n$1/4$-approximation guarantees in expectation in linear time even for\\nnon-monotone objective functions. Our result provides a constant-factor\\napproximation algorithm with the fewest oracle queries for non-monotone\\nsubmodular maximization with a cardinality constraint. Experiments confirm that\\n(modified) SG can run far faster than and achieve as good objective values as\\nexisting algorithms.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.06663</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.06663</id><submitter>Chris Reinke</submitter><version version=\"v1\"><date>Mon, 19 Aug 2019 09:32:46 GMT</date><size>8742kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 16:40:53 GMT</date><size>8810kb</size><source_type>D</source_type></version><title>Intrinsically Motivated Discovery of Diverse Patterns in Self-Organizing\\n  Systems</title><authors>Chris Reinke, Mayalen Etcheverry, Pierre-Yves Oudeyer</authors><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many complex dynamical systems, artificial or natural, one can observe\\nself-organization of patterns emerging from local rules. Cellular automata,\\nlike the Game of Life (GOL), have been widely used as abstract models enabling\\nthe study of various aspects of self-organization and morphogenesis, such as\\nthe emergence of spatially localized patterns. However, findings of\\nself-organized patterns in such models have so far relied on manual tuning of\\nparameters and initial states, and on the human eye to identify interesting\\npatterns. In this paper, we formulate the problem of automated discovery of\\ndiverse self-organized patterns in such high-dimensional complex dynamical\\nsystems, as well as a framework for experimentation and evaluation. Using a\\ncontinuous GOL as a testbed, we show that recent intrinsically-motivated\\nmachine learning algorithms (POP-IMGEPs), initially developed for learning of\\ninverse models in robotics, can be transposed and used in this novel\\napplication area. These algorithms combine intrinsically-motivated goal\\nexploration and unsupervised learning of goal space representations. Goal space\\nrepresentations describe the interesting features of patterns for which diverse\\nvariations should be discovered. In particular, we compare various approaches\\nto define and learn goal space representations from the perspective of\\ndiscovering diverse spatially localized patterns. Moreover, we introduce an\\nextension of a state-of-the-art POP-IMGEP algorithm which incrementally learns\\na goal representation using a deep auto-encoder, and the use of CPPN primitives\\nfor generating initialization parameters. We show that it is more efficient\\nthan several baselines and equally efficient as a system pre-trained on a\\nhand-made database of patterns identified by human experts.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.06754</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.06754</id><submitter>Daniel Rivero</submitter><version version=\"v1\"><date>Fri, 16 Aug 2019 10:58:11 GMT</date><size>80kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 17 Sep 2019 07:57:31 GMT</date><size>354kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 1 Oct 2019 11:47:12 GMT</date><size>1193kb</size><source_type>D</source_type></version><title>A New Deterministic Technique for Symbolic Regression</title><authors>Daniel Rivero, Enrique Fernandez-Blanco</authors><categories>cs.LG stat.ML</categories><comments>22 pages. Work in progress, results not definite</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a new method for Symbolic Regression that allows to find\\nmathematical expressions from a dataset. This method has a strong mathematical\\nbasis. As opposed to other methods such as Genetic Programming, this method is\\ndeterministic, and does not involve the creation of a population of initial\\nsolutions. Instead of it, a simple expression is being grown until it fits the\\ndata. The experiments performed show that the results are as good as other\\nMachine Learning methods, in a very low computational time. Another advantage\\nof this technique is that the complexity of the expressions can be limited, so\\nthe system can return mathematical expressions that can be easily analysed by\\nthe user, in opposition to other techniques like GSGP.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.06848</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.06848</id><submitter>Vassilios Dallas</submitter><version version=\"v1\"><date>Fri, 26 Jul 2019 20:54:40 GMT</date><size>1777kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 27 Sep 2019 21:58:04 GMT</date><size>1777kb</size><source_type>D</source_type></version><title>Classification of chaotic time series with deep learning</title><authors>Nicolas Boull\\\\\\'e, Vassilios Dallas, Yuji Nakatsukasa, D. Samaddar</authors><categories>eess.SP cs.LG math.DS nlin.CD physics.comp-ph stat.ML</categories><comments>14 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use standard deep neural networks to classify univariate time series\\ngenerated by discrete and continuous dynamical systems based on their chaotic\\nor non-chaotic behaviour. Our approach to circumvent the lack of precise models\\nfor some of the most challenging real-life applications is to train different\\nneural networks on a data set from a dynamical system with a basic or\\nlow-dimensional phase space and then use these networks to classify univariate\\ntime series of a dynamical system with more intricate or high-dimensional phase\\nspace. We illustrate this generalisation approach using the logistic map, the\\nsine-circle map, the Lorenz system, and the Kuramoto--Sivashinsky equation. We\\nobserve that a convolutional neural network without batch normalisation layers\\noutperforms state-of-the-art neural networks for time series classification and\\nis able to generalise and classify time series as chaotic or not with high\\naccuracy.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.06933</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.06933</id><submitter>Ali Hatamizadeh</submitter><version version=\"v1\"><date>Mon, 19 Aug 2019 17:12:00 GMT</date><size>2848kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 21 Aug 2019 18:35:36 GMT</date><size>2854kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 8 Oct 2019 09:15:48 GMT</date><size>2848kb</size><source_type>D</source_type></version><title>DALS: Deep Active Lesion Segmentation</title><authors>Ali Hatamizadeh, Assaf Hoogi, Debleena Sengupta, Wuyue Lu, Brian\\n  Wilcox, Daniel Rubin and Demetri Terzopoulos</authors><categories>eess.IV cs.CV cs.LG</categories><comments>Accepted to Machine Learning in Medical Imaging (MLMI 2019)</comments><journal-ref>MLMI 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lesion segmentation is an important problem in computer-assisted diagnosis\\nthat remains challenging due to the prevalence of low contrast, irregular\\nboundaries that are unamenable to shape priors. We introduce Deep Active Lesion\\nSegmentation (DALS), a fully automated segmentation framework for that\\nleverages the powerful nonlinear feature extraction abilities of fully\\nConvolutional Neural Networks (CNNs) and the precise boundary delineation\\nabilities of Active Contour Models (ACMs). Our DALS framework benefits from an\\nimproved level-set ACM formulation with a per-pixel-parameterized energy\\nfunctional and a novel multiscale encoder-decoder CNN that learns an\\ninitialization probability map along with parameter maps for the ACM. We\\nevaluate our lesion segmentation model on a new Multiorgan Lesion Segmentation\\n(MLS) dataset that contains images of various organs, including brain, liver,\\nand lung, across different imaging modalities---MR and CT. Our results\\ndemonstrate favorable performance compared to competing methods, especially for\\nsmall training datasets.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.07107</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.07107</id><submitter>Debanjan Borthakur</submitter><version version=\"v1\"><date>Mon, 19 Aug 2019 23:44:01 GMT</date><size>2628kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 21:47:27 GMT</date><size>3521kb</size><source_type>D</source_type></version><title>Fuzzy C-Means Clustering and Sonification of HRV Features</title><authors>Debanjan Borthakur, Victoria Grace, Paul Batchelor, Harishchandra\\n  Dubey, Kunal Mankodiya</authors><categories>cs.HC cs.LG cs.SD eess.AS stat.ML</categories><comments>5 pages, 5 figures</comments><journal-ref>2019 the IEEE/ACM 4th International Conference on Connected\\n  Health: Applications, Systems and Engineering Technologies: EdgeDL\\n  WorkshopAt: Washington, D.C, sep- 25-27</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear and non-linear measures of heart rate variability (HRV) are widely\\ninvestigated as non-invasive indicators of health. Stress has a profound impact\\non heart rate, and different meditation techniques have been found to modulate\\nheartbeat rhythm. This paper aims to explore the process of identifying\\nappropriate metrices from HRV analysis for sonification. Sonification is a type\\nof auditory display involving the process of mapping data to acoustic\\nparameters. This work explores the use of auditory display in aiding the\\nanalysis of HRV leveraged by unsupervised machine learning techniques.\\nUnsupervised clustering helps select the appropriate features to improve the\\nsonification interpretability. Vocal synthesis sonification techniques are\\nemployed to increase comprehension and learnability of the processed data\\ndisplayed through sound. These analyses are early steps in building a real-time\\nsound-based biofeedback training system.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.07181</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.07181</id><submitter>Raphael Shu</submitter><version version=\"v1\"><date>Tue, 20 Aug 2019 06:14:18 GMT</date><size>1392kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 22 Aug 2019 05:17:44 GMT</date><size>1392kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 10 Sep 2019 01:35:11 GMT</date><size>1966kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Sat, 5 Oct 2019 07:03:22 GMT</date><size>1966kb</size><source_type>D</source_type></version><title>Latent-Variable Non-Autoregressive Neural Machine Translation with\\n  Deterministic Inference using a Delta Posterior</title><authors>Raphael Shu, Jason Lee, Hideki Nakayama, Kyunghyun Cho</authors><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although neural machine translation models reached high translation quality,\\nthe autoregressive nature makes inference difficult to parallelize and leads to\\nhigh translation latency. Inspired by recent refinement-based approaches, we\\npropose a latent-variable non-autoregressive model with continuous latent\\nvariables and deterministic inference procedure. In contrast to existing\\napproaches, we use a deterministic inference algorithm to find the target\\nsequence that maximizes the lowerbound to the log-probability. During\\ninference, the length of translation automatically adapts itself. Our\\nexperiments show that the lowerbound can be greatly increased by running the\\ninference algorithm, resulting in significantly improved translation quality.\\nOur proposed model closes the performance gap between non-autoregressive and\\nautoregressive approaches on ASPEC Ja-En dataset with 8.6x faster decoding. On\\nWMT\\'14 En-De dataset, our model narrows the gap with autoregressive baseline to\\n2.0 BLEU points with 12.5x speedup. By decoding multiple intial latent\\nvariables in parallel and rescore using a teacher model, the proposed model\\nfurther brings the gap down to 1.0 BLEU point on WMT\\'14 En-De task with 6.8x\\nspeedup.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.07235</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.07235</id><submitter>Tiago Ramalho</submitter><version version=\"v1\"><date>Tue, 20 Aug 2019 09:21:14 GMT</date><size>1327kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 06:27:40 GMT</date><size>1314kb</size><source_type>D</source_type></version><title>Density estimation in representation space to predict model uncertainty</title><authors>Tiago Ramalho, Miguel Miranda</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning models frequently make incorrect predictions with high\\nconfidence when presented with test examples that are not well represented in\\ntheir training dataset. We propose a novel and straightforward approach to\\nestimate prediction uncertainty in a pre-trained neural network model. Our\\nmethod estimates the training data density in representation space for a novel\\ninput. A neural network model then uses this information to determine whether\\nwe expect the pre-trained model to make a correct prediction. This uncertainty\\nmodel is trained by predicting in-distribution errors, but can detect\\nout-of-distribution data without having seen any such example. We test our\\nmethod for a state-of-the art image classification model in the settings of\\nboth in-distribution uncertainty estimation as well as out-of-distribution\\ndetection.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.07245</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.07245</id><submitter>Luyao Huang</submitter><version version=\"v1\"><date>Tue, 20 Aug 2019 09:37:42 GMT</date><size>30kb</size></version><version version=\"v2\"><date>Mon, 7 Oct 2019 16:38:11 GMT</date><size>30kb</size></version><title>GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge</title><authors>Luyao Huang, Chi Sun, Xipeng Qiu, Xuanjing Huang</authors><categories>cs.CL</categories><comments>EMNLP-IJCNLP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Word Sense Disambiguation (WSD) aims to find the exact sense of an ambiguous\\nword in a particular context. Traditional supervised methods rarely take into\\nconsideration the lexical resources like WordNet, which are widely utilized in\\nknowledge-based methods. Recent studies have shown the effectiveness of\\nincorporating gloss (sense definition) into neural networks for WSD. However,\\ncompared with traditional word expert supervised methods, they have not\\nachieved much improvement. In this paper, we focus on how to better leverage\\ngloss knowledge in a supervised neural WSD system. We construct context-gloss\\npairs and propose three BERT-based models for WSD. We fine-tune the pre-trained\\nBERT model and achieve new state-of-the-art results on WSD task.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.07380</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.07380</id><submitter>Omar Rivasplata</submitter><version version=\"v1\"><date>Mon, 19 Aug 2019 13:27:08 GMT</date><size>307kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 21 Aug 2019 10:18:05 GMT</date><size>306kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 23 Aug 2019 08:16:40 GMT</date><size>306kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Mon, 30 Sep 2019 12:32:30 GMT</date><size>305kb</size><source_type>D</source_type></version><version version=\"v5\"><date>Fri, 4 Oct 2019 17:23:16 GMT</date><size>307kb</size><source_type>D</source_type></version><title>PAC-Bayes with Backprop</title><authors>Omar Rivasplata, Vikram M Tankasali, Csaba Szepesvari</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the family of methods &quot;PAC-Bayes with Backprop&quot; (PBB) to train\\nprobabilistic neural networks by minimizing PAC-Bayes bounds. We present two\\ntraining objectives, one derived from a previously known PAC-Bayes bound, and a\\nsecond one derived from a novel PAC-Bayes bound. Both training objectives are\\nevaluated on MNIST and on various UCI data sets. Our experiments show two\\nstriking observations: we obtain competitive test set error estimates (~1.4% on\\nMNIST) and at the same time we compute non-vacuous bounds with much tighter\\nvalues (~2.3% on MNIST) than previous results. These observations suggest that\\nneural nets trained by PBB may lead to self-bounding learning, where the\\navailable data can be used to simultaneously learn a predictor and certify its\\nrisk, with no need to follow a data-splitting protocol.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.07924</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.07924</id><submitter>Babak Salimi</submitter><version version=\"v1\"><date>Tue, 20 Aug 2019 17:23:00 GMT</date><size>972kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 25 Sep 2019 05:24:37 GMT</date><size>3053kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 1 Oct 2019 02:07:32 GMT</date><size>1339kb</size><source_type>D</source_type></version><title>Data Management for Causal Algorithmic Fairness</title><authors>Babak Salimi, Bill Howe, Dan Suciu</authors><categories>cs.DB cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1902.08283</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fairness is increasingly recognized as a critical component of machine\\nlearning systems. However, it is the underlying data on which these systems are\\ntrained that often reflects discrimination, suggesting a data management\\nproblem. In this paper, we first make a distinction between associational and\\ncausal definitions of fairness in the literature and argue that the concept of\\nfairness requires causal reasoning. We then review existing works and identify\\nfuture opportunities for applying data management techniques to causal\\nalgorithmic fairness.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.08118</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.08118</id><submitter>Yang Li</submitter><version version=\"v1\"><date>Tue, 13 Aug 2019 18:57:30 GMT</date><size>368kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 17:45:55 GMT</date><size>359kb</size><source_type>D</source_type></version><title>Neural Plasticity Networks</title><authors>Yang Li, Shihao Ji</authors><categories>cs.NE cs.LG stat.ML</categories><comments>fixed some figure issues. arXiv admin note: text overlap with\\n  arXiv:1904.04432</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural plasticity is an important functionality of human brain, in which\\nnumber of neurons and synapses can shrink or expand in response to stimuli\\nthroughout the span of life. We model this dynamic learning process as an\\n$L_0$-norm regularized binary optimization problem, in which each unit of a\\nneural network (e.g., weight, neuron or channel, etc.) is attached with a\\nstochastic binary gate, whose parameters determine the level of activity of a\\nunit in the network. At the beginning, only a small portion of binary gates\\n(therefore the corresponding neurons) are activated, while the remaining\\nneurons are in a hibernation mode. As the learning proceeds, some neurons might\\nbe activated or deactivated if doing so can be justified by the cost-benefit\\ntradeoff measured by the $L_0$-norm regularized objective. As the training gets\\nmature, the probability of transition between activation and deactivation will\\ndiminish until a final hardening stage. We demonstrate that all of these\\nlearning dynamics can be modulated by a single parameter $k$ seamlessly. Our\\nneural plasticity network (NPN) can prune or expand a network depending on the\\ninitial capacity of network provided by the user; it also unifies dropout (when\\n$k=0$), traditional training of DNNs (when $k=\\\\infty$) and interpolates between\\nthese two. To the best of our knowledge, this is the first learning framework\\nthat unifies network sparsification and network expansion in an end-to-end\\ntraining pipeline. Extensive experiments on synthetic dataset and multiple\\nimage classification benchmarks demonstrate the superior performance of NPN. We\\nshow that both network sparsification and network expansion can yield compact\\nmodels of similar architectures and of similar predictive accuracies that are\\nclose to or sometimes even higher than baseline networks. We plan to release\\nour code to facilitate the research in this area.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.08123</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.08123</id><submitter>James F. Brady</submitter><version version=\"v1\"><date>Wed, 21 Aug 2019 21:35:03 GMT</date><size>651kb</size></version><version version=\"v2\"><date>Fri, 13 Sep 2019 22:08:07 GMT</date><size>651kb</size></version><version version=\"v3\"><date>Mon, 30 Sep 2019 21:39:55 GMT</date><size>653kb</size></version><title>Computing System Congestion Management Using Exponential Smoothing\\n  Forecasting</title><authors>James F Brady</authors><categories>cs.PF</categories><comments>7 figures, 20 pages including computer program listing v2 - clarified\\n  some of the notation v3 - reference to C program GitHub location added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An overloaded computer must finish what it starts and not start what will\\nfail or hang. A congestion management algorithm the author developed, and\\nSiemens Corporation patented for telecom products, effectively manages traffic\\noverload with its unique formulation of Exponential Smoothing forecasting.\\nSiemens filed for exclusive rights to this technique in 2003 and obtained US\\npatent US7301903B2 in 2007 with this author, an employee at the time of the\\nfiling, the sole inventor. A computer program, written in C language, which\\nexercises the methodology is listed at the end of this document and available\\non GitHub.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.08167</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.08167</id><submitter>Zhiguo Wang</submitter><version version=\"v1\"><date>Thu, 22 Aug 2019 02:00:53 GMT</date><size>132kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 02:28:53 GMT</date><size>132kb</size><source_type>D</source_type></version><title>Multi-passage BERT: A Globally Normalized BERT Model for Open-domain\\n  Question Answering</title><authors>Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nallapati, Bing Xiang</authors><categories>cs.CL cs.AI</categories><comments>To appear in EMNLP 2019</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  BERT model has been successfully applied to open-domain QA tasks. However,\\nprevious work trains BERT by viewing passages corresponding to the same\\nquestion as independent training instances, which may cause incomparable scores\\nfor answers from different passages. To tackle this issue, we propose a\\nmulti-passage BERT model to globally normalize answer scores across all\\npassages of the same question, and this change enables our QA model find better\\nanswers by utilizing more passages. In addition, we find that splitting\\narticles into passages with the length of 100 words by sliding window improves\\nperformance by 4%. By leveraging a passage ranker to select high-quality\\npassages, multi-passage BERT gains additional 2%. Experiments on four standard\\nbenchmarks showed that our multi-passage BERT outperforms all state-of-the-art\\nmodels on all benchmarks. In particular, on the OpenSQuAD dataset, our model\\ngains 21.4% EM and 21.5% $F_1$ over all non-BERT models, and 5.8% EM and 6.5%\\n$F_1$ over BERT-based models.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.08530</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.08530</id><submitter>Yue Cao</submitter><version version=\"v1\"><date>Thu, 22 Aug 2019 17:59:30 GMT</date><size>473kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 5 Oct 2019 11:18:38 GMT</date><size>474kb</size><source_type>D</source_type></version><title>VL-BERT: Pre-training of Generic Visual-Linguistic Representations</title><authors>Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai</authors><categories>cs.CV cs.CL cs.LG</categories><comments>Work in progress</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new pre-trainable generic representation for visual-linguistic\\ntasks, called Visual-Linguistic BERT (VL-BERT for short). VL-BERT adopts the\\nsimple yet powerful Transformer model as the backbone, and extends it to take\\nboth visual and linguistic embedded features as input. In it, each element of\\nthe input is either of a word from the input sentence, or a region-of-interest\\n(RoI) from the input image. It is designed to fit for most of the\\nvisual-linguistic downstream tasks. To better exploit the generic\\nrepresentation, we pre-train VL-BERT on the massive-scale Conceptual Captions\\ndataset, together with text-only corpus. Extensive empirical analysis\\ndemonstrates that the pre-training procedure can better align the\\nvisual-linguistic clues and benefit the downstream tasks, such as visual\\ncommonsense reasoning, visual question answering and referring expression\\ncomprehension. It is worth noting that VL-BERT achieved the first place of\\nsingle model on the leaderboard of the VCR benchmark.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.08532</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.08532</id><submitter>Nate Phillips Mr.</submitter><version version=\"v1\"><date>Wed, 21 Aug 2019 23:32:57 GMT</date><size>3891kb</size><source_type>D</source_type></version><title>Design, Assembly, Calibration, and Measurement of an Augmented Reality\\n  Haploscope</title><authors>Nate Phillips, Kristen Massey, Mohammed Safayet Arefin, and J. Edward\\n  Swan II</authors><categories>cs.GR cs.HC</categories><comments>Accepted and presented at the IEEE VR 2018 Workshop on Perceptual and\\n  Cognitive Issues in AR (PERCAR); pre-print version</comments><journal-ref>Proceedings of PERCAR: The Fifth IEEE Virtual Reality Workshop on\\n  Perceptual and Cognitive Issues in AR, 2019 IEEE Conference on Virtual\\n  Reality and 3D User Interfaces, Osaka, Japan, March 23-27, pages 1770-1774,\\n  2019</journal-ref><doi>10.1109/VR.2019.8798335</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A haploscope is an optical system which produces a carefully controlled\\nvirtual image. Since the development of Wheatstone\\'s original stereoscope in\\n1838, haploscopes have been used to measure perceptual properties of human\\nstereoscopic vision. This paper presents an augmented reality (AR) haploscope,\\nwhich allows the viewing of virtual objects superimposed against the real\\nworld. Our lab has used generations of this device to make a careful series of\\nperceptual measurements of AR phenomena, which have been described in\\npublications over the previous 8 years. This paper systematically describes the\\ndesign, assembly, calibration, and measurement of our AR haploscope. These\\nmethods have been developed and improved in our lab over the past 10 years.\\nDespite the fact that 180 years have elapsed since the original report of\\nWheatstone\\'s stereoscope, we have not previously found a paper that describes\\nthese kinds of details.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.08601</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.08601</id><submitter>Dinh-Cuong Hoang</submitter><version version=\"v1\"><date>Thu, 22 Aug 2019 21:15:10 GMT</date><size>4474kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 17:19:07 GMT</date><size>4280kb</size><source_type>D</source_type></version><title>Object-RPE: Dense 3D Reconstruction and Pose Estimation with\\n  Convolutional Neural Networks for Warehouse Robots</title><authors>Dinh-Cuong Hoang, Todor Stoyanov, and Achim J. Lilienthal</authors><categories>cs.RO</categories><comments>Presented at European Conference on Mobile Robots (ECMR), Prague,\\n  2019. arXiv admin note: text overlap with arXiv:1903.10782</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach for recognizing all objects in a scene and estimating\\ntheir full pose from an accurate 3D instance-aware semantic reconstruction\\nusing an RGB-D camera. Our framework couples convolutional neural networks\\n(CNNs) and a state-of-the-art dense Simultaneous Localisation and Mapping\\n(SLAM) system, ElasticFusion, to achieve both high-quality semantic\\nreconstruction as well as robust 6D pose estimation for relevant objects. While\\nthe main trend in CNN-based 6D pose estimation has been to infer object\\'s\\nposition and orientation from single views of the scene, our approach explores\\nperforming pose estimation from multiple viewpoints, under the conjecture that\\ncombining multiple predictions can improve the robustness of an object\\ndetection system. The resulting system is capable of producing high-quality\\nobject-aware semantic reconstructions of room-sized environments, as well as\\naccurately detecting objects and their 6D poses. The developed method has been\\nverified through experimental validation on the YCB-Video dataset and a newly\\ncollected warehouse object dataset. Experimental results confirmed that the\\nproposed system achieves improvements over state-of-the-art methods in terms of\\nsurface reconstruction and object pose prediction. Our code and video are\\navailable at https://sites.google.com/view/object-rpe.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.08681</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.08681</id><submitter>Diganta Misra</submitter><version version=\"v1\"><date>Fri, 23 Aug 2019 06:22:06 GMT</date><size>1314kb</size></version><version version=\"v2\"><date>Wed, 2 Oct 2019 16:59:14 GMT</date><size>2214kb</size></version><title>Mish: A Self Regularized Non-Monotonic Neural Activation Function</title><authors>Diganta Misra</authors><categories>cs.LG cs.CV cs.NE stat.ML</categories><comments>13 pages, 26 figures and 6 tables. Draft Version -2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of non-linearity in a Neural Network is introduced by an\\nactivation function which serves an integral role in the training and\\nperformance evaluation of the network. Over the years of theoretical research,\\nmany activation functions have been proposed, however, only a few are widely\\nused in mostly all applications which include ReLU (Rectified Linear Unit),\\nTanH (Tan Hyperbolic), Sigmoid, Leaky ReLU and Swish. In this work, a novel\\nneural activation function called as Mish is proposed. The experiments show\\nthat Mish tends to work better than both ReLU and Swish along with other\\nstandard activation functions in many deep networks across challenging\\ndatasets. For instance, in Squeeze Excite Net- 18 for CIFAR 100 classification,\\nthe network with Mish had an increase in Top-1 test accuracy by 0.494% and\\n1.671% as compared to the same network with Swish and ReLU respectively. The\\nsimilarity to Swish along with providing a boost in performance and its\\nsimplicity in implementation makes it easier for researchers and developers to\\nuse Mish in their Neural Network Models.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.08702</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.08702</id><submitter>Oliver Braganza</submitter><version version=\"v1\"><date>Fri, 23 Aug 2019 07:45:30 GMT</date><size>473kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 10 Sep 2019 08:59:23 GMT</date><size>474kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 2 Oct 2019 14:27:15 GMT</date><size>474kb</size><source_type>D</source_type></version><title>Economically rational sample-size choice and irreproducibility</title><authors>Oliver Braganza</authors><categories>econ.GN cs.SY eess.SY q-fin.EC stat.ME</categories><comments>Working Paper, related to Proxyeconomics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several systematic studies have suggested that a large fraction of published\\nresearch is not reproducible. One probable reason for low reproducibility is\\ninsufficient sample size, resulting in low power and low positive predictive\\nvalue. It has been suggested that insufficient sample-size choice is driven by\\na combination of scientific competition and \\'positive publication bias\\'. Here\\nwe formalize this intuition in a simple model, in which scientists choose\\neconomically rational sample sizes, balancing the cost of experimentation with\\nincome from publication. Specifically, assuming that a scientist\\'s income\\nderives only from \\'positive\\' findings (positive publication bias) and that\\nindividual samples cost a fixed amount, allows to leverage basic statistical\\nformulas into an economic optimality prediction. We find that if effects have\\ni) low base probability, ii) small effect size or iii) low grant income per\\npublication, then the rational (economically optimal) sample size is small.\\nFurthermore, for plausible distributions of these parameters we find a robust\\nemergence of a bimodal distribution of obtained statistical power and low\\noverall reproducibility rates, matching empirical findings. Overall, the model\\ndescribes a simple mechanism explaining both the prevalence and the persistence\\nof small sample sizes. It suggests economic rationality, or economic pressures,\\nas a principal driver of irreproducibility.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.09008</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.09008</id><submitter>Apratim Bhattacharyya</submitter><version version=\"v1\"><date>Sat, 24 Aug 2019 08:02:34 GMT</date><size>3486kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 10:44:50 GMT</date><size>3779kb</size><source_type>D</source_type></version><title>Conditional Flow Variational Autoencoders for Structured Sequence\\n  Prediction</title><authors>Apratim Bhattacharyya, Michael Hanselmann, Mario Fritz, Bernt Schiele,\\n  Christoph-Nikolas Straehle</authors><categories>cs.CV cs.LG stat.ML</categories><comments>To appear at Bayesian Deep Learning and Machine Learning for\\n  Autonomous Driving @NeurIPS 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prediction of future states of the environment and interacting agents is a\\nkey competence required for autonomous agents to operate successfully in the\\nreal world. Prior work for structured sequence prediction based on latent\\nvariable models imposes a uni-modal standard Gaussian prior on the latent\\nvariables. This induces a strong model bias which makes it challenging to fully\\ncapture the multi-modality of the distribution of the future states. In this\\nwork, we introduce Conditional Flow Variational Autoencoders (CF-VAE) using our\\nnovel conditional normalizing flow based prior to capture complex multi-modal\\nconditional distributions for effective structured sequence prediction.\\nMoreover, we propose two novel regularization schemes which stabilizes training\\nand deals with posterior collapse for stable training and better fit to the\\ntarget data distribution. Our experiments on three multi-modal structured\\nsequence prediction datasets -- MNIST Sequences, Stanford Drone and HighD --\\nshow that the proposed method obtains state of art results across different\\nevaluation metrics.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.09080</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.09080</id><submitter>Mohammad Reza Besharati</submitter><version version=\"v1\"><date>Sat, 24 Aug 2019 03:10:38 GMT</date><size>2002kb</size></version><version version=\"v2\"><date>Tue, 1 Oct 2019 08:10:24 GMT</date><size>1571kb</size></version><version version=\"v3\"><date>Mon, 7 Oct 2019 21:09:09 GMT</date><size>1569kb</size></version><title>DAST Model: Deciding About Semantic Complexity of a Text</title><authors>MohammadReza Besharati, Mohammad Izadi</authors><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measuring text complexity is an essential task in several fields and\\napplications (such as NLP, semantic web, smart education, etc.). The semantic\\nlayer of text is more tacit than its syntactic structure and, as a result,\\ncalculation of semantic complexity is more difficult than syntactic complexity.\\nWhile there are famous and powerful academic and commercial syntactic\\ncomplexity measures, the problem of measuring semantic complexity is still a\\nchallenging one. In this paper, we introduce the DAST model, which stands for\\nDeciding About Semantic Complexity of a Text. DAST proposes an intuitionistic\\napproach to semantics that lets us have a well-defined model for the semantics\\nof a text and its complexity: semantic is considered as a lattice of intuitions\\nand, as a result, semantic complexity is defined as the result of a calculation\\non this lattice. A set theoretic formal definition of semantic complexity, as a\\n6-tuple formal system, is provided. By using this formal system, a method for\\nmeasuring semantic complexity is presented. The evaluation of the proposed\\napproach is done by a set of three human-judgment experiments. The results show\\nthat DAST model is capable of deciding about semantic complexity of text.\\nFurthermore, the analysis of the results leads us to introduce a Markovian\\nmodel for the process of common-sense, multiple-steps and semantic-complexity\\nreasoning in people. The results of Experiments demonstrate that our method\\noutperforms the random baseline with improvement in precision and accuracy.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.09094</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.09094</id><submitter>Sandeep Juneja</submitter><version version=\"v1\"><date>Sat, 24 Aug 2019 05:31:49 GMT</date><size>42kb</size></version><version version=\"v2\"><date>Tue, 8 Oct 2019 07:13:06 GMT</date><size>78kb</size><source_type>D</source_type></version><title>Optimal $\\\\delta$-Correct Best-Arm Selection for General Distributions</title><authors>Shubhada Agrawal, Sandeep Juneja and Peter Glynn</authors><categories>cs.LG math.PR stat.ML</categories><comments>49 pages, 2 figures</comments><msc-class>65C05, 60-08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a finite set of unknown distributions, or arms, that can be sampled, we\\nconsider the problem of identifying the one with the largest mean using a\\ndelta-correct algorithm (an adaptive, sequential algorithm that restricts the\\nprobability of error to a specified delta) that has minimum sample complexity.\\nLower bounds for delta-correct algorithms are well known. Delta-correct\\nalgorithms that match the lower bound asymptotically as delta reduces to zero\\nhave been previously developed when arm distributions are restricted to a\\nsingle parameter exponential family. In this paper, we first observe a negative\\nresult that some restrictions are essential, as otherwise under a delta-correct\\nalgorithm, distributions with unbounded support would require an infinite\\nnumber of samples in expectation. We then propose a delta-correct algorithm\\nthat matches the lower bound as delta reduces to zero under the mild\\nrestriction that a known bound on the expectation of a non-negative,\\ncontinuous, increasing convex function (for example, the squared moment) of the\\nunderlying random variables, exists. We also propose batch processing and\\nidentify near-optimal batch sizes to substantially speed up the proposed\\nalgorithm. The best-arm problem has many learning applications, including\\nrecommendation systems and product selection. It is also a well studied classic\\nproblem in the simulation community.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.09101</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.09101</id><submitter>Haiyang Mei</submitter><version version=\"v1\"><date>Sat, 24 Aug 2019 06:57:04 GMT</date><size>9275kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 10:44:28 GMT</date><size>9279kb</size><source_type>D</source_type></version><title>Where Is My Mirror?</title><authors>Xin Yang, Haiyang Mei, Ke Xu, Xiaopeng Wei, Baocai Yin, Rynson W.H.\\n  Lau</authors><categories>cs.CV</categories><comments>Accepted by ICCV 2019. Project homepage:\\n  https://mhaiyang.github.io/ICCV2019_MirrorNet/index.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mirrors are everywhere in our daily lives. Existing computer vision systems\\ndo not consider mirrors, and hence may get confused by the reflected content\\ninside a mirror, resulting in a severe performance degradation. However,\\nseparating the real content outside a mirror from the reflected content inside\\nit is non-trivial. The key challenge is that mirrors typically reflect contents\\nsimilar to their surroundings, making it very difficult to differentiate the\\ntwo. In this paper, we present a novel method to segment mirrors from an input\\nimage. To the best of our knowledge, this is the first work to address the\\nmirror segmentation problem with a computational approach. We make the\\nfollowing contributions. First, we construct a large-scale mirror dataset that\\ncontains mirror images with corresponding manually annotated masks. This\\ndataset covers a variety of daily life scenes, and will be made publicly\\navailable for future research. Second, we propose a novel network, called\\nMirrorNet, for mirror segmentation, by modeling both semantical and low-level\\ncolor/texture discontinuities between the contents inside and outside of the\\nmirrors. Third, we conduct extensive experiments to evaluate the proposed\\nmethod, and show that it outperforms the carefully chosen baselines from the\\nstate-of-the-art detection and segmentation methods.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.09295</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.09295</id><submitter>Quan-Lin Li</submitter><version version=\"v1\"><date>Sun, 25 Aug 2019 10:06:23 GMT</date><size>482kb</size></version><version version=\"v2\"><date>Tue, 1 Oct 2019 06:47:19 GMT</date><size>483kb</size></version><title>A Complete Algebraic Transformational Solution for the Optimal Dynamic\\n  Policy in Inventory Rationing across Two Demand Classes</title><authors>Quan-Lin Li, Yi-Meng Li, Jing-Yu Ma, Heng-Li Liu</authors><categories>math.OC cs.CE cs.SY eess.SY</categories><comments>62 pages; 7 figures</comments><msc-class>90B05, 90B22, 90B30, 60J27, 90C40, 93E20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we apply the sensitivity-based optimization to propose and\\ndevelop a complete algebraic transformational solution for the optimal dynamic\\nrationing policy in inventory rationing across two demand classes. Our results\\nprovide a unified framework to set up a new transformational threshold type\\nstructure for the optimal dynamic rationing policy. Based on this, we can\\nprovide a complete description that the optimal dynamic rationing policy is\\neither of critical rationing level (i.e. threshold type or a static rationing\\npolicy) or of no critical rationing level. Also, two basic classifications can\\nbe described by means of our algebraic transformational solution. To this end,\\nwe first establish a policy-based birth-death process and set up a more general\\nreward (or cost) function with respect to both states and policies of the\\nbirth-death process, hence this gives our policy optimal problem. Then we set\\nup a policy-based Poisson equation, which, together with a performance\\ndifference equation, characterizes monotonicity and optimality of the long-run\\naverage profit of the rationing inventory system. Finally, we apply the\\nsensitivity-based optimization to construct a threshold type policy to further\\nstudy the rationing inventory system. Furthermore, we use some numerical\\nexperiments to verify our theoretic results and computational validity, and\\nspecifically, compare the optimal dynamic rationing policy with the optimal\\nthreshold type rationing policy from two different policy spaces. We hope that\\nthe methodology and results developed in this paper can shed light to the study\\nof rationing inventory systems, and will open a series of potentially promising\\nresearch by means of the sensitivity-based optimization and our algebraic\\ntransformational solution.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.09336</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.09336</id><submitter>Kaihan Li</submitter><version version=\"v1\"><date>Sun, 25 Aug 2019 14:25:31 GMT</date><size>96kb</size></version><version version=\"v2\"><date>Mon, 30 Sep 2019 12:10:56 GMT</date><size>96kb</size></version><title>Resource Allocation for Non-Orthogonal Multiple Access (NOMA) Enabled\\n  LPWA Networks</title><authors>Kaihan Li, Fatma Benkhelifa and Julie McCann</authors><categories>eess.SP cs.IT math.IT</categories><comments>IEEE Global Communications Conference (Globecom\\'2019)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the resource allocation for uplink\\nnon-orthogonal multiple access (NOMA) enabled low-power wide-area (LPWA)\\nnetworks to support the massive connectivity of users/nodes. Here, LPWA nodes\\ncommunicate with a central gateway through resource blocks like channels,\\ntransmission times, bandwidths, etc. The nodes sharing the same resource blocks\\nsuffer from intra-cluster interference and possibly inter-cluster interference,\\nwhich makes current LPWA networks unable to support the massive connectivity.\\nUsing the minimum transmission rate metric to highlight the interference\\nreduction that results from the addition of NOMA, and while assuring user\\nthroughput fairness, we decompose the minimum rate maximization optimization\\nproblem into three sub-problems. First, a low-complexity sub-optimal nodes\\nclustering scheme is proposed assigning nodes to channels based on their\\nnormalized channel gains. Then, two types of transmission time allocation\\nalgorithms are proposed that either assure fair or unfair transmission time\\nallocation between LPWA nodes sharing the same channel. For a given channel and\\ntransmission time allocation, we further propose an optimal power allocation\\nscheme. Simulation evaluations demonstrate approximately 100dB improvement of\\nthe selected metric for a single network with 4000 active nodes.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.09532</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.09532</id><submitter>Alberto Poncelas</submitter><version version=\"v1\"><date>Mon, 26 Aug 2019 08:55:00 GMT</date><size>95kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 13 Sep 2019 15:47:57 GMT</date><size>82kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 8 Oct 2019 12:59:03 GMT</date><size>81kb</size><source_type>D</source_type></version><title>Transductive Data-Selection Algorithms for Fine-Tuning Neural Machine\\n  Translation</title><authors>Alberto Poncelas, Gideon Maillette de Buy Wenniger, Andy Way</authors><categories>cs.CL</categories><comments>Proceedings of The 8th Workshop on Patent and Scientific Literature\\n  Translation, 2019, pages 13--23, Dublin</comments><journal-ref>Proceedings of The 8th Workshop on Patent and Scientific\\n  Literature Translation, 2019</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Machine Translation models are trained to translate a variety of documents\\nfrom one language into another. However, models specifically trained for a\\nparticular characteristics of the documents tend to perform better. Fine-tuning\\nis a technique for adapting an NMT model to some domain. In this work, we want\\nto use this technique to adapt the model to a given test set. In particular, we\\nare using transductive data selection algorithms which take advantage the\\ninformation of the test set to retrieve sentences from a larger parallel set.\\n  In cases where the model is available at translation time (when the test set\\nis provided), it can be adapted with a small subset of data, thereby achieving\\nbetter performance than a generic model or a domain-adapted model.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.09621</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.09621</id><submitter>Jake Hanson</submitter><version version=\"v1\"><date>Sat, 3 Aug 2019 00:49:09 GMT</date><size>1257kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 21:34:27 GMT</date><size>2652kb</size><source_type>D</source_type></version><title>Integrated Information Theory and Isomorphic Feed-Forward Philosophical\\n  Zombies</title><authors>Jake R. Hanson and Sara I. Walker</authors><categories>cs.IT math.IT</categories><comments>13 pages</comments><msc-class>68T27</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Any theory amenable to scientific inquiry must have testable consequences.\\nThis minimal criterion is uniquely challenging for the study of consciousness,\\nas we do not know if it is possible to confirm via observation from the outside\\nwhether or not a physical system knows what it feels like to have an inside - a\\nchallenge referred to as the &quot;hard problem&quot; of consciousness. To arrive at a\\ntheory of consciousness, the hard problem has motivated the development of\\nphenomenological approaches that adopt assumptions of what properties\\nconsciousness has based on first-hand experience and, from these, derive the\\nphysical processes that give rise to these properties. A leading theory\\nadopting this approach is Integrated Information Theory (IIT), which assumes\\nour subjective experience is a &quot;unified whole&quot;, subsequently yielding a\\nrequirement for physical feedback as a necessary condition for consciousness.\\nHere, we develop a mathematical framework to assess the validity of this\\nassumption by testing it in the context of isomorphic physical systems with and\\nwithout feedback. The isomorphism allows us to isolate changes in $\\\\Phi$\\nwithout affecting the size or functionality of the original system. Indeed, we\\nshow that the only mathematical difference between a &quot;conscious&quot; system with\\n$\\\\Phi&gt;0$ and an isomorphic &quot;philosophical zombies&quot; with $\\\\Phi=0$ is a\\npermutation of the binary labels used to internally represent functional\\nstates. This implies $\\\\Phi$ is sensitive to functionally arbitrary aspects of a\\nparticular labeling scheme, with no clear justification in terms of\\nphenomenological differences. In light of this, we argue any quantitative\\ntheory of consciousness, including IIT, should be invariant under isomorphisms\\nif it is to avoid the existence of isomorphic philosophical zombies and the\\nepistemological problems they pose.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.09804</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.09804</id><submitter>Satish Chandra</submitter><version version=\"v1\"><date>Mon, 26 Aug 2019 17:18:32 GMT</date><size>43kb</size></version><version version=\"v2\"><date>Tue, 27 Aug 2019 09:59:33 GMT</date><size>1135kb</size></version><version version=\"v3\"><date>Fri, 6 Sep 2019 01:20:40 GMT</date><size>43kb</size></version><version version=\"v4\"><date>Wed, 18 Sep 2019 23:29:26 GMT</date><size>43kb</size></version><version version=\"v5\"><date>Mon, 23 Sep 2019 23:13:13 GMT</date><size>43kb</size></version><version version=\"v6\"><date>Wed, 2 Oct 2019 00:07:53 GMT</date><size>43kb</size></version><title>Neural Code Search Evaluation Dataset</title><authors>Hongyu Li and Seohyun Kim and Satish Chandra</authors><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  There has been an increase of interest in code search using natural language.\\nAssessing the performance of such code search models can be difficult without a\\nreadily available evaluation suite. In this paper, we present an evaluation\\ndataset consisting of natural language query and code snippet pairs, with the\\nhope that future work in this area can use this dataset as a common benchmark.\\nWe also provide the results of two code search models ([1] and [6]) from recent\\nwork.\\n  The evaluation dataset is available at\\nhttps://github.com/facebookresearch/Neural-Code-Search-Evaluation-Dataset\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.10129</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.10129</id><submitter>Ruaridh Clark</submitter><version version=\"v1\"><date>Tue, 27 Aug 2019 11:05:48 GMT</date><size>3468kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 5 Oct 2019 17:10:12 GMT</date><size>3601kb</size><source_type>D</source_type></version><title>Network Communities of Dynamical Influence</title><authors>Ruaridh Clark, Giuliano Punzo and Malcolm Macdonald</authors><categories>cs.SI nlin.AO physics.soc-ph</categories><comments>15 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Fuelled by a desire for greater connectivity, networked systems now pervade\\nour society at an unprecedented level that will affect it in ways we do not yet\\nunderstand. In contrast, nature has already developed efficient networks that\\ncan instigate rapid response and consensus, when key elements are stimulated.\\nWe present a technique for identifying these key elements by investigating the\\nrelationships between a system\\'s most dominant eigenvectors. This approach\\nreveals the most effective vertices for leading a network to rapid consensus\\nwhen stimulated, as well as the communities that form under their dynamical\\ninfluence. In applying this technique, the effectiveness of starling flocks was\\nfound to be due, in part, to the low outdegree of every bird, where increasing\\nthe number of outgoing connections can produce a less responsive flock. A\\nlarger outdegree also affects the location of the birds with the most\\ninfluence, where these influentially connected birds become more centrally\\nlocated and in a poorer position to observe a predator and, hence, instigate an\\nevasion manoeuvre. Finally, the technique was found to be effective in large\\nvoxel-wise brain connectomes where subjects can be identified from their\\ninfluential communities.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.10553</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.10553</id><submitter>Jiawang Bian</submitter><version version=\"v1\"><date>Wed, 28 Aug 2019 05:25:46 GMT</date><size>1199kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 00:34:14 GMT</date><size>1348kb</size><source_type>D</source_type></version><title>Unsupervised Scale-consistent Depth and Ego-motion Learning from\\n  Monocular Video</title><authors>Jia-Wang Bian, Zhichao Li, Naiyan Wang, Huangying Zhan, Chunhua Shen,\\n  Ming-Ming Cheng, Ian Reid</authors><categories>cs.CV</categories><comments>Accepted to NeurIPS 2019. Code is available at\\n  https://github.com/JiawangBian/SC-SfMLearner-Release</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work has shown that CNN-based depth and ego-motion estimators can be\\nlearned using unlabelled monocular videos. However, the performance is limited\\nby unidentified moving objects that violate the underlying static scene\\nassumption in geometric image reconstruction. More significantly, due to lack\\nof proper constraints, networks output scale-inconsistent results over\\ndifferent samples, i.e., the ego-motion network cannot provide full camera\\ntrajectories over a long video sequence because of the per-frame scale\\nambiguity. This paper tackles these challenges by proposing a geometry\\nconsistency loss for scale-consistent predictions and an induced\\nself-discovered mask for handling moving objects and occlusions. Since we do\\nnot leverage multi-task learning like recent works, our framework is much\\nsimpler and more efficient. Comprehensive evaluation results demonstrate that\\nour depth estimator achieves the state-of-the-art performance on the KITTI\\ndataset. Moreover, we show that our ego-motion network is able to predict a\\nglobally scale-consistent camera trajectory for long video sequences, and the\\nresulting visual odometry accuracy is competitive with the recent model that is\\ntrained using stereo videos. To the best of our knowledge, this is the first\\nwork to show that deep networks trained using unlabelled monocular videos can\\npredict globally scale-consistent camera trajectories over a long video\\nsequence.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.10920</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.10920</id><submitter>Guan-Horng Liu</submitter><version version=\"v1\"><date>Wed, 28 Aug 2019 19:36:23 GMT</date><size>794kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 20:40:02 GMT</date><size>1013kb</size><source_type>D</source_type></version><title>Deep Learning Theory Review: An Optimal Control and Dynamical Systems\\n  Perspective</title><authors>Guan-Horng Liu, Evangelos A. Theodorou</authors><categories>cs.LG cs.SY eess.SY stat.ML</categories><comments>Under Submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attempts from different disciplines to provide a fundamental understanding of\\ndeep learning have advanced rapidly in recent years, yet a unified framework\\nremains relatively limited. In this article, we provide one possible way to\\nalign existing branches of deep learning theory through the lens of dynamical\\nsystem and optimal control. By viewing deep neural networks as discrete-time\\nnonlinear dynamical systems, we can analyze how information propagates through\\nlayers using mean field theory. When optimization algorithms are further recast\\nas controllers, the ultimate goal of training processes can be formulated as an\\noptimal control problem. In addition, we can reveal convergence and\\ngeneralization properties by studying the stochastic dynamics of optimization\\nalgorithms. This viewpoint features a wide range of theoretical study from\\ninformation bottleneck to statistical physics. It also provides a principled\\nway for hyper-parameter tuning when optimal control theory is introduced. Our\\nframework fits nicely with supervised learning and can be extended to other\\nlearning problems, such as Bayesian learning, adversarial training, and\\nspecific forms of meta learning, without efforts. The review aims to shed\\nlights on the importance of dynamics and optimal control when developing deep\\nlearning theory.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.10959</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.10959</id><submitter>Qing Qu</submitter><version version=\"v1\"><date>Wed, 28 Aug 2019 21:52:28 GMT</date><size>6037kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 05:25:59 GMT</date><size>6305kb</size><source_type>D</source_type></version><title>Short-and-Sparse Deconvolution -- A Geometric Approach</title><authors>Yenson Lau, Qing Qu, Han-Wen Kuo, Pengcheng Zhou, Yuqian Zhang, John\\n  Wright</authors><categories>eess.SP cs.LG eess.IV math.OC stat.ML</categories><comments>*YL and QQ contributed equally to this work; 30 figures, 45 pages;\\n  This version: added an experiment comparing with other methods, corrected\\n  typos and added references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Short-and-sparse deconvolution (SaSD) is the problem of extracting localized,\\nrecurring motifs in signals with spatial or temporal structure. Variants of\\nthis problem arise in applications such as image deblurring, microscopy, neural\\nspike sorting, and more. The problem is challenging in both theory and\\npractice, as natural optimization formulations are nonconvex. Moreover,\\npractical deconvolution problems involve smooth motifs (kernels) whose spectra\\ndecay rapidly, resulting in poor conditioning and numerical challenges. This\\npaper is motivated by recent theoretical advances, which characterize the\\noptimization landscape of a particular nonconvex formulation of SaSD. This is\\nused to derive a $provable$ algorithm which exactly solves certain\\nnon-practical instances of the SaSD problem. We leverage the key ideas from\\nthis theory (sphere constraints, data-driven initialization) to develop a\\n$practical$ algorithm, which performs well on data arising from a range of\\napplication areas. We highlight key additional challenges posed by the\\nill-conditioning of real SaSD problems, and suggest heuristics (acceleration,\\ncontinuation, reweighting) to mitigate them. Experiments demonstrate both the\\nperformance and generality of the proposed method.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.11111</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.11111</id><submitter>Christian Joppi</submitter><version version=\"v1\"><date>Thu, 29 Aug 2019 09:13:13 GMT</date><size>10490kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 30 Aug 2019 07:10:02 GMT</date><size>10490kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 4 Sep 2019 07:18:41 GMT</date><size>10522kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Fri, 4 Oct 2019 07:56:46 GMT</date><size>10522kb</size><source_type>D</source_type></version><title>Texture Retrieval in the Wild through detection-based attributes</title><authors>Christian Joppi, Marco Godi, Andrea Giachetti, Fabio Pellacini, Marco\\n  Cristani</authors><categories>cs.CV</categories><comments>ICIAP - International Conference on Image Analysis and Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Capturing the essence of a textile image in a robust way is important to\\nretrieve it in a large repository, especially if it has been acquired in the\\nwild (by taking a photo of the textile of interest). In this paper we show that\\na texel-based representation fits well with this task. In particular, we refer\\nto Texel-Att, a recent texel-based descriptor which has shown to capture fine\\ngrained variations of a texture, for retrieval purposes. After a brief\\nexplanation of Texel-Att, we will show in our experiments that this descriptor\\nis robust to distortions resulting from acquisitions in the wild by setting up\\nan experiment in which textures from the ElBa (an Element-Based texture\\ndataset) are artificially distorted and then used to retrieve the original\\nimage. We compare our approach with existing descriptors using a simple ranking\\nframework based on distance functions. Results show that even under extreme\\nconditions (such as a down-sampling with a factor of 10), we perform better\\nthan alternative approaches.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.11298</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.11298</id><submitter>Tao Wen</submitter><version version=\"v1\"><date>Thu, 29 Aug 2019 15:33:05 GMT</date><size>1576kb</size></version><version version=\"v2\"><date>Sun, 6 Oct 2019 03:40:09 GMT</date><size>1578kb</size></version><title>Identification of influencers in complex networks by local information\\n  dimensionality</title><authors>Tao Wen and Yong Deng</authors><categories>cs.SI physics.soc-ph</categories><journal-ref>Information Sciences 2019</journal-ref><doi>10.1016/j.ins.2019.10.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The identification of influential spreaders in complex networks is a popular\\ntopic in studies of network characteristics. Many centrality measures have been\\nproposed to address this problem, but most have limitations. In this paper, a\\nmethod for identifying influencers in complex networks via the local\\ninformation dimensionality. The proposed method considers the local structural\\nproperties around the central node; therefore, the scale of locality only\\nincreases to half of the maximum value of the shortest distance from the\\ncentral node. Thus, the proposed method considers the quasilocal information\\nand reduces the computational complexity. The information (number of nodes) in\\nboxes is described via the Shannon entropy, which is more reasonable. A node is\\nmore influential when its local information dimensionality is higher. In order\\nto show the effectiveness of the proposed method, five existing centrality\\nmeasures are used as comparison methods to rank influential nodes in six real\\nworld complex networks. In addition, a susceptible infected (SI) model and\\nKendall\\'s tau coefficient are applied to show the correlation between different\\nmethods. Experiment results show the superiority of the proposed method.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.11498</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.11498</id><submitter>Domonkos Vamossy</submitter><version version=\"v1\"><date>Fri, 30 Aug 2019 01:06:02 GMT</date><size>7481kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 19:50:35 GMT</date><size>7583kb</size><source_type>D</source_type></version><title>Predicting Consumer Default: A Deep Learning Approach</title><authors>Stefania Albanesi and Domonkos F. Vamossy</authors><categories>econ.GN cs.LG q-fin.EC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a model to predict consumer default based on deep learning. We\\nshow that the model consistently outperforms standard credit scoring models,\\neven though it uses the same data. Our model is interpretable and is able to\\nprovide a score to a larger class of borrowers relative to standard credit\\nscoring models while accurately tracking variations in systemic risk. We argue\\nthat these properties can provide valuable insights for the design of policies\\ntargeted at reducing consumer default and alleviating its burden on borrowers\\nand lenders, as well as macroprudential regulation.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.11533</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.11533</id><submitter>Jun Kitagawa</submitter><version version=\"v1\"><date>Fri, 30 Aug 2019 04:57:29 GMT</date><size>93kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 17:30:11 GMT</date><size>95kb</size><source_type>D</source_type></version><title>A Newton algorithm for semi-discrete optimal transport with storage fees\\n  and quantitative convergence of cells</title><authors>Mohit Bansil and Jun Kitagawa</authors><categories>math.NA cs.NA math.AP</categories><comments>Added result on equivalence of Hausdorff convergence of Laguerre\\n  cells and uniform convergence of dual potentials. 42 pages, comments welcome</comments><msc-class>49M25 65K10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we will continue analysis of the variant of semi-discrete\\noptimal transport problem with storage fees, previously analyzed by the\\nauthors, by proving convergence of a damped Newton algorithm for a specific\\nchoice of storage fee function, along with quantitative convergence of the\\nassociated Laguerre cells under limits of various parameters associated with\\nthe problem. A convergence result for cells in measure is proven without the\\nadditional assumption of a Poincar{\\\\`e}-Wirtinger inequality on the source\\nmeasure, while convergence in Hausdorff metric is shown when assuming such an\\ninequality. Additionally, it is shown that the Hausdorff convergence of\\nLaguerre cells is equivalent to uniform convergence of the associated dual\\npotentials, in a quantitative manner. These convergence results also yield\\napproximations to the classical semi-discrete optimal transport problem.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1908.11642</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1908.11642</id><submitter>Johannes Doleschal</submitter><version version=\"v1\"><date>Fri, 30 Aug 2019 10:39:21 GMT</date><size>44kb</size></version><version version=\"v2\"><date>Mon, 7 Oct 2019 13:46:59 GMT</date><size>65kb</size></version><title>Weight Annotation in Information Extraction</title><authors>Johannes Doleschal and Benny Kimelfeld and Wim Martens and Liat\\n  Peterfreund</authors><categories>cs.DB cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The framework of document spanners abstracts the task of information\\nextraction from text as a function that maps every document (a string) into a\\nrelation over the document\\'s spans (intervals identified by their start and end\\nindices). For instance, the regular spanners are the closure under the\\nRelational Algebra (RA) of the regular expressions with capture variables, and\\nthe expressive power of the regular spanners is precisely captured by the class\\nof vset-automata - a restricted class of transducers that mark the endpoints of\\nselected spans.\\n  In this work, we embark on the investigation of document spanners that can\\nannotate extractions with auxiliary information such as confidence, support,\\nand confidentiality measures. To this end, we adopt the abstraction of\\nprovenance semirings by Green et al., where tuples of a relation are annotated\\nwith the elements of a commutative semiring, and where the annotation\\npropagates through the (positive) RA operators via the semiring operators.\\nHence, the proposed spanner extension, referred to as an annotator, maps every\\nstring into an annotated relation over the spans. As a specific instantiation,\\nwe explore weighted vset-automata that, similarly to weighted automata and\\ntransducers, attach semiring elements to transitions. We investigate key\\naspects of expressiveness, such as the closure under the positive RA, and key\\naspects of computational complexity, such as the enumeration of annotated\\nanswers and their ranked enumeration in the case of numeric semirings. For a\\nnumber of these problems, fundamental properties of the underlying semiring,\\nsuch as positivity, are crucial for establishing tractability.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.00040</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.00040</id><submitter>Chunting Zhou</submitter><version version=\"v1\"><date>Fri, 30 Aug 2019 19:07:56 GMT</date><size>228kb</size><source_type>D</source_type></version><title>Handling Syntactic Divergence in Low-resource Machine Translation</title><authors>Chunting Zhou, Xuezhe Ma, Junjie Hu, Graham Neubig</authors><categories>cs.CL</categories><comments>Accepted by EMNLP 2019 (short paper)</comments><journal-ref>EMNLP 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite impressive empirical successes of neural machine translation (NMT) on\\nstandard benchmarks, limited parallel data impedes the application of NMT\\nmodels to many language pairs. Data augmentation methods such as\\nback-translation make it possible to use monolingual data to help alleviate\\nthese issues, but back-translation itself fails in extreme low-resource\\nscenarios, especially for syntactically divergent languages. In this paper, we\\npropose a simple yet effective solution, whereby target-language sentences are\\nre-ordered to match the order of the source and used as an additional source of\\ntraining-time supervision. Experiments with simulated low-resource\\nJapanese-to-English, and real low-resource Uyghur-to-English scenarios find\\nsignificant improvements over other semi-supervised alternatives.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.00299</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.00299</id><submitter>Kien Nguyen</submitter><version version=\"v1\"><date>Sun, 1 Sep 2019 00:30:58 GMT</date><size>2783kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 22 Sep 2019 22:05:13 GMT</date><size>2053kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 30 Sep 2019 18:39:34 GMT</date><size>2335kb</size><source_type>D</source_type></version><title>A Privacy-Preserving, Accountable and Spam-Resilient Geo-Marketplace</title><authors>Kien Nguyen, Gabriel Ghinita, Muhammad Naveed, Cyrus Shahabi</authors><categories>cs.CR</categories><comments>SIGSPATIAL\\'19, 10 pages</comments><doi>10.1145/3347146.3359072</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile devices with rich features can record videos, traffic parameters or\\nair quality readings along user trajectories. Although such data may be\\nvaluable, users are seldom rewarded for collecting them. Emerging digital\\nmarketplaces allow owners to advertise their data to interested buyers. We\\nfocus on geo-marketplaces, where buyers search data based on geo-tags. Such\\nmarketplaces present significant challenges. First, if owners upload data with\\nrevealed geo-tags, they expose themselves to serious privacy risks. Second,\\nowners must be accountable for advertised data, and must not be allowed to\\nsubsequently alter geo-tags. Third, such a system may be vulnerable to\\nintensive spam activities, where dishonest owners flood the system with fake\\nadvertisements. We propose a geo-marketplace that addresses all these concerns.\\nWe employ searchable encryption, digital commitments, and blockchain to protect\\nthe location privacy of owners while at the same time incorporating\\naccountability and spam-resilience mechanisms. We implement a prototype with\\ntwo alternative designs that obtain distinct trade-offs between trust\\nassumptions and performance. Our experiments on real location data show that\\none can achieve the above design goals with practical performance and\\nreasonable financial overhead.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.00894</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.00894</id><submitter>Yu Chen</submitter><version version=\"v1\"><date>Tue, 3 Sep 2019 00:11:20 GMT</date><size>31kb</size></version><version version=\"v2\"><date>Sun, 6 Oct 2019 11:28:30 GMT</date><size>24kb</size></version><title>Estimating Approximation Errors of Elitist Evolutionary Algorithms</title><authors>Cong Wang and Yu Chen and Jun He and Chengwang Xie</authors><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When evolutionary algorithms (EAs) are unlikely to locate precise global\\noptimal solutions with satisfactory performances, it is important to substitute\\nalternative theoretical routine for the analysis of hitting time/running time.\\nIn order to narrow the gap between theories and applications, this paper is\\ndedicated to perform an analysis on approximation error of EAs. First, we\\nproposed a general result on upper bound and lower bound of approximation\\nerrors. Then, several case studies are performed to present the routine of\\nerror analysis, and theoretical results show the close connections between\\napproximation errors and eigenvalues of transition matrices. The analysis\\nvalidates applicability of error analysis, demonstrates significance of\\nestimation results, and then, exhibits its potential to be applied for\\ntheoretical analysis of elitist EAs.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.00973</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.00973</id><submitter>Darius Foo</submitter><version version=\"v1\"><date>Tue, 3 Sep 2019 06:31:33 GMT</date><size>137kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 04:04:11 GMT</date><size>128kb</size><source_type>D</source_type></version><title>The Dynamics of Software Composition Analysis</title><authors>Darius Foo, Jason Yeo, Hao Xiao, Asankhaya Sharma</authors><categories>cs.SE cs.PL</categories><comments>ASE 2019, LBR</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developers today use significant amounts of open source code, surfacing the\\nneed for ways to automatically audit and upgrade library dependencies, and\\ngiving rise to the subfield of Software Composition Analysis (SCA). SCA\\nproducts are concerned with three tasks: discovering dependencies, checking the\\nreachability of vulnerable code for false positive elimination, and automated\\nremediation. The latter two tasks rely on call graphs of application and\\nlibrary code to check whether vulnerability-specific sinks identified in\\nlibraries are used by applications. However, statically-constructed call graphs\\nintroduce both false positives and false negatives on real-world projects. In\\nthis paper, we develop a novel, modular means of combining call graphs derived\\nfrom both static and dynamic analysis to improve the performance of false\\npositive elimination. Our experiments indicate significant performance\\nimprovements.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.01150</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.01150</id><submitter>Lingxiao Wang</submitter><version version=\"v1\"><date>Thu, 29 Aug 2019 15:38:19 GMT</date><size>63kb</size></version><version version=\"v2\"><date>Mon, 7 Oct 2019 00:25:26 GMT</date><size>69kb</size></version><title>Neural Policy Gradient Methods: Global Optimality and Rates of\\n  Convergence</title><authors>Lingxiao Wang, Qi Cai, Zhuoran Yang, Zhaoran Wang</authors><categories>cs.LG math.OC stat.ML</categories><comments>70 pages. The first two authors contribute equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Policy gradient methods with actor-critic schemes demonstrate tremendous\\nempirical successes, especially when the actors and critics are parameterized\\nby neural networks. However, it remains less clear whether such &quot;neural&quot; policy\\ngradient methods converge to globally optimal policies and whether they even\\nconverge at all. We answer both the questions affirmatively in the\\noverparameterized regime. In detail, we prove that neural natural policy\\ngradient converges to a globally optimal policy at a sublinear rate. Also, we\\nshow that neural vanilla policy gradient converges sublinearly to a stationary\\npoint. Meanwhile, by relating the suboptimality of the stationary points to the\\nrepresentation power of neural actor and critic classes, we prove the global\\noptimality of all stationary points under mild regularity conditions.\\nParticularly, we show that a key to the global optimality and convergence is\\nthe &quot;compatibility&quot; between the actor and critic, which is ensured by sharing\\nneural architectures and random initializations across the actor and critic. To\\nthe best of our knowledge, our analysis establishes the first global optimality\\nand convergence guarantees for neural policy gradient methods.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.01153</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.01153</id><submitter>Yang Li</submitter><version version=\"v1\"><date>Fri, 30 Aug 2019 14:16:23 GMT</date><size>1321kb</size></version><title>Dynamic State Estimation of Generators Under Cyber Attacks</title><authors>Yang Li, Zhi Li, Liang Chen</authors><categories>eess.SY cs.SY eess.SP</categories><comments>Accepted by IEEE Access</comments><journal-ref>IEEE Access 7 (2019) 125253-125267</journal-ref><doi>10.1109/ACCESS.2019.2939055</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate and reliable estimation of generator\\'s dynamic state vectors in real\\ntime are critical to the monitoring and control of power systems. A robust\\nCubature Kalman Filter (RCKF) based approach is proposed for dynamic state\\nestimation (DSE) of generators under cyber attacks in this paper. First, two\\ntypes of cyber attacks, namely false data injection and denial of service\\nattacks, are modelled and thereby introduced into DSE of a generator by mixing\\nthe attack vectors with the measurement data; Second, under cyber attacks with\\ndifferent degrees of sophistication, the RCKF algorithm and the Cubature Kalman\\nFilter (CKF) algorithm are adopted to the DSE, and then the two algorithms are\\ncompared and discussed. The novelty of this study lies primarily in our attempt\\nto introduce cyber attacks into DSE of generators. The simulation results on\\nthe IEEE 9-bus system and the New England 16-machine 68-bus system verify the\\neffectiveness and superiority of the RCKF.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.01461</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.01461</id><submitter>Dhruv Mubayi</submitter><version version=\"v1\"><date>Tue, 3 Sep 2019 21:20:38 GMT</date><size>10kb</size></version><version version=\"v2\"><date>Sun, 29 Sep 2019 01:23:23 GMT</date><size>11kb</size></version><title>A note on pseudorandom Ramsey graphs</title><authors>Dhruv Mubayi and Jacques Verstraete</authors><categories>math.CO cs.DM</categories><comments>10 pages, minor changes to the first version</comments><msc-class>05D10, 05C55, 05B25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For fixed $s \\\\ge 3$, we prove that if optimal $K_s$-free pseudorandom graphs\\nexist, then the Ramsey number\\n  $r(s,t) = t^{s-1+o(1)}$ as $t \\\\rightarrow \\\\infty$. Our method also improves\\nthe best lower bounds for $r(C_{\\\\ell},t)$ obtained by Bohman and Keevash from\\nthe random $C_{\\\\ell}$-free process by polylogarithmic factors for all odd $\\\\ell\\n\\\\geq 5$ and $\\\\ell \\\\in \\\\{6,10\\\\}$. For $\\\\ell = 4$ it matches their lower bound\\nfrom the $C_4$-free process.\\n  We also prove, via a different approach, that $r(C_5, t)&gt; (1+o(1))t^{11/8}$\\nand $r(C_7, t)&gt; (1+o(1))t^{11/9}$. These improve the exponent of $t$ in the\\nprevious best results and appear to be the first examples of graphs $F$ with\\ncycles for which such an improvement of the exponent for $r(F, t)$ is shown\\nover the bounds given by the random $F$-free process and random graphs.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.01529</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.01529</id><submitter>Liqun Qi</submitter><version version=\"v1\"><date>Wed, 4 Sep 2019 02:52:56 GMT</date><size>10kb</size></version><version version=\"v2\"><date>Thu, 5 Sep 2019 03:49:10 GMT</date><size>10kb</size></version><version version=\"v3\"><date>Mon, 9 Sep 2019 14:45:57 GMT</date><size>11kb</size></version><version version=\"v4\"><date>Tue, 10 Sep 2019 07:31:16 GMT</date><size>11kb</size></version><version version=\"v5\"><date>Sun, 15 Sep 2019 09:19:37 GMT</date><size>11kb</size></version><version version=\"v6\"><date>Mon, 30 Sep 2019 10:01:11 GMT</date><size>11kb</size></version><title>Spectral Norm and Nuclear Norm of a Third Order Tensor</title><authors>Liqun Qi and Shenglong Hu</authors><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spectral norm and the nuclear norm of a third order tensor play an\\nimportant role in the tensor completion and recovery problem. We show that the\\nspectral norm of a third order tensor is equal to the square root of the\\nspectral norm of three positive semi-definite biquadratic tensors, and the\\nsquare roots of the nuclear norms of those three positive semi-definite\\nbiquadratic tensors are lower bounds of the nuclear norm of that third order\\ntensor. This provides a way to estimate and to evaluate the spectral norm and\\nthe nuclear norm of that third order tensor. Some upper and lower bounds for\\nthe spectral norm and nuclear norm of a third order tensor, by spectral radii\\nand nuclear norms of some symmetric matrices, are presented.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.01771</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.01771</id><submitter>Emre Neftci</submitter><version version=\"v1\"><date>Wed, 4 Sep 2019 13:09:27 GMT</date><size>7367kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 09:45:24 GMT</date><size>7367kb</size><source_type>D</source_type></version><title>Spiking Neural Networks for Inference and Learning: A Memristor-based\\n  Design Perspective</title><authors>M. E. Fouda, F. Kurdahi, A. Eltawil, E. Neftci</authors><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On metrics of density and power efficiency, neuromorphic technologies have\\nthe potential to surpass mainstream computing technologies in tasks where\\nreal-time functionality, adaptability, and autonomy are essential. While\\nalgorithmic advances in neuromorphic computing are proceeding successfully, the\\npotential of memristors to improve neuromorphic computing have not yet born\\nfruit, primarily because they are often used as a drop-in replacement to\\nconventional memory. However, interdisciplinary approaches anchored in machine\\nlearning theory suggest that multifactor plasticity rules matching neural and\\nsynaptic dynamics to the device capabilities can take better advantage of\\nmemristor dynamics and its stochasticity. Furthermore, such plasticity rules\\ngenerally show much higher performance than that of classical Spike Time\\nDependent Plasticity (STDP) rules. This chapter reviews the recent development\\nin learning with spiking neural network models and their possible\\nimplementation with memristor-based hardware.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.01965</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.01965</id><submitter>Darij Grinberg</submitter><version version=\"v1\"><date>Wed, 4 Sep 2019 17:46:28 GMT</date><size>36kb</size></version><version version=\"v2\"><date>Fri, 20 Sep 2019 19:35:53 GMT</date><size>43kb</size></version><version version=\"v3\"><date>Mon, 7 Oct 2019 13:27:28 GMT</date><size>43kb</size></version><title>A greedoid and a matroid inspired by Bhargava\\'s $p$-orderings</title><authors>Darij Grinberg, Fedor Petrov</authors><categories>math.CO cs.DM</categories><comments>46 pages. v3 streamlines the article: Old alternative proof of the\\n  matroid property removed; notion of greedy flags removed (and everything\\n  restated in terms of greedy permutations); appendix shortened. The proof of\\n  the greedoid property is now independent of Sections 4 and 5. Minor\\n  corrections made. Comments are welcome!</comments><msc-class>05B35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a finite set $E$. Assume that each $e \\\\in E$ has a &quot;weight&quot; $w\\n\\\\left(e\\\\right) \\\\in \\\\mathbb{R}$ assigned to it, and any two distinct $e, f \\\\in\\nE$ have a &quot;distance&quot; $d \\\\left(e, f\\\\right) = d \\\\left(f, e\\\\right) \\\\in \\\\mathbb{R}$\\nassigned to them, such that the distances satisfy the ultrametric triangle\\ninequality $d(a,b)\\\\leqslant \\\\max \\\\left\\\\{d(a,c),d(b,c)\\\\right\\\\}$. We look for a\\nsubset of $E$ of given size with maximum perimeter (where the perimeter is\\ndefined by summing the weights of all elements and their pairwise distances).\\nWe show that any such subset can be found by a greedy algorithm (which starts\\nwith the empty set, and then adds new elements one by one, maximizing the\\nperimeter at each step). We use this to define numerical invariants, and also\\nto show that the maximum-perimeter subsets of all sizes form a strong greedoid,\\nand the maximum-perimeter subsets of any given size are the bases of a matroid.\\nThis essentially generalizes the &quot;$P$-orderings&quot; constructed by Bhargava in\\norder to define his generalized factorials, and is also similar to the strong\\ngreedoid of maximum diversity subsets in phylogenetic trees studied by Moulton,\\nSemple and Steel.\\n  We further discuss some numerical invariants of $E, w, d$ stemming from this\\nconstruction, as well as an analogue where maximum-perimeter subsets are\\nreplaced by maximum-perimeter tuples (i.e., elements can appear multiple\\ntimes).\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.02072</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.02072</id><submitter>Tianlang Chen</submitter><version version=\"v1\"><date>Wed, 4 Sep 2019 19:49:58 GMT</date><size>6516kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 15:57:07 GMT</date><size>6724kb</size><source_type>D</source_type></version><title>Large-scale Tag-based Font Retrieval with Generative Feature Learning</title><authors>Tianlang Chen, Zhaowen Wang, Ning Xu, Hailin Jin, Jiebo Luo</authors><categories>cs.CV</categories><comments>accepted by ICCV 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Font selection is one of the most important steps in a design workflow.\\nTraditional methods rely on ordered lists which require significant domain\\nknowledge and are often difficult to use even for trained professionals. In\\nthis paper, we address the problem of large-scale tag-based font retrieval\\nwhich aims to bring semantics to the font selection process and enable people\\nwithout expert knowledge to use fonts effectively. We collect a large-scale\\nfont tagging dataset of high-quality professional fonts. The dataset contains\\nnearly 20,000 fonts, 2,000 tags, and hundreds of thousands of font-tag\\nrelations. We propose a novel generative feature learning algorithm that\\nleverages the unique characteristics of fonts. The key idea is that font images\\nare synthetic and can therefore be controlled by the learning algorithm. We\\ndesign an integrated rendering and learning process so that the visual feature\\nfrom one image can be used to reconstruct another image with different text.\\nThe resulting feature captures important font design details while is robust to\\nnuisance factors such as text. We propose a novel attention mechanism to\\nre-weight the visual feature for joint visual-text modeling. We combine the\\nfeature and the attention mechanism in a novel recognition-retrieval model.\\nExperimental results show that our method significantly outperforms the\\nstate-of-the-art for the important problem of large-scale tag-based font\\nretrieval.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.02164</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.02164</id><submitter>Wenhu Chen</submitter><version version=\"v1\"><date>Thu, 5 Sep 2019 00:25:17 GMT</date><size>2274kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 10 Sep 2019 01:59:41 GMT</date><size>2282kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 8 Oct 2019 05:58:51 GMT</date><size>2476kb</size><source_type>D</source_type></version><title>TabFact: A Large-scale Dataset for Table-based Fact Verification</title><authors>Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang,\\n  Shiyang Li, Xiyou Zhou and William Yang Wang</authors><categories>cs.CL cs.AI</categories><comments>Table-based Fact Verification Dataset on Structured Information</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of verifying whether a textual hypothesis holds based on the\\ngiven evidence, also known as fact verification, plays an important role in the\\nstudy of natural language understanding and semantic representation. However,\\nexisting studies are mainly restricted to dealing with unstructured evidence\\n(e.g., natural language sentences and documents, news, etc), while verification\\nunder structured evidence, such as tables, graphs, and databases, remains\\nunexplored. This paper specifically aims to study the fact verification given\\nsemi-structured data as evidence. To this end, we construct a large-scale\\ndataset called TabFact with 16k Wikipedia tables as the evidence for 118k\\nhuman-annotated natural language statements, which are labeled as either\\nENTAILED or REFUTED. TabFact is challenging since it involves both soft\\nlinguistic reasoning and hard symbolic reasoning. To address these reasoning\\nchallenges, we design two different models: Table-BERT and Latent Program\\nAlgorithm (LPA). Table-BERT leverages the state-of-the-art pre-trained language\\nmodel to encode the linearized tables and statements into continuous vectors\\nfor verification. LPA parses statements into LISP-like programs and executes\\nthem against the tables to obtain the returned binary value for verification.\\nBoth methods achieve similar accuracy but still lag far behind human\\nperformance. We also perform a comprehensive analysis to demonstrate great\\nfuture opportunities. The data and code of the dataset are provided in\\n\\\\url{https://github.com/wenhuchen/Table-Fact-Checking}.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.02190</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.02190</id><submitter>Jiazhen Gu</submitter><version version=\"v1\"><date>Thu, 5 Sep 2019 02:56:33 GMT</date><size>1652kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 14:49:21 GMT</date><size>1647kb</size><source_type>D</source_type></version><title>Detecting Deep Neural Network Defects with Data Flow Analysis</title><authors>Jiazhen Gu, Huanlin Xu, Yangfan Zhou, Xin Wang, Hui Xu, Michael Lyu</authors><categories>cs.LG eess.SP stat.ML</categories><comments>2 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks (DNNs) are shown to be promising solutions in many\\nchallenging artificial intelligence tasks. However, it is very hard to figure\\nout whether the low precision of a DNN model is an inevitable result, or caused\\nby defects. This paper aims at addressing this challenging problem. We find\\nthat the internal data flow footprints of a DNN model can provide insights to\\nlocate the root cause effectively. We develop DeepMorph (DNN Tomography) to\\nanalyze the root cause, which can guide a DNN developer to improve the model.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.02220</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.02220</id><submitter>Kevin He</submitter><version version=\"v1\"><date>Thu, 5 Sep 2019 06:05:21 GMT</date><size>205kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 5 Oct 2019 17:53:09 GMT</date><size>206kb</size><source_type>D</source_type></version><title>An Experiment on Network Density and Sequential Learning</title><authors>Krishna Dasaratha, Kevin He</authors><categories>econ.TH cs.SI econ.GN q-fin.EC</categories><comments>Incorporates the experimental results from a previous version of\\n  arXiv:1703.02105</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We conduct a sequential social-learning experiment where subjects take turns\\nguessing a hidden state based on private signals and the guesses of a subset of\\ntheir predecessors. A network determines the observable predecessors, and we\\ncompare subjects\\' accuracy on sparse and dense networks. Accuracy gains from\\nsocial learning are twice as large on sparse networks compared to dense\\nnetworks. Models of naive inference where agents ignore correlation between\\nobservations predict this comparative static in network density, while the\\nfinding is difficult to reconcile with rational-learning models.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.02352</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.02352</id><submitter>Peilun Wu</submitter><version version=\"v1\"><date>Thu, 5 Sep 2019 12:11:07 GMT</date><size>75kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 16:02:08 GMT</date><size>164kb</size><source_type>D</source_type></version><title>A Transfer Learning Approach for Network Intrusion Detection</title><authors>Peilun Wu, Hui Guo and Richard Buckland</authors><categories>cs.LG cs.CR cs.NI</categories><doi>10.1109/ICBDA.2019.8713213</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolution Neural Network (ConvNet) offers a high potential to generalize\\ninput data. It has been widely used in many application areas, such as visual\\nimagery, where comprehensive learning datasets are available and a ConvNet\\nmodel can be well trained and perform the required function effectively.\\nConvNet can also be applied to network intrusion detection. However, the\\ncurrently available datasets related to the network intrusion are often\\ninadequate, which makes the ConvNet learning deficient, hence the trained model\\nis not competent in detecting unknown intrusions. In this paper, we propose a\\nConvNet model using transfer learning for network intrusion detection. The\\nmodel consists of two concatenated ConvNets and is built on a two-stage\\nlearning process: learning a base dataset and transferring the learned\\nknowledge to the learning of the target dataset. Our experiments on the NSL-KDD\\ndataset show that the proposed model can improve the detection accuracy not\\nonly on the test dataset containing mostly known attacks (KDDTest+) but also on\\nthe test dataset featuring many novel attacks (KDDTest-21) -- about 2.68\\\\%\\nimprovement on KDDTest+ and 22.02\\\\% on KDDTest-21 can be achieved, as compared\\nto the traditional ConvNet model.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.02353</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.02353</id><submitter>Laszlo Csirmaz</submitter><version version=\"v1\"><date>Thu, 5 Sep 2019 12:16:47 GMT</date><size>15kb</size></version><version version=\"v2\"><date>Mon, 9 Sep 2019 09:38:25 GMT</date><size>15kb</size></version><version version=\"v3\"><date>Wed, 2 Oct 2019 11:11:13 GMT</date><size>15kb</size></version><title>Sticky matroids and convolution</title><authors>Laszlo Csirmaz</authors><categories>math.CO cs.IT math.IT</categories><msc-class>03G10, 05B35, 06C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the characterization of the lattice of cyclic flats of a\\nmatroid, the convolution of a ranked lattice and a discrete measure is defined,\\ngeneralizing polymatroid convolution. Using the convolution technique we prove\\nthat if a matroid has a non-principal modular cut then it is not sticky. A\\nsimilar statement for matroids has been proved in [8] using different\\ntechnique.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.02364</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.02364</id><submitter>Carl Barton</submitter><version version=\"v1\"><date>Thu, 5 Sep 2019 12:47:54 GMT</date><size>577kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 19 Sep 2019 18:18:49 GMT</date><size>577kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 19 Sep 2019 18:19:44 GMT</date><size>577kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Tue, 1 Oct 2019 09:59:01 GMT</date><size>580kb</size><source_type>D</source_type></version><title>A Simple Reduction for Full-Permuted Pattern Matching Problems on\\n  Multi-Track Strings</title><authors>Carl Barton, Ewan Birney, Tomas Fitzgerald</authors><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study a variant of string pattern matching which deals with\\ntuples of strings known as \\\\textit{multi-track strings}. Multi-track strings\\nare a generalisation of strings (or \\\\textit{single-track strings}) that have\\nprimarily found uses in problems related to searching multiple genomes and\\nmusic information retrieval. A multi-track string $\\\\mathcal{T} = (t_1, t_2,\\nt_3, \\\\ldots , t_N)$ of length $n$ and track count $N$ is a multi-set of $N$\\nstrings of length $n$ with characters drawn from a common alphabet of size\\n$\\\\sigma_U$. Given two multi-track strings $\\\\mathcal{T} = (t_1, t_2, t_3, \\\\ldots\\n, t_N)$ and $ \\\\mathcal{P} = (p_1, p_2, p_3, \\\\ldots , p_N)$ of length $n$ and\\ntrack count $N$, there is a \\\\textit{full-permuted-match} between $\\\\mathcal{P}$\\nand $\\\\mathcal{T}$ if $t_{r_i} = p_i$ for all $i \\\\in \\\\{1,2,3,\\\\ldots N \\\\}$ and\\nsome permutation $(r_1, r_2, r_3\\\\ldots,r_N)$ of $(1, 2, 3,\\\\ldots,N)$, we denote\\nthis $\\\\mathcal{P}\\\\asymp\\\\mathcal{T}$.\\n  Efficient algorithms for some full-permuted-match problems on multi-track\\nstrings have recently been presented. In this paper we show a reduction from a\\nmulti-track string of length $n$ and track count $N$ with alphabet size\\n$\\\\sigma_U$, to a single-track string of length $2n-1$ with alphabet size\\n$\\\\sigma_U^N$. Through this reduction we allow any string algorithm to be used\\non multi-track string problems using $\\\\asymp$ as the match relation. For\\npolynomial time algorithms on single-track strings of length $n$ there is a\\nmultiplicative penalty of not more than $\\\\mathcal{O}(N)$-time for the same\\nalgorithm on mt-strings of length $n$ and track count $N$.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.02511</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.02511</id><submitter>Bo Zhou</submitter><version version=\"v1\"><date>Thu, 5 Sep 2019 16:31:40 GMT</date><size>1355kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 27 Sep 2019 21:48:31 GMT</date><size>1354kb</size><source_type>D</source_type></version><title>CT Data Curation for Liver Patients: Phase Recognition in Dynamic\\n  Contrast-Enhanced CT</title><authors>Bo Zhou, Adam P. Harrison, Jiawen Yao, Chi-Tung Cheng, Jing Xiao,\\n  Chien-Hung Liao, Le Lu</authors><categories>eess.IV cs.CV</categories><comments>11 pages, accepted by 2019 MICCAI - Medical Image Learning with Less\\n  Labels and Imperfect Data Workshop</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As the demand for more descriptive machine learning models grows within\\nmedical imaging, bottlenecks due to data paucity will exacerbate. Thus,\\ncollecting enough large-scale data will require automated tools to harvest\\ndata/label pairs from messy and real-world datasets, such as hospital PACS.\\nThis is the focus of our work, where we present a principled data curation tool\\nto extract multi-phase CT liver studies and identify each scan\\'s phase from a\\nreal-world and heterogenous hospital PACS dataset. Emulating a typical\\ndeployment scenario, we first obtain a set of noisy labels from our\\ninstitutional partners that are text mined using simple rules from DICOM tags.\\nWe train a deep learning system, using a customized and streamlined 3D SE\\narchitecture, to identify non-contrast, arterial, venous, and delay phase\\ndynamic CT liver scans, filtering out anything else, including other types of\\nliver contrast studies. To exploit as much training data as possible, we also\\nintroduce an aggregated cross entropy loss that can learn from scans only\\nidentified as &quot;contrast&quot;. Extensive experiments on a dataset of 43K scans of\\n7680 patient imaging studies demonstrate that our 3DSE architecture, armed with\\nour aggregated loss, can achieve a mean F1 of 0.977 and can correctly harvest\\nup to 92.7% of studies, which significantly outperforms the text-mined and\\nstandard-loss approach, and also outperforms other, and more complex, model\\narchitectures.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.02688</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.02688</id><submitter>Thomas Athey</submitter><version version=\"v1\"><date>Fri, 6 Sep 2019 01:45:27 GMT</date><size>2755kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 16:14:24 GMT</date><size>1162kb</size><source_type>D</source_type></version><title>AutoGMM: Automatic Gaussian Mixture Modeling in Python</title><authors>Thomas L. Athey, Joshua T. Vogelstein</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian mixture modeling is a fundamental tool in clustering, as well as\\ndiscriminant analysis and semiparametric density estimation. However,\\nestimating the optimal model for any given number of components is an NP-hard\\nproblem, and estimating the number of components is in some respects an even\\nharder problem. In R, a popular package called mclust addresses both of these\\nproblems. However, Python has lacked such a package. We therefore introduce\\nAutoGMM, a Python algorithm for automatic Gaussian mixture modeling. AutoGMM\\nbuilds upon scikit-learn\\'s AgglomerativeClustering and GaussianMixture classes,\\nwith certain modifications to make the results more stable. Empirically, on\\nseveral different applications, AutoGMM performs approximately as well as\\nmclust. This algorithm is freely available and therefore further shrinks the\\ngap between functionality of R and Python for data science.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.02765</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.02765</id><submitter>Zhuoran Ji</submitter><version version=\"v1\"><date>Fri, 6 Sep 2019 08:36:05 GMT</date><size>403kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 06:04:24 GMT</date><size>546kb</size><source_type>D</source_type></version><title>ILP-M Conv: Optimize Convolution Algorithm for Single-Image Convolution\\n  Neural Network Inference on Mobile GPUs</title><authors>Zhuoran Ji</authors><categories>cs.DC cs.CV cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolution neural networks are widely used for mobile applications. However,\\nGPU convolution algorithms are designed for mini-batch neural network training,\\nthe single-image convolution neural network inference algorithm on mobile GPUs\\nis not well-studied. After discussing the usage difference and examining the\\nexisting convolution algorithms, we proposed the HNTMP convolution algorithm.\\nThe HNTMP convolution algorithm achieves $14.6 \\\\times$ speedup than the most\\npopular \\\\textit{im2col} convolution algorithm, and $2.30 \\\\times$ speedup than\\nthe fastest existing convolution algorithm (direct convolution) as far as we\\nknow.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.03039</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.03039</id><submitter>Jonas Kemp</submitter><version version=\"v1\"><date>Fri, 6 Sep 2019 17:49:56 GMT</date><size>2781kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 20:04:00 GMT</date><size>2780kb</size><source_type>D</source_type></version><title>Improved Patient Classification with Language Model Pretraining Over\\n  Clinical Notes</title><authors>Jonas Kemp, Alvin Rajkomar, Andrew M. Dai</authors><categories>cs.LG cs.CL stat.ML</categories><comments>Accepted at NeurIPS ML4H 2019, extended abstract track</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clinical notes in electronic health records contain highly heterogeneous\\nwriting styles, including non-standard terminology or abbreviations. Using\\nthese notes in predictive modeling has traditionally required preprocessing\\n(e.g. taking frequent terms or topic modeling) that removes much of the\\nrichness of the source data. We propose a pretrained hierarchical recurrent\\nneural network model that parses minimally processed clinical notes in an\\nintuitive fashion, and show that it improves performance for multiple\\nclassification tasks on the Medical Information Mart for Intensive Care III\\n(MIMIC-III) dataset, improving top-5 recall to 89.7% (increase of 4.8%) for\\nprimary diagnosis classification and AUPRC to 35.2% (increase of 2.1%) for\\nmultilabel diagnosis classification compared to models that treat the notes as\\nan unordered collection of terms, using no pretraining. We also apply an\\nattribution technique to several examples to identify the words and the nearby\\ncontext that the model uses to make its prediction, and show the importance of\\nthe words\\' context.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.03559</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.03559</id><submitter>Espen Sande</submitter><version version=\"v1\"><date>Sun, 8 Sep 2019 22:54:28 GMT</date><size>63kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 16:25:08 GMT</date><size>82kb</size><source_type>D</source_type></version><title>Explicit error estimates for spline approximation of arbitrary\\n  smoothness in isogeometric analysis</title><authors>Espen Sande, Carla Manni and Hendrik Speleers</authors><categories>math.NA cs.NA</categories><comments>39 pages, 4 figures. Improved the presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we provide a priori error estimates with explicit constants for\\nboth the $L^2$-projection and the Ritz projection onto spline spaces of\\narbitrary smoothness defined on arbitrary grids. This extends and completes the\\nresults recently obtained for spline spaces of maximal smoothness. The\\npresented error estimates indicate that smoother spline spaces exhibit a better\\napproximation behavior per degree of freedom, even for low smoothness of the\\nfunctions to be approximated. This is in complete agreement with the numerical\\nevidence found in the literature. We begin with presenting results for\\nunivariate spline spaces, and then we address multivariate tensor-product\\nspline spaces and isogeometric spline spaces generated by means of a mapped\\ngeometry, both in the single-patch and in the multi-patch case.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.03615</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.03615</id><submitter>Chun-Ting Liu</submitter><version version=\"v1\"><date>Mon, 9 Sep 2019 03:28:55 GMT</date><size>488kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 01:55:48 GMT</date><size>694kb</size><source_type>D</source_type></version><title>Neural Architecture Search in Embedding Space</title><authors>Chun-Ting Liu</authors><categories>cs.LG stat.ML</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The neural architecture search (NAS) algorithm with reinforcement learning\\ncan be a powerful and novel framework for the automatic discovering process of\\nneural architectures. However, its application is restricted by noncontinuous\\nand high-dimensional search spaces, which result in difficulty in optimization.\\nTo resolve these problems, we proposed NAS in embedding space (NASES), which is\\na novel framework. Unlike other NAS with reinforcement learning approaches that\\nsearch over a discrete and high-dimensional architecture space, this approach\\nenables reinforcement learning to search in an embedding space by using\\narchitecture encoders and decoders. The current experiment demonstrated that\\nthe performance of the final architecture network using the NASES procedure is\\ncomparable with that of other popular NAS approaches for the image\\nclassification task on CIFAR-10. The beneficial-performance and effectiveness\\nof NASES was impressive even when only the architecture-embedding searching and\\npre-training controller were applied without other NAS tricks such as parameter\\nsharing. Specifically, considerable reduction in searches was achieved by\\nreducing the average number of searching to &lt; 100 architectures to achieve a\\nfinal architecture for the NASES procedure.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.03812</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.03812</id><submitter>Anastasia Ingacheva</submitter><version version=\"v1\"><date>Mon, 9 Sep 2019 12:45:19 GMT</date><size>6201kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 6 Oct 2019 07:41:38 GMT</date><size>5548kb</size><source_type>D</source_type></version><title>HoughNet: neural network architecture for vanishing points detection</title><authors>Alexander Sheshkus, Anastasia Ingacheva, Vladimir Arlazarov, Dmitry\\n  Nikolaev</authors><categories>cs.CV cs.AI</categories><comments>6 pages, 6 figures, 2 tables, 28 references, conference</comments><journal-ref>15th International Conference on Document Analysis and Recognition\\n  (ICDAR 2019)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a novel neural network architecture based on Fast\\nHough Transform layer. The layer of this type allows our neural network to\\naccumulate features from linear areas across the entire image instead of local\\nareas. We demonstrate its potential by solving the problem of vanishing points\\ndetection in the images of documents. Such problem occurs when dealing with\\ncamera shots of the documents in uncontrolled conditions. In this case, the\\ndocument image can suffer several specific distortions including projective\\ntransform. To train our model, we use MIDV-500 dataset and provide testing\\nresults. The strong generalization ability of the suggested method is proven\\nwith its applying to a completely different ICDAR 2011 dewarping contest. In\\npreviously published papers considering these dataset authors measured the\\nquality of vanishing point detection by counting correctly recognized words\\nwith open OCR engine Tesseract. To compare with them, we reproduce this\\nexperiment and show that our method outperforms the state-of-the-art result.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.03835</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.03835</id><submitter>Haochuan Lu</submitter><version version=\"v1\"><date>Fri, 6 Sep 2019 10:15:21 GMT</date><size>3135kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 26 Sep 2019 11:47:40 GMT</date><size>3135kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sat, 28 Sep 2019 09:55:00 GMT</date><size>3133kb</size><source_type>D</source_type></version><title>Data Sanity Check for Deep Learning Systems via Learnt Assertions</title><authors>Haochuan Lu and Huanlin Xu and Nana Liu and Yangfan Zhou and Xin Wang</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reliability is a critical consideration to DL-based systems. But the\\nstatistical nature of DL makes it quite vulnerable to invalid inputs, i.e.,\\nthose cases that are not considered in the training phase of a DL model. This\\npaper proposes to perform data sanity check to identify invalid inputs, so as\\nto enhance the reliability of DL-based systems. We design and implement a tool\\nto detect behavior deviation of a DL model when processing an input case. This\\ntool extracts the data flow footprints and conducts an assertion-based\\nvalidation mechanism. The assertions are built automatically, which are\\nspecifically-tailored for DL model data flow analysis. Our experiments\\nconducted with real-world scenarios demonstrate that such an assertion-based\\ndata sanity check mechanism is effective in identifying invalid input cases.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.04242</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.04242</id><submitter>Guanhua Zhang</submitter><version version=\"v1\"><date>Tue, 10 Sep 2019 02:35:34 GMT</date><size>32kb</size></version><version version=\"v2\"><date>Sat, 5 Oct 2019 14:40:31 GMT</date><size>32kb</size></version><title>Mitigating Annotation Artifacts in Natural Language Inference Datasets\\n  to Improve Cross-dataset Generalization Ability</title><authors>Guanhua Zhang, Bing Bai, Junqi Zhang, Kun Bai, Conghui Zhu, Tiejun\\n  Zhao</authors><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural language inference (NLI) aims at predicting the relationship between\\na given pair of premise and hypothesis. However, several works have found that\\nthere widely exists a bias pattern called annotation artifacts in NLI datasets,\\nmaking it possible to identify the label only by looking at the hypothesis.\\nThis irregularity makes the evaluation results over-estimated and affects\\nmodels\\' generalization ability. In this paper, we consider a more trust-worthy\\nsetting, i.e., cross-dataset evaluation. We explore the impacts of annotation\\nartifacts in cross-dataset testing. Furthermore, we propose a training\\nframework to mitigate the impacts of the bias pattern. Experimental results\\ndemonstrate that our methods can alleviate the negative effect of the artifacts\\nand improve the generalization ability of models.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.04261</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.04261</id><submitter>Bo Wang</submitter><version version=\"v1\"><date>Tue, 10 Sep 2019 03:26:01 GMT</date><size>763kb</size><source_type>D</source_type></version><title>Bayesian Network Based Risk and Sensitivity Analysis for Production\\n  Process Stability Control</title><authors>Wei Xie and Bo Wang and Cheng Li and Jared Auclair and Peter Baker</authors><categories>stat.ML cs.LG cs.SY eess.SY</categories><comments>42 pages, 4 figures, under review, submitted to European Journal of\\n  Operational Research (EJOR)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The biomanufacturing industry is growing rapidly and becoming one of the key\\ndrivers of personalized medicine and life science. However, biopharmaceutical\\nproduction faces critical challenges, including complexity, high variability,\\nlong lead time and rapid changes in technologies, processes, and regulatory\\nenvironment. Driven by these challenges, we explore the bio-technology domain\\nknowledge and propose a rigorous risk and sensitivity analysis framework for\\nbiomanufacturing innovation. Built on the causal relationships of raw material\\nquality attributes, production process, and bio-drug properties in safety and\\nefficacy, we develop a Bayesian Network (BN) to model the complex probabilistic\\ninterdependence between process parameters and quality attributes of raw\\nmaterials/in-process materials/drug substance. It integrates various sources of\\ndata and leads to an interpretable probabilistic knowledge graph of the\\nend-to-end production process. Then, we introduce a systematic risk analysis to\\nassess the criticality of process parameters and quality attributes. The\\ncomplex production processes often involve many process parameters and quality\\nattributes impacting the product quality variability. However, the real-world\\n(batch) data are often limited, especially for customized and personalized\\nbio-drugs. We propose uncertainty quantification and sensitivity analysis to\\nanalyze the impact of model risk. Given very limited process data, the\\nempirical results show that we can provide reliable and interpretable risk and\\nsensitivity analysis. Thus, the proposed framework can provide the science- and\\nrisk-based guidance on the process monitoring, data collection, and process\\nparameters specifications to facilitate the production process learning and\\nstability control.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.04605</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.04605</id><submitter>Jose Rodrigues Jr</submitter><version version=\"v1\"><date>Tue, 10 Sep 2019 16:30:24 GMT</date><size>522kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 11 Sep 2019 12:05:15 GMT</date><size>521kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 30 Sep 2019 08:45:00 GMT</date><size>521kb</size><source_type>D</source_type></version><title>Patient trajectory prediction in the Mimic-III dataset, challenges and\\n  pitfalls</title><authors>Jose F Rodrigues-Jr, Gabriel Spadon, Bruno Brandoli, Sihem Amer-Yahia</authors><categories>cs.LG cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated medical prognosis has gained interest as artificial intelligence\\nevolves and the potential for computer-aided medicine becomes evident.\\nNevertheless, it is challenging to design an effective system that, given a\\npatient\\'s medical history, is able to predict probable future conditions.\\nPrevious works, mostly carried out over private datasets, have tackled the\\nproblem by using artificial neural network architectures that cannot deal with\\nlow-cardinality datasets, or by means of non-generalizable inference\\napproaches. We introduce a Deep Learning architecture whose design results from\\nan intensive experimental process. The final architecture is based on two\\nparallel Minimal Gated Recurrent Unit networks working in bi-directional\\nmanner, which was extensively tested with the open-access Mimic-III dataset.\\nOur results demonstrate significant improvements in automated medical\\nprognosis, as measured with Recall@k. We summarize our experience as a set of\\nrelevant insights for the design of Deep Learning architectures. Our work\\nimproves the performance of computer-aided medicine and can serve as a guide in\\ndesigning artificial neural networks used in prediction tasks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.04797</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.04797</id><submitter>Raunak Dey</submitter><version version=\"v1\"><date>Wed, 11 Sep 2019 00:11:14 GMT</date><size>3572kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 14 Sep 2019 16:38:34 GMT</date><size>3572kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 8 Oct 2019 05:25:43 GMT</date><size>3572kb</size><source_type>D</source_type></version><title>Hybrid Cascaded Neural Network for Liver Lesion Segmentation</title><authors>Raunak Dey, Yi Hong</authors><categories>eess.IV cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic liver lesion segmentation is a challenging task while having a\\nsignificant impact on assisting medical professionals in the designing of\\neffective treatment and planning proper care. In this paper we propose a\\ncascaded system that combines both 2D and 3D convolutional neural networks to\\neffectively segment hepatic lesions. Our 2D network operates on a slice by\\nslice basis to segment the liver and larger tumors, while we use a 3D network\\nto detect small lesions that are often missed in a 2D segmentation design. We\\nemploy this algorithm on the LiTS challenge obtaining a Dice score per case of\\n68.1%, which performs the best among all non pre-trained models and the second\\nbest among published methods. We also perform two-fold cross-validation to\\nreveal the over- and under-segmentation issues in the LiTS annotations.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.04961</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.04961</id><submitter>Silvia Rossi</submitter><version version=\"v1\"><date>Wed, 11 Sep 2019 10:29:58 GMT</date><size>168kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 13:42:22 GMT</date><size>234kb</size><source_type>D</source_type></version><title>Emotional Distraction for Children Anxiety Reduction During Vaccination</title><authors>Martina Ruocco, Marwa Larafa, Silvia Rossi</authors><categories>cs.RO cs.HC</categories><report-no>SREC/2019/08</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social assistive robots are starting to be widely used in pediatric\\nhealth-care environments with the aim of distracting and entertaining children,\\nand so of reducing a possible state of anxiety. In this paper, we present some\\ninitial results of a study (N=69) conducted in a Health-Vaccines Center, where\\nthe distraction role of a social robot, which interacts with a child showing an\\nemotional behavior, is compared with the same not showing any emotional social\\ncue. Outcome criteria for the evaluation of the intervention included the\\nparents reported level of anxiety before, during and after the procedure.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.05299</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.05299</id><submitter>Yuta Saito</submitter><version version=\"v1\"><date>Wed, 11 Sep 2019 18:43:32 GMT</date><size>43kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 07:11:53 GMT</date><size>43kb</size><source_type>D</source_type></version><title>Counterfactual Cross-Validation: Effective Causal Model Selection from\\n  Observational Data</title><authors>Yuta Saito, Shota Yasui</authors><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What is the most effective way to select the best causal model among\\npotential candidates? In this paper, we propose a method to effectively select\\nthe best individual-level treatment effect (ITE) predictors from a set of\\ncandidates using only an observational validation set. In model selection or\\nhyperparameter tuning, we are interested in choosing the best model or the\\nvalue of hyperparameter from potential candidates. Thus, we focus on accurately\\npreserving the rank order of the ITE prediction performance of candidate causal\\nmodels. The proposed evaluation metric is theoretically proved to preserve the\\ntrue ranking of the model performance in expectation and to minimize the upper\\nbound of the finite sample uncertainty in model selection. Consistent with the\\ntheoretical result, empirical experiments demonstrate that our proposed method\\nis more likely to select the best model and set of hyperparameter in both model\\nselection and hyperparameter tuning.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.05630</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.05630</id><submitter>Walid Abdullah Al</submitter><version version=\"v1\"><date>Mon, 2 Sep 2019 09:12:36 GMT</date><size>6394kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 04:28:33 GMT</date><size>6436kb</size><source_type>D</source_type></version><title>Reinforcing Medical Image Classifier to Improve Generalization on Small\\n  Datasets</title><authors>Walid Abdullah Al, Il Dong Yun</authors><categories>cs.LG cs.CV eess.IV stat.ML</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advents of deep learning, improved image classification with complex\\ndiscriminative models has been made possible. However, such deep models with\\nincreased complexity require a huge set of labeled samples to generalize the\\ntraining. Such classification models can easily overfit when applied for\\nmedical images because of limited training data, which is a common problem in\\nthe field of medical image analysis. This paper proposes and investigates a\\nreinforced classifier for improving the generalization under a few available\\ntraining data. Partially following the idea of reinforcement learning, the\\nproposed classifier uses a generalization-feedback from a subset of the\\ntraining data to update its parameter instead of only using the conventional\\ncross-entropy loss about the training data. We evaluate the improvement of the\\nproposed classifier by applying it on three different classification problems\\nagainst the standard deep classifiers equipped with existing\\noverfitting-prevention techniques. Besides an overall improvement in\\nclassification performance, the proposed classifier showed remarkable\\ncharacteristics of generalized learning, which can have great potential in\\nmedical classification tasks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.05665</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.05665</id><submitter>Sangjae Bae</submitter><version version=\"v1\"><date>Mon, 9 Sep 2019 18:01:36 GMT</date><size>783kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 06:34:25 GMT</date><size>1431kb</size><source_type>D</source_type></version><title>Cooperation-Aware Lane Change Maneuver in Dense Traffic based on Model\\n  Predictive Control with Recurrent Neural Network</title><authors>Sangjae Bae, Dhruv Saxena, Alireza Nakhaei, Chiho Choi, Kikuo\\n  Fujimura, and Scott Moura</authors><categories>cs.RO cs.AI</categories><comments>Submitted to 2020 American Control Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a real-time lane change control framework of autonomous\\ndriving in dense traffic, which exploits cooperative behaviors of other\\ndrivers. This paper focuses on heavy traffic where vehicles cannot change lanes\\nwithout cooperating with other drivers. In this case, classical robust controls\\nmay not apply since there is no safe area to merge to without interacting with\\nthe other drivers. That said, modeling complex and interactive human behaviors\\nis highly non-trivial from the perspective of control engineers. We propose a\\nmathematical control framework based on Model Predictive Control (MPC)\\nencompassing a state-of-the-art Recurrent Neural network (RNN) architecture. In\\nparticular, RNN predicts interactive motions of other drivers in response to\\npotential actions of the autonomous vehicle, which are then systematically\\nevaluated in safety constraints. We also propose a real-time heuristic\\nalgorithm to find locally optimal control inputs. Finally, quantitative and\\nqualitative analysis on simulation studies are presented to illustrate the\\nbenefits of the proposed framework.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.05738</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.05738</id><submitter>Anthony Bagnall Dr</submitter><version version=\"v1\"><date>Thu, 12 Sep 2019 15:01:30 GMT</date><size>76kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 16 Sep 2019 19:38:02 GMT</date><size>85kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 7 Oct 2019 10:46:27 GMT</date><size>94kb</size><source_type>D</source_type></version><title>A tale of two toolkits, report the first: benchmarking time series\\n  classification algorithms for correctness and efficiency</title><authors>Anthony Bagnall, Franz Kir\\\\\\'aly, Markus L\\\\&quot;oning, Matthew Middlehurst\\n  and George Oastler</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  sktime is an open source, Python based, sklearn compatible toolkit for time\\nseries analysis developed by researchers at the University of East Anglia\\n(UEA), University College London and the Alan Turing Institute. A key initial\\ngoal for sktime was to provide time series classification functionality\\nequivalent to that available in a related java package, tsml, also developed at\\nUEA. We describe the implementation of six such classifiers in sktime and\\ncompare them to their tsml equivalents. We demonstrate correctness through\\nequivalence of accuracy on a range of standard test problems and compare the\\nbuild time of the different implementations. We find that there is significant\\ndifference in accuracy on only one of the six algorithms we look at (Proximity\\nForest). This difference is causing us some pain in debugging. We found a much\\nwider range of difference in efficiency. Again, this was not unexpected, but it\\ndoes highlight ways both toolkits could be improved.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.05773</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.05773</id><submitter>Tomer Weiss</submitter><version version=\"v1\"><date>Thu, 12 Sep 2019 16:10:31 GMT</date><size>9251kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 12:01:38 GMT</date><size>9251kb</size><source_type>D</source_type></version><title>PILOT: Physics-Informed Learned Optimal Trajectories for Accelerated MRI</title><authors>Tomer Weiss, Ortal Senouf, Sanketh Vedula, Oleg Michailovich, Michael\\n  Zibulevsky, Alex Bronstein</authors><categories>eess.IV cs.CV physics.med-ph</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Magnetic Resonance Imaging (MRI) has long been considered to be among &quot;the\\ngold standards&quot; of diagnostic medical imaging. The long acquisition times,\\nhowever, render MRI prone to motion artifacts, let alone their adverse\\ncontribution to the relative high costs of MRI examination. Over the last few\\ndecades, multiple studies have focused on the development of both physical and\\npost-processing methods for accelerated acquisition of MRI scans. These two\\napproaches, however, have so far been addressed separately. On the other hand,\\nrecent works in optical computational imaging have demonstrated growing success\\nof concurrent learning-based design of data acquisition and image\\nreconstruction schemes. In this work, we propose a novel approach to the\\nlearning of optimal schemes for conjoint acquisition and reconstruction of MRI\\nscans, with the optimization carried out simultaneously with respect to the\\ntime-efficiency of data acquisition and the quality of resulting\\nreconstructions. To be of a practical value, the schemes are encoded in the\\nform of general k-space trajectories, whose associated magnetic gradients are\\nconstrained to obey a set of predefined hardware requirements (as defined in\\nterms of, e.g., peak currents and maximum slew rates of magnetic gradients).\\nWith this proviso in mind, we propose a novel algorithm for the end-to-end\\ntraining of a combined acquisition-reconstruction pipeline using a deep neural\\nnetwork with differentiable forward- and back-propagation operators. We also\\ndemonstrate the effectiveness of the proposed solution in application to both\\nimage reconstruction and image segmentation, reporting substantial improvements\\nin terms of acceleration factors as well as the quality of these end tasks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.05886</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.05886</id><submitter>Lingda Wang</submitter><version version=\"v1\"><date>Thu, 12 Sep 2019 18:04:12 GMT</date><size>320kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 16 Sep 2019 14:19:02 GMT</date><size>321kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 4 Oct 2019 15:06:18 GMT</date><size>320kb</size><source_type>D</source_type></version><version version=\"v4\"><date>Mon, 7 Oct 2019 15:00:06 GMT</date><size>320kb</size><source_type>D</source_type></version><title>Be Aware of Non-Stationarity: Nearly Optimal Algorithms for\\n  Piecewise-Stationary Cascading Bandits</title><authors>Lingda Wang, Huozhi Zhou, Bingcong Li, Lav R. Varshney, Zhizhen Zhao</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cascading bandit (CB) is a variant of both the multi-armed bandit (MAB) and\\nthe cascade model (CM), where a learning agent aims to maximize the total\\nreward by recommending $K$ out of $L$ items to a user. We focus on a common\\nreal-world scenario where the user\\'s preference can change in a\\npiecewise-stationary manner. Two efficient algorithms, \\\\texttt{GLRT-CascadeUCB}\\nand \\\\texttt{GLRT-CascadeKL-UCB}, are developed. The key idea behind the\\nproposed algorithms is incorporating an almost parameter-free change-point\\ndetector, the Generalized Likelihood Ratio Test (GLRT), within classical upper\\nconfidence bound (UCB) based algorithms. Gap-dependent regret upper bounds of\\nthe proposed algorithms are derived, both on the order of\\n$\\\\mathcal{O}(\\\\sqrt{NLT\\\\log{T}})$, where $N$ is the number of\\npiecewise-stationary segments, and $T$ is the time horizon. We also derive a\\nminimax lower bound on the order of $\\\\mathcal{O}(\\\\sqrt{NLT})$ for\\npiecewise-stationary CB, showing that our proposed algorithms are optimal up to\\na poly-logarithmic factor $\\\\sqrt{\\\\log T}$. Lastly, we present numerical\\nexperiments on both synthetic and real-world datasets to show that\\n\\\\texttt{GLRT-CascadeUCB} and \\\\texttt{GLRT-CascadeKL-UCB} outperform\\nstate-of-the-art algorithms in the literature.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.06317</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.06317</id><submitter>Shigeki Karita</submitter><version version=\"v1\"><date>Fri, 13 Sep 2019 16:27:08 GMT</date><size>883kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 11:11:38 GMT</date><size>889kb</size><source_type>D</source_type></version><title>A Comparative Study on Transformer vs RNN in Speech Applications</title><authors>Shigeki Karita, Nanxin Chen, Tomoki Hayashi, Takaaki Hori, Hirofumi\\n  Inaguma, Ziyan Jiang, Masao Someki, Nelson Enrique Yalta Soplin, Ryuichi\\n  Yamamoto, Xiaofei Wang, Shinji Watanabe, Takenori Yoshimura, Wangyou Zhang</authors><categories>cs.CL cs.SD eess.AS</categories><comments>Accepted at ASRU 2019</comments><journal-ref>IEEE Automatic Speech Recognition and Understanding Workshop 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequence-to-sequence models have been widely used in end-to-end speech\\nprocessing, for example, automatic speech recognition (ASR), speech translation\\n(ST), and text-to-speech (TTS). This paper focuses on an emergent\\nsequence-to-sequence model called Transformer, which achieves state-of-the-art\\nperformance in neural machine translation and other natural language processing\\napplications. We undertook intensive studies in which we experimentally\\ncompared and analyzed Transformer and conventional recurrent neural networks\\n(RNN) in a total of 15 ASR, one multilingual ASR, one ST, and two TTS\\nbenchmarks. Our experiments revealed various training tips and significant\\nperformance benefits obtained with Transformer for each task including the\\nsurprising superiority of Transformer in 13/15 ASR benchmarks in comparison\\nwith RNN. We are preparing to release Kaldi-style reproducible recipes using\\nopen source and publicly available datasets for all the ASR, ST, and TTS tasks\\nfor the community to succeed our exciting outcomes.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.06351</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.06351</id><submitter>Desh Raj</submitter><version version=\"v1\"><date>Fri, 13 Sep 2019 17:56:13 GMT</date><size>115kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 22:55:18 GMT</date><size>116kb</size><source_type>D</source_type></version><title>Probing the Information Encoded in X-vectors</title><authors>Desh Raj, David Snyder, Daniel Povey, Sanjeev Khudanpur</authors><categories>eess.AS cs.CL cs.SD</categories><comments>Accepted at IEEE Workshop on Automatic Speech Recognition and\\n  Understanding (ASRU) 2019</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Deep neural network based speaker embeddings, such as x-vectors, have been\\nshown to perform well in text-independent speaker recognition/verification\\ntasks. In this paper, we use simple classifiers to investigate the contents\\nencoded by x-vector embeddings. We probe these embeddings for information\\nrelated to the speaker, channel, transcription (sentence, words, phones), and\\nmeta information about the utterance (duration and augmentation type), and\\ncompare these with the information encoded by i-vectors across a varying number\\nof dimensions. We also study the effect of data augmentation during extractor\\ntraining on the information captured by x-vectors. Experiments on the RedDots\\ndata set show that x-vectors capture spoken content and channel-related\\ninformation, while performing well on speaker verification tasks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.06436</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.06436</id><submitter>Isaac Gerg</submitter><version version=\"v1\"><date>Fri, 13 Sep 2019 20:30:17 GMT</date><size>9032kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 20:21:33 GMT</date><size>9032kb</size><source_type>D</source_type></version><title>Coupling Rendering and Generative Adversarial Networks for Artificial\\n  SAS Image Generation</title><authors>Albert Reed, Isaac Gerg, John McKay, Daniel Brown, David Williams, and\\n  Suren Jayasuriya</authors><categories>eess.IV cs.CV</categories><comments>10 pages, 9 figures. Submitted to IEEE OCEANS 2019 (Seattle). Updated\\n  acknowledgements</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Acquisition of Synthetic Aperture Sonar (SAS) datasets is bottlenecked by the\\ncostly deployment of SAS imaging systems, and even when data acquisition is\\npossible,the data is often skewed towards containing barren seafloor rather\\nthan objects of interest. We present a novel pipeline, called SAS GAN, which\\ncouples an optical renderer with a generative adversarial network (GAN) to\\nsynthesize realistic SAS images of targets on the seafloor. This coupling\\nenables high levels of SAS image realism while enabling control over image\\ngeometry and parameters. We demonstrate qualitative results by presenting\\nexamples of images created with our pipeline. We also present quantitative\\nresults through the use of t-SNE and the Fr\\\\\\'echet Inception Distance to argue\\nthat our generated SAS imagery potentially augments SAS datasets more\\neffectively than an off-the-shelf GAN.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.06474</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.06474</id><submitter>Wenjun Mei</submitter><version version=\"v1\"><date>Fri, 13 Sep 2019 22:19:34 GMT</date><size>674kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 18 Sep 2019 20:17:15 GMT</date><size>674kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 30 Sep 2019 16:12:24 GMT</date><size>674kb</size><source_type>D</source_type></version><title>Occam\\'s Razor in Opinion Dynamics: The Weighted-Median Influence Process</title><authors>Wenjun Mei, Francesco Bullo, Ge Chen, Florian D\\\\&quot;orfler</authors><categories>cs.SI cs.SY eess.SY math.DS</categories><msc-class>91C99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays public opinion formation is deeply influenced by social networks and\\nfaces unprecedented challenges such as opinion radicalization, echo chambers,\\nand ideologization of public debates. Mathematical modeling of opinion dynamics\\nplays a fundamental role in understanding the microscopic mechanisms of social\\ninteractions behind these macroscopic phenomena. The weighted-averaging opinion\\nupdate is arguably the most widely adopted microscopic mechanism for opinion\\ndynamics. However, such models based on weighted averaging are restricted in\\ntheir predictive power and limited to stylized continuous opinion spectra. Here\\nwe point out that these models\\' limitation in predictability is not due to the\\nlack of complexity, but because the weighted-averaging mechanism itself\\nfeatures a non-negligible unrealistic implication. By resolving this\\nunrealistic feature in the framework of cognitive dissonance theory, we propose\\na novel opinion dynamics model based on a weighted-median mechanism instead.\\nSurprisingly, such an inconspicuous change in microscopic mechanism leads to\\ndramatic macroscopic consequences. In the spirit of Occam\\'s razor, our new\\nmodel, despite its simplicity in form, exhibits a sophisticated\\nconsensus-disagreement phase transition depending on the influence network\\nstructure. Our model gives perhaps the simplest answers to various open\\nproblems in sociology and political science, such as the connection between\\nsocial marginalization and opinion radicalization, the mechanism for echo\\nchambers, and the formation of multipolar opinion distributions. Remarkably,\\nthe weighted-median opinion dynamics are the first model applicable to ordered\\nmultiple-choice issues, which are prevalent in modern-day public debates and\\nelections.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.06614</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.06614</id><submitter>Qiujia Li</submitter><version version=\"v1\"><date>Sat, 14 Sep 2019 15:40:27 GMT</date><size>332kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 11:09:08 GMT</date><size>332kb</size><source_type>D</source_type></version><title>Integrating Source-channel and Attention-based Sequence-to-sequence\\n  Models for Speech Recognition</title><authors>Qiujia Li, Chao Zhang, Philip C. Woodland</authors><categories>eess.AS cs.CL cs.LG cs.SD</categories><comments>To appear in Proc. ASRU2019, December 14-18, 2019, Sentosa, Singapore</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel automatic speech recognition (ASR) framework\\ncalled Integrated Source-Channel and Attention (ISCA) that combines the\\nadvantages of traditional systems based on the noisy source-channel model (SC)\\nand end-to-end style systems using attention-based sequence-to-sequence models.\\nThe traditional SC system framework includes hidden Markov models and\\nconnectionist temporal classification (CTC) based acoustic models, language\\nmodels (LMs), and a decoding procedure based on a lexicon, whereas the\\nend-to-end style attention-based system jointly models the whole process with a\\nsingle model. By rescoring the hypotheses produced by traditional systems using\\nend-to-end style systems based on an extended noisy source-channel model, ISCA\\nallows structured knowledge to be easily incorporated via the SC-based model\\nwhile exploiting the complementarity of the attention-based model. Experiments\\non the AMI meeting corpus show that ISCA is able to give a relative word error\\nrate reduction up to 21% over an individual system, and by 13% over an\\nalternative method which also involves combining CTC and attention-based\\nmodels.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.06709</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.06709</id><submitter>Silas Fong</submitter><version version=\"v1\"><date>Sun, 15 Sep 2019 01:49:40 GMT</date><size>3953kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 14:35:48 GMT</date><size>3953kb</size><source_type>D</source_type></version><title>Low-Latency Network-Adaptive Error Control for Interactive Streaming</title><authors>Silas L. Fong, Salma Emara, Baochun Li, Ashish Khisti, Wai-Tian Tan,\\n  Xiaoqing Zhu, and John Apostolopoulos</authors><categories>cs.NI</categories><comments>14 pages, 28 figures, numerous typos of T=5 in Example 1 have been\\n  corrected to T=4, submitted to IEEE TCSVT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel network-adaptive algorithm that is suitable for\\nalleviating network packet losses for low-latency interactive communications\\nbetween a source and a destination. Our network-adaptive algorithm estimates in\\nreal time the best parameters of a recently proposed streaming code that uses\\nforward error correction (FEC) to correct both arbitrary and burst losses,\\nwhich cause crackling noise and undesirable jitters respectively in audio. In\\nparticular, the destination estimates appropriate coding parameters based on\\nits observed packet loss pattern and sends them back to the source for updating\\nthe underlying code. In addition, a new explicit construction of practical\\nlow-latency streaming codes that achieve the optimal tradeoff between the\\ncapability of correcting arbitrary losses and the capability of correcting\\nburst losses is provided. Simulation evaluations based on statistical losses\\nand real-world packet loss traces reveal the following: (i) Our proposed\\nnetwork-adaptive algorithm combined with our optimal streaming codes can\\nachieve significantly higher performance compared to uncoded and non-adaptive\\nFEC schemes over UDP (User Datagram Protocol); (ii) Our explicit streaming\\ncodes can significantly outperform traditional MDS (maximum-distance separable)\\nstreaming schemes when they are used along with our network-adaptive algorithm.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.06794</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.06794</id><submitter>N. Jesper Larsson</submitter><version version=\"v1\"><date>Sun, 15 Sep 2019 12:48:00 GMT</date><size>347kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 05:13:05 GMT</date><size>347kb</size><source_type>D</source_type></version><title>Run-Length Encoding in a Finite Universe</title><authors>N. Jesper Larsson</authors><categories>cs.IT cs.DS math.IT</categories><comments>SPIRE 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text compression schemes and compact data structures usually combine\\nsophisticated probability models with basic coding methods whose average\\ncodeword length closely match the entropy of known distributions. In the\\nfrequent case where basic coding represents run-lengths of outcomes that have\\nprobability $p$, i.e. the geometric distribution $\\\\Pr(i)=p^i(1-p)$, a\\n\\\\emph{Golomb code} is an optimal instantaneous code, which has the additional\\nadvantage that codewords can be computed using only an integer parameter\\ncalculated from $p$, without need for a large or sophisticated data structure.\\nGolomb coding does not, however, gracefully handle the case where run-lengths\\nare bounded by a known integer~$n$. In this case, codewords allocated for the\\ncase $i&gt;n$ are wasted. While negligible for large $n$, this makes Golomb coding\\nunattractive in situations where $n$ is recurrently small, e.g., when\\nrepresenting many short lists of integers drawn from limited ranges, or when\\nthe range of $n$ is narrowed down by a recursive algorithm. We address the\\nproblem of choosing a code for this case, considering efficiency from both\\ninformation-theoretic and computational perspectives, and arrive at a simple\\ncode that allows computing a codeword using only $O(1)$ simple computer\\noperations and $O(1)$ machine words. We demonstrate experimentally that the\\nresulting representation length is very close (equal in a majority of tested\\ncases) to the optimal Huffman code, to the extent that the expected difference\\nis practically negligible. We describe efficient branch-free implementation of\\nencoding and decoding.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.06806</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.06806</id><submitter>Thomas Nedelec</submitter><version version=\"v1\"><date>Sun, 15 Sep 2019 14:08:34 GMT</date><size>207kb</size></version><version version=\"v2\"><date>Fri, 4 Oct 2019 15:34:17 GMT</date><size>207kb</size></version><title>Adversarial learning for revenue-maximizing auctions</title><authors>Thomas Nedelec, Jules Baudet, Vianney Perchet, Noureddine El Karoui</authors><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new numerical framework to learn optimal bidding strategies in\\nrepeated auctions when the seller uses past bids to optimize her mechanism.\\nCrucially, we do not assume that the bidders know what optimization mechanism\\nis used by the seller. We recover essentially all state-of-the-art analytical\\nresults for the single-item framework derived previously in the setup where the\\nbidder knows the optimization mechanism used by the seller and extend our\\napproach to multi-item settings, in which no optimal shading strategies were\\npreviously known. Our approach yields substantial increases in bidder utility\\nin all settings. Our approach also has a strong potential for practical usage\\nsince it provides a simple way to optimize bidding strategies on modern\\nmarketplaces where buyers face unknown data-driven mechanisms.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.06809</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.06809</id><submitter>Siyuan Ji</submitter><version version=\"v1\"><date>Sun, 15 Sep 2019 14:14:53 GMT</date><size>270kb</size></version><version version=\"v2\"><date>Mon, 30 Sep 2019 14:52:30 GMT</date><size>270kb</size></version><title>Using Model Theory for Architecture Definition in Complex System Design</title><authors>Charles E. Dickerson, Michael K. Wilkinson, Eugenie Hunsicker, Siyuan\\n  Ji, Yves Bernard, and Peter Denno</authors><categories>cs.SE</categories><comments>This abbreviated version of the paper has been created for the INCOSE\\n  UK Chapter Architecture Working Group</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Architecture Definition, which is central to system design, is one of the two\\nmost used technical processes in the practice of model based systems\\nengineering. In this paper a fundamental approach to architecture definition is\\npresented and demonstrated. The success of its application to engineering\\nproblems depends on a precise but practical definition of the term\\narchitecture. In the standard for Architecture Description, ISO/IEC/IEEE\\n42010:2011, a definition was adopted that has been subsumed into later\\nstandards. In 2018 the working group JTC1/SC7/WG42 on System Architecture began\\na review of the standard, holding sessions late in the year. This paper extends\\nand complements a position paper submitted during the meetings, in which Tarski\\nmodel theory in conjunction with ISO/IEC 24707:2018 (logic-based languages) was\\nused to better understand relationships between system models and concepts\\nrelated to architecture. Definitions of architecture and system are now offered\\nindependent of the working group that have a mathematical foundation but are\\nstated in simple intuitive terms. The nature of the definitions supports a\\nfundamental expression of architecture definition that can be applied\\nthroughout the system lifecycle. The engineering utility and benefits to\\ncomplex system design are demonstrated in a diesel engine emissions reduction\\ncase study.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.06845</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.06845</id><submitter>Diederik Aerts</submitter><version version=\"v1\"><date>Sun, 15 Sep 2019 17:40:57 GMT</date><size>1083kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 5 Oct 2019 17:15:59 GMT</date><size>1112kb</size><source_type>D</source_type></version><title>Quantum Structure in Cognition: Human Language as a Boson Gas of\\n  Entangled Words</title><authors>Diederik Aerts and Lester Beltran</authors><categories>q-bio.NC cs.CL quant-ph</categories><comments>45 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We model a piece of text of human language telling a story by means of the\\nquantum structure describing a Bose gas in a state close to a Bose-Einstein\\ncondensate near absolute zero temperature. For this we introduce energy levels\\nfor the words (concepts) used in the story and we also introduce the new notion\\nof \\'cogniton\\' as the quantum of human thought. Words (concepts) are then\\ncognitons in different energy states as it is the case for photons in different\\nenergy states, or states of different radiative frequency, when the considered\\nboson gas is that of the quanta of the electromagnetic field. We show that\\nBose-Einstein statistics delivers a very good model for these pieces of texts\\ntelling stories, both for short stories and for long stories of the size of\\nnovels. We analyze an unexpected connection with Zipf\\'s law in human language,\\nthe Zipf ranking relating to the energy levels of the words, and the\\nBose-Einstein graph coinciding with the Zipf graph. We investigate the issue of\\n\\'identity and indistinguishability\\' from this new perspective and conjecture\\nthat the way one can easily understand how two of \\'the same concepts\\' are\\n\\'absolutely identical and indistinguishable\\' in human language is also the way\\nin which quantum particles are absolutely identical and indistinguishable in\\nphysical reality, providing in this way new evidence for our conceptuality\\ninterpretation of quantum theory.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.06892</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.06892</id><submitter>Shubham Jain</submitter><version version=\"v1\"><date>Sun, 15 Sep 2019 21:43:19 GMT</date><size>3268kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 03:59:26 GMT</date><size>3276kb</size><source_type>D</source_type></version><title>TiM-DNN: Ternary in-Memory accelerator for Deep Neural Networks</title><authors>Shubham Jain, Sumeet Kumar Gupta, Anand Raghunathan</authors><categories>cs.LG cs.AR cs.CV cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of lower precision has emerged as a popular technique to optimize the\\ncompute and storage requirements of complex Deep Neural Networks (DNNs). In the\\nquest for lower precision, recent studies have shown that ternary DNNs, which\\nrepresent weights and activations by signed ternary values, represent a\\npromising sweet spot, and achieve accuracy close to full-precision networks on\\ncomplex tasks such as language modeling and image classification. We propose\\nTiM-DNN, a programmable, in-memory accelerator that is specifically designed to\\nexecute ternary DNNs. TiM-DNN supports various ternary representations\\nincluding unweighted (-1,0,1), symmetric weighted (-a,0,a), and asymmetric\\nweighted (-a,0,b) ternary systems. TiM-DNN is designed using TiM tiles --\\nspecialized memory arrays that perform massively parallel signed vector-matrix\\nmultiplications on ternary values with a single access. TiM tiles are in turn\\ncomposed of Ternary Processing Cells (TPCs), new bit-cells that function as\\nboth ternary storage units and signed scalar multiplication units. We evaluate\\nan implementation of TiM-DNN in 32nm technology using an architectural\\nsimulator calibrated with SPICE simulations and RTL synthesis. TiM-DNN achieves\\na peak performance of 114 TOPs/s, consumes 0.9W power, and occupies 1.96mm2\\nchip area, representing a 300X and 388X improvement in TOPS/W and TOPS/mm2,\\nrespectively, compared to a state-of-the-art NVIDIA Tesla V100 GPU. In\\ncomparison to popular DNN accelerators, TiM-DNN achieves 55.2X-240X and\\n160X-291X improvement in TOPS/W and TOPS/mm2, respectively. We compare TiM-DNN\\nwith a well-optimized near-memory accelerator for ternary DNNs across a suite\\nof state-of-the-art DNN benchmarks including both deep convolutional and\\nrecurrent neural networks, demonstrating 3.9x-4.7x improvement in system-level\\nenergy and 3.2x-4.2x speedup.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.06988</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.06988</id><submitter>Sidhanth Mohanty</submitter><version version=\"v1\"><date>Mon, 16 Sep 2019 05:03:38 GMT</date><size>57kb</size></version><version version=\"v2\"><date>Sat, 5 Oct 2019 22:03:49 GMT</date><size>57kb</size></version><title>Explicit near-Ramanujan graphs of every degree</title><authors>Sidhanth Mohanty, Ryan O\\'Donnell, Pedro Paredes</authors><categories>cs.DS cs.DM math.CO</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For every constant $d \\\\geq 3$ and $\\\\epsilon &gt; 0$, we give a deterministic\\n$\\\\mathrm{poly}(n)$-time algorithm that outputs a $d$-regular graph on\\n$\\\\Theta(n)$ vertices that is $\\\\epsilon$-near-Ramanujan; i.e., its eigenvalues\\nare bounded in magnitude by $2\\\\sqrt{d-1} + \\\\epsilon$ (excluding the single\\ntrivial eigenvalue of~$d$).\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.07053</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.07053</id><submitter>Yuantao Fan</submitter><version version=\"v1\"><date>Mon, 16 Sep 2019 08:31:08 GMT</date><size>4309kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 23 Sep 2019 02:45:27 GMT</date><size>5634kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 30 Sep 2019 18:00:37 GMT</date><size>2867kb</size><source_type>D</source_type></version><title>Transfer learning for Remaining Useful Life Prediction Based on\\n  Consensus Self-Organizing Models</title><authors>Yuantao Fan and S{\\\\l}awomir Nowaczyk and Thorsteinn R\\\\&quot;ognvaldsson</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The traditional paradigm for developing machine prognostics usually relies on\\ngeneralization from data acquired in experiments under controlled conditions\\nprior to deployment of the equipment. Detecting or predicting failures and\\nestimating machine health in this way assumes that future field data will have\\na very similar distribution to the experiment data. However, many complex\\nmachines operate under dynamic environmental conditions and are used in many\\ndifferent ways. This makes collecting comprehensive data very challenging, and\\nthe assumption that pre-deployment data and post-deployment data follow very\\nsimilar distributions is unlikely to hold. Transfer Learning (TL) refers to\\nmethods for transferring knowledge learned in one setting (the source domain)\\nto another setting (the target domain). In this work, we present a TL method\\nfor predicting Remaining Useful Life (RUL) of equipment, under the assumption\\nthat labels are available only for the source domain and not the target domain.\\nThis setting corresponds to generalizing from a limited number of\\nrun-to-failure experiments performed prior to deployment into making\\nprognostics with data coming from deployed equipment that is being used under\\nmultiple new operating conditions and experiencing previously unseen faults. We\\nemploy a deviation detection method, Consensus Self-Organizing Models (COSMO),\\nto create transferable features for building the RUL regression model. These\\nfeatures capture how different target equipment is in comparison to its peers.\\nThe efficiency of the proposed TL method is demonstrated using the NASA\\nTurbofan Engine Degradation Simulation Data Set. Models using the COSMO\\ntransferable features show better performance than other methods on predicting\\nRUL when the target domain is more complex than the source domain.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.07061</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.07061</id><submitter>Haofeng Li</submitter><version version=\"v1\"><date>Mon, 16 Sep 2019 08:44:02 GMT</date><size>374kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 17:03:23 GMT</date><size>374kb</size><source_type>D</source_type></version><title>Motion Guided Attention for Video Salient Object Detection</title><authors>Haofeng Li, Guanqi Chen, Guanbin Li, Yizhou Yu</authors><categories>cs.CV</categories><comments>10 pages, 4 figures, ICCV 2019, code:\\n  https://github.com/lhaof/Motion-Guided-Attention</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video salient object detection aims at discovering the most visually\\ndistinctive objects in a video. How to effectively take object motion into\\nconsideration during video salient object detection is a critical issue.\\nExisting state-of-the-art methods either do not explicitly model and harvest\\nmotion cues or ignore spatial contexts within optical flow images. In this\\npaper, we develop a multi-task motion guided video salient object detection\\nnetwork, which learns to accomplish two sub-tasks using two sub-networks, one\\nsub-network for salient object detection in still images and the other for\\nmotion saliency detection in optical flow images. We further introduce a series\\nof novel motion guided attention modules, which utilize the motion saliency\\nsub-network to attend and enhance the sub-network for still images. These two\\nsub-networks learn to adapt to each other by end-to-end training. Experimental\\nresults demonstrate that the proposed method significantly outperforms existing\\nstate-of-the-art algorithms on a wide range of benchmarks. We hope our simple\\nand effective approach will serve as a solid baseline and help ease future\\nresearch in video salient object detection. Code and models will be made\\navailable.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.07088</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.07088</id><submitter>Chieh Yu Chen</submitter><version version=\"v1\"><date>Mon, 16 Sep 2019 09:45:49 GMT</date><size>7305kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 02:33:51 GMT</date><size>7305kb</size><source_type>D</source_type></version><title>BasketballGAN: Generating Basketball Play Simulation Through Sketching</title><authors>Hsin-Ying Hsieh, Chieh-Yu Chen, Yu-Shuen Wang, Jung-Hong Chuang</authors><categories>cs.MM cs.HC cs.LG</categories><comments>9 pages, Accepted paper at ACMMM 2019, code is available at\\n  https://github.com/chychen/BasketballGAN</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a data-driven basketball set play simulation. Given an offensive\\nset play sketch, our method simulates potential scenarios that may occur in the\\ngame. The simulation provides coaches and players with insights on how a given\\nset play can be executed. To achieve the goal, we train a conditional\\nadversarial network on NBA movement data to imitate the behaviors of how\\nplayers move around the court through two major components: a generator that\\nlearns to generate natural player movements based on a latent noise and a user\\nsketched set play; and a discriminator that is used to evaluate the realism of\\nthe basketball play. To improve the quality of simulation, we minimize 1.) a\\ndribbler loss to prevent the ball from drifting away from the dribbler; 2.) a\\ndefender loss to prevent the dribbler from not being defended; 3.) a ball\\npassing loss to ensure the straightness of passing trajectories; and 4) an\\nacceleration loss to minimize unnecessary players\\' movements. To evaluate our\\nsystem, we objectively compared real and simulated basketball set plays.\\nBesides, a subjective test was conducted to judge whether a set play was real\\nor generated by our network. On average, the mean correct rates to the binary\\ntests were 56.17 \\\\%. Experiment results and the evaluations demonstrated the\\neffectiveness of our system.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.07278</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.07278</id><submitter>Zhenjie Ren</submitter><version version=\"v1\"><date>Mon, 16 Sep 2019 15:31:25 GMT</date><size>38kb</size></version><version version=\"v2\"><date>Thu, 3 Oct 2019 19:41:20 GMT</date><size>58kb</size><source_type>D</source_type></version><title>Mean-field Langevin System, Optimal Control and Deep Neural Networks</title><authors>Kaitong Hu and Anna Kazeykina and Zhenjie Ren</authors><categories>math.PR cs.LG math.OC</categories><comments>25 pages</comments><msc-class>60H30, 37M25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a regularised relaxed optimal control problem and, in\\nparticular, we are concerned with the case where the control variable is of\\nlarge dimension. We introduce a system of mean-field Langevin equations, the\\ninvariant measure of which is shown to be the optimal control of the initial\\nproblem under mild conditions. Therefore, this system of processes can be\\nviewed as a continuous-time numerical algorithm for computing the optimal\\ncontrol. As an application, this result endorses the solvability of the\\nstochastic gradient descent algorithm for a wide class of deep neural networks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.07372</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.07372</id><submitter>Prayitno Prayitno</submitter><version version=\"v1\"><date>Sun, 15 Sep 2019 07:26:39 GMT</date><size>541kb</size></version><version version=\"v2\"><date>Mon, 30 Sep 2019 21:30:34 GMT</date><size>0kb</size><source_type>I</source_type></version><title>Modeling Traffic Congestion with Spatiotemporal Big Data for An\\n  Intelligent Freeway Monitoring System</title><authors>Karisma Trinanda Putra, Jing-Doo Wang, Eko Prasetyo, Prayitno</authors><categories>cs.CY cs.DC</categories><comments>The article was published without the co-Author\\'s notice, and it is\\n  withdrawn due to his objection</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Traffic congestion is a complex, nonlinear spatiotemporal modeling problem.\\nBy collecting and analyzing a vast quantity and different categories of\\ninformation, traffic flow, and road congestion can be predicted and controlled\\non an intelligent transportation system. This report provides an analysis of\\ntraveling time across Taiwan from North to South, vice versa. We analyze\\ntraffic in a national freeway between Tainan and Kaohsiung section, which\\nrepresents the common trip of the population in Southern Taiwan. The data is\\nrecorded using the Electronic Toll Collection System (ETC) provided by Ministry\\nof Transportation in Taiwan. We use MapReduce framework to process data into a\\nsmaller task which can be distributed on several computer clusters to speed up\\nthe process. The results show that the spatiotemporal model of traffic flow is\\nstrongly influenced by direction, working hour, and holidays with a recurring\\npattern for each week. The distinctive pattern inside the spatiotemporal\\ndataset can be used on an AI-powered decision-making system for future\\ndevelopment.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.07486</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.07486</id><submitter>Franz Scherr</submitter><version version=\"v1\"><date>Mon, 16 Sep 2019 21:12:24 GMT</date><size>1333kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 11:03:45 GMT</date><size>1763kb</size><source_type>D</source_type></version><title>Reservoirs learn to learn</title><authors>Anand Subramoney and Franz Scherr and Wolfgang Maass</authors><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider reservoirs in the form of liquid state machines, i.e.,\\nrecurrently connected networks of spiking neurons with randomly chosen weights.\\nSo far only the weights of a linear readout were adapted for a specific task.\\nWe wondered whether the performance of liquid state machines can be improved if\\nthe recurrent weights are chosen with a purpose, rather than randomly. After\\nall, weights of recurrent connections in the brain are also not assumed to be\\nrandomly chosen. Rather, these weights were probably optimized during\\nevolution, development, and prior learning experiences for specific task\\ndomains. In order to examine the benefits of choosing recurrent weights within\\na liquid with a purpose, we applied the Learning-to-Learn (L2L) paradigm to our\\nmodel: We optimized the weights of the recurrent connections -- and hence the\\ndynamics of the liquid state machine -- for a large family of potential\\nlearning tasks, which the network might have to learn later through\\nmodification of the weights of readout neurons. We found that this two-tiered\\nprocess substantially improves the learning speed of liquid state machines for\\nspecific tasks. In fact, this learning speed increases further if one does not\\ntrain the weights of linear readouts at all, and relies instead on the internal\\ndynamics and fading memory of the network for remembering salient information\\nthat it could extract from preceding examples for the current learning task.\\nThis second type of learning has recently been proposed to underlie fast\\nlearning in the prefrontal cortex and motor cortex, and hence it is of interest\\nto explore its performance also in models. Since liquid state machines share\\nmany properties with other types of reservoirs, our results raise the question\\nwhether L2L conveys similar benefits also to these other reservoirs.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.07556</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.07556</id><submitter>Bolin Chen</submitter><version version=\"v1\"><date>Tue, 17 Sep 2019 02:34:21 GMT</date><size>740kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 18 Sep 2019 02:55:01 GMT</date><size>740kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 2 Oct 2019 01:31:03 GMT</date><size>740kb</size><source_type>D</source_type></version><title>Enhancing JPEG Steganography using Iterative Adversarial Examples</title><authors>Huaxiao Mo, Tingting Song, Bolin Chen, Weiqi Luo, Jiwu Huang</authors><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Neural Networks (CNN) based methods have significantly improved\\nthe performance of image steganalysis compared with conventional ones based on\\nhand-crafted features. However, many existing literatures on computer vision\\nhave pointed out that those effective CNN-based methods can be easily fooled by\\nadversarial examples. In this paper, we propose a novel steganography framework\\nbased on adversarial example in an iterative manner. The proposed framework\\nfirst starts from an existing embedding cost, such as J-UNIWARD in this work,\\nand then updates the cost iteratively based on adversarial examples derived\\nfrom a series of steganalytic networks until achieving satisfactory results. We\\ncarefully analyze two important factors that would affect the security\\nperformance of the proposed framework, i.e. the percentage of selected\\ngradients with larger amplitude and the adversarial intensity to modify\\nembedding cost. The experimental results evaluated on three modern steganalytic\\nmodels, including GFR, SCA-GFR and SRNet, show that the proposed framework is\\nvery promising to enhance the security performances of JPEG steganography.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.07577</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.07577</id><submitter>Mehrdad Shoeiby</submitter><version version=\"v1\"><date>Tue, 17 Sep 2019 03:53:20 GMT</date><size>2100kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 02:22:59 GMT</date><size>2100kb</size><source_type>D</source_type></version><title>Multi-FAN: Multi-Spectral Mosaic Super-Resolution Via Multi-Scale\\n  Feature Aggregation Network</title><authors>Mehrdad Shoeiby, Sadegh Aliakbarian, Saeed Anwar, Lars Petersson</authors><categories>eess.IV cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel method to super-resolve multi-spectral images\\ncaptured by modern real-time single-shot mosaic image sensors, also known as\\nmulti-spectral cameras. Our contribution is two-fold. Firstly, we super-resolve\\nmulti-spectral images from mosaic images rather than image cubes, which helps\\nto take into account the spatial offset of each wavelength. Secondly, we\\nintroduce an external multi-scale feature aggregation network (Multi-FAN) which\\nconcatenates the feature maps with different levels of semantic information\\nthroughout a super-resolution (SR) network. A cascade of convolutional layers\\nthen implicitly selects the most valuable feature maps to generate a mosaic\\nimage. This mosaic image is then merged with the mosaic image generated by the\\nSR network to produce a quantitatively superior image. We apply our Multi-FAN\\nto RCAN (Residual Channel Attention Network), which is the state-of-the-art SR\\nalgorithm. We show that Multi-FAN improves both quantitative results and well\\nas inference time.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.07755</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.07755</id><submitter>Markus Eberts</submitter><version version=\"v1\"><date>Tue, 17 Sep 2019 13:01:12 GMT</date><size>99kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 09:58:10 GMT</date><size>99kb</size><source_type>D</source_type></version><title>Span-based Joint Entity and Relation Extraction with Transformer\\n  Pre-training</title><authors>Markus Eberts, Adrian Ulges</authors><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce SpERT, an attention model for span-based joint entity and\\nrelation extraction. Our approach employs the pre-trained Transformer network\\nBERT as its core. We use BERT embeddings as shared inputs for a light-weight\\nreasoning, which features entity recognition and filtering, as well as relation\\nclassification with a localized, marker-free context representation. The model\\nis trained on strong within-sentence negative samples, which are efficiently\\nextracted in a single BERT pass. These aspects facilitate a search over all\\nspans in the sentence. In ablation studies, we demonstrate the benefits of\\npre-training, strong negative sampling and localized context. Our model\\noutperforms prior work by up to 4.7% F1 score on several datasets for joint\\nentity and relation extraction.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.07775</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.07775</id><submitter>Junhua Liu</submitter><version version=\"v1\"><date>Thu, 12 Sep 2019 11:09:47 GMT</date><size>643kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 6 Oct 2019 12:52:32 GMT</date><size>643kb</size><source_type>D</source_type></version><title>Crowd-aware itinerary recommendation: a game-theoretic approach to\\n  optimize social welfare</title><authors>Junhua Liu, Chu Guo, Kristin L. Wood, Kwan Hui Lim</authors><categories>cs.AI cs.GT cs.MA</categories><comments>10 pages, 2 figures</comments><msc-class>68T20</msc-class><acm-class>I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The demand for Itinerary Planning grows rapidly in recent years as the\\neconomy and standard of living are improving globally. Nonetheless, itinerary\\nrecommendation remains a complex and difficult task, especially for one that is\\nqueuing time- and crowd-aware. This difficulty is due to the large amount of\\nparameters involved, i.e., attraction popularity, queuing time, walking time,\\noperating hours, etc. Many recent or existing works adopt a data-driven\\napproach and propose solutions with single-person perspectives, but do not\\naddress real-world problems as a result of natural crowd behavior, such as the\\nSelfish Routing problem, which describes the consequence of ineffective network\\nand sub-optimal social outcome by leaving agents to decide freely. In this\\nwork, we propose the Strategic and Crowd-Aware Itinerary Recommendation (SCAIR)\\nalgorithm which takes a game-theoretic approach to address the Selfish Routing\\nproblem and optimize social welfare in real-world situations. To address the\\nNP-hardness of the social welfare optimization problem, we further propose a\\nMarkov Decision Process (MDP) approach which enables our simulations to be\\ncarried out in poly-time. We then use real-world data to evaluate the proposed\\nalgorithm, with benchmarks of two intuitive strategies commonly adopted in real\\nlife, and a recent algorithm published in the literature. Our simulation\\nresults highlight the existence of the Selfish Routing problem and show that\\nSCAIR outperforms the benchmarks in handling this issue with real-world data.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.07916</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.07916</id><submitter>Kunal Garg</submitter><version version=\"v1\"><date>Tue, 17 Sep 2019 16:25:47 GMT</date><size>2131kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 19:49:04 GMT</date><size>2131kb</size><source_type>D</source_type></version><title>Safety-Critical Adaptive Control with Nonlinear Reference Model Systems</title><authors>Ehsan Arabi, Kunal Garg, Dimitra Panagou</authors><categories>eess.SY cs.SY</categories><comments>Submitted to American Control Conference (ACC) 2020, under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a model reference adaptive control architecture is proposed\\nfor uncertain nonlinear systems to achieve prescribed performance guarantees.\\nSpecifically, a general nonlinear reference model system is considered that\\ncaptures an ideal and safe system behavior. An adaptive control architecture is\\nthen proposed to suppress the effects of system uncertainties without any prior\\nknowledge of their magnitude and rate upper bounds. More importantly, the\\nproposed control architecture enforces the system state trajectories to evolve\\nwithin a user-specified prescribed distance from the reference system\\ntrajectories, satisfying the safety constraints. This eliminates the ad-hoc\\ntuning process for the adaptation rate that is conventionally required in model\\nreference adaptive control to ensure safety. The efficacy of the proposed\\ncontrol architecture is also demonstrated through an illustrative numerical\\nexample.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.07961</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.07961</id><submitter>M\\\\\\'at\\\\\\'e Gerencs\\\\\\'er</submitter><version version=\"v1\"><date>Tue, 17 Sep 2019 17:51:43 GMT</date><size>42kb</size></version><version version=\"v2\"><date>Wed, 2 Oct 2019 09:16:51 GMT</date><size>42kb</size></version><title>Approximation of SDEs -- a stochastic sewing approach</title><authors>Oleg Butkovsky and Konstantinos Dareiotis and M\\\\\\'at\\\\\\'e Gerencs\\\\\\'er</authors><categories>math.PR cs.NA math.NA</categories><comments>30 pages</comments><msc-class>60H10, 60H07, 60H35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new take on the error analysis of approximations of stochastic\\ndifferential equations (SDEs), utilising the stochastic sewing lemma [L\\\\^e\\n\\'18]. This approach allows one to exploit regularisation by noise effects in\\nobtaining convergence rates. In our first application we show convergence (to\\nour knowledge for the first time) of the Euler-Maruyama scheme for SDEs driven\\nby fractional Brownian motions with non-regular drift. When the Hurst parameter\\nis $H\\\\in(0,1)$ and the drift is $\\\\mathcal{C}^\\\\alpha$, $\\\\alpha&gt;2-1/H$, we show\\nthe strong $L_p$ and almost sure rates of convergence to be\\n$1/2+\\\\alpha(1/2\\\\wedge H)-\\\\varepsilon$, for any $\\\\varepsilon&gt;0$. As another\\napplication we consider the approximation of SDEs driven by multiplicative\\nstandard Brownian noise where we derive the almost optimal rate of convergence\\n$1/2-\\\\varepsilon$ of the Euler-Maruyama scheme for $\\\\mathcal{C}^\\\\alpha$ drift,\\nfor any $\\\\varepsilon,\\\\alpha&gt;0$.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.08053</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.08053</id><submitter>Mohammad Shoeybi</submitter><version version=\"v1\"><date>Tue, 17 Sep 2019 19:42:54 GMT</date><size>3472kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 19 Sep 2019 00:30:15 GMT</date><size>3472kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sat, 5 Oct 2019 03:27:58 GMT</date><size>3472kb</size><source_type>D</source_type></version><title>Megatron-LM: Training Multi-Billion Parameter Language Models Using\\n  Model Parallelism</title><authors>Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared\\n  Casper, and Bryan Catanzaro</authors><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work in unsupervised language modeling demonstrates that training\\nlarge neural language models advances the state of the art in Natural Language\\nProcessing applications. However, for very large models, memory constraints\\nlimit the size of models that can be practically trained. Model parallelism\\nallows us to train larger models, because the parameters can be split across\\nmultiple processors. In this work, we implement a simple, efficient intra-layer\\nmodel parallel approach that enables training state of the art transformer\\nlanguage models with billions of parameters. Our approach does not require a\\nnew compiler or library changes, is orthogonal and complimentary to pipeline\\nmodel parallelism, and can be fully implemented with the insertion of a few\\ncommunication operations in native PyTorch. We illustrate this approach by\\nconverging an 8.3 billion parameter transformer language model using 512 GPUs,\\nmaking it the largest transformer model ever trained at 24x times the size of\\nBERT and 5.6x times the size of GPT-2. We sustain up to 15.1 PetaFLOPs per\\nsecond across the entire application with 76% scaling efficiency, compared to a\\nstrong single processor baseline that sustains 39 TeraFLOPs per second, which\\nis 30% of peak FLOPs. The model is trained on 174GB of text, requiring 12\\nZettaFLOPs over 9.2 days to converge. Transferring this language model achieves\\nstate of the art (SOTA) results on the WikiText103 (10.8 compared to SOTA\\nperplexity of 16.4) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%)\\ndatasets. We release training and evaluation code, as well as the weights of\\nour smaller portable model, for reproducibility.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.08210</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.08210</id><submitter>Jiangsheng You Dr.</submitter><version version=\"v1\"><date>Wed, 18 Sep 2019 04:58:25 GMT</date><size>607kb</size></version><version version=\"v2\"><date>Thu, 26 Sep 2019 01:04:01 GMT</date><size>618kb</size></version><title>Data Mapping for Restricted Boltzmann Machine</title><authors>Jiangsheng You</authors><categories>cs.LG stat.ML</categories><comments>14 pages with 12 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Restricted Boltzmann machine (RBM) is two-layer neural nets constructed as a\\nprobabilistic model and its training is to maximize a product of probabilities\\nby the contrastive divergence (CD) scheme. In this paper a data mapping is used\\nto describe the relationship between visible and hidden layers and the training\\nis to minimize a squared error of the reconstructed visible layer by the\\ngradient descent or a finite difference approximation. This paper presents\\nthree new findings: 1) nodes on visible and hidden layers can take real-valued\\nmatrix data without a probabilistic interpretation; 2) the famous CD1 is a\\nfinite difference approximation of gradient descent after ignoring the\\nsecond-order error; 3) activation can take non-sigmoid functions such as\\nidentity, relu and softsign. The data mapping provides a unified framework on\\ndimensionality reduction, feature extraction and data representation pioneered\\nand developed by Hinton and his colleagues. As an approximation of gradient\\ndescent, the finite difference learning is applicable to both directed and\\nundirected graphs. Numerical results are performed to confirm these new\\nfindings on very low dimensionality reduction, matrix data and flexible\\nactivations.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.08326</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.08326</id><submitter>Hong Wang</submitter><version version=\"v1\"><date>Wed, 18 Sep 2019 10:05:24 GMT</date><size>3458kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 16:18:32 GMT</date><size>3398kb</size><source_type>D</source_type></version><title>A Survey on Rain Removal from Video and Single Image</title><authors>Hong Wang, Yichen Wu, Minghan Li, Qian Zhao, and Deyu Meng</authors><categories>eess.IV cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rain streaks might severely degenerate the performance of video/image\\nprocessing tasks. The investigations on rain removal from video or a single\\nimage has thus been attracting much research attention in the field of computer\\nvision and pattern recognition, and various methods have been proposed against\\nthis task in the recent years. However, there is still not a comprehensive\\nsurvey paper to summarize current rain removal methods and fairly compare their\\ngeneralization performance, and especially, still not a off-the-shelf toolkit\\nto accumulate recent representative methods for easy performance comparison and\\ncapability evaluation. Aiming at this meaningful task, in this study we present\\na comprehensive review for current rain removal methods for video and a single\\nimage. Specifically, these methods are categorized into model-driven and\\ndata-driven approaches, and more elaborate branches of each approach are\\nfurther introduced. Intrinsic capabilities, especially generalization, of\\nrepresentative state-of-the-art methods of each approach have been evaluated\\nand analyzed by experiments implemented on synthetic and real data both\\nvisually and quantitatively. Furthermore, we release a comprehensive\\nrepository, including direct links to 74 rain removal papers, source codes of 9\\nmethods for video rain removal and 20 ones for single image rain removal, 19\\nrelated project pages, 6 synthetic datasets and 4 real ones, and 4 commonly\\nused image quality metrics, to facilitate reproduction and performance\\ncomparison of current existing methods for general users. Some limitations and\\nresearch issues worthy to be further investigated have also been discussed for\\nfuture research of this direction.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.08560</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.08560</id><submitter>Wen-Loong Ma</submitter><version version=\"v1\"><date>Wed, 18 Sep 2019 16:29:53 GMT</date><size>9307kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 18:06:28 GMT</date><size>9307kb</size><source_type>D</source_type></version><title>From Bipedal Walking to Quadrupedal Locomotion: Full-Body Dynamics\\n  Decomposition for Rapid Gait Generation</title><authors>Wen-Loong Ma, Aaron D Ames</authors><categories>cs.RO cs.SY eess.SY</categories><comments>Submitted to RA-L with ICRA 2020 presentation option</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper systematically decomposes quadrupeds into bipeds to rapidly\\ngenerate walking gaits, and then recomposes these gaits to obtain quadrupedal\\nlocomotion. We begin by decomposing the full-order, nonlinear and hybrid\\ndynamics of a three-dimensional quadrupedal robot, including its continuous and\\ndiscrete dynamics, into two bipedal systems that are subject to external\\nforces. Using the hybrid zero dynamics (HZD) framework, gaits for these bipedal\\nrobots can be rapidly generated (on the order of seconds) along with\\ncorresponding controllers. The decomposition is performed in such a way that\\nthe bipedal walking gaits and controllers can be composed to yield dynamic\\nwalking gaits for the original quadrupedal robot --- the result, therefore, is\\nthe rapid generation of dynamic quadruped gaits utilizing the full-order\\ndynamics. This methodology is demonstrated through the rapid generation (3.96\\nseconds on average) of four stepping-in-place gaits and one ambling gait at\\n0.35 m/s on a quadrupedal robot --- the Vision 60, with 36 state variables and\\n12 control inputs --- both in simulation and through outdoor experiments. This\\nsuggested a new approach for fast quadrupedal trajectory planning using\\nfull-body dynamics, without the need for empirical model simplification,\\nwherein methods from dynamic bipedal walking can be directly applied to\\nquadrupeds.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.08857</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.08857</id><submitter>Frank Nielsen</submitter><version version=\"v1\"><date>Thu, 19 Sep 2019 08:32:44 GMT</date><size>51kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 23:06:57 GMT</date><size>63kb</size><source_type>D</source_type></version><title>A note on the quasiconvex Jensen divergences and the quasiconvex Bregman\\n  divergences derived thereof</title><authors>Frank Nielsen and Ga\\\\&quot;etan Hadjeres</authors><categories>cs.IT cs.LG math.IT</categories><comments>19 pages, 3 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We first introduce the class of strictly quasiconvex and strictly\\nquasiconcave Jensen divergences which are oriented (asymmetric) distances, and\\nstudy some of their properties. We then define the strictly quasiconvex Bregman\\ndivergences as the limit case of scaled and skewed quasiconvex Jensen\\ndivergences, and report a simple closed-form formula which shows that these\\ndivergences are only pseudo-divergences at countably many inflection points of\\nthe generators. To remedy this problem, we propose the $\\\\delta$-averaged\\nquasiconvex Bregman divergences which integrate the pseudo-divergences over a\\nsmall neighborhood in order obtain a proper divergence. The formula of\\n$\\\\delta$-averaged quasiconvex Bregman divergences extend even to\\nnon-differentiable strictly quasiconvex generators. These quasiconvex Bregman\\ndivergences between distinct elements have the property to always have one\\norientation finite while the other orientation is infinite. We show that these\\nquasiconvex Bregman divergences can also be interpreted as limit cases of\\ngeneralized skewed Jensen divergences with respect to comparative convexity by\\nusing power means. Finally, we illustrate how these quasiconvex Bregman\\ndivergences naturally appear as equivalent divergences for the Kullback-Leibler\\ndivergences between probability densities belonging to a same parametric family\\nof distributions with nested supports.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.08920</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.08920</id><submitter>Hugo Gilbert</submitter><version version=\"v1\"><date>Thu, 19 Sep 2019 11:30:54 GMT</date><size>32kb</size></version><version version=\"v2\"><date>Sun, 22 Sep 2019 11:06:13 GMT</date><size>33kb</size></version><version version=\"v3\"><date>Mon, 7 Oct 2019 15:22:18 GMT</date><size>33kb</size></version><title>Parameterized Complexity of Manipulating Sequential Allocation</title><authors>Hugo Gilbert</authors><categories>cs.GT</categories><comments>Changes w.r.t. previous version: correction of few typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sequential allocation protocol is a simple and popular mechanism to\\nallocate indivisible goods, in which the agents take turns to pick the items\\naccording to a predefined sequence. While this protocol is not strategy-proof,\\nit has been shown recently that finding a successful manipulation for an agent\\nis an NP-hard problem (Aziz et al., 2017). Conversely, it is also known that\\nfinding an optimal manipulation can be solved in polynomial time in a few\\ncases: if there are only two agents or if the manipulator has a binary or a\\nlexicographic utility function. In this work, we take a parameterized approach\\nto provide several new complexity results on this manipulation problem.\\nNotably, we show that finding an optimal manipulation can be performed in\\npolynomial time if the number of agents is a constant and that it is\\nfixed-parameter tractable with respect to a parameter measuring the distance\\nbetween the preference rankings of the agents. Moreover, we provide an integer\\nprogram and a dynamic programming scheme to solve the manipulation problem and\\nwe show that a single manipulator can increase the utility of her bundle by a\\nmultiplicative factor which is at most 2. Overall, our results show that\\nmanipulating the sequential allocation protocol can be performed efficiently\\nfor a wide range of instances.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.08987</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.08987</id><submitter>Mohammed Zubair Mohammed Shamim</submitter><version version=\"v1\"><date>Wed, 18 Sep 2019 17:35:18 GMT</date><size>2080kb</size></version><title>Automated detection of oral pre-cancerous tongue lesions using deep\\n  learning for early diagnosis of oral cavity cancer</title><authors>Mohammed Zubair M. Shamim, Sadatullah Syed, Mohammad Shiblee, Mohammed\\n  Usman and Syed Ali</authors><categories>eess.IV cs.CV cs.LG stat.ML</categories><comments>25 pages, 10 figures</comments><report-no>01</report-no><msc-class>68Txx</msc-class><doi>10.13140/RG.2.2.28808.16643</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discovering oral cavity cancer (OCC) at an early stage is an effective way to\\nincrease patient survival rate. However, current initial screening process is\\ndone manually and is expensive for the average individual, especially in\\ndeveloping countries worldwide. This problem is further compounded due to the\\nlack of specialists in such areas. Automating the initial screening process\\nusing artificial intelligence (AI) to detect pre-cancerous lesions can prove to\\nbe an effective and inexpensive technique that would allow patients to be\\ntriaged accordingly to receive appropriate clinical management. In this study,\\nwe have applied and evaluated the efficacy of six deep convolutional neural\\nnetwork (DCNN) models using transfer learning, for identifying pre-cancerous\\ntongue lesions directly using a small data set of clinically annotated\\nphotographic images to diagnose early signs of OCC. DCNN model based on Vgg19\\narchitecture was able to differentiate between benign and pre-cancerous tongue\\nlesions with a mean classification accuracy of 0.98, sensitivity 0.89 and\\nspecificity 0.97. Additionally, the ResNet50 DCNN model was able to distinguish\\nbetween five types of tongue lesions i.e. hairy tongue, fissured tongue,\\ngeographic tongue, strawberry tongue and oral hairy leukoplakia with a mean\\nclassification accuracy of 0.97. Preliminary results using an (AI+Physician)\\nensemble model demonstrate that an automated initial screening process of\\ntongue lesions using DCNNs can achieve near-human level classification\\nperformance for diagnosing early signs of OCC in patients.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.09007</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.09007</id><submitter>Zi Qing Zhu</submitter><version version=\"v1\"><date>Thu, 19 Sep 2019 14:06:59 GMT</date><size>1282kb</size></version><version version=\"v2\"><date>Tue, 8 Oct 2019 15:00:02 GMT</date><size>1280kb</size></version><title>Community Detection Across Multiple Social Networks based on Overlapping\\n  Users</title><authors>Ziqing Zhu, Tao Zhou, Chenghao Jia, Weijia Liu, Jiuxin Cao</authors><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid development of Internet technology, online social networks\\n(OSNs) have got fast development and become increasingly popular. Meanwhile,\\nthe research works across multiple social networks attract more and more\\nattention from researchers, and community detection is an important one across\\nOSNs for online security problems, such as the user behavior analysis and\\nabnormal community discovery. In this paper, a community detection method is\\nproposed across multiple social networks based on overlapping users. First, the\\nconcept of overlapping users is defined, then an algorithm CMN NMF is designed\\nto discover the stub communities from overlapping users based on the social\\nrelevance. After that, we extend each stub community in different social\\nnetworks by adding the users with strong similarity, and in the end different\\ncommunities are excavated out across networks. Experimental results show the\\nadvantage on effectiveness of our method over other methods under real data\\nsets.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.09029</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.09029</id><submitter>Jeremy Lacomis</submitter><version version=\"v1\"><date>Thu, 19 Sep 2019 14:57:31 GMT</date><size>434kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 15:42:43 GMT</date><size>434kb</size><source_type>D</source_type></version><title>DIRE: A Neural Approach to Decompiled Identifier Naming</title><authors>Jeremy Lacomis, Pengcheng Yin, Edward J. Schwartz, Miltiadis\\n  Allamanis, Claire Le Goues, Graham Neubig, Bogdan Vasilescu</authors><categories>cs.SE</categories><comments>2019 International Conference on Automated Software Engineering</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The decompiler is one of the most common tools for examining binaries without\\ncorresponding source code. It transforms binaries into high-level code,\\nreversing the compilation process. Decompilers can reconstruct much of the\\ninformation that is lost during the compilation process (e.g., structure and\\ntype information). Unfortunately, they do not reconstruct semantically\\nmeaningful variable names, which are known to increase code understandability.\\nWe propose the Decompiled Identifier Renaming Engine (DIRE), a novel\\nprobabilistic technique for variable name recovery that uses both lexical and\\nstructural information recovered by the decompiler. We also present a technique\\nfor generating corpora suitable for training and evaluating models of\\ndecompiled code renaming, which we use to create a corpus of 164,632 unique\\nx86-64 binaries generated from C projects mined from GitHub. Our results show\\nthat on this corpus DIRE can predict variable names identical to the names in\\nthe original source code up to 74.3% of the time.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.09228</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.09228</id><submitter>James Hare</submitter><version version=\"v1\"><date>Mon, 9 Sep 2019 19:58:50 GMT</date><size>7723kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 27 Sep 2019 18:19:15 GMT</date><size>6875kb</size><source_type>D</source_type></version><title>Non-Bayesian Social Learning with Uncertain Models</title><authors>James Z. Hare and Cesar A. Uribe and Lance Kaplan and Ali Jadbabaie</authors><categories>cs.AI cs.MA cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-Bayesian social learning theory provides a framework that models\\ndistributed inference for a group of agents interacting over a social network.\\nIn this framework, each agent iteratively forms and communicates beliefs about\\nan unknown state of the world with their neighbors using a learning rule.\\nExisting approaches assume agents have access to precise statistical models (in\\nthe form of likelihoods) for the state of the world. However in many\\nsituations, such models must be learned from finite data. We propose a social\\nlearning rule that takes into account uncertainty in the statistical models\\nusing second-order probabilities. Therefore, beliefs derived from uncertain\\nmodels are sensitive to the amount of past evidence collected for each\\nhypothesis. We characterize how well the hypotheses can be tested on a social\\nnetwork, as consistent or not with the state of the world. We explicitly show\\nthe dependency of the generated beliefs with respect to the amount of prior\\nevidence. Moreover, as the amount of prior evidence goes to infinity, learning\\noccurs and is consistent with traditional social learning theory.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.09264</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.09264</id><submitter>Meyer Scetbon</submitter><version version=\"v1\"><date>Thu, 19 Sep 2019 23:59:44 GMT</date><size>1003kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 01:17:31 GMT</date><size>1006kb</size><source_type>D</source_type></version><title>Comparing distributions: $\\\\ell_1$ geometry improves kernel two-sample\\n  testing</title><authors>M. Scetbon, G. Varoquaux</authors><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Are two sets of observations drawn from the same distribution? This problem\\nis a two-sample test. Kernel methods lead to many appealing properties. Indeed\\nstate-of-the-art approaches use the $L^2$ distance between kernel-based\\ndistribution representatives to derive their test statistics. Here, we show\\nthat $L^p$ distances (with $p\\\\geq 1$) between these distribution\\nrepresentatives give metrics on the space of distributions that are\\nwell-behaved to detect differences between distributions as they metrize the\\nweak convergence. Moreover, for analytic kernels, we show that the $L^1$\\ngeometry gives improved testing power for scalable computational procedures.\\nSpecifically, we derive a finite dimensional approximation of the metric given\\nas the $\\\\ell_1$ norm of a vector which captures differences of expectations of\\nanalytic functions evaluated at spatial locations or frequencies (i.e,\\nfeatures). The features can be chosen to maximize the differences of the\\ndistributions and give interpretable indications of how they differs. Using an\\n$\\\\ell_1$ norm gives better detection because differences between\\nrepresentatives are dense as we use analytic kernels (non-zero almost\\neverywhere). The tests are consistent, while much faster than state-of-the-art\\nquadratic-time kernel-based tests. Experiments on artificial and real-world\\nproblems demonstrate improved power/time tradeoff than the state of the art,\\nbased on $\\\\ell_2$ norms, and in some cases, better outright power than even the\\nmost expensive quadratic-time tests.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.09285</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.09285</id><submitter>Asma Ghandeharioun</submitter><version version=\"v1\"><date>Fri, 20 Sep 2019 01:22:53 GMT</date><size>221kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 5 Oct 2019 18:33:32 GMT</date><size>221kb</size><source_type>D</source_type></version><title>Characterizing Sources of Uncertainty to Proxy Calibration and\\n  Disambiguate Annotator and Data Bias</title><authors>Asma Ghandeharioun, Brian Eoff, Brendan Jou, Rosalind W. Picard</authors><categories>cs.LG stat.ML</categories><comments>Accepted for presentation at 2019 ICCV Workshop on Interpreting and\\n  Explaining Visual Artificial Intelligence Models</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supporting model interpretability for complex phenomena where annotators can\\nlegitimately disagree, such as emotion recognition, is a challenging machine\\nlearning task. In this work, we show that explicitly quantifying the\\nuncertainty in such settings has interpretability benefits. We use a simple\\nmodification of a classical network inference using Monte Carlo dropout to give\\nmeasures of epistemic and aleatoric uncertainty. We identify a significant\\ncorrelation between aleatoric uncertainty and human annotator disagreement\\n($r\\\\approx.3$). Additionally, we demonstrate how difficult and subjective\\ntraining samples can be identified using aleatoric uncertainty and how\\nepistemic uncertainty can reveal data bias that could result in unfair\\npredictions. We identify the total uncertainty as a suitable surrogate for\\nmodel calibration, i.e. the degree we can trust model\\'s predicted confidence.\\nIn addition to explainability benefits, we observe modest performance boosts\\nfrom incorporating model uncertainty.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.09345</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.09345</id><submitter>Shuaiwen Wang</submitter><version version=\"v1\"><date>Fri, 20 Sep 2019 07:01:51 GMT</date><size>140kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 06:50:30 GMT</date><size>140kb</size><source_type>D</source_type></version><title>Does SLOPE outperform bridge regression?</title><authors>Shuaiwen Wang, Haolei Weng, Arian Maleki</authors><categories>stat.ML cs.LG math.ST stat.TH</categories><comments>51 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recently proposed SLOPE estimator (arXiv:1407.3824) has been shown to\\nadaptively achieve the minimax $\\\\ell_2$ estimation rate under high-dimensional\\nsparse linear regression models (arXiv:1503.08393). Such minimax optimality\\nholds in the regime where the sparsity level $k$, sample size $n$, and\\ndimension $p$ satisfy $k/p \\\\rightarrow 0$, $k\\\\log p/n \\\\rightarrow 0$. In this\\npaper, we characterize the estimation error of SLOPE under the complementary\\nregime where both $k$ and $n$ scale linearly with $p$, and provide new insights\\ninto the performance of SLOPE estimators. We first derive a concentration\\ninequality for the finite sample mean square error (MSE) of SLOPE. The quantity\\nthat MSE concentrates around takes a complicated and implicit form. With\\ndelicate analysis of the quantity, we prove that among all SLOPE estimators,\\nLASSO is optimal for estimating $k$-sparse parameter vectors that do not have\\ntied non-zero components in the low noise scenario. On the other hand, in the\\nlarge noise scenario, the family of SLOPE estimators are sub-optimal compared\\nwith bridge regression such as the Ridge estimator.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.09349</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.09349</id><submitter>Juan Luis Gonzalez Bello</submitter><version version=\"v1\"><date>Fri, 20 Sep 2019 07:18:39 GMT</date><size>6504kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 17:23:17 GMT</date><size>6504kb</size><source_type>D</source_type></version><title>Deep 3D-Zoom Net: Unsupervised Learning of Photo-Realistic 3D-Zoom</title><authors>Juan Luis Gonzalez Bello and Munchurl Kim</authors><categories>eess.IV cs.CV eess.SP</categories><comments>Check our video at https://www.youtube.com/watch?v=Gz76VYwUzZ8</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 3D-zoom operation is the positive translation of the camera in the\\nZ-axis, perpendicular to the image plane. In contrast, the optical zoom changes\\nthe focal length and the digital zoom is used to enlarge a certain region of an\\nimage to the original image size. In this paper, we are the first to formulate\\nan unsupervised 3D-zoom learning problem where images with an arbitrary zoom\\nfactor can be generated from a given single image. An unsupervised framework is\\nconvenient, as it is a challenging task to obtain a 3D-zoom dataset of natural\\nscenes due to the need for special equipment to ensure camera movement is\\nrestricted to the Z-axis. In addition, the objects in the scenes should not\\nmove when being captured, which hinders the construction of a large dataset of\\noutdoor scenes. We present a novel unsupervised framework to learn how to\\ngenerate arbitrarily 3D-zoomed versions of a single image, not requiring a\\n3D-zoom ground truth, called the Deep 3D-Zoom Net. The Deep 3D-Zoom Net\\nincorporates the following features: (i) transfer learning from a pre-trained\\ndisparity estimation network via a back re-projection reconstruction loss; (ii)\\na fully convolutional network architecture that models depth-image-based\\nrendering (DIBR), taking into account high-frequency details without the need\\nfor estimating the intermediate disparity; and (iii) incorporating a\\ndiscriminator network that acts as a no-reference penalty for unnaturally\\nrendered areas. Even though there is no baseline to fairly compare our results,\\nour method outperforms previous novel view synthesis research in terms of\\nrealistic appearance on large camera baselines. We performed extensive\\nexperiments to verify the effectiveness of our method on the KITTI and\\nCityscapes datasets.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.09362</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.09362</id><submitter>Zhe Zeng Miss</submitter><version version=\"v1\"><date>Fri, 20 Sep 2019 07:56:29 GMT</date><size>1159kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 07:19:49 GMT</date><size>435kb</size><source_type>D</source_type></version><title>Hybrid Probabilistic Inference with Logical Constraints: Tractability\\n  and Message Passing</title><authors>Zhe Zeng, Fanqi Yan, Paolo Morettin, Antonio Vergari, Guy Van den\\n  Broeck</authors><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weighted model integration (WMI) is a very appealing framework for\\nprobabilistic inference: it allows to express the complex dependencies of\\nreal-world hybrid scenarios where variables are heterogeneous in nature (both\\ncontinuous and discrete) via the language of Satisfiability Modulo Theories\\n(SMT); as well as computing probabilistic queries with arbitrarily complex\\nlogical constraints. Recent work has shown WMI inference to be reducible to a\\nmodel integration (MI) problem, under some assumptions, thus effectively\\nallowing hybrid probabilistic reasoning by volume computations. In this paper,\\nwe introduce a novel formulation of MI via a message passing scheme that allows\\nto efficiently compute the marginal densities and statistical moments of all\\nthe variables in linear time. As such, we are able to amortize inference for\\narbitrarily rich MI queries when they conform to the problem structure, here\\nrepresented as the primal graph associated to the SMT formula. Furthermore, we\\ntheoretically trace the tractability boundaries of exact MI. Indeed, we prove\\nthat in terms of the structural requirements on the primal graph that make our\\nMI algorithm tractable - bounding its diameter and treewidth - the bounds are\\nnot only sufficient, but necessary for tractable inference via MI.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.09502</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.09502</id><submitter>Alexander Ororbia</submitter><version version=\"v1\"><date>Fri, 20 Sep 2019 13:45:23 GMT</date><size>7187kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 23 Sep 2019 00:54:37 GMT</date><size>7188kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Fri, 27 Sep 2019 19:11:08 GMT</date><size>7186kb</size><source_type>D</source_type></version><title>An Empirical Exploration of Deep Recurrent Connections and Memory Cells\\n  Using Neuro-Evolution</title><authors>Travis J. Desell, AbdElRahman A. ElSaid and Alexander G. Ororbia</authors><categories>cs.NE</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neuro-evolution and neural architecture search algorithms have gained\\nincreasing interest due to the challenges involved in designing optimal\\nartificial neural networks (ANNs). While these algorithms have been shown to\\npossess the potential to outperform the best human crafted architectures, a\\nless common use of them is as a tool for analysis of ANN structural components\\nand connectivity structures. In this work, we focus on this particular use-case\\nto develop a rigorous examination and comparison framework for analyzing\\nrecurrent neural networks (RNNs) applied to time series prediction using the\\nnovel neuro-evolutionary process known as Evolutionary eXploration of\\nAugmenting Memory Models (EXAMM). Specifically, we use our EXAMM-based analysis\\nto investigate the capabilities of recurrent memory cells and the\\ngeneralization ability afforded by various complex recurrent connectivity\\npatterns that span one or more steps in time, i.e., deep recurrent connections.\\nEXAMM, in this study, was used to train over 10.56 million RNNs in 5,280\\nrepeated experiments with varying components. While many modern, often\\nhand-crafted RNNs rely on complex memory cells (which have internal recurrent\\nconnections that only span a single time step) operating under the assumption\\nthat these sufficiently latch information and handle long term dependencies,\\nour results show that networks evolved with deep recurrent connections perform\\nsignificantly better than those without. More importantly, in some cases, the\\nbest performing RNNs consisted of only simple neurons and deep time skip\\nconnections, without any memory cells. These results strongly suggest that\\nutilizing deep time skip connections in RNNs for time series data prediction\\nnot only deserves further, dedicated study, but also demonstrate the potential\\nof neuro-evolution as a means to better study, understand, and train effective\\nRNNs.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.09593</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.09593</id><submitter>Vu Nguyen</submitter><version version=\"v1\"><date>Fri, 20 Sep 2019 16:14:34 GMT</date><size>3836kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 13:48:56 GMT</date><size>5450kb</size><source_type>D</source_type></version><title>Bayesian Optimization for Iterative Learning</title><authors>Vu Nguyen and Sebastian Schulze and Michael A Osborne</authors><categories>cs.LG stat.ML</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The success of deep (reinforcement) learning systems crucially depends on the\\ncorrect choice of hyperparameters which are notoriously sensitive and expensive\\nto evaluate. Training these systems typically requires running iterative\\nprocesses over multiple epochs or episodes. Traditional approaches only\\nconsider final performances of a hyperparameter although intermediate\\ninformation from the learning curve is readily available. In this paper, we\\npresent a Bayesian optimization approach which exploits the iterative structure\\nof learning algorithms for efficient hyperparameter tuning. First, we transform\\neach training curve into a numeric score. Second, we selectively augment the\\ndata using the auxiliary information from the curve. This augmentation step\\nenables modeling efficiency while preventing the ill-conditioned issue of\\nGaussian process covariance matrix happened when adding the whole curve. We\\ndemonstrate the efficiency of our algorithm by tuning hyperparameters for the\\ntraining of deep reinforcement learning agents and convolutional neural\\nnetworks. Our algorithm outperforms all existing baselines in identifying\\noptimal hyperparameters in minimal time.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.09633</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.09633</id><submitter>Keith Burghardt</submitter><version version=\"v1\"><date>Fri, 20 Sep 2019 17:54:49 GMT</date><size>541kb</size><source_type>D</source_type></version><title>Quantifying the Impact of Cognitive Biases in Question-Answering Systems</title><authors>Keith Burghardt, Tad Hogg, Kristina Lerman</authors><categories>cs.HC cs.SI</categories><comments>9 pages, 5 figures</comments><journal-ref>a short version is in the Proceeding of the Twelfth International\\n  Conference on Web and Social Media (ICWSM-18), pp. 568-571 (2018)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourcing can identify high-quality solutions to problems; however,\\nindividual decisions are constrained by cognitive biases. We investigate some\\nof these biases in an experimental model of a question-answering system. In\\nboth natural and controlled experiments, we observe a strong position bias in\\nfavor of answers appearing earlier in a list of choices. This effect is\\nenhanced by three cognitive factors: the attention an answer receives, its\\nperceived popularity, and cognitive load, measured by the number of choices a\\nuser has to process. While separately weak, these effects synergistically\\namplify position bias and decouple user choices of best answers from their\\nintrinsic quality. We end our paper by discussing the novel ways we can apply\\nthese findings to substantially improve how high-quality answers are found in\\nquestion-answering systems.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.09725</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.09725</id><submitter>Qiqi Hou</submitter><version version=\"v1\"><date>Fri, 20 Sep 2019 21:36:30 GMT</date><size>6887kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 23:24:17 GMT</date><size>6888kb</size><source_type>D</source_type></version><title>Context-Aware Image Matting for Simultaneous Foreground and Alpha\\n  Estimation</title><authors>Qiqi Hou, Feng Liu</authors><categories>cs.CV</categories><comments>This is the camera ready version of ICCV2019 paper</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Natural image matting is an important problem in computer vision and\\ngraphics. It is an ill-posed problem when only an input image is available\\nwithout any external information. While the recent deep learning approaches\\nhave shown promising results, they only estimate the alpha matte. This paper\\npresents a context-aware natural image matting method for simultaneous\\nforeground and alpha matte estimation. Our method employs two encoder networks\\nto extract essential information for matting. Particularly, we use a matting\\nencoder to learn local features and a context encoder to obtain more global\\ncontext information. We concatenate the outputs from these two encoders and\\nfeed them into decoder networks to simultaneously estimate the foreground and\\nalpha matte. To train this whole deep neural network, we employ both the\\nstandard Laplacian loss and the feature loss: the former helps to achieve high\\nnumerical performance while the latter leads to more perceptually plausible\\nresults. We also report several data augmentation strategies that greatly\\nimprove the network\\'s generalization performance. Our qualitative and\\nquantitative experiments show that our method enables high-quality matting for\\na single natural image. Our inference codes and models have been made publicly\\navailable at https://github.com/hqqxyy/Context-Aware-Matting.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.09756</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.09756</id><submitter>Sameer Kumar</submitter><version version=\"v1\"><date>Sat, 21 Sep 2019 01:12:38 GMT</date><size>559kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 25 Sep 2019 17:03:37 GMT</date><size>600kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 2 Oct 2019 18:37:01 GMT</date><size>614kb</size><source_type>D</source_type></version><title>Scale MLPerf-0.6 models on Google TPU-v3 Pods</title><authors>Sameer Kumar, Victor Bitorff, Dehao Chen, Chiachen Chou, Blake\\n  Hechtman, HyoukJoong Lee, Naveen Kumar, Peter Mattson, Shibo Wang, Tao Wang,\\n  Yuanzhong Xu, Zongwei Zhou</authors><categories>cs.LG cs.AI cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent submission of Google TPU-v3 Pods to the industry wide MLPerf v0.6\\ntraining benchmark demonstrates the scalability of a suite of industry relevant\\nML models. MLPerf defines a suite of models, datasets and rules to follow when\\nbenchmarking to ensure results are comparable across hardware, frameworks and\\ncompanies. Using this suite of models, we discuss the optimizations and\\ntechniques including choice of optimizer, spatial partitioning and weight\\nupdate sharding necessary to scale to 1024 TPU chips. Furthermore, we identify\\nproperties of models that make scaling them challenging, such as limited data\\nparallelism and unscaled weights. These optimizations contribute to record\\nperformance in transformer, Resnet-50 and SSD in the Google MLPerf-0.6\\nsubmission.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.09757</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.09757</id><submitter>Yixing Xu</submitter><version version=\"v1\"><date>Sat, 21 Sep 2019 01:21:16 GMT</date><size>2524kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 03:06:55 GMT</date><size>2524kb</size><source_type>D</source_type></version><title>Positive-Unlabeled Compression on the Cloud</title><authors>Yixing Xu, Yunhe Wang, Hanting Chen, Kai Han, Chunjing Xu, Dacheng\\n  Tao, Chang Xu</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many attempts have been done to extend the great success of convolutional\\nneural networks (CNNs) achieved on high-end GPU servers to portable devices\\nsuch as smart phones. Providing compression and acceleration service of deep\\nlearning models on the cloud is therefore of significance and is attractive for\\nend users. However, existing network compression and acceleration approaches\\nusually fine-tuning the svelte model by requesting the entire original training\\ndata (\\\\eg ImageNet), which could be more cumbersome than the network itself and\\ncannot be easily uploaded to the cloud. In this paper, we present a novel\\npositive-unlabeled (PU) setting for addressing this problem. In practice, only\\na small portion of the original training set is required as positive examples\\nand more useful training examples can be obtained from the massive unlabeled\\ndata on the cloud through a PU classifier with an attention based multi-scale\\nfeature extractor. We further introduce a robust knowledge distillation (RKD)\\nscheme to deal with the class imbalance problem of these newly augmented\\ntraining examples. The superiority of the proposed method is verified through\\nexperiments conducted on the benchmark models and datasets. We can use only\\n$8\\\\%$ of uniformly selected data from the ImageNet to obtain an efficient model\\nwith comparable performance to the baseline ResNet-34.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.09767</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.09767</id><submitter>Zehui Yao</submitter><version version=\"v1\"><date>Sat, 21 Sep 2019 03:40:02 GMT</date><size>1558kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 10:18:21 GMT</date><size>1544kb</size><source_type>D</source_type></version><title>IntersectGAN: Learning Domain Intersection for Generating Images with\\n  Multiple Attributes</title><authors>Zehui Yao, Boyan Zhang, Zhiyong Wang, Wanli Ouyang, Dong Xu, Dagan\\n  Feng</authors><categories>cs.CV</categories><doi>10.1145/3343031.3350908</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative adversarial networks (GANs) have demonstrated great success in\\ngenerating various visual content. However, images generated by existing GANs\\nare often of attributes (e.g., smiling expression) learned from one image\\ndomain. As a result, generating images of multiple attributes requires many\\nreal samples possessing multiple attributes which are very resource expensive\\nto be collected. In this paper, we propose a novel GAN, namely IntersectGAN, to\\nlearn multiple attributes from different image domains through an intersecting\\narchitecture. For example, given two image domains $X_1$ and $X_2$ with certain\\nattributes, the intersection $X_1 \\\\cap X_2$ denotes a new domain where images\\npossess the attributes from both $X_1$ and $X_2$ domains. The proposed\\nIntersectGAN consists of two discriminators $D_1$ and $D_2$ to distinguish\\nbetween generated and real samples of different domains, and three generators\\nwhere the intersection generator is trained against both discriminators. And an\\noverall adversarial loss function is defined over three generators. As a\\nresult, our proposed IntersectGAN can be trained on multiple domains of which\\neach presents one specific attribute, and eventually eliminates the need of\\nreal sample images simultaneously possessing multiple attributes. By using the\\nCelebFaces Attributes dataset, our proposed IntersectGAN is able to produce\\nhigh quality face images possessing multiple attributes (e.g., a face with\\nblack hair and a smiling expression). Both qualitative and quantitative\\nevaluations are conducted to compare our proposed IntersectGAN with other\\nbaseline methods. Besides, several different applications of IntersectGAN have\\nbeen explored with promising results.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.09803</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.09803</id><submitter>Huangying Zhan</submitter><version version=\"v1\"><date>Sat, 21 Sep 2019 10:00:21 GMT</date><size>2524kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 12:26:26 GMT</date><size>2524kb</size><source_type>D</source_type></version><title>Visual Odometry Revisited: What Should Be Learnt?</title><authors>Huangying Zhan, Chamara Saroj Weerasekera, Jiawang Bian, Ian Reid</authors><categories>cs.CV</categories><comments>Demo video: https://youtu.be/Nl8mFU4SJKY Code:\\n  https://github.com/Huangying-Zhan/DF-VO</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we present a monocular visual odometry (VO) algorithm which\\nleverages geometry-based methods and deep learning. Most existing VO/SLAM\\nsystems with superior performance are based on geometry and have to be\\ncarefully designed for different application scenarios. Moreover, most\\nmonocular systems suffer from scale-drift issue. Some recent deep learning\\nworks learn VO in an end-to-end manner but the performance of these deep\\nsystems is still not comparable to geometry-based methods. In this work, we\\nrevisit the basics of VO and explore the right way for integrating deep\\nlearning with epipolar geometry and Perspective-n-Point (PnP) method.\\nSpecifically, we train two convolutional neural networks (CNNs) for estimating\\nsingle-view depths and two-view optical flows as intermediate outputs. With the\\ndeep predictions, we design a simple but robust frame-to-frame VO algorithm\\n(DF-VO) which outperforms pure deep learning-based and geometry-based methods.\\nMore importantly, our system does not suffer from the scale-drift issue being\\naided by a scale consistent single-view depth CNN. Extensive experiments on\\nKITTI dataset shows the robustness of our system and a detailed ablation study\\nshows the effect of different factors in our system.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.09992</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.09992</id><submitter>Uzi Pereg</submitter><version version=\"v1\"><date>Sun, 22 Sep 2019 12:38:29 GMT</date><size>91kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 17:20:21 GMT</date><size>137kb</size><source_type>D</source_type></version><title>Entanglement-Assisted Capacity of Quantum Channels with Side Information</title><authors>Uzi Pereg</authors><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Entanglement-assisted communication over a random-parameter quantum channel\\nwith either causal or non-causal channel side information (CSI) at the encoder\\nis considered. This describes a scenario where the quantum channel depends on\\nthe quantum state of the input environment. While Bob, the decoder, has no\\naccess to this state, Alice, the transmitter, performs a sequence of projective\\nmeasurements on her environment and encodes her message accordingly. Dupuis\\nestablished the entanglement-assisted capacity with non-causal CSI. Here, we\\nestablish characterization in the causal setting, and also give an alternative\\nproof technique and further observations for the non-causal setting.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.10031</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.10031</id><submitter>Peilun Wu</submitter><version version=\"v1\"><date>Sun, 22 Sep 2019 15:34:27 GMT</date><size>371kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 6 Oct 2019 16:40:12 GMT</date><size>474kb</size><source_type>D</source_type></version><title>LuNet: A Deep Neural Network for Network Intrusion Detection</title><authors>Peilun Wu and Hui Guo</authors><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network attack is a significant security issue for modern society. From small\\nmobile devices to large cloud platforms, almost all computing products, used in\\nour daily life, are networked and potentially under the threat of network\\nintrusion. With the fast-growing network users, network intrusions become more\\nand more frequent, volatile and advanced. Being able to capture intrusions in\\ntime for such a large scale network is critical and very challenging. To this\\nend, the machine learning (or AI) based network intrusion detection (NID), due\\nto its intelligent capability, has drawn increasing attention in recent years.\\nCompared to the traditional signature-based approaches, the AI-based solutions\\nare more capable of detecting variants of advanced network attacks. However,\\nthe high detection rate achieved by the existing designs is usually accompanied\\nby a high rate of false alarms, which may significantly discount the overall\\neffectiveness of the intrusion detection system. In this paper, we consider the\\nexistence of spatial and temporal features in the network traffic data and\\npropose a hierarchical CNN+RNN neural network, LuNet. In LuNet, the\\nconvolutional neural network (CNN) and the recurrent neural network (RNN) learn\\ninput traffic data in sync with a gradually increasing granularity such that\\nboth spatial and temporal features of the data can be effectively extracted.\\nOur experiments on two network traffic datasets show that compared to the\\nstate-of-the-art network intrusion detection techniques, LuNet not only offers\\na high level of detection capability but also has a much low rate of false\\npositive-alarm.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.10074</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.10074</id><submitter>Nikolai Matni</submitter><version version=\"v1\"><date>Sun, 22 Sep 2019 19:18:27 GMT</date><size>2134kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 15:21:36 GMT</date><size>1056kb</size><source_type>D</source_type></version><title>Distributed and Localized Model Predictive Control via System Level\\n  Synthesis</title><authors>Carmen Amo Alonso and Nikolai Matni</authors><categories>math.OC cs.SY eess.SY</categories><comments>Extended version of ACC 2020 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the Distributed and Localized Model Predictive Control (DLMPC)\\nalgorithm for large-scale structured linear systems, wherein only local state\\nand model information needs to be exchanged between subsystems for the\\ncomputation and implementation of control actions. We use the System Level\\nSynthesis (SLS) framework to reformulate the MPC problem as an optimization\\nproblem over closed loop system responses, and show that this allows us to\\nnaturally impose localized communication constraints between sub-controllers,\\nsuch that only local state and system model information needs to be exchanged\\nfor both computation and implementation of closed loop MPC control policies. In\\nparticular, we show that the structure of the resulting optimization problem\\ncan be exploited to develop an Alternating Direction Method of Multipliers\\n(ADMM) based algorithm that allows for distributed and localized computation of\\ncontrol decisions. Moreover, our approach can accommodate constraints and\\nobjective functions that couple the behavior of different subsystems, so long\\nas the coupled systems are able to communicate directly with each other,\\nallowing for a broader class of MPC problems to be solved via distributed\\noptimization. We conclude with numerical simulations to demonstrate the\\nusefulness of our method, and in particular, we demonstrate that the\\ncomputational complexity of the subproblems solved by each subsystem in DLMPC\\nis independent of the size of the global system.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.10274</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.10274</id><submitter>Nicolas Robinson-Garcia</submitter><version version=\"v1\"><date>Mon, 23 Sep 2019 10:47:27 GMT</date><size>531kb</size></version><version version=\"v2\"><date>Tue, 1 Oct 2019 08:25:25 GMT</date><size>530kb</size></version><title>The differing meanings of indicators under different policy contexts.\\n  The case of internationalisation</title><authors>Nicolas Robinson-Garcia and Ismael Rafols</authors><categories>cs.DL</categories><comments>Unrevised version to be published in the book \\'Evaluative\\n  informetrics - the art of metrics-based research assessment. Festschrift in\\n  honour of Henk F. Moed\\' edited by Cinzia Daraio and Wolfgang Glanzel.\\n  Corrected labeling of axes in Figure 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this chapter we build upon Moed\\'s conceptual contributions on the\\nimportance of the policy context when using and interpreting scientometric\\nindicators. We focus on the use of indicators in research evaluation regarding\\ninternationalisation policies. The globalization of higher education presents\\nimportant challenges to institutions worldwide, which are confronted with\\ntensions derived from the need to respond both, to their local necessities and\\ndemands while participating in global networks. In this context, indicators\\nhave served as measures for monitoring and enforcing internationalisation\\npolicies, in many cases interpreting them regardless of the policy context in\\nwhich they are enforced. We will analyse three examples of indicators related\\nto internationalisation. The first one is about international collaborations,\\nunder the assumption that a greater number of internationally co-authored\\npublications will benefit a national science system as it will result in higher\\ncitation impact. The second one relates to the promotion of English language as\\nthe dominant language of science. The third case analyses how different policy\\ncontexts shape the selection and construction of indicators, sometimes in a\\npartial way which does not properly reflect the phenomenon under study. The\\nexamples illustrate that the interpretation and policy implications of the\\n\\'same\\' S&amp;T indicators differ depending on specific contexts.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.10300</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.10300</id><submitter>Edouard Pauwels</submitter><version version=\"v1\"><date>Mon, 23 Sep 2019 11:39:16 GMT</date><size>39kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 15:00:51 GMT</date><size>42kb</size><source_type>D</source_type></version><title>Conservative set valued fields, automatic differentiation, stochastic\\n  gradient method and deep learning</title><authors>J\\\\\\'er\\\\^ome Bolte and Edouard Pauwels</authors><categories>math.OC cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern problems in AI or in numerical analysis require nonsmooth approaches\\nwith a flexible calculus. We introduce generalized derivatives called\\nconservative fields for which we develop a calculus and provide representation\\nformulas. Functions having a conservative field are called path differentiable:\\nconvex, concave, Clarke regular and any semialgebraic Lipschitz continuous\\nfunctions are path differentiable. Using Whitney stratification techniques for\\nsemialgebraic and definable sets, our model provides variational formulas for\\nnonsmooth automatic differentiation oracles, as for instance the famous\\nbackpropagation algorithm in deep learning. Our differential model is applied\\nto establish the convergence in values of nonsmooth stochastic gradient methods\\nas they are implemented in practice.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.10360</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.10360</id><submitter>ZhenLiang Ni</submitter><version version=\"v1\"><date>Mon, 23 Sep 2019 13:34:00 GMT</date><size>1347kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 27 Sep 2019 14:12:04 GMT</date><size>1347kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Wed, 2 Oct 2019 09:24:54 GMT</date><size>1347kb</size><source_type>D</source_type></version><title>RAUNet: Residual Attention U-Net for Semantic Segmentation of Cataract\\n  Surgical Instruments</title><authors>Zhen-Liang Ni, Gui-Bin Bian, Xiao-Hu Zhou, Zeng-Guang Hou, Xiao-Liang\\n  Xie, Chen Wang, Yan-Jie Zhou, Rui-Qi Li, and Zhen Li</authors><categories>cs.CV</categories><comments>Accepted by the 26th International Conference on Neural Information\\n  Processing (ICONIP2019). arXiv admin note: cs.CV =&gt; eess.IV cs.CV</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semantic segmentation of surgical instruments plays a crucial role in\\nrobot-assisted surgery. However, accurate segmentation of cataract surgical\\ninstruments is still a challenge due to specular reflection and class imbalance\\nissues. In this paper, an attention-guided network is proposed to segment the\\ncataract surgical instrument. A new attention module is designed to learn\\ndiscriminative features and address the specular reflection issue. It captures\\nglobal context and encodes semantic dependencies to emphasize key semantic\\nfeatures, boosting the feature representation. This attention module has very\\nfew parameters, which helps to save memory. Thus, it can be flexibly plugged\\ninto other networks. Besides, a hybrid loss is introduced to train our network\\nfor addressing the class imbalance issue, which merges cross entropy and\\nlogarithms of Dice loss. A new dataset named Cata7 is constructed to evaluate\\nour network. To the best of our knowledge, this is the first cataract surgical\\ninstrument dataset for semantic segmentation. Based on this dataset, RAUNet\\nachieves state-of-the-art performance 97.71% mean Dice and 95.62% mean IOU.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.10391</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.10391</id><submitter>Daniel Rueckert</submitter><version version=\"v1\"><date>Mon, 23 Sep 2019 14:35:55 GMT</date><size>1536kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 24 Sep 2019 14:26:05 GMT</date><size>1536kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Mon, 30 Sep 2019 12:41:42 GMT</date><size>1782kb</size><source_type>D</source_type></version><title>Model-Based and Data-Driven Strategies in Medical Image Computing</title><authors>Daniel Rueckert and Julia A. Schnabel</authors><categories>cs.CV</categories><comments>Accepted in IEEE Proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-based approaches for image reconstruction, analysis and interpretation\\nhave made significant progress over the last decades. Many of these approaches\\nare based on either mathematical, physical or biological models. A challenge\\nfor these approaches is the modelling of the underlying processes (e.g. the\\nphysics of image acquisition or the patho-physiology of a disease) with\\nappropriate levels of detail and realism. With the availability of large\\namounts of imaging data and machine learning (in particular deep learning)\\ntechniques, data-driven approaches have become more widespread for use in\\ndifferent tasks in reconstruction, analysis and interpretation. These\\napproaches learn statistical models directly from labelled or unlabeled image\\ndata and have been shown to be very powerful for extracting clinically useful\\ninformation from medical imaging. While these data-driven approaches often\\noutperform traditional model-based approaches, their clinical deployment often\\nposes challenges in terms of robustness, generalization ability and\\ninterpretability. In this article, we discuss what developments have motivated\\nthe shift from model-based approaches towards data-driven strategies and what\\npotential problems are associated with the move towards purely data-driven\\napproaches, in particular deep learning. We also discuss some of the open\\nchallenges for data-driven approaches, e.g. generalization to new unseen data\\n(e.g. transfer learning), robustness to adversarial attacks and\\ninterpretability. Finally, we conclude with a discussion on how these\\napproaches may lead to the development of more closely coupled imaging\\npipelines that are optimized in an end-to-end fashion.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.10424</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.10424</id><submitter>Sujan Vijayaraj</submitter><version version=\"v1\"><date>Mon, 23 Sep 2019 15:29:32 GMT</date><size>443kb</size></version><version version=\"v2\"><date>Sat, 5 Oct 2019 10:25:11 GMT</date><size>453kb</size></version><title>Impartial binary decisions through qubits</title><authors>Sujan Vijayaraj, S. Nandakumar</authors><categories>quant-ph cs.GT</categories><comments>5 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Binary decisions are the simplest form of decisions that are made in our\\ndaily lives. Examples include choosing a two-way path in a maze, accepting or\\ndeclining an offer, etc. These decisions are also made by computers, machines\\nand various electronic components. But decisions made on these devices can be\\npartial and deterministic, and hence compromised. In this paper, a simple\\nframework to implement binary decisions using one or many qubits is presented.\\nSuch systems are based on a separate hardware infrastructure rather than\\ncomputer codes. This helps enable true randomness and impartial decision\\nmaking. The multi-armed bandit problem is used to highlight the decision making\\nability of qubits by predictive modelling based on quantum Bayesianism.\\nBipartite and multipartite entangled states are also used to solve specific\\ncases of the problem.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.10430</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.10430</id><submitter>Gregor Wiedemann</submitter><version version=\"v1\"><date>Mon, 23 Sep 2019 15:38:02 GMT</date><size>273kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 13:26:04 GMT</date><size>274kb</size><source_type>D</source_type></version><title>Does BERT Make Any Sense? Interpretable Word Sense Disambiguation with\\n  Contextualized Embeddings</title><authors>Gregor Wiedemann, Steffen Remus, Avi Chawla, Chris Biemann</authors><categories>cs.CL</categories><comments>10 pages, 3 figures, 6 tables, Accepted for Konferenz zur\\n  Verarbeitung nat\\\\&quot;urlicher Sprache / Conference on Natural Language\\n  Processing (KONVENS) 2019, Erlangen/Germany</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contextualized word embeddings (CWE) such as provided by ELMo (Peters et al.,\\n2018), Flair NLP (Akbik et al., 2018), or BERT (Devlin et al., 2019) are a\\nmajor recent innovation in NLP. CWEs provide semantic vector representations of\\nwords depending on their respective context. Their advantage over static word\\nembeddings has been shown for a number of tasks, such as text classification,\\nsequence tagging, or machine translation. Since vectors of the same word type\\ncan vary depending on the respective context, they implicitly provide a model\\nfor word sense disambiguation (WSD). We introduce a simple but effective\\napproach to WSD using a nearest neighbor classification on CWEs. We compare the\\nperformance of different CWE models for the task and can report improvements\\nabove the current state of the art for two standard WSD benchmark datasets. We\\nfurther show that the pre-trained BERT model is able to place polysemic words\\ninto distinct \\'sense\\' regions of the embedding space, while ELMo and Flair NLP\\ndo not seem to possess this ability.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.10470</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.10470</id><submitter>Vishvak Murahari</submitter><version version=\"v1\"><date>Mon, 23 Sep 2019 16:47:15 GMT</date><size>3137kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 03:01:48 GMT</date><size>3137kb</size><source_type>D</source_type></version><title>Improving Generative Visual Dialog by Answering Diverse Questions</title><authors>Vishvak Murahari, Prithvijit Chattopadhyay, Dhruv Batra, Devi Parikh,\\n  Abhishek Das</authors><categories>cs.LG cs.AI cs.CL cs.CV stat.ML</categories><comments>EMNLP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prior work on training generative Visual Dialog models with reinforcement\\nlearning(Das et al.) has explored a Qbot-Abot image-guessing game and shown\\nthat this \\'self-talk\\' approach can lead to improved performance at the\\ndownstream dialog-conditioned image-guessing task. However, this improvement\\nsaturates and starts degrading after a few rounds of interaction, and does not\\nlead to a better Visual Dialog model. We find that this is due in part to\\nrepeated interactions between Qbot and Abot during self-talk, which are not\\ninformative with respect to the image. To improve this, we devise a simple\\nauxiliary objective that incentivizes Qbot to ask diverse questions, thus\\nreducing repetitions and in turn enabling Abot to explore a larger state space\\nduring RL ie. be exposed to more visual concepts to talk about, and varied\\nquestions to answer. We evaluate our approach via a host of automatic metrics\\nand human studies, and demonstrate that it leads to better dialog, ie. dialog\\nthat is more diverse (ie. less repetitive), consistent (ie. has fewer\\nconflicting exchanges), fluent (ie. more human-like),and detailed, while still\\nbeing comparably image-relevant as prior work and ablations.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.10491</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.10491</id><submitter>Pramesh Singh</submitter><version version=\"v1\"><date>Mon, 23 Sep 2019 17:19:50 GMT</date><size>493kb</size><source_type>D</source_type></version><title>Reduced network extremal ensemble learning (RenEEL) scheme for community\\n  detection in complex networks</title><authors>Jiahao Guo, Pramesh Singh, Kevin E. Bassler</authors><categories>physics.soc-ph cond-mat.stat-mech cs.LG cs.SI</categories><journal-ref>Scientific Reports 9, 14234 (2019)</journal-ref><doi>10.1038/s41598-019-50739-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an ensemble learning scheme for community detection in complex\\nnetworks. The scheme uses a Machine Learning algorithmic paradigm we call\\nExtremal Ensemble Learning. It uses iterative extremal updating of an ensemble\\nof network partitions, which can be found by a conventional base algorithm, to\\nfind a node partition that maximizes modularity. At each iteration, core groups\\nof nodes that are in the same community in every ensemble partition are\\nidentified and used to form a reduced network. Partitions of the reduced\\nnetwork are then found and used to update the ensemble. The smaller size of the\\nreduced network makes the scheme efficient. We use the scheme to analyze the\\ncommunity structure in a set of commonly studied benchmark networks and find\\nthat it outperforms all other known methods for finding the partition with\\nmaximum modularity.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.10554</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.10554</id><submitter>Federico Albanese</submitter><version version=\"v1\"><date>Mon, 23 Sep 2019 18:23:56 GMT</date><size>537kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 14:52:36 GMT</date><size>537kb</size><source_type>D</source_type></version><title>A data-driven model for Mass Media influence in electoral context</title><authors>Federico Albanese, Claudio J. Tessone, Viktoriya Semeshenko, Pablo\\n  Balenzuela</authors><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mass Media outlets have occupied the central role of the political scenario,\\nand are persuasive in the process of opinion formation of the citizens. In\\nparticular, the study of the relationship between Mass Media and behaviour of\\ncitizens can be monitored during election times, given the accessibility of\\nnews related to the candidates and polls that precede the election\\'s day. In\\nthis paper we present a novel two-dimensional data driven Mass Media model\\nbased on semantic analysis of newspapers and national election surveys, which\\nwe use to analyse how a single influence mechanism should behave in order to\\nreproduce the behaviour of the voters. Using simple and feasible rules for\\ndynamics, we were able to find a notable agreement between the model\\'s\\npredictions and the polls which help us to understand the underlying mechanisms\\nof the interactions between reader and media.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.10660</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.10660</id><submitter>Daiki Matsunaga</submitter><version version=\"v1\"><date>Tue, 24 Sep 2019 00:19:32 GMT</date><size>260kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 23:47:20 GMT</date><size>260kb</size><source_type>D</source_type></version><title>Exploring Graph Neural Networks for Stock Market Predictions with\\n  Rolling Window Analysis</title><authors>Daiki Matsunaga, Toyotaro Suzumura, Toshihiro Takahashi</authors><categories>cs.LG q-fin.ST</categories><comments>Under Review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been a surge of interest in the use of machine learning\\nto help aid in the accurate predictions of financial markets. Despite the\\nexciting advances in this cross-section of finance and AI, many of the current\\napproaches are limited to using technical analysis to capture historical trends\\nof each stock price and thus limited to certain experimental setups to obtain\\ngood prediction results. On the other hand, professional investors additionally\\nuse their rich knowledge of inter-market and inter-company relations to map the\\nconnectivity of companies and events, and use this map to make better market\\npredictions. For instance, they would predict the movement of a certain\\ncompany\\'s stock price based not only on its former stock price trends but also\\non the performance of its suppliers or customers, the overall industry,\\nmacroeconomic factors and trade policies. This paper investigates the\\neffectiveness of work at the intersection of market predictions and graph\\nneural networks, which hold the potential to mimic the ways in which investors\\nmake decisions by incorporating company knowledge graphs directly into the\\npredictive model. The main goal of this work is to test the validity of this\\napproach across different markets and longer time horizons for backtesting\\nusing rolling window analysis. In this work, we concentrate on the prediction\\nof individual stock prices in the Japanese Nikkei 225 market over a period of\\nroughly 20 years. For the knowledge graph, we use the Nikkei Value Search data,\\nwhich is a rich dataset showing mainly supplier relations among Japanese and\\nforeign companies. Our preliminary results show a 29.5% increase and a 2.2-fold\\nincrease in the return ratio and Sharpe ratio, respectively, when compared to\\nthe market benchmark, as well as a 6.32% increase and 1.3-fold increase,\\nrespectively, compared to the baseline LSTM model.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.10681</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.10681</id><submitter>Peixiang Zhong</submitter><version version=\"v1\"><date>Tue, 24 Sep 2019 02:08:29 GMT</date><size>395kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 14:00:22 GMT</date><size>395kb</size><source_type>D</source_type></version><title>Knowledge-Enriched Transformer for Emotion Detection in Textual\\n  Conversations</title><authors>Peixiang Zhong, Di Wang, Chunyan Miao</authors><categories>cs.CL cs.AI cs.LG</categories><comments>EMNLP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Messages in human conversations inherently convey emotions. The task of\\ndetecting emotions in textual conversations leads to a wide range of\\napplications such as opinion mining in social networks. However, enabling\\nmachines to analyze emotions in conversations is challenging, partly because\\nhumans often rely on the context and commonsense knowledge to express emotions.\\nIn this paper, we address these challenges by proposing a Knowledge-Enriched\\nTransformer (KET), where contextual utterances are interpreted using\\nhierarchical self-attention and external commonsense knowledge is dynamically\\nleveraged using a context-aware affective graph attention mechanism.\\nExperiments on multiple textual conversation datasets demonstrate that both\\ncontext and commonsense knowledge are consistently beneficial to the emotion\\ndetection performance. In addition, the experimental results show that our KET\\nmodel outperforms the state-of-the-art models on most of the tested datasets in\\nF1 score.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.10685</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.10685</id><submitter>Qi Luo</submitter><version version=\"v1\"><date>Tue, 24 Sep 2019 02:52:08 GMT</date><size>465kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 6 Oct 2019 11:42:46 GMT</date><size>466kb</size><source_type>D</source_type></version><title>Phase Retrieval via Smooth Amplitude Flow</title><authors>Q. Luo, H. Wang</authors><categories>eess.SP cs.IT math.IT</categories><comments>18 pages, 6 figures, two referrences added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Phase retrieval (PR) is an inverse problem about recovering a signal from\\nphaseless linear measurements. This problem can be effectively solved by\\nminimizing a nonconvex amplitude-based loss function. However, this loss\\nfunction is non-smooth. To address the non-smoothness, a series of methods have\\nbeen proposed by adding truncating, reweighting and smoothing operations to\\nadjust the gradient or the loss function and achieved better performance. But\\nthese operations bring about extra rules and parameters that need to be\\ncarefully designed. Unlike previous works, we present a smooth amplitude flow\\nmethod (SAF) which minimizes a novel loss function, without additionally\\nmodifying the gradient or the loss function during gradient descending. Such a\\nnew heuristic can be regarded as a smooth version of the original non-smooth\\namplitude-based loss function. We prove that SAF can converge geometrically to\\na global optimal point via the gradient algorithm with an elaborate\\ninitialization stage with a high probability. Substantial numerical tests\\nempirically illustrate that the proposed heuristic is significantly superior to\\nthe original amplitude-based loss function and SAF also outperforms other\\nstate-of-the-art methods in terms of the recovery rate and the converging\\nspeed. Specially, it is numerically shown that SAF can stably recover the\\noriginal signal when number of measurements is smaller than the\\ninformation-theoretic limit for both the real and the complex Gaussian models.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.10773</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.10773</id><submitter>Minhao Cheng</submitter><version version=\"v1\"><date>Tue, 24 Sep 2019 09:27:08 GMT</date><size>1072kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 08:52:24 GMT</date><size>1073kb</size><source_type>D</source_type></version><title>Sign-OPT: A Query-Efficient Hard-label Adversarial Attack</title><authors>Minhao Cheng, Simranjit Singh, Patrick Chen, Pin-Yu Chen, Sijia Liu,\\n  Cho-Jui Hsieh</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the most practical problem setup for evaluating adversarial\\nrobustness of a machine learning system with limited access: the hard-label\\nblack-box attack setting for generating adversarial examples, where limited\\nmodel queries are allowed and only the decision is provided to a queried data\\ninput. Several algorithms have been proposed for this problem but they\\ntypically require huge amount (&gt;20,000) of queries for attacking one example.\\nAmong them, one of the state-of-the-art approaches (Cheng et al., 2019) showed\\nthat hard-label attack can be modeled as an optimization problem where the\\nobjective function can be evaluated by binary search with additional model\\nqueries, thereby a zeroth order optimization algorithm can be applied. In this\\npaper, we adopt the same optimization formulation but propose to directly\\nestimate the sign of gradient at any direction instead of the gradient itself,\\nwhich enjoys the benefit of single query. Using this single query oracle for\\nretrieving sign of directional derivative, we develop a novel query-efficient\\nSign-OPT approach for hard-label black-box attack. We provide a convergence\\nanalysis of the new algorithm and conduct experiments on several models on\\nMNIST, CIFAR-10 and ImageNet. We find that Sign-OPT attack consistently\\nrequires 5X to 10X fewer queries when compared to the current state-of-the-art\\napproaches, and usually converges to an adversarial example with smaller\\nperturbation.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.10853</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.10853</id><submitter>Tobias Weinzierl</submitter><version version=\"v1\"><date>Tue, 24 Sep 2019 12:54:55 GMT</date><size>53kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 08:15:17 GMT</date><size>53kb</size><source_type>D</source_type></version><title>A high-level characterisation and generalisation of\\n  communication-avoiding programming techniques</title><authors>Tobias Weinzierl</authors><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today\\'s hardware\\'s explosion of concurrency plus the explosion of data we\\nbuild upon in both machine learning and scientific simulations have\\nmultifaceted impact on how we write our codes. They have changed our notion of\\nperformance and, hence, of what a good code is: Good code has, first of all, to\\nbe able to exploit the unprecedented levels of parallelism. To do so, it has to\\nmanage to move the compute data into the compute facilities on time. As\\ncommunication and memory bandwidth cannot keep pace with the growth in compute\\ncapabilities and as latency increases---at least relative to what the hardware\\ncould do---communication-avoiding techniques gain importance. We characterise\\nand classify the field of communication-avoiding algorithms. A review of some\\nexamples of communication-avoiding programming by means of our new terminology\\nshows that we are well-advised to broaden our notion of\\n&quot;communication-avoiding&quot; and to look beyond numerical linear algebra. An\\nabstraction, generalisation and weakening of the term enriches our toolset of\\nhow to tackle the data movement challenges. Through this, we eventually gain\\naccess to a richer set of tools that we can use to deliver proper code for\\ncurrent and upcoming hardware generations.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.10879</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.10879</id><submitter>Jinfu Chen</submitter><version version=\"v1\"><date>Tue, 24 Sep 2019 13:23:16 GMT</date><size>778kb</size></version><version version=\"v2\"><date>Sat, 28 Sep 2019 09:56:18 GMT</date><size>1096kb</size></version><title>A Taxonomic Review of Adaptive Random Testing: Current Status,\\n  Classifications, and Issues</title><authors>Jinfu Chen, Hilary Ackah-Arthur, Chengying Mao, Patrick Kwaku Kudjo</authors><categories>cs.SE</categories><comments>The first draft of this review paper was completed in June 2017</comments><report-no>2017-06-30-REPORT</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random testing (RT) is a black-box software testing technique that tests\\nprograms by generating random test inputs. It is a widely used technique for\\nsoftware quality assurance, but there has been much debate by practitioners\\nconcerning its failure-detection effectiveness. RT is argued to be possibly\\nless effective by some researchers as it does not utilize any information about\\nthe program under test. Efforts to mainly improve the failure-detection\\ncapability of RT, have led to the proposition of Adaptive Random Testing (ART).\\nART takes advantage of the location information of previous non-fault-detecting\\ntest cases to enhance effectiveness as compared to RT. The approach has gained\\npopularity and has a large number of theoretical studies and methods that\\nemploy different notions. In this review, our goal is to provide an overview of\\nexisting ART studies. We classify all ART studies and assess existing ART\\nmethods for numeric programs with a focus on their motivation, strategy, and\\nfindings. The study also discusses several worthy avenues related to ART. The\\nreview uses 109 ART papers in several journals, workshops, and conference\\nproceedings. The results of the review show that significant research efforts\\nhave been made towards the field of ART, however further empirical studies are\\nstill required to make the technique applicable in different test scenarios in\\norder to impact on the industry.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11013</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11013</id><submitter>Jo\\\\~ao Santos</submitter><version version=\"v1\"><date>Tue, 24 Sep 2019 15:50:22 GMT</date><size>341kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 15:00:21 GMT</date><size>0kb</size><source_type>I</source_type></version><title>Towards the Uses of Blockchain in Mobile Health Services and\\n  Applications: A Survey</title><authors>Jo\\\\~ao Amaral Santos, Pedro R. M. In\\\\\\'acio, Bruno M. Silva</authors><categories>cs.CR cs.DC</categories><comments>Re-organization of the paper is required until it is published</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of Bitcoin and blockchain, the growth and adaptation of\\ncryptographic features and capabilities were quickly extended to new and\\nunderexplored areas, such as healthcare. Currently, blockchain is being\\nimplemented mainly as a mechanism to secure Electronic Health Records (EHRs).\\nHowever, new studies have shown that this technology can be a powerful tool in\\nempowering patients to control their own health data, as well for enabling a\\nfool-proof health data history and establishing medical responsibility. With\\nthe advent of mobile health (m-Health) sustained on service-oriented\\narchitectures, the adaptation of blockchain mechanisms into m-Health\\napplications creates the possibility for a more decentralized and available\\nhealthcare service. Hence, this paper presents a review of the current security\\nbest practices for m-Health including blockchain technologies in healthcare.\\nMoreover, it discusses and elaborates on identified open-issues and\\npotentialities regarding the uses of Blockchain. Finally, the paper proposes\\nconceptual solutions for future blockchain implementations for m-Health\\nServices and Applications.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11059</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11059</id><submitter>Luowei Zhou</submitter><version version=\"v1\"><date>Tue, 24 Sep 2019 17:17:26 GMT</date><size>453kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 02:16:27 GMT</date><size>453kb</size><source_type>D</source_type></version><title>Unified Vision-Language Pre-Training for Image Captioning and VQA</title><authors>Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso,\\n  Jianfeng Gao</authors><categories>cs.CV</categories><comments>The code and the pre-trained models are available at\\n  https://github.com/LuoweiZhou/VLP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a unified Vision-Language Pre-training (VLP) model. The\\nmodel is unified in that (1) it can be fine-tuned for either vision-language\\ngeneration (e.g., image captioning) or understanding (e.g., visual question\\nanswering) tasks, and (2) it uses a shared multi-layer transformer network for\\nboth encoding and decoding, which differs from many existing methods where the\\nencoder and decoder are implemented using separate models. The unified VLP\\nmodel is pre-trained on a large amount of image-text pairs using the\\nunsupervised learning objectives of two tasks: bidirectional and\\nsequence-to-sequence (seq2seq) masked vision-language prediction. The two tasks\\ndiffer solely in what context the prediction conditions on. This is controlled\\nby utilizing specific self-attention masks for the shared transformer network.\\nTo the best of our knowledge, VLP is the first reported model that achieves\\nstate-of-the-art results on both vision-language generation and understanding\\ntasks, as disparate as image captioning and visual question answering, across\\nthree challenging benchmark datasets: COCO Captions, Flickr30k Captions, and\\nVQA 2.0. The code and the pre-trained models are available at\\nhttps://github.com/LuoweiZhou/VLP.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11124</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11124</id><submitter>Yifan Xue</submitter><version version=\"v1\"><date>Tue, 24 Sep 2019 18:51:21 GMT</date><size>838kb</size></version><version version=\"v2\"><date>Sun, 29 Sep 2019 19:54:35 GMT</date><size>837kb</size></version><title>Supervised Vector Quantized Variational Autoencoder for Learning\\n  Interpretable Global Representations</title><authors>Yifan Xue, Michael Ding and Xinghua Lu</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning interpretable representations of data remains a central challenge in\\ndeep learning. When training a deep generative model, the observed data are\\noften associated with certain categorical labels, and, in parallel with\\nlearning to regenerate data and simulate new data, learning an interpretable\\nrepresentation of each class of data is also a process of acquiring knowledge.\\nHere, we present a novel generative model, referred to as the Supervised Vector\\nQuantized Variational AutoEncoder (S-VQ-VAE), which combines the power of\\nsupervised and unsupervised learning to obtain a unique, interpretable global\\nrepresentation for each class of data. Compared with conventional generative\\nmodels, our model has three key advantages: first, it is an integrative model\\nthat can simultaneously learn a feature representation for individual data\\npoint and a global representation for each class of data; second, the learning\\nof global representations with embedding codes is guided by supervised\\ninformation, which clearly defines the interpretation of each code; and third,\\nthe global representations capture crucial characteristics of different\\nclasses, which reveal similarity and differences of statistical structures\\nunderlying different groups of data. We evaluated the utility of S-VQ-VAE on a\\nmachine learning benchmark dataset, the MNIST dataset, and on gene expression\\ndata from the Library of Integrated Network-Based Cellular Signatures (LINCS).\\nWe proved that S-VQ-VAE was able to learn the global genetic characteristics of\\nsamples perturbed by the same class of perturbagen (PCL), and further revealed\\nthe mechanism correlations between PCLs. Such knowledge is crucial for\\npromoting new drug development for complex diseases like cancer.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11145</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11145</id><submitter>Timo C. Wunderlich</submitter><version version=\"v1\"><date>Tue, 24 Sep 2019 19:29:30 GMT</date><size>2179kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 09:02:01 GMT</date><size>2179kb</size><source_type>D</source_type></version><title>Brain-Inspired Hardware for Artificial Intelligence: Accelerated\\n  Learning in a Physical-Model Spiking Neural Network</title><authors>Timo C. Wunderlich, Akos F. Kungl, Eric M\\\\&quot;uller, Johannes Schemmel,\\n  Mihai Petrovici</authors><categories>cs.NE cs.AI cs.ET</categories><journal-ref>Artificial Neural Networks and Machine Learning - ICANN 2019:\\n  Theoretical Neural Computation. ICANN 2019. Lecture Notes in Computer\\n  Science, vol 11727. Springer, Cham</journal-ref><doi>10.1007/978-3-030-30487-4_10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future developments in artificial intelligence will profit from the existence\\nof novel, non-traditional substrates for brain-inspired computing. Neuromorphic\\ncomputers aim to provide such a substrate that reproduces the brain\\'s\\ncapabilities in terms of adaptive, low-power information processing. We present\\nresults from a prototype chip of the BrainScaleS-2 mixed-signal neuromorphic\\nsystem that adopts a physical-model approach with a 1000-fold acceleration of\\nspiking neural network dynamics relative to biological real time. Using the\\nembedded plasticity processor, we both simulate the Pong arcade video game and\\nimplement a local plasticity rule that enables reinforcement learning, allowing\\nthe on-chip neural network to learn to play the game. The experiment\\ndemonstrates key aspects of the employed approach, such as accelerated and\\nflexible learning, high energy efficiency and resilience to noise.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11160</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11160</id><submitter>Rojin Aslani</submitter><version version=\"v1\"><date>Tue, 24 Sep 2019 20:27:41 GMT</date><size>343kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 19:26:59 GMT</date><size>343kb</size><source_type>D</source_type></version><title>Energy Efficiency Maximization Via Joint Sub-Carrier Assignment and\\n  Power Control for OFDMA Full Duplex Networks</title><authors>Rojin Aslani, Mehdi Rasti, Ata Khalili</authors><categories>cs.IT eess.SP math.IT</categories><comments>This paper has been accepted by IEEE Transactions on Vehicular\\n  Technology</comments><doi>10.1109/TVT.2019.2944909</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop an energy efficient resource allocation scheme for\\northogonal frequency division multiple access (OFDMA) networks with in-band\\nfull-duplex (IBFD) communication between the base station and user equipments\\n(UEs) considering a realistic self-interference (SI) model. Our primary aim is\\nto maximize the system energy efficiency (EE) through a joint power control and\\nsub-carrier assignment in both the downlink (DL) and uplink (UL), where the\\nquality of service requirements of the UEs in DL and UL are guaranteed. The\\nformulated problem is non-convex due to the non-linear fractional objective\\nfunction and the non-convex feasible set which is generally intractable. In\\norder to handle this difficulty, we first use fractional programming to\\ntransform the fractional objective function to the subtractive form. Then, by\\nemploying Dinkelbach method, we propose an iterative algorithm in which an\\ninner problem is solved in each iteration. Applying majorization-minimization\\napproximation, we make the inner problem convex. Also, by introducing a penalty\\nfunction to handle integer sub-carrier assignment variables, we propose an\\niterative algorithm for addressing the inner problem. We show that our proposed\\nalgorithm converges to the locally optimal solution which is also demonstrated\\nby our simulation results. In addition, simulation results show that by\\napplying the IBFD capability in OFDMA networks with efficient SI cancellation\\ntechniques, our proposed resource allocation algorithm attains a 75% increase\\nin the EE as compared to the half-duplex system.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11197</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11197</id><submitter>Tanwi Mallick</submitter><version version=\"v1\"><date>Tue, 24 Sep 2019 21:38:29 GMT</date><size>1131kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 14:51:19 GMT</date><size>1132kb</size><source_type>D</source_type></version><title>Graph-Partitioning-Based Diffusion Convolution Recurrent Neural Network\\n  for Large-Scale Traffic Forecasting</title><authors>Tanwi Mallick, Prasanna Balaprakash, Eric Rask, and Jane Macfarlane</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traffic forecasting approaches are critical to developing adaptive strategies\\nfor mobility. Traffic patterns have complex spatial and temporal dependencies\\nthat make accurate forecasting on large highway networks a challenging task.\\nRecently, diffusion convolutional recurrent neural networks (DCRNNs) have\\nachieved state-of-the-art results in traffic forecasting by capturing the\\nspatiotemporal dynamics of the traffic. Despite the promising results, adopting\\nDCRNN for large highway networks still remains elusive because of computational\\nand memory bottlenecks. We present an approach to apply DCRNN for a large\\nhighway network. We use a graph-partitioning approach to decompose a large\\nhighway network into smaller networks and train them simultaneously on a\\ncluster with graphics processing units (GPU). For the first time, we forecast\\nthe traffic of the entire California highway network with 11,160 traffic sensor\\nlocations simultaneously. We show that our approach can be trained within 3\\nhours of wall-clock time using 64 GPUs to forecast speed with high accuracy.\\nFurther improvements in the accuracy are attained by including overlapping\\nsensor locations from nearby partitions and finding high-performing\\nhyperparameter configurations for the DCRNN using DeepHyper, a hyperparameter\\ntuning package. We demonstrate that a single DCRNN model can be used to train\\nand forecast the speed and flow simultaneously and the results preserve\\nfundamental traffic flow dynamics. We expect our approach for modeling a large\\nhighway network in short wall-clock time as a potential core capability in\\nadvanced highway traffic monitoring systems, where forecasts can be used to\\nadjust traffic management strategies proactively given anticipated future\\nconditions.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11199</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11199</id><submitter>Yu Tang</submitter><version version=\"v1\"><date>Tue, 24 Sep 2019 21:44:45 GMT</date><size>1946kb</size></version><version version=\"v2\"><date>Tue, 1 Oct 2019 04:42:01 GMT</date><size>1946kb</size></version><title>Security Risk Analysis of the Shorter-Queue Routing Policy for Two\\n  Symmetric Servers</title><authors>Yu Tang, Yining Wen, Li Jin</authors><categories>eess.SY cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we study the classical shortest queue problem under the\\ninfluence of malicious attacks, which is relevant to a variety of engineering\\nsystem including transportation, manufacturing, and communications. We consider\\na homogeneous Poisson arrival process of jobs and two parallel exponential\\nservers with symmetric service rates. A system operator route incoming jobs to\\nthe shorter queue; if the queues are equal, the job is routed randomly. A\\nmalicious attacker is able to intercept the operator\\'s routing instruction and\\noverwrite it with a randomly generated one. The operator is able to defend\\nindividual jobs to ensure correct routing. Both attacking and defending induce\\ntechnological costs. The attacker\\'s (resp. operator\\'s) decision is the\\nprobability of attacking (resp. defending) the routing of each job. We first\\nquantify the queuing cost for given strategy profiles by deriving a theoretical\\nupper bound for the cost. Then, we formulate a non-zero-sum attacker-defender\\ngame, characterize the equilibria in multiple regimes, and quantify the\\nsecurity risk. We find that the attacker\\'s best strategy is either to attack\\nall jobs or not to attack, and the defender\\'s strategy is strongly influenced\\nby the arrival rate of jobs. Finally, as a benchmark, we compare the security\\nrisks of the feedback-controlled system to a corresponding open-loop system\\nwith Bernoulli routing. We show that the shorter-queue policy has a higher\\n(resp. lower) security risk than the Bernoulli policy if the demand is lower\\n(resp. higher) than the service rate of one server.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11204</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11204</id><submitter>Emily Hannigan</submitter><version version=\"v1\"><date>Tue, 24 Sep 2019 22:01:10 GMT</date><size>1289kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 00:01:00 GMT</date><size>1301kb</size><source_type>D</source_type></version><title>Automatic Snake Gait Generation Using Model Predictive Control</title><authors>Emily Hannigan, Bing Song, Gagan Khandate, Maximilian Haas-Heger, Ji\\n  Yin and Matei Ciocarlie</authors><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a method for generating undulatory gaits for snake\\nrobots. Instead of starting from a pre-defined movement pattern such as a\\nserpenoid curve, we use a Model Predictive Control approach to automatically\\ngenerate effective locomotion gaits via trajectory optimization. An important\\nadvantage of this approach is that the resulting gaits are automatically\\nadapted to the environment that is being modeled as part of the snake dynamics.\\nTo illustrate this, we use a novel model for anisotropic dry friction, along\\nwith existing models for viscous friction and fluid dynamic effects such as\\ndrag and added mass. For each of these models, gaits generated without any\\nchange in the method or its parameters are as efficient as Pareto-optimal\\nserpenoid gaits tuned individually for each environment. Furthermore, the\\nproposed method can also produce more complex or irregular gaits, e.g. for\\nobstacle avoidance or executing sharp turns.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11213</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11213</id><submitter>Kevin Doherty</submitter><version version=\"v1\"><date>Tue, 24 Sep 2019 22:34:35 GMT</date><size>4157kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 19:45:32 GMT</date><size>4162kb</size><source_type>D</source_type></version><title>Probabilistic Data Association via Mixture Models for Robust Semantic\\n  SLAM</title><authors>Kevin Doherty, David Baxter, Edward Schneeweiss, John Leonard</authors><categories>cs.RO</categories><comments>Authors D. Baxter and E. Schneeweiss contributed equally to this\\n  work. Submitted to the IEEE International Conference on Robotics and\\n  Automation (ICRA) 2020</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern robotic systems sense the environment geometrically, through sensors\\nlike cameras, lidar, and sonar, as well as semantically, often through visual\\nmodels learned from data, such as object detectors. We aim to develop robots\\nthat can use all of these sources of information for reliable navigation, but\\neach is corrupted by noise. Rather than assume that object detection will\\neventually achieve near perfect performance across the lifetime of a robot, in\\nthis work we represent and cope with the semantic and geometric uncertainty\\ninherent in methods like object detection. Specifically, we model data\\nassociation ambiguity, which is typically non-Gaussian, in a way that is\\namenable to solution within the common nonlinear Gaussian formulation of\\nsimultaneous localization and mapping (SLAM). We do so by eliminating data\\nassociation variables from the inference process through max-marginalization,\\npreserving standard Gaussian posterior assumptions. The result is a\\nmax-mixture-type model that accounts for multiple data association hypotheses\\nas well as incorrect loop closures. We provide experimental results on indoor\\nand outdoor semantic navigation tasks with noisy odometry and object detection\\nand find that the ability of the proposed approach to represent multiple\\nhypotheses, including the &quot;null&quot; hypothesis, gives substantial robustness\\nadvantages in comparison to alternative semantic SLAM approaches.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11236</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11236</id><submitter>Bardienus Duisterhof</submitter><version version=\"v1\"><date>Wed, 25 Sep 2019 00:10:33 GMT</date><size>9595kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 14:24:07 GMT</date><size>9596kb</size><source_type>D</source_type></version><title>Learning to Seek: Autonomous Source Seeking with Deep Reinforcement\\n  Learning Onboard a Nano Drone Microcontroller</title><authors>Bardienus P. Duisterhof, Srivatsan Krishnan, Jonathan J. Cruz, Colby\\n  R. Banbury, William Fu, Aleksandra Faust, Guido C. H. E. de Croon, Vijay\\n  Janapa Reddi</authors><categories>cs.RO cs.AI cs.LG cs.SY eess.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fully autonomous navigation using nano drones has numerous applications in\\nthe real world, ranging from search and rescue to source seeking. Nano drones\\nare well-suited for source seeking because of their agility, low price, and\\nubiquitous character. Unfortunately, their constrained form factor limits\\nflight time, sensor payload, and compute capability. These challenges are a\\ncrucial limitation for the use of source-seeking nano drones in GPS-denied and\\nhighly cluttered environments. Hereby, we introduce a fully autonomous deep\\nreinforcement learning-based light-seeking nano drone. The 33-gram nano drone\\nperforms all computation on-board the ultra-low-power microcontroller (MCU). We\\npresent the method for efficiently training, converting, and utilizing deep\\nreinforcement learning policies. Our training methodology and novel\\nquantization scheme allow fitting the trained policy in 3 kB of memory. The\\nquantization scheme uses representative input data and input scaling to arrive\\nat a full 8-bit model. Finally, we evaluate the approach in simulation and\\nflight tests using a Bitcraze CrazyFlie, achieving 80% success rate on average\\nin a highly cluttered and randomized test environment. Even more, the drone\\nfinds the light source in 29% fewer steps compared to a baseline simulation\\n(obstacle avoidance without source information). To our knowledge, this is the\\nfirst deep reinforcement learning method that enables source seeking within a\\nhighly constrained nano drone demonstrating robust flight behavior. Our general\\nmethodology is suitable for any (source seeking) highly constrained platform\\nusing deep reinforcement learning.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11248</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11248</id><submitter>John Gideon</submitter><version version=\"v1\"><date>Wed, 25 Sep 2019 01:14:05 GMT</date><size>1806kb</size></version><version version=\"v2\"><date>Thu, 3 Oct 2019 01:11:49 GMT</date><size>1805kb</size></version><title>When to Intervene: Detecting Abnormal Mood using Everyday Smartphone\\n  Conversations</title><authors>John Gideon, Katie Matton, Steve Anderau, Melvin G McInnis, Emily\\n  Mower Provost</authors><categories>cs.LG cs.HC stat.ML</categories><comments>Submitted to IEEE Transactions on Affective Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bipolar disorder (BPD) is a chronic mental illness characterized by extreme\\nmood and energy changes from mania to depression. These changes drive behaviors\\nthat often lead to devastating personal or social consequences. BPD is managed\\nclinically with regular interactions with care providers, who assess mood,\\nenergy levels, and the form and content of speech. Recent work has proposed\\nsmartphones for monitoring mood using speech. However, these works do not\\npredict when to intervene. Predicting when to intervene is challenging because\\nthere is not a single measure that is relevant for every person: different\\nindividuals may have different levels of symptom severity considered typical.\\nAdditionally, this typical mood, or baseline, may change over time, making a\\nsingle symptom threshold insufficient. This work presents an innovative\\napproach that expands clinical mood monitoring to predict when interventions\\nare necessary using an anomaly detection framework, which we call Temporal\\nNormalization. We first validate the model using a dataset annotated for\\nclinical interventions and then incorporate this method in a deep learning\\nframework to predict mood anomalies from natural, unstructured, telephone\\nspeech data. The combination of these approaches provides a framework to enable\\nreal-world speech-focused mood monitoring.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11252</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11252</id><submitter>Yang Lv</submitter><version version=\"v1\"><date>Wed, 25 Sep 2019 01:50:32 GMT</date><size>128kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 14:33:07 GMT</date><size>253kb</size><source_type>D</source_type></version><title>Neighborhood-Enhanced and Time-Aware Model for Session-based\\n  Recommendation</title><authors>Yang Lv, Liangsheng Zhuang, Pengyu Luo</authors><categories>cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Session based recommendation has become one of the research hotpots in the\\nfield of recommendation systems due to its highly practical value.Previous deep\\nlearning methods mostly focus on the sequential characteristics within the\\ncurrent session,and neglect the context similarity and temporal similarity\\nbetween sessions which contain abundant collaborative information.In this\\npaper,we propose a novel neural networks framework,namely Neighborhood Enhanced\\nand Time Aware Recommendation Machine(NETA) for session based recommendation.\\nFirstly,we introduce an efficient neighborhood retrieve mechanism to find out\\nsimilar sessions which includes collaborative information.Then we design a\\nguided attention with time-aware mechanism to extract collaborative\\nrepresentation from neighborhood sessions.Especially,temporal recency between\\nsessions is considered separately.Finally, we design a simple co-attention\\nmechanism to determine the importance of complementary collaborative\\nrepresentation when predicting the next item.Extensive experiments conducted on\\ntwo real-world datasets demonstrate the effectiveness of our proposed model.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11290</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11290</id><submitter>Ke Chen</submitter><version version=\"v1\"><date>Wed, 25 Sep 2019 05:14:36 GMT</date><size>86kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 19:48:43 GMT</date><size>87kb</size><source_type>D</source_type></version><title>Structured random sketching for PDE inverse problems</title><authors>Ke Chen, Qin Li, Kit Newton, Steve Wright</authors><categories>math.NA cs.NA math.PR</categories><comments>remove equation labels</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For an overdetermined system $\\\\mathsf{A}\\\\mathsf{x} \\\\approx \\\\mathsf{b}$ with\\n$\\\\mathsf{A}$ and $\\\\mathsf{b}$ given, the least-square (LS) formulation $\\\\min_x\\n\\\\, \\\\|\\\\mathsf{A}\\\\mathsf{x}-\\\\mathsf{b}\\\\|_2$ is often used to find an acceptable\\nsolution $\\\\mathsf{x}$. The cost of solving this problem depends on the\\ndimensions of $\\\\mathsf{A}$, which are large in many practical instances. This\\ncost can be reduced by the use of random sketching, in which we choose a matrix\\n$\\\\mathsf{S}$ with fewer rows than $\\\\mathsf{A}$ and $\\\\mathsf{b}$, and solve the\\nsketched LS problem $\\\\min_x \\\\, \\\\|\\\\mathsf{S}(\\\\mathsf{A}\\n\\\\mathsf{x}-\\\\mathsf{b})\\\\|_2$ to obtain an approximate solution to the original\\nLS problem. Significant theoretical and practical progress has been made in the\\nlast decade in designing the appropriate structure and distribution for the\\nsketching matrix $\\\\mathsf{S}$. When $\\\\mathsf{A}$ and $\\\\mathsf{b}$ arise from\\ndiscretizations of a PDE-based inverse problem, tensor structure is often\\npresent in $\\\\mathsf{A}$ and $\\\\mathsf{b}$. For reasons of practical efficiency,\\n$\\\\mathsf{S}$ should be designed to have a structure consistent with that of\\n$\\\\mathsf{A}$. Can we claim similar approximation properties for the solution of\\nthe sketched LS problem with structured $\\\\mathsf{S}$ as for fully-random\\n$\\\\mathsf{S}$? We give estimates that relate the quality of the solution of the\\nsketched LS problem to the size of the structured sketching matrices, for two\\ndifferent structures. Our results are among the first known for random\\nsketching matrices whose structure is suitable for use in PDE inverse problems.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11310</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11310</id><submitter>Chongshuang Chen</submitter><version version=\"v1\"><date>Wed, 25 Sep 2019 06:56:31 GMT</date><size>356kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 03:47:51 GMT</date><size>352kb</size><source_type>D</source_type></version><title>Joint optimization of train blocking and shipment path:An integrated\\n  model and a sequential algorithm</title><authors>Chongshuang Chen, Jun Zhao</authors><categories>math.OC cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The INFORMS RAS 2019 Problem Solving Competition is focused on the integrated\\ntrain blocking and shipment path (TBSP) optimization for tonnage-based\\noperating railways. In nature, the TBSP problem could be viewed as a\\nmulti-commodity network design problem with a double-layer network structure.\\nBy introducing a directed physical railway network and a directed train\\nservices (blocks) network, we formulate completely the TBSP problem as a mixed\\ninteger linear programming (MILP) model that incorporates all decisions,\\nobjectives and constraints especially the merge flow (called intree rule here)\\nin an integrated manner. The scale of the MILP model can be reduced efficiently\\nif we only enumerate the arc selection and block sequence variables for each\\nshipment on the legal paths from its origin to its destination satisfying the\\ngiven detour ratio. We further develop a sequential algorithm that decomposes\\nthe TBSP problem into the shipment path subproblem and train blocking\\nsubproblem which are solved sequentially. Computational tests on the three\\ngiven data sets show that the reduced MILP model can solve DataSet\\\\_1 to\\noptimality in 8.48 seconds and DataSet\\\\_2 with a gap of 0.16\\\\% in 6 hours on a\\nGPU workstation. The reduced model also can provide strong lower bounds for\\nDataSet\\\\_2 and DataSet\\\\_3. The sequential algorithm can find a high quality\\nsolution with 0.04\\\\% gap within 0.26 seconds for DataSet\\\\_1, 0.42\\\\% gap within\\n4.53 seconds for DataSet\\\\_2 and 1.54\\\\% gap within 0.58 hours for DataSet\\\\_3\\nrespectively on a Thinkpad laptop.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11413</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11413</id><submitter>Alex Samoletov</submitter><version version=\"v1\"><date>Wed, 25 Sep 2019 11:31:34 GMT</date><size>1009kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 09:07:15 GMT</date><size>1009kb</size><source_type>D</source_type></version><title>Temperature expressions and ergodicity of the Nos\\\\\\'e-Hoover\\n  deterministic schemes</title><authors>A. Samoletov and B. Vasiev</authors><categories>physics.comp-ph cs.NA math.NA physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thermostats are dynamic equations used to model thermodynamic variables in\\nmolecular dynamics. The applicability of thermostats is based on the ergodic\\nhypothesis. The most commonly used thermostats are designed according to the\\nNos\\\\\\'e-Hoover scheme, although it is known that it often violates ergodicity.\\nHere, following a method from our recent study \\\\citep{SamoletovVasiev2017}, we\\nhave extended the classic Nos\\\\\\'e-Hoover scheme with an additional temperature\\ncontrol tool. However, as with the NH scheme, a single thermostat variable is\\nused. In the present study we analyze the statistical properties of the\\nmodified equations of motion with an emphasis on ergodicity. Simultaneous\\nthermostatting of all phase variables with minimal extra computational costs is\\nan advantage of the specific theoretical scheme presented here.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11459</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11459</id><submitter>Gregor Simm</submitter><version version=\"v1\"><date>Wed, 25 Sep 2019 12:56:50 GMT</date><size>5168kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 09:49:40 GMT</date><size>5026kb</size><source_type>D</source_type></version><title>A Generative Model for Molecular Distance Geometry</title><authors>Gregor N. C. Simm and Jos\\\\\\'e Miguel Hern\\\\\\'andez-Lobato</authors><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing equilibrium states for many-body systems, such as molecules, is a\\nlong-standing challenge. In the absence of methods for generating statistically\\nindependent samples, great computational effort is invested in simulating these\\nsystems using, for example, Markov chain Monte Carlo. We present a\\nprobabilistic model that generates such samples for molecules from their graph\\nrepresentations. Our model learns a low-dimensional manifold that preserves the\\ngeometry of local atomic neighborhoods through a principled learning\\nrepresentation that is based on Euclidean distance geometry. We create a new\\ndataset for molecular conformation generation with which we show experimentally\\nthat our generative model achieves state-of-the-art accuracy. Finally, we show\\nhow to use our model as a proposal distribution in an importance sampling\\nscheme to compute molecular properties.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11483</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11483</id><submitter>Jordan Ott</submitter><version version=\"v1\"><date>Mon, 23 Sep 2019 20:10:50 GMT</date><size>6489kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 19:58:04 GMT</date><size>6655kb</size><source_type>D</source_type></version><title>Learning in the Machine: To Share or Not to Share?</title><authors>Jordan Ott, Erik Linstead, Nicholas LaHaye, Pierre Baldi</authors><categories>cs.LG cs.NE q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weight-sharing is one of the pillars behind Convolutional Neural Networks and\\ntheir successes. However, in physical neural systems such as the brain,\\nweight-sharing is implausible. This discrepancy raises the fundamental question\\nof whether weight-sharing is necessary. If so, to which degree of precision? If\\nnot, what are the alternatives? The goal of this study is to investigate these\\nquestions, primarily through simulations where the weight-sharing assumption is\\nrelaxed. Taking inspiration from neural circuitry, we explore the use of Free\\nConvolutional Networks and neurons with variable connection patterns. Using\\nFree Convolutional Networks, we show that while weight-sharing is a pragmatic\\noptimization approach, it is not a necessity in computer vision applications.\\nFurthermore, Free Convolutional Networks match the performance observed in\\nstandard architectures when trained using properly translated data (akin to\\nvideo). Under the assumption of translationally augmented data, Free\\nConvolutional Networks learn translationally invariant representations that\\nyield an approximate form of weight sharing.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11522</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11522</id><submitter>Guillermo Valle-P\\\\\\'erez</submitter><version version=\"v1\"><date>Wed, 25 Sep 2019 14:29:45 GMT</date><size>2000kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 15:57:00 GMT</date><size>2000kb</size><source_type>D</source_type></version><title>Neural networks are a priori biased towards Boolean functions with low\\n  entropy</title><authors>Chris Mingard, Joar Skalse, Guillermo Valle-P\\\\\\'erez, David\\n  Mart\\\\\\'inez-Rubio, Vladimir Mikulik, Ard A. Louis</authors><categories>cs.LG stat.ML</categories><comments>Under review as a conference paper at ICLR 2020</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Understanding the inductive bias of neural networks is critical to explaining\\ntheir ability to generalise. Here, for one of the simplest neural networks -- a\\nsingle-layer perceptron with $n$ input neurons, one output neuron, and no\\nthreshold bias term -- we prove that upon random initialisation of weights, the\\na priori probability $P(t)$ that it represents a Boolean function that\\nclassifies $t$ points in $\\\\{0,1\\\\}^n$ as $1$ has a remarkably simple form: $\\nP(t) = 2^{-n} \\\\,\\\\, {\\\\rm for} \\\\,\\\\, 0\\\\leq t &lt; 2^n$. Since a perceptron can\\nexpress far fewer Boolean functions with small or large values of $t$ (low\\n&quot;entropy&quot;) than with intermediate values of $t$ (high &quot;entropy&quot;) there is, on\\naverage, a strong intrinsic a-priori bias towards individual functions with low\\nentropy. Furthermore, within a class of functions with fixed $t$, we often\\nobserve a further intrinsic bias towards functions of lower complexity.\\nFinally, we prove that, regardless of the distribution of inputs, the bias\\ntowards low entropy becomes monotonically stronger upon adding ReLU layers, and\\nempirically show that increasing the variance of the bias term has a similar\\neffect.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11535</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11535</id><submitter>Xiao Huang</submitter><version version=\"v1\"><date>Wed, 25 Sep 2019 14:53:57 GMT</date><size>2084kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 23:10:55 GMT</date><size>2084kb</size><source_type>D</source_type></version><title>Learning A Unified Named Entity Tagger From Multiple Partially Annotated\\n  Corpora For Efficient Adaptation</title><authors>Xiao Huang, Li Dong, Elizabeth Boschee, Nanyun Peng</authors><categories>cs.CL</categories><comments>9 pages of main content + 4 pages of references and appendix. 4\\n  figures and 2 tables in the main content. Accepted by CoNLL 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Named entity recognition (NER) identifies typed entity mentions in raw text.\\nWhile the task is well-established, there is no universally used tagset: often,\\ndatasets are annotated for use in downstream applications and accordingly only\\ncover a small set of entity types relevant to a particular task. For instance,\\nin the biomedical domain, one corpus might annotate genes, another chemicals,\\nand another diseases---despite the texts in each corpus containing references\\nto all three types of entities. In this paper, we propose a deep structured\\nmodel to integrate these &quot;partially annotated&quot; datasets to jointly identify all\\nentity types appearing in the training corpora. By leveraging multiple\\ndatasets, the model can learn robust input representations; by building a joint\\nstructured model, it avoids potential conflicts caused by combining several\\nmodels\\' predictions at test time. Experiments show that the proposed model\\nsignificantly outperforms strong multi-task learning baselines when training on\\nmultiple, partially annotated datasets and testing on datasets that contain\\ntags from more than one of the training corpora.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11598</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11598</id><submitter>Chuan-Chi Lai</submitter><version version=\"v1\"><date>Wed, 25 Sep 2019 16:35:32 GMT</date><size>329kb</size></version><version version=\"v2\"><date>Sun, 6 Oct 2019 06:56:20 GMT</date><size>329kb</size></version><title>A Predictive On-Demand Placement of UAV Base Stations Using Echo State\\n  Network</title><authors>Haoran Peng, Chao Chen, Chuan-Chi Lai, Li-Chun Wang, Zhu Han</authors><categories>cs.NI cs.LG eess.SP</categories><comments>6 pages, 8 figures, accepted by 2019 IEEE/CIC International\\n  Conference on Communications in China (ICCC)</comments><doi>10.1109/ICCChina.2019.8855868</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The unmanned aerial vehicles base stations (UAV-BSs) have great potential in\\nbeing widely used in many dynamic application scenarios. In those scenarios,\\nthe movements of served user equipments (UEs) are inevitable, so the UAV-BSs\\nneeds to be re-positioned dynamically for providing seamless services. In this\\npaper, we propose a system framework consisting of UEs clustering, UAV-BS\\nplacement, UEs trajectories prediction, and UAV-BS reposition matching scheme,\\nto serve the UEs seamlessly as well as minimize the energy cost of UAV-BSs\\'\\nreposition trajectories. An Echo State Network (ESN) based algorithm for\\npredicting the future trajectories of UEs and a Kuhn-Munkres-based algorithm\\nfor finding the energy-efficient reposition trajectories of UAV-BSs is\\ndesigned, respectively. We conduct a simulation using a real open dataset for\\nperformance validation. The simulation results indicate that the proposed\\nframework achieves high prediction accuracy and provides the energy-efficient\\nmatching scheme.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11628</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11628</id><submitter>Yaodong Yang Mr.</submitter><version version=\"v1\"><date>Wed, 25 Sep 2019 17:21:45 GMT</date><size>1349kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 26 Sep 2019 15:38:30 GMT</date><size>2293kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sat, 28 Sep 2019 22:50:53 GMT</date><size>0kb</size><source_type>I</source_type></version><title>$\\\\alpha^{\\\\alpha}$-Rank: Scalable Multi-agent Evaluation through\\n  Evolution</title><authors>Yaodong Yang, Rasul Tutunov, Phu Sakulwongtana, Haitham Bou Ammar, Jun\\n  Wang</authors><categories>cs.MA cs.LG</categories><comments>The authors decide to retract the current version for an improvement</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although challenging, strategy profile evaluation in large connected learner\\nnetworks is crucial for enabling the next wave of machine learning\\napplications. Recently, $\\\\alpha$-Rank, an evolutionary algorithm, has been\\nproposed as a solution for ranking joint policy profiles in multi-agent\\nsystems. $\\\\alpha$-Rank claimed scalability through a polynomial time\\nimplementation with respect to the total number of pure strategy profiles. In\\nthis paper, we formally prove that such a claim is not grounded. In fact, we\\nshow that $\\\\alpha$-Rank exhibits an exponential complexity in number of agents,\\nhindering its application beyond a small finite number of joint profiles.\\nRealizing such a limitation, we contribute by proposing a scalable evaluation\\nprotocol that we title $\\\\alpha^{\\\\alpha}$-Rank. Our method combines evolutionary\\ndynamics with stochastic optimization and double oracles for \\\\emph{truly}\\nscalable ranking with linear (in number of agents) time and memory\\ncomplexities. Our contributions allow us, for the first time, to conduct\\nlarge-scale evaluation experiments of multi-agent systems, where we show\\nsuccessful results on large joint strategy profiles with sizes in the order of\\n$\\\\mathcal{O}(2^{25})$ (i.e., $\\\\approx \\\\text{$33$ million strategies}$) -- a\\nsetting not evaluable using current techniques.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11644</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11644</id><submitter>arXiv Admin</submitter><version version=\"v1\"><date>Wed, 25 Sep 2019 17:44:55 GMT</date><size>160kb</size></version><version version=\"v2\"><date>Tue, 1 Oct 2019 18:15:48 GMT</date><size>0kb</size><source_type>I</source_type></version><title>An Improvement Over Threads Communications on Multi-Core Processors</title><authors>Reza Fotohi, Mehdi Effatparvar, Fateme Sarkohaki, Shahram Behzad,\\n  Jaber Hoseini balov</authors><categories>cs.OS</categories><comments>This submission has been withdrawn by arXiv administrators due to\\n  inappropriate text reuse from external sources</comments><journal-ref>2012, Volume 6, Issue 12, pp 379-384</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Multicore is an integrated circuit chip that uses two or more computational\\nengines (cores) places in a single processor. This new approach is used to\\nsplit the computational work of a threaded application and spread it over\\nmultiple execution cores, so that the computer system can benefits from a\\nbetter performance and better responsiveness of the system. A thread is a unit\\nof execution inside a process that is created and maintained to execute a set\\nof actions/ instructions. Threads can be implemented differently from an\\noperating system to another, but the operating system is in most cases\\nresponsible to schedule the execution of different threads. Multi-threading\\nimproving efficiency of processor performance with a cost-effective memory\\nsystem. In this paper, we explore one approach to improve communications for\\nmultithreaded. Pre-send is a software Controlled data forwarding technique that\\nsends data to destination\\'s cache before it is needed, eliminating cache misses\\nin the destination\\'s cache as well as reducing the coherence traffic on the\\nbus. we show how we could improve the overall system performance by addition of\\nthese architecture optimizations to multi-core processors.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11655</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11655</id><submitter>AkshatKumar Nigam Mr</submitter><version version=\"v1\"><date>Wed, 25 Sep 2019 17:59:17 GMT</date><size>2709kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 23:41:14 GMT</date><size>2709kb</size><source_type>D</source_type></version><title>Augmenting Genetic Algorithms with Deep Neural Networks for Exploring\\n  the Chemical Space</title><authors>AkshatKumar Nigam, Pascal Friederich, Mario Krenn, Al\\\\\\'an Aspuru-Guzik</authors><categories>cs.NE cs.LG physics.chem-ph physics.comp-ph</categories><comments>Comments: 8 Pages, 7 figures, 2 tables. Comments are welcome! (code\\n  will be uploaded when paper is formally published)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Challenges in natural sciences can often be phrased as optimization problems.\\nMachine learning techniques have recently been applied to solve such problems.\\nOne example in chemistry is the design of tailor-made organic materials and\\nmolecules, which requires efficient methods to explore the chemical space. We\\npresent a genetic algorithm (GA) that is enhanced with a neural network (DNN)\\nbased discriminator model to improve the diversity of generated molecules and\\nat the same time steer the GA. We show that our algorithm outperforms other\\ngenerative models in optimization tasks. We furthermore present a way to\\nincrease interpretability of genetic algorithms, which helped us to derive\\ndesign principles.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11713</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11713</id><submitter>Cristian Candia</submitter><version version=\"v1\"><date>Wed, 25 Sep 2019 18:53:53 GMT</date><size>3059kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 18:37:16 GMT</date><size>3029kb</size><source_type>D</source_type></version><title>Strategic reciprocity improves academic performance in public elementary\\n  school children</title><authors>Cristian Candia, V\\\\\\'ictor Landaeta-Torres, C\\\\\\'esar A. Hidalgo, Carlos\\n  Rodriguez-Sickert</authors><categories>physics.soc-ph cs.SI physics.app-ph physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks are pivotal for learning. Yet, we still lack a full\\nunderstanding of the mechanisms connecting networks with learning outcomes.\\nHere, we present the results of a large scale study (946 elementary school\\nchildren from 45 different classrooms) designed to understand the social\\nstrategies used by elementary school children. We mapped the social networks of\\nstudents using both, a non-anonymous version of a prisoner\\'s dilemma and a\\nsurvey of nominated friendships, and compared the strategies played by students\\nwith their GPAs. We found that higher GPA students invest more strategically in\\ntheir relationships, cooperating more generously with friends and less\\ngenerously with non-friends than lower GPA students. Our findings suggest that\\nthe higher selectivity of social capital investments by high performing\\nstudents may be one of the mechanisms helping them reap the learning benefits\\nof their social networks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11764</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11764</id><submitter>Chen Zhu</submitter><version version=\"v1\"><date>Wed, 25 Sep 2019 20:50:32 GMT</date><size>243kb</size></version><version version=\"v2\"><date>Mon, 30 Sep 2019 18:53:21 GMT</date><size>243kb</size></version><version version=\"v3\"><date>Sat, 5 Oct 2019 04:05:46 GMT</date><size>63kb</size></version><title>FreeLB: Enhanced Adversarial Training for Language Understanding</title><authors>Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, Jingjing Liu</authors><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adversarial training, which minimizes the maximal risk for label-preserving\\ninput perturbations, has proved to be effective for improving the\\ngeneralization of language models. In this work, we propose a novel adversarial\\ntraining algorithm - FreeLB, that promotes higher robustness and invariance in\\nthe embedding space, by adding adversarial perturbations to word embeddings and\\nminimizing the resultant adversarial risk inside different regions around input\\nsamples. To validate the effectiveness of the proposed approach, we apply it to\\nTransformer-based models for natural language understanding and commonsense\\nreasoning tasks. Experiments on the GLUE benchmark show that when applied only\\nto the finetuning stage, it is able to improve the overall test scores of\\nBERT-based model from 78.3 to 79.4, and RoBERTa-large model from 88.5 to 88.8.\\nIn addition, the proposed approach achieves state-of-the-art single-model test\\naccuracies of 85.44% and 67.75% on ARC-Easy and ARC-Challenge. Experiments on\\nCommonsenseQA benchmark further demonstrate that FreeLB can be generalized and\\nboost the performance of RoBERTa-large model on other tasks as well.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11821</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11821</id><submitter>Yueh-Hua Wu</submitter><version version=\"v1\"><date>Wed, 25 Sep 2019 23:52:30 GMT</date><size>332kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 06:19:21 GMT</date><size>332kb</size><source_type>D</source_type></version><title>Model Imitation for Model-Based Reinforcement Learning</title><authors>Yueh-Hua Wu, Ting-Han Fan, Peter J. Ramadge, and Hao Su</authors><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-based reinforcement learning (MBRL) aims to learn a dynamic model to\\nreduce the number of interactions with real-world environments. However, due to\\nestimation error, rollouts in the learned model, especially those of long\\nhorizon, fail to match the ones in real-world environments. This mismatching\\nhas seriously impacted the sample complexity of MBRL. The phenomenon can be\\nattributed to the fact that previous works employ supervised learning to learn\\nthe one-step transition models, which has inherent difficulty ensuring the\\nmatching of distributions from multi-step rollouts. Based on the claim, we\\npropose to learn the synthesized model by matching the distributions of\\nmulti-step rollouts sampled from the synthesized model and the real ones via\\nWGAN. We theoretically show that matching the two can minimize the difference\\nof cumulative rewards between the real transition and the learned one. Our\\nexperiments also show that the proposed model imitation method outperforms the\\nstate-of-the-art in terms of sample complexity and average return.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11825</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11825</id><submitter>Yu Sun</submitter><version version=\"v1\"><date>Thu, 26 Sep 2019 00:21:16 GMT</date><size>994kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 08:09:29 GMT</date><size>995kb</size><source_type>D</source_type></version><title>Unsupervised Domain Adaptation through Self-Supervision</title><authors>Yu Sun, Eric Tzeng, Trevor Darrell, Alexei A. Efros</authors><categories>cs.LG cs.CV stat.ML</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  This paper addresses unsupervised domain adaptation, the setting where\\nlabeled training data is available on a source domain, but the goal is to have\\ngood performance on a target domain with only unlabeled data. Like much of\\nprevious work, we seek to align the learned representations of the source and\\ntarget domains while preserving discriminability. The way we accomplish\\nalignment is by learning to perform auxiliary self-supervised task(s) on both\\ndomains simultaneously. Each self-supervised task brings the two domains closer\\ntogether along the direction relevant to that task. Training this jointly with\\nthe main task classifier on the source domain is shown to successfully\\ngeneralize to the unlabeled target domain. The presented objective is\\nstraightforward to implement and easy to optimize. We achieve state-of-the-art\\nresults on four out of seven standard benchmarks, and competitive results on\\nsegmentation adaptation. We also demonstrate that our method composes well with\\nanother popular pixel-level adaptation method.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11847</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11847</id><submitter>Lawrence Ong</submitter><version version=\"v1\"><date>Thu, 26 Sep 2019 02:21:23 GMT</date><size>1071kb</size><source_type>D</source_type></version><title>Optimal-Rate Characterisation for Pliable Index Coding using Absent\\n  Receivers</title><authors>Lawrence Ong, Badri N. Vellambi, J\\\\&quot;org Kliewer</authors><categories>cs.IT math.IT</categories><comments>Authors\\' copy</comments><journal-ref>in Proc. IEEE Int. Symp. Inf. Theory (ISIT), Paris, France, July\\n  7-12 2019, pp. 522-526</journal-ref><doi>10.1109/ISIT.2019.8849527</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterise the optimal broadcast rate for a few classes of\\npliable-index-coding problems. This is achieved by devising new lower bounds\\nthat utilise the set of absent receivers to construct decoding chains with\\nskipped messages. This work complements existing works by considering problems\\nthat are not complete-S, i.e., problems considered in this work do not require\\nthat all receivers with a certain side-information cardinality to be either\\npresent or absent from the problem. We show that for a certain class, the set\\nof receivers is critical in the sense that adding any receiver strictly\\nincreases the broadcast rate.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11849</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11849</id><submitter>Alexander Ororbia</submitter><version version=\"v1\"><date>Thu, 26 Sep 2019 02:27:11 GMT</date><size>2743kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 27 Sep 2019 19:11:48 GMT</date><size>2725kb</size><source_type>D</source_type></version><title>The Ant Swarm Neuro-Evolution Procedure for Optimizing Recurrent\\n  Networks</title><authors>AbdElRahman A. ElSaid, Alexander G. Ororbia and Travis J. Desell</authors><categories>cs.NE</categories><comments>15 pages, 22 pages appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hand-crafting effective and efficient structures for recurrent neural\\nnetworks (RNNs) is a difficult, expensive, and time-consuming process. To\\naddress this challenge, we propose a novel neuro-evolution algorithm based on\\nant colony optimization (ACO), called ant swarm neuro-evolution (ASNE), for\\ndirectly optimizing RNN topologies. The procedure selects from multiple modern\\nrecurrent cell types such as Delta-RNN, GRU, LSTM, MGU and UGRNN cells, as well\\nas recurrent connections which may span multiple layers and/or steps of time.\\nIn order to introduce an inductive bias that encourages the formation of\\nsparser synaptic connectivity patterns, we investigate several variations of\\nthe core algorithm. We do so primarily by formulating different functions that\\ndrive the underlying pheromone simulation process (which mimic L1 and L2\\nregularization in standard machine learning) as well as by introducing ant\\nagents with specialized roles (inspired by how real ant colonies operate),\\ni.e., explorer ants that construct the initial feed forward structure and\\nsocial ants which select nodes from the feed forward connections to\\nsubsequently craft recurrent memory structures. We also incorporate a\\nLamarckian strategy for weight initialization which reduces the number of\\nbackpropagation epochs required to locally train candidate RNNs, speeding up\\nthe neuro-evolution process. Our results demonstrate that the sparser RNNs\\nevolved by ASNE significantly outperform traditional one and two layer\\narchitectures consisting of modern memory cells, as well as the well-known NEAT\\nalgorithm. Furthermore, we improve upon prior state-of-the-art results on the\\ntime series dataset utilized in our experiments.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11850</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11850</id><submitter>Lawrence Ong</submitter><version version=\"v1\"><date>Thu, 26 Sep 2019 02:29:38 GMT</date><size>153kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 11:27:40 GMT</date><size>1101kb</size><source_type>D</source_type></version><title>Improved Lower Bounds for Pliable Index Coding using Absent Receivers</title><authors>Lawrence Ong, Badri N. Vellambi, J\\\\&quot;org Kliewer, Parastoo Sadeghi</authors><categories>cs.IT math.IT</categories><comments>An extended version of the same-titled paper submitted to a\\n  conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies pliable index coding, in which a sender broadcasts\\ninformation to multiple receivers through a shared broadcast medium, and the\\nreceivers each have some message a priori and want any message they do not\\nhave. An approach, based on receivers that are absent from the problem, was\\npreviously proposed to find lower bounds on the optimal broadcast rate. In this\\npaper, we introduce new techniques to obtained better lower bounds, and derive\\nthe optimal broadcast rates for new classes of the problems, including all\\nproblems with up to four absent receivers.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11861</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11861</id><submitter>Jiwei Li</submitter><version version=\"v1\"><date>Thu, 26 Sep 2019 03:06:47 GMT</date><size>949kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 18:57:37 GMT</date><size>949kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Sun, 6 Oct 2019 04:01:39 GMT</date><size>950kb</size><source_type>D</source_type></version><title>Large-scale Pretraining for Neural Machine Translation with Tens of\\n  Billions of Sentence Pairs</title><authors>Yuxian Meng, Xiangyuan Ren, Zijun Sun, Xiaoya Li, Arianna Yuan, Fei\\n  Wu, Jiwei Li</authors><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the problem of training neural machine\\ntranslation (NMT) systems with a dataset of more than 40 billion bilingual\\nsentence pairs, which is larger than the largest dataset to date by orders of\\nmagnitude. Unprecedented challenges emerge in this situation compared to\\nprevious NMT work, including severe noise in the data and prohibitively long\\ntraining time. We propose practical solutions to handle these issues and\\ndemonstrate that large-scale pretraining significantly improves NMT\\nperformance. We are able to push the BLEU score of WMT17 Chinese-English\\ndataset to 32.3, with a significant performance boost of +3.2 over existing\\nstate-of-the-art results.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11926</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11926</id><submitter>Guilin Li</submitter><version version=\"v1\"><date>Thu, 26 Sep 2019 06:26:58 GMT</date><size>896kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sat, 28 Sep 2019 15:29:40 GMT</date><size>896kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 1 Oct 2019 03:20:17 GMT</date><size>896kb</size><source_type>D</source_type></version><title>StacNAS: Towards stable and consistent optimization for differentiable\\n  Neural Architecture Search</title><authors>Guilin Li, Xing Zhang, Zitong Wang, Zhenguo Li, Tong Zhang</authors><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Earlier methods for Neural Architecture Search were computationally\\nexpensive. Recently proposed Differentiable Neural Architecture Search\\nalgorithms such as DARTS can effectively speed up the computation. However, the\\ncurrent formulation relies on a relaxation of the original problem that leads\\nto unstable and suboptimal solutions. We argue that these problems are caused\\nby three fundamental reasons: (1) The difficulty of bi-level optimization; (2)\\nMulticollinearity of correlated operations such as max pooling and average\\npooling; (3) The discrepancy between the optimization complexity of the search\\nstage and the final training. In this paper, we propose a grouped variable\\npruning algorithm based on one-level optimization, which leads to a more stable\\nand consistent optimization solution for differentiable NAS. Extensive\\nexperiments verify the superiority of the proposed method regarding both\\naccuracy and stability. Our new approach obtains state-of-the-art accuracy on\\nCIFAR-10, CIFAR-100 and ImageNet.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11937</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11937</id><submitter>Huapeng Wu</submitter><version version=\"v1\"><date>Thu, 26 Sep 2019 06:54:00 GMT</date><size>1388kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 09:09:30 GMT</date><size>1388kb</size><source_type>D</source_type></version><title>Multi-grained Attention Networks for Single Image Super-Resolution</title><authors>Huapeng Wu, Zhengxia Zou, Jie Gui, Wen-Jun Zeng, Jieping Ye, Jun\\n  Zhang, Hongyi Liu, Zhihui Wei</authors><categories>eess.IV cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Convolutional Neural Networks (CNN) have drawn great attention in image\\nsuper-resolution (SR). Recently, visual attention mechanism, which exploits\\nboth of the feature importance and contextual cues, has been introduced to\\nimage SR and proves to be effective to improve CNN-based SR performance. In\\nthis paper, we make a thorough investigation on the attention mechanisms in a\\nSR model and shed light on how simple and effective improvements on these ideas\\nimprove the state-of-the-arts. We further propose a unified approach called\\n&quot;multi-grained attention networks (MGAN)&quot; which fully exploits the advantages\\nof multi-scale and attention mechanisms in SR tasks. In our method, the\\nimportance of each neuron is computed according to its surrounding regions in a\\nmulti-grained fashion and then is used to adaptively re-scale the feature\\nresponses. More importantly, the &quot;channel attention&quot; and &quot;spatial attention&quot;\\nstrategies in previous methods can be essentially considered as two special\\ncases of our method. We also introduce multi-scale dense connections to extract\\nthe image features at multiple scales and capture the features of different\\nlayers through dense skip connections. Ablation studies on benchmark datasets\\ndemonstrate the effectiveness of our method. In comparison with other\\nstate-of-the-art SR methods, our method shows the superiority in terms of both\\naccuracy and model size.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11974</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11974</id><submitter>Ze Yang</submitter><version version=\"v1\"><date>Thu, 26 Sep 2019 08:34:05 GMT</date><size>181kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 06:33:29 GMT</date><size>181kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 1 Oct 2019 17:55:14 GMT</date><size>112kb</size><source_type>D</source_type></version><title>Read, Attend and Comment: A Deep Architecture for Automatic News Comment\\n  Generation</title><authors>Ze Yang, Can Xu, Wei Wu, Zhoujun Li</authors><categories>cs.CL cs.IR cs.LG</categories><comments>Accepted by EMNLP2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic news comment generation is a new testbed for techniques of natural\\nlanguage generation. In this paper, we propose a &quot;read-attend-comment&quot;\\nprocedure for news comment generation and formalize the procedure with a\\nreading network and a generation network. The reading network comprehends a\\nnews article and distills some important points from it, then the generation\\nnetwork creates a comment by attending to the extracted discrete points and the\\nnews title. We optimize the model in an end-to-end manner by maximizing a\\nvariational lower bound of the true objective using the back-propagation\\nalgorithm. Experimental results on two datasets indicate that our model can\\nsignificantly outperform existing methods in terms of both automatic evaluation\\nand human judgment.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.11983</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.11983</id><submitter>Qingbo Wu</submitter><version version=\"v1\"><date>Thu, 26 Sep 2019 08:57:34 GMT</date><size>5019kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 29 Sep 2019 04:56:02 GMT</date><size>0kb</size><source_type>I</source_type></version><version version=\"v3\"><date>Sun, 6 Oct 2019 03:55:12 GMT</date><size>5449kb</size><source_type>D</source_type></version><title>Subjective and Objective De-raining Quality Assessment Towards Authentic\\n  Rain Image</title><authors>Qingbo Wu and Lei Wang and King N. Ngan and Hongliang Li and Fanman\\n  Meng and Linfeng Xu</authors><categories>eess.IV cs.CV</categories><comments>In this revision, we add the comparison with our previous exploration\\n  towards the de-raining quality assessment in Ref. [16]. Some typos in Tables\\n  III and IV are corrected, where the missed minus signs are added back for\\n  some OU metrics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Images acquired by outdoor vision systems easily suffer poor visibility and\\nannoying interference due to the rainy weather, which brings great challenge\\nfor accurately understanding and describing the visual contents. Recent\\nresearches have devoted great efforts on the task of rain removal for improving\\nthe image visibility. However, there is very few exploration about the quality\\nassessment of de-rained image, even it is crucial for accurately measuring the\\nperformance of various de-raining algorithms. In this paper, we first create a\\nde-raining quality assessment (DQA) database that collects 206 authentic rain\\nimages and their de-rained versions produced by 6 representative single image\\nrain removal algorithms. Then, a subjective study is conducted on our DQA\\ndatabase, which collects the subject-rated scores of all de-rained images. To\\nquantitatively measure the quality of de-rained image with non-uniform\\nartifacts, we propose a bi-directional feature embedding network (B-FEN) which\\nintegrates the features of global perception and local difference together.\\nExperiments confirm that the proposed method significantly outperforms many\\nexisting universal blind image quality assessment models. To help the research\\ntowards perceptually preferred de-raining algorithm, we will publicly release\\nour DQA database and B-FEN source code on https://github.com/wqb-uestc.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12004</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12004</id><submitter>Peter Chini</submitter><version version=\"v1\"><date>Thu, 26 Sep 2019 09:47:25 GMT</date><size>107kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 7 Oct 2019 08:27:52 GMT</date><size>107kb</size><source_type>D</source_type></version><title>Complexity of Liveness in Parameterized Systems</title><authors>Peter Chini, Roland Meyer, Prakash Saivasan</authors><categories>cs.FL cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the fine-grained complexity of liveness verification for\\nleader contributor systems. These consist of a designated leader thread and an\\narbitrary number of identical contributor threads communicating via a shared\\nmemory. The liveness verification problem asks whether there is an infinite\\ncomputation of the system in which the leader reaches a final state infinitely\\noften. Like its reachability counterpart, the problem is known to be\\nNP-complete. Our results show that, even from a fine-grained point of view, the\\ncomplexities differ only by a polynomial factor.\\n  Liveness verification decomposes into reachability and cycle detection. We\\npresent a fixed point iteration solving the latter in polynomial time. For\\nreachability, we reconsider the two standard parameterizations. When\\nparameterized by the number of states of the leader L and the size of the data\\ndomain D, we show an (L + D)^O(L + D)-time algorithm. It improves on a previous\\nalgorithm, thereby settling an open problem. When parameterized by the number\\nof states of the contributor C, we reuse an O*(2^C)-time algorithm. We show how\\nto connect both algorithms with the cycle detection to obtain algorithms for\\nliveness verification. The running times of the composed algorithms match those\\nof reachability, proving that the fine-grained lower bounds for liveness\\nverification are met.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12038</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12038</id><submitter>Qimai Li</submitter><version version=\"v1\"><date>Thu, 26 Sep 2019 11:47:54 GMT</date><size>1126kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 27 Sep 2019 18:19:39 GMT</date><size>1132kb</size><source_type>D</source_type></version><title>Attributed Graph Learning with 2-D Graph Convolution</title><authors>Qimai Li, Xiaotong Zhang, Han Liu, Xiao-Ming Wu</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph convolutional neural networks have demonstrated promising performance\\nin attributed graph learning, thanks to the use of graph convolution that\\neffectively combines graph structures and node features for learning node\\nrepresentations. However, one intrinsic limitation of the commonly adopted 1-D\\ngraph convolution is that it only exploits graph connectivity for feature\\nsmoothing, which may lead to inferior performance on sparse and noisy\\nreal-world attributed networks. To address this problem, we propose to explore\\nrelational information among node attributes to complement node relations for\\nrepresentation learning. In particular, we propose to use 2-D graph convolution\\nto jointly model the two kinds of relations and develop a computationally\\nefficient dimensionwise separable 2-D graph convolution (DSGC). Theoretically,\\nwe show that DSGC can reduce intra-class variance of node features on both the\\nnode dimension and the attribute dimension to facilitate learning. Empirically,\\nwe demonstrate that by incorporating attribute relations, DSGC achieves\\nsignificant performance gain over state-of-the-art methods on node\\nclassification and clustering on several real-world attributed networks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12054</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12054</id><submitter>Alexis Stoven-Dubois</submitter><version version=\"v1\"><date>Thu, 26 Sep 2019 12:39:02 GMT</date><size>3005kb</size></version><title>Fuzzy Gesture Expression Model for an Interactive and Safe Robot Partner</title><authors>Alexis Stoven-Dubois, Janos Botzheim and Naoyuki Kubota</authors><categories>cs.RO</categories><comments>11 pages, 8 figures, accepted for publication in Journal of Network\\n  Intelligence</comments><acm-class>I.2.9</acm-class><journal-ref>Journal of Network Intelligence Vol. 1 Number 4 (2016) pgs.\\n  119-129</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interaction with a robot partner requires many elements, including not only\\nspeech but also embodiment. Thus, gestural and facial expressions are important\\nfor communication. Furthermore, understanding human movements is essential for\\nsafe and natural interchange. This paper proposes an interactive fuzzy\\nemotional model for the robot partner\\'s gesture expression, following its\\nfacial emotional model.\\n  First, we describe the physical interaction between the user and its robot\\npartner. Next, we propose a kinematic model for the robot partner based on the\\nDenavit-Hartenberg convention and solve the inverse kinematic transformation\\nthrough Bacterial Memetic Algorithm. Then, the emotional model along its\\ninteractivity with the user is discussed. Finally, we show experimental results\\nof the proposed model.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12095</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12095</id><submitter>Scott Stoller</submitter><version version=\"v1\"><date>Tue, 24 Sep 2019 18:32:46 GMT</date><size>61kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 27 Sep 2019 21:32:29 GMT</date><size>62kb</size><source_type>D</source_type></version><title>A Decision Tree Learning Approach for Mining Relationship-Based Access\\n  Control Policies</title><authors>Thang Bui and Scott D. Stoller</authors><categories>cs.CR</categories><comments>arXiv admin note: text overlap with arXiv:1903.07530,\\n  arXiv:1708.04749</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relationship-based access control (ReBAC) provides a high level of\\nexpressiveness and flexibility that promotes security and information sharing,\\nby allowing policies to be expressed in terms of chains of relationships\\nbetween entities. ReBAC policy mining algorithms have the potential to\\nsignificantly reduce the cost of migration from legacy access control systems\\nto ReBAC, by partially automating the development of a ReBAC policy.\\n  This paper presents a new algorithm, based on decision trees, for mining\\nReBAC policies from access control lists (ACLs) and information about entities.\\nThe algorithm first learns an authorization policy in the form of a decision\\ntree, and then extracts a set of candidate authorization rules from the\\ndecision tree. Next, it constructs the final mined policy by eliminating\\nnegative conditions from the candidate rules and then simplifying them.\\nCompared to state-of-the-art ReBAC mining algorithms, DTRM is simpler,\\nsignificantly faster, achieves similar policy quality, and can mine policies in\\na richer language.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12200</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12200</id><submitter>Serkan Cabi</submitter><version version=\"v1\"><date>Thu, 26 Sep 2019 15:45:23 GMT</date><size>3895kb</size><source_type>D</source_type></version><title>A Framework for Data-Driven Robotics</title><authors>Serkan Cabi, Sergio G\\\\\\'omez Colmenarejo, Alexander Novikov, Ksenia\\n  Konyushkova, Scott Reed, Rae Jeong, Konrad Zolna, Yusuf Aytar, David Budden,\\n  Mel Vecerik, Oleg Sushkov, David Barker, Jonathan Scholz, Misha Denil, Nando\\n  de Freitas, Ziyu Wang</authors><categories>cs.RO cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework for data-driven robotics that makes use of a large\\ndataset of recorded robot experience and scales to several tasks using learned\\nreward functions. We show how to apply this framework to accomplish three\\ndifferent object manipulation tasks on a real robot platform. Given\\ndemonstrations of a task together with task-agnostic recorded experience, we\\nuse a special form of human annotation as supervision to learn a reward\\nfunction, which enables us to deal with real-world tasks where the reward\\nsignal cannot be acquired directly. Learned rewards are used in combination\\nwith a large dataset of experience from different tasks to learn a robot policy\\noffline using batch RL. We show that using our approach it is possible to train\\nagents to perform a variety of challenging manipulation tasks including\\nstacking rigid objects and handling cloth.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12211</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12211</id><submitter>Emil Je\\\\v{r}\\\\\\'abek</submitter><version version=\"v1\"><date>Thu, 26 Sep 2019 16:03:17 GMT</date><size>117kb</size></version><version version=\"v2\"><date>Wed, 2 Oct 2019 07:32:11 GMT</date><size>75kb</size></version><title>On the complexity of the clone membership problem</title><authors>Emil Je\\\\v{r}\\\\\\'abek</authors><categories>cs.CC cs.LO</categories><comments>24 pages; cleaned up image</comments><msc-class>68Q17, 08A40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the complexity of the Boolean clone membership problem (CMP):\\ngiven a set of Boolean functions $F$ and a Boolean function $f$, determine if\\n$f$ is in the clone generated by $F$, i.e., if it can be expressed by a circuit\\nwith $F$-gates. Here, $f$ and elements of $F$ are given as circuits or formulas\\nover the usual De Morgan basis. B\\\\&quot;ohler and Schnoor (2007) proved that for any\\nfixed $F$, the problem is coNP-complete, with a few exceptions where it is in\\nP. Vollmer (2009) incorrectly claimed that the full problem CMP is also\\ncoNP-complete. We prove that CMP is in fact $\\\\Theta^P_2$-complete, and we\\ncomplement B\\\\&quot;ohler and Schnoor\\'s results by showing that for fixed $f$, the\\nproblem is NP-complete unless $f$ is a projection.\\n  More generally, we study the problem $B$-CMP where $F$ and $f$ are given by\\ncircuits using gates from $B$. For most choices of $B$, we classify the\\ncomplexity of $B$-CMP as being $\\\\Theta^P_2$-complete (possibly under randomized\\nreductions), coDP-complete, or in P.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12224</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12224</id><submitter>Zhixin Piao</submitter><version version=\"v1\"><date>Thu, 26 Sep 2019 16:23:59 GMT</date><size>8002kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 14:52:32 GMT</date><size>8002kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Tue, 1 Oct 2019 16:42:27 GMT</date><size>8002kb</size><source_type>D</source_type></version><title>Liquid Warping GAN: A Unified Framework for Human Motion Imitation,\\n  Appearance Transfer and Novel View Synthesis</title><authors>Wen Liu, Zhixin Piao, Jie Min, Wenhan Luo, Lin Ma, Shenghua Gao</authors><categories>cs.CV cs.LG eess.IV</categories><comments>accepted by ICCV2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We tackle the human motion imitation, appearance transfer, and novel view\\nsynthesis within a unified framework, which means that the model once being\\ntrained can be used to handle all these tasks. The existing task-specific\\nmethods mainly use 2D keypoints (pose) to estimate the human body structure.\\nHowever, they only expresses the position information with no abilities to\\ncharacterize the personalized shape of the individual person and model the\\nlimbs rotations. In this paper, we propose to use a 3D body mesh recovery\\nmodule to disentangle the pose and shape, which can not only model the joint\\nlocation and rotation but also characterize the personalized body shape. To\\npreserve the source information, such as texture, style, color, and face\\nidentity, we propose a Liquid Warping GAN with Liquid Warping Block (LWB) that\\npropagates the source information in both image and feature spaces, and\\nsynthesizes an image with respect to the reference. Specifically, the source\\nfeatures are extracted by a denoising convolutional auto-encoder for\\ncharacterizing the source identity well. Furthermore, our proposed method is\\nable to support a more flexible warping from multiple sources. In addition, we\\nbuild a new dataset, namely Impersonator (iPER) dataset, for the evaluation of\\nhuman motion imitation, appearance transfer, and novel view synthesis.\\nExtensive experiments demonstrate the effectiveness of our method in several\\naspects, such as robustness in occlusion case and preserving face identity,\\nshape consistency and clothes details. All codes and datasets are available on\\nhttps://svip-lab.github.io/project/impersonator.html\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12243</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12243</id><submitter>Yi Huang</submitter><version version=\"v1\"><date>Thu, 26 Sep 2019 16:42:13 GMT</date><size>1826kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 02:08:53 GMT</date><size>1825kb</size><source_type>D</source_type></version><title>Data Smashing 2.0: Sequence Likelihood (SL) Divergence For Fast Time\\n  Series Comparison</title><authors>Yi Huang and Ishanu Chattopadhyay</authors><categories>stat.ML cs.LG q-fin.MF</categories><comments>typos corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recognizing subtle historical patterns is central to modeling and forecasting\\nproblems in time series analysis. Here we introduce and develop a new approach\\nto quantify deviations in the underlying hidden generators of observed data\\nstreams, resulting in a new efficiently computable universal metric for time\\nseries. The proposed metric is in the sense that we can compare and contrast\\ndata streams regardless of where and how they are generated and without any\\nfeature engineering step. The approach proposed in this paper is conceptually\\ndistinct from our previous work on data smashing, and vastly improves\\ndiscrimination performance and computing speed. The core idea here is the\\ngeneralization of the notion of KL divergence often used to compare probability\\ndistributions to a notion of divergence in time series. We call this the\\nsequence likelihood (SL) divergence, which may be used to measure deviations\\nwithin a well-defined class of discrete-valued stochastic processes. We devise\\nefficient estimators of SL divergence from finite sample paths and subsequently\\nformulate a universal metric useful for computing distance between time series\\nproduced by hidden stochastic generators.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12289</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12289</id><submitter>Qingyun Dou</submitter><version version=\"v1\"><date>Thu, 26 Sep 2019 17:52:15 GMT</date><size>175kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Wed, 2 Oct 2019 19:17:11 GMT</date><size>175kb</size><source_type>D</source_type></version><title>Attention Forcing for Sequence-to-sequence Model Training</title><authors>Qingyun Dou, Yiting Lu, Joshua Efiong and Mark J. F. Gales</authors><categories>cs.LG cs.CL eess.AS stat.ML</categories><comments>11 pages, 4 figures, conference</comments><acm-class>I.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Auto-regressive sequence-to-sequence models with attention mechanism have\\nachieved state-of-the-art performance in many tasks such as machine translation\\nand speech synthesis. These models can be difficult to train. The standard\\napproach, teacher forcing, guides a model with reference output history during\\ntraining. The problem is that the model is unlikely to recover from its\\nmistakes during inference, where the reference output is replaced by generated\\noutput. Several approaches deal with this problem, largely by guiding the model\\nwith generated output history. To make training stable, these approaches often\\nrequire a heuristic schedule or an auxiliary classifier. This paper introduces\\nattention forcing, which guides the model with generated output history and\\nreference attention. This approach can train the model to recover from its\\nmistakes, in a stable fashion, without the need for a schedule or a classifier.\\nIn addition, it allows the model to generate output sequences aligned with the\\nreferences, which can be important for cascaded systems like many speech\\nsynthesis systems. Experiments on speech synthesis show that attention forcing\\nyields significant performance gain. Experiments on machine translation show\\nthat for tasks where various re-orderings of the output are valid, guiding the\\nmodel with generated output history is challenging, while guiding the model\\nwith reference attention is beneficial.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12292</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12292</id><submitter>Ziwei Ji</submitter><version version=\"v1\"><date>Thu, 26 Sep 2019 17:56:28 GMT</date><size>18kb</size></version><version version=\"v2\"><date>Sun, 29 Sep 2019 02:21:50 GMT</date><size>18kb</size></version><title>Polylogarithmic width suffices for gradient descent to achieve\\n  arbitrarily small test error with shallow ReLU networks</title><authors>Ziwei Ji, Matus Telgarsky</authors><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent theoretical work has guaranteed that overparameterized networks\\ntrained by gradient descent achieve arbitrarily low training error, and\\nsometimes even low test error. The required width, however, is always\\npolynomial in at least one of the sample size $n$, the (inverse) target error\\n$1/\\\\epsilon$, and the (inverse) failure probability $1/\\\\delta$. This work shows\\nthat $\\\\widetilde{O}(1/\\\\epsilon)$ iterations of gradient descent with\\n$\\\\widetilde{\\\\Omega}(1/\\\\epsilon^2)$ training examples on two-layer ReLU networks\\nof any width exceeding $\\\\mathrm{polylog}(n,1/\\\\epsilon,1/\\\\delta)$ suffice to\\nachieve a test misclassification error of $\\\\epsilon$. The analysis further\\nrelies upon a margin property of the limiting kernel, which is guaranteed\\npositive, and can distinguish between true labels and random labels.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12329</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12329</id><submitter>Xiangyun Meng</submitter><version version=\"v1\"><date>Thu, 26 Sep 2019 18:46:07 GMT</date><size>5861kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 8 Oct 2019 02:06:39 GMT</date><size>5860kb</size><source_type>D</source_type></version><title>Scaling Local Control to Large-Scale Topological Navigation</title><authors>Xiangyun Meng, Nathan Ratliff, Yu Xiang and Dieter Fox</authors><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual topological navigation has been revitalized recently thanks to the\\nadvancement of deep learning that substantially improves robot perception.\\nHowever, the scalability and reliability issue remain challenging due to the\\ncomplexity and ambiguity of real world images and mechanical constraints of\\nreal robots. We present an intuitive solution to show that by accurately\\nmeasuring the capability of a local controller, large-scale visual topological\\nnavigation can be achieved while being scalable and robust. Our approach\\nachieves state-of-the-art results in trajectory following and planning in\\nlarge-scale environments. It also generalizes well to real robots and new\\nenvironments without retraining or finetuning.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12354</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12354</id><submitter>Qingyang Tan</submitter><version version=\"v1\"><date>Thu, 26 Sep 2019 19:38:58 GMT</date><size>17477kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 14:28:37 GMT</date><size>17477kb</size><source_type>D</source_type></version><title>Realtime Simulation of Thin-Shell Deformable Materials using CNN-Based\\n  Mesh Embedding</title><authors>Qingyang Tan, Zherong Pan, Lin Gao, Dinesh Manocha</authors><categories>cs.GR cs.CV cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of accelerating thin-shell deformable object\\nsimulations by dimension reduction. We present a new algorithm to embed a\\nhigh-dimensional configuration space of deformable objects in a low-dimensional\\nfeature space, where the configurations of objects and feature points have\\napproximate one-to-one mapping. Our key technique is a graph-based\\nconvolutional neural network (CNN) defined on meshes with arbitrary topologies\\nand a new mesh embedding approach based on physics-inspired loss term. We have\\napplied our approach to accelerate high-resolution thin shell simulations\\ncorresponding to cloth-like materials, where the configuration space has tens\\nof thousands of degrees of freedom. We show that our physics-inspired embedding\\napproach leads to higher accuracy compared with prior mesh embedding methods.\\nFinally, we show that the temporal evolution of the mesh in the feature space\\ncan also be learned using a recurrent neural network (RNN) leading to fully\\nlearnable physics simulators. After training our learned simulator runs\\n$10-100\\\\times$ faster and the accuracy is high enough for robot manipulation\\ntasks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12379</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12379</id><submitter>Vicent Cholvi</submitter><version version=\"v1\"><date>Thu, 26 Sep 2019 20:40:47 GMT</date><size>46kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 07:25:28 GMT</date><size>46kb</size><source_type>D</source_type></version><title>Packet-oblivious stable routing in multi-hop wireless networks</title><authors>Vicent Cholvi, Pawe{\\\\l} Garncarek, Tomasz Jurdzinski and Dariusz R.\\n  Kowalski</authors><categories>cs.NI cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study the fundamental problem of scheduling communication in\\nmulti-hop wireless networks.\\n  We focus on packet-oblivious routing protocols; that is, algorithms that do\\nnot take into account any historical information about packets or carried out\\nby packets. Such protocols are well motivated in practice, as real forwarding\\nprotocols and corresponding data-link layer architectures are typically\\npacket-oblivious. We provide a local-knowledge protocol, i.e., which is working\\nwithout using any topological information, except for some upper bounds on the\\nnumber of links and the network\\'s degree, that is stable for a wide spectrum of\\npacket injection rates. It is based on novel transmission schedules, called\\nuniversally strong selectors, which, combined with some known queuing policies\\n(LIS, SIS, NFS, FTG), makes it the best known local-knowledge packet-oblivious\\nrouting protocol regarding the injection rate for which stability is\\nguaranteed. We also propose a global-knowledge protocol, which is stable if the\\npacket injection rate per link is smaller than the inverse of the chromatic\\nnumber of the collision graph associated with the network. Although the\\nprotocol does not take into account any historical information, it has to be\\nseeded by some information about the network topology.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12384</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12384</id><submitter>Xiangrui Zeng</submitter><version version=\"v1\"><date>Thu, 26 Sep 2019 20:57:17 GMT</date><size>129kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Thu, 3 Oct 2019 19:23:13 GMT</date><size>128kb</size><source_type>D</source_type></version><title>CS Sparse K-means: An Algorithm for Cluster-Specific Feature Selection\\n  in High-Dimensional Clustering</title><authors>Xiangrui Zeng, Hongyu Zheng</authors><categories>stat.ME cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Feature selection is an important and challenging task in high dimensional\\nclustering. For example, in genomics, there may only be a small number of genes\\nthat are differentially expressed, which are informative to the overall\\nclustering structure. Existing feature selection methods, such as Sparse\\nK-means, rarely tackle the problem of accounting features that can only\\nseparate a subset of clusters. In genomics, it is highly likely that a gene can\\nonly define one subtype against all the other subtypes or distinguish a pair of\\nsubtypes but not others. In this paper, we propose a K-means based clustering\\nalgorithm that discovers informative features as well as which cluster pairs\\nare separable by each selected features. The method is essentially an EM\\nalgorithm, in which we introduce lasso-type constraints on each cluster pair in\\nthe M step, and make the E step possible by maximizing the raw cross-cluster\\ndistance instead of minimizing the intra-cluster distance. The results were\\ndemonstrated on simulated data and a leukemia gene expression dataset.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12432</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12432</id><submitter>Hamidreza Mahyar</submitter><version version=\"v1\"><date>Thu, 26 Sep 2019 23:18:40 GMT</date><size>1966kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 21:06:52 GMT</date><size>2001kb</size><source_type>D</source_type></version><title>A Matrix Factorization Model for Hellinger-based Trust Management in\\n  Social Internet of Things</title><authors>Soroush Aalibagi, Hamidreza Mahyar, Ali Movaghar, and H. Eugene\\n  Stanley</authors><categories>cs.LG cs.CR cs.SI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Social Internet of Things (SIoT), integration of Internet of Things and\\nSocial networks paradigms, has been introduced to build a network of smart\\nnodes which are capable of establishing social links. In order to deal with\\nmisbehavioral service provider nodes, service requestor nodes must evaluate\\ntheir trustworthiness levels. In this paper, we propose a novel trust\\nmanagement mechanism in the SIoT to predict the most reliable service provider\\nfor a service requestor, that leads to reduce the risk of exposing to malicious\\nnodes. We model an SIoT with a flexible bipartite graph (containing two sets of\\nnodes: service providers and requestors), then build the corresponding social\\nnetwork among service requestor nodes, using Hellinger distance. After that, we\\ndevelop a social trust model, by using nodes\\' centrality and similarity\\nmeasures, to extract behavioral trust between the network nodes. Finally, a\\nmatrix factorization technique is designed to extract latent features of SIoT\\nnodes to mitigate the data sparsity and cold start problems. We analyze the\\neffect of parameters in the proposed trust prediction mechanism on prediction\\naccuracy. The results indicate that feedbacks from the neighboring nodes of a\\nspecific service requestor with high Hellinger similarity in our mechanism\\noutperforms the best existing methods. We also show that utilizing social trust\\nmodel, which only considers the similarity measure, significantly improves the\\naccuracy of the prediction mechanism. Furthermore, we evaluate the\\neffectiveness of the proposed trust management system through a real-world SIoT\\napplication. Our results demonstrate that the proposed mechanism is resilient\\nto different types of network attacks and it can accurately find the proper\\nservice provider with high trustworthiness.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12511</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12511</id><submitter>Abhishek Halder</submitter><version version=\"v1\"><date>Fri, 27 Sep 2019 06:32:46 GMT</date><size>82kb</size></version><version version=\"v2\"><date>Mon, 30 Sep 2019 03:53:59 GMT</date><size>82kb</size></version><title>Finite Horizon Density Steering for Multi-input State Feedback\\n  Linearizable Systems</title><authors>Kenneth F. Caluya, Abhishek Halder</authors><categories>math.OC cs.SY eess.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the feedback synthesis problem for steering the joint\\nstate density or ensemble subject to multi-input state feedback linearizable\\ndynamics. This problem is of interest to many practical applications including\\nthat of dynamically shaping a robotic swarm. Our results here show that it is\\npossible to exploit the structural nonlinearities to derive the feedback\\ncontrollers steering the joint density from a prescribed shape to another while\\nminimizing the expected control effort to do so. The developments herein build\\non our previous work, and extend the theory of the Schr\\\\&quot;{o}dinger bridge\\nproblem subject to feedback linearizable dynamics.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12518</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12518</id><submitter>Lior Kamma</submitter><version version=\"v1\"><date>Fri, 27 Sep 2019 06:56:47 GMT</date><size>23kb</size></version><version version=\"v2\"><date>Mon, 30 Sep 2019 14:39:11 GMT</date><size>23kb</size></version><title>Margin-Based Generalization Lower Bounds for Boosted Classifiers</title><authors>Allan Gr{\\\\o}nlund, Lior Kamma, Kasper Green Larsen, Alexander\\n  Mathiasen, Jelani Nelson</authors><categories>cs.LG cs.DS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boosting is one of the most successful ideas in machine learning. The most\\nwell-accepted explanations for the low generalization error of boosting\\nalgorithms such as AdaBoost stem from margin theory. The study of margins in\\nthe context of boosting algorithms was initiated by Schapire, Freund, Bartlett\\nand Lee (1998) and has inspired numerous boosting algorithms and generalization\\nbounds. To date, the strongest known generalization (upper bound) is the $k$th\\nmargin bound of Gao and Zhou (2013). Despite the numerous generalization upper\\nbounds that have been proved over the last two decades, nothing is known about\\nthe tightness of these bounds. In this paper, we give the first margin-based\\nlower bounds on the generalization error of boosted classifiers. Our lower\\nbounds nearly match the $k$th margin bound and thus almost settle the\\ngeneralization performance of boosted classifiers in terms of margins.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12563</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12563</id><submitter>Pieter Hartel</submitter><version version=\"v1\"><date>Fri, 27 Sep 2019 08:53:50 GMT</date><size>93kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Mon, 30 Sep 2019 07:35:11 GMT</date><size>94kb</size><source_type>D</source_type></version><title>Gas limit aware mutation testing of smart contracts at scale</title><authors>Pieter Hartel, Richard Schumi</authors><categories>cs.SE</categories><comments>11 pages, 6 tables, 1 figure; added one reference, corrected some\\n  typos and rephrased a few sentences</comments><acm-class>D.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The blockchain is a key technology that has been adopted in many application\\nareas to increase security and reliability and to avoid the need for a central\\ntrusted authority. One of its essential underlying foundations are smart\\ncontracts, which are executable programs for managing data or assets on the\\nblockchain. It is crucial that smart contracts are tested thoroughly due to\\ntheir immutable nature and since even small bugs can lead to huge monetary\\nlosses. However, it is not enough to just test smart contracts, it is also\\nimportant to ensure the quality and completeness of the tests. Hence, we\\nintroduce new smart contract specific mutation operators as well as a novel\\nkilling condition that is able to detect a deviation in the gas consumptions,\\ni.e., in the monetary value that is required to perform transactions. Moreover,\\nwe establish a baseline for mutation testing of smart contracts by applying our\\nmethod to a replay test suite and by testing about a thousand contracts.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12584</identifier>\\n <datestamp>2019-10-09</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12584</id><submitter>Wei Li</submitter><version version=\"v1\"><date>Fri, 27 Sep 2019 09:53:12 GMT</date><size>554kb</size></version><version version=\"v2\"><date>Mon, 7 Oct 2019 02:07:10 GMT</date><size>554kb</size></version><version version=\"v3\"><date>Tue, 8 Oct 2019 04:31:08 GMT</date><size>554kb</size></version><title>Upper Bound of Collective Attacks on Quantum Key Distribution</title><authors>Wei Li, Shengmei Zhao</authors><categories>cs.CR quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluating the theoretical limit of the amount of information Eve can steal\\nfrom a quantum key distribution protocol under given conditions is one of the\\nmost important things that need to be done in security proof. In addition to\\nsource loopholes and detection loopholes, channel attacks are considered to be\\nthe main ways of information leakage, while collective attacks are considered\\nto be the most powerful active channel attacks. Here we deduce in detail the\\ncapability limit of Eve\\'s collective attack in non-entangled quantum key\\ndistribution, like BB84 and measurement-device-independent protocols, and\\nentangled quantum key distribution, like device-independent protocol, in which\\ncollective attack is composed of quantum weak measurement and quantum\\nunambiguous state discrimination detection. The theoretical results show that\\ncollective attacks are equivalent in entangled and non-entangled quantum key\\ndistribution protocols. We also find that compared with the security proof\\nbased on entanglement purification, the security proof based on collective\\nattack not only improves the system\\'s tolerable bit error rate, but also\\nimproves the key rate.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12617</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12617</id><submitter>Mahdi Sarbazi</submitter><version version=\"v1\"><date>Fri, 27 Sep 2019 11:06:39 GMT</date><size>2042kb</size></version><title>Improving Resource Allocation in Software-Defined Networks using\\n  Clustering</title><authors>Mahdi Sarbazi, Mehdi SadeghZadeh, seyyed Javad Mir Abedini</authors><categories>cs.NI</categories><comments>12 pages, 10 figure. 1 table. Cluster Comput (2019)</comments><doi>10.1007/s10586-019-02985-3</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Software-defined networks (SDNs) are a huge evolution in simplifying\\nimplementation and network operation which have reduced costs and made the\\nnetwork programmable. Although SDNs are a suitable option for solving some of\\nthe previous problems, but they have some challenges as any other new\\ntechnology. Resource allocation and balance control in network is one of the\\nmain challenges of this technology which is studied in this paper. In this\\nstudy, a new approach is proposed for improving memory resource allocation in\\nnetwork using load distribution clusters. Since in the proposed method,\\nK-mean++ algorithm is used for clustering, load balancing of clusters can be\\nused to preserve load balance of the network. In the proposed method, data with\\nhigher recall is transmitted to high-quality clusters in terms of average\\nnumber of hubs and lower average delay between server and user. In the proposed\\nmethod, by increasing number of clusters, higher memory is created in the\\nnetwork.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12638</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12638</id><submitter>Jinchen Xuan</submitter><version version=\"v1\"><date>Fri, 27 Sep 2019 12:00:41 GMT</date><size>1433kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 6 Oct 2019 06:27:06 GMT</date><size>1433kb</size><source_type>D</source_type></version><title>On the Anomalous Generalization of GANs</title><authors>Jinchen Xuan, Yunchang Yang, Ze Yang, Di He, Liwei Wang</authors><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative models, especially Generative Adversarial Networks (GANs), have\\nreceived significant attention recently. However, it has been observed that in\\nterms of some attributes, e.g. the number of simple geometric primitives in an\\nimage, GANs are not able to learn the target distribution in practice.\\nMotivated by this observation, we discover two specific problems of GANs\\nleading to anomalous generalization behaviour, which we refer to as the sample\\ninsufficiency and the pixel-wise combination. For the first problem of sample\\ninsufficiency, we show theoretically and empirically that the batchsize of the\\ntraining samples in practice may be insufficient for the discriminator to learn\\nan accurate discrimination function. It could result in unstable training\\ndynamics for the generator, leading to anomalous generalization. For the second\\nproblem of pixel-wise combination, we find that besides recognizing the\\npositive training samples as real, under certain circumstances, the\\ndiscriminator could be fooled to recognize the pixel-wise combinations (e.g.\\npixel-wise average) of the positive training samples as real. However, those\\ncombinations could be visually different from the real samples in the target\\ndistribution. With the fooled discriminator as reference, the generator would\\nobtain biased supervision further, leading to the anomalous generalization\\nbehaviour. Additionally, in this paper, we propose methods to mitigate the\\nanomalous generalization of GANs. Extensive experiments on benchmark show our\\nproposed methods improve the FID score up to 30\\\\% on natural image dataset.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12681</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12681</id><submitter>Emre Yilmaz</submitter><version version=\"v1\"><date>Fri, 27 Sep 2019 13:38:20 GMT</date><size>17kb</size></version><version version=\"v2\"><date>Mon, 30 Sep 2019 06:17:59 GMT</date><size>17kb</size></version><title>End-to-End Code-Switching ASR for Low-Resourced Language Pairs</title><authors>Xianghu Yue, Grandee Lee, Emre Y{\\\\i}lmaz, Fang Deng, Haizhou Li</authors><categories>cs.CL eess.AS</categories><comments>Accepted for publication at IEEE ASRU Workshop 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the significant progress in end-to-end (E2E) automatic speech\\nrecognition (ASR), E2E ASR for low resourced code-switching (CS) speech has not\\nbeen well studied. In this work, we describe an E2E ASR pipeline for the\\nrecognition of CS speech in which a low-resourced language is mixed with a high\\nresourced language. Low-resourcedness in acoustic data hinders the performance\\nof E2E ASR systems more severely than the conventional ASR systems.~To mitigate\\nthis problem in the transcription of archives with code-switching Frisian-Dutch\\nspeech, we integrate a designated decoding scheme and perform rescoring with\\nneural network-based language models to enable better utilization of the\\navailable textual resources. We first incorporate a multi-graph decoding\\napproach which creates parallel search spaces for each monolingual and mixed\\nrecognition tasks to maximize the utilization of the textual resources from\\neach language. Further, language model rescoring is performed using a recurrent\\nneural network pre-trained with cross-lingual embedding and further adapted\\nwith the limited amount of in-domain CS text. The ASR experiments demonstrate\\nthe effectiveness of the described techniques in improving the recognition\\nperformance of an E2E CS ASR system in a low-resourced scenario.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12743</identifier>\\n <datestamp>2019-10-03</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12743</id><submitter>Reza Bahmanyar</submitter><version version=\"v1\"><date>Fri, 27 Sep 2019 15:22:23 GMT</date><size>7696kb</size><source_type>D</source_type></version><title>MRCNet: Crowd Counting and Density Map Estimation in Aerial and Ground\\n  Imagery</title><authors>Reza Bahmanyar, Elenora Vig, and Peter Reinartz</authors><categories>cs.CV cs.LG eess.IV stat.ML</categories><journal-ref>BMVC Workshop on Object Detection and Recognition for Security\\n  Screenin (BMVC-ODRSS) 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In spite of the many advantages of aerial imagery for crowd monitoring and\\nmanagement at mass events, datasets of aerial images of crowds are still\\nlacking in the field. As a remedy, in this work we introduce a novel crowd\\ndataset, the DLR Aerial Crowd Dataset (DLR-ACD), which is composed of 33 large\\naerial images acquired from 16 flight campaigns over mass events with 226,291\\npersons annotated. To the best of our knowledge, DLR-ACD is the first aerial\\ncrowd dataset and will be released publicly. To tackle the problem of accurate\\ncrowd counting and density map estimation in aerial images of crowds, this work\\nalso proposes a new encoder-decoder convolutional neural network, the so-called\\nMulti-Resolution Crowd Network MRCNet. The encoder is based on the VGG-16\\nnetwork and the decoder is composed of a set of bilinear upsampling and\\nconvolutional layers. Using two losses, one at an earlier level and another at\\nthe last level of the decoder, MRCNet estimates crowd counts and\\nhigh-resolution crowd density maps as two different but interrelated tasks. In\\naddition, MRCNet utilizes contextual and detailed local information by\\ncombining high- and low-level features through a number of lateral connections\\ninspired by the Feature Pyramid Network (FPN) technique. We evaluated MRCNet on\\nthe proposed DLR-ACD dataset as well as on the ShanghaiTech dataset, a\\nCCTV-based crowd counting benchmark. The results demonstrate that MRCNet\\noutperforms the state-of-the-art crowd counting methods in estimating the crowd\\ncounts and density maps for both aerial and CCTV-based images.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12798</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12798</id><submitter>Hao Wang</submitter><version version=\"v1\"><date>Tue, 24 Sep 2019 10:50:47 GMT</date><size>293kb</size></version><title>Quantitative analysis of Matthew effect and sparsity problem of\\n  recommender systems</title><authors>Hao Wang, Zonghu Wang, Weishi Zhang</authors><categories>cs.IR cs.LG</categories><journal-ref>2018 IEEE 3rd International Conference on Cloud Computing and Big\\n  Data Analysis (ICCCBDA)</journal-ref><doi>10.1109/ICCCBDA.2018.8386490</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender systems have received great commercial success. Recommendation\\nhas been used widely in areas such as e-commerce, online music FM, online news\\nportal, etc. However, several problems related to input data structure pose\\nserious challenge to recommender system performance. Two of these problems are\\nMatthew effect and sparsity problem. Matthew effect heavily skews recommender\\nsystem output towards popular items. Data sparsity problem directly affects the\\ncoverage of recommendation result. Collaborative filtering is a simple\\nbenchmark ubiquitously adopted in the industry as the baseline for recommender\\nsystem design. Understanding the underlying mechanism of collaborative\\nfiltering is crucial for further optimization. In this paper, we do a thorough\\nquantitative analysis on Matthew effect and sparsity problem in the particular\\ncontext setting of collaborative filtering. We compare the underlying mechanism\\nof user-based and item-based collaborative filtering and give insight to\\nindustrial recommender system builders.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12837</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12837</id><submitter>Andrei Cramariuc</submitter><version version=\"v1\"><date>Fri, 27 Sep 2019 16:02:02 GMT</date><size>7351kb</size><source_type>D</source_type></version><title>SegMap: Segment-based mapping and localization using data-driven\\n  descriptors</title><authors>Renaud Dub\\\\\\'e, Andrei Cramariuc, Daniel Dugas, Hannes Sommer, Marcin\\n  Dymczyk, Juan Nieto, Roland Siegwart, and Cesar Cadena</authors><categories>cs.RO cs.CV</categories><comments>arXiv admin note: substantial text overlap with arXiv:1804.09557</comments><doi>10.1177/0278364919863090</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Precisely estimating a robot\\'s pose in a prior, global map is a fundamental\\ncapability for mobile robotics, e.g. autonomous driving or exploration in\\ndisaster zones. This task, however, remains challenging in unstructured,\\ndynamic environments, where local features are not discriminative enough and\\nglobal scene descriptors only provide coarse information. We therefore present\\nSegMap: a map representation solution for localization and mapping based on the\\nextraction of segments in 3D point clouds. Working at the level of segments\\noffers increased invariance to view-point and local structural changes, and\\nfacilitates real-time processing of large-scale 3D data. SegMap exploits a\\nsingle compact data-driven descriptor for performing multiple tasks: global\\nlocalization, 3D dense map reconstruction, and semantic information extraction.\\nThe performance of SegMap is evaluated in multiple urban driving and search and\\nrescue experiments. We show that the learned SegMap descriptor has superior\\nsegment retrieval capabilities, compared to state-of-the-art handcrafted\\ndescriptors. In consequence, we achieve a higher localization accuracy and a 6%\\nincrease in recall over state-of-the-art. These segment-based localizations\\nallow us to reduce the open-loop odometry drift by up to 50%. SegMap is\\nopen-source available along with easy to run demonstrations.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12838</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12838</id><submitter>Alberto Barbado Gonzalez</submitter><version version=\"v1\"><date>Fri, 27 Sep 2019 16:28:01 GMT</date><size>372kb</size></version><title>Responsible AI by Design</title><authors>Richard Benjamins, Alberto Barbado, Daniel Sierra</authors><categories>cs.CY</categories><comments>9 pages, 3 tables. To appear in the Proceedings of the Human-Centered\\n  AI: Trustworthiness of AI Models &amp; Data (HAI) track at AAAI Fall Symposium,\\n  DC, November 7-9, 2019</comments><acm-class>K.4.1; K.4.2; K.4.3</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Recently, a lot of attention has been given to undesired consequences of\\nArtificial Intelligence (AI), such as unfair bias leading to discrimination, or\\nthe lack of explanations of the results of AI systems. There are several\\nimportant questions to answer before AI can be deployed at scale in our\\nbusinesses and societies. Most of these issues are being discussed by experts\\nand the wider communities, and it seems there is broad consensus on where they\\ncome from. There is, however, less consensus on, and experience with how to\\npractically deal with those issues in organizations that develop and use AI,\\nboth from a technical and organizational perspective. In this paper, we discuss\\nthe practical case of a large organization that is putting in place a\\ncompany-wide methodology to minimize the risk of undesired consequences of AI.\\nWe hope that other organizations can learn from this and that our experience\\ncontributes to making the best of AI while minimizing its risks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12863</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12863</id><submitter>Laura Sanit\\\\`a</submitter><version version=\"v1\"><date>Fri, 27 Sep 2019 18:28:06 GMT</date><size>28kb</size></version><title>Pivot Rules for Circuit-Augmentation Algorithms in Linear Optimization</title><authors>Jes\\\\\\'us A. De Loera, Sean Kafer, Laura Sanit\\\\`a</authors><categories>math.CO cs.DM math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Circuit-augmentation algorithms are a generalization of the Simplex method,\\nwhere in each step one is allowed to move along a set of directions, called\\ncircuits, that is a superset of the edges of a polytope. We show that in this\\ngeneral context the greatest-improvement and Dantzig pivot rules are NP-hard.\\nDifferently, the steepest-descent pivot rule can be computed in polynomial\\ntime, and the number of augmentations required to reach an optimal solution\\naccording to this rule is strongly-polynomial for 0/1 LPs.\\n  Interestingly, we show that this more general framework can be exploited also\\nto make conclusions about the Simplex method itself. In particular, as a\\nbyproduct of our results, we prove that (i) computing the shortest monotone\\npath to an optimal solution on the 1-skeleton of a polytope is NP-hard, and\\nhard to approximate within a factor better than 2, and (ii) for 0/1 polytopes,\\na monotone path of polynomial length can be constructed using steepest\\nimproving edges.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12864</identifier>\\n <datestamp>2019-10-07</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12864</id><submitter>Hancheng Min</submitter><version version=\"v1\"><date>Fri, 27 Sep 2019 18:29:28 GMT</date><size>493kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Fri, 4 Oct 2019 17:43:05 GMT</date><size>832kb</size><source_type>D</source_type></version><title>Accurate Reduced Order Models for Coherent Synchronous Generators</title><authors>Hancheng Min, Fernando Paganini and Enrique Mallada</authors><categories>eess.SY cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel framework to approximate the aggregate frequency\\ndynamics of coherent synchronous generators. By leveraging recent results on\\ndynamics concentration of tightly connected networks, we develop a hierarchy of\\nreduced order models --based on frequency weighted balanced truncation-- that\\naccurately approximate the aggregate system response. Our results outperform\\nexisting aggregation techniques and can be shown to monotonically improve the\\napproximation as the hierarchy order increases.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12867</identifier>\\n <datestamp>2019-10-04</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12867</id><submitter>Quentin Le Gall</submitter><version version=\"v1\"><date>Fri, 27 Sep 2019 18:34:25 GMT</date><size>2470kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 08:23:50 GMT</date><size>2470kb</size><source_type>D</source_type></version><version version=\"v3\"><date>Thu, 3 Oct 2019 16:03:31 GMT</date><size>2470kb</size><source_type>D</source_type></version><title>Relay-assisted Device-to-Device Networks: Connectivity and Uberization\\n  Opportunities</title><authors>Quentin Le Gall, Bart{\\\\l}omiej B{\\\\l}aszczyszyn, Elie Cali, Taoufik\\n  En-Najjary</authors><categories>cs.NI</categories><comments>7 pages, 5 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been shown that deploying device-to-device (D2D) networks in urban\\nenvironments requires equipping a considerable proportion of crossroads with\\nrelays. This represents a necessary economic investment for an operator. In\\nthis work, we tackle the problem of the economic feasibility of such\\nrelay-assisted D2D networks. First, we propose a stochastic model taking into\\naccount a positive surface for streets and crossroads, thus allowing for a more\\nrealistic estimation of the minimal number of needed relays. Secondly, we\\nintroduce a cost model for the deployment of relays, allowing one to study\\noperators\\' D2D deployment strategies. We investigate the example of an\\nuberizing neo-operator willing to set up a network entirely relying on D2D and\\nshow that a return on the initial investment in relays is possible in a\\nrealistic period of time, even if the network is funded by a very low revenue\\nper D2D user. Our results bring quantitative arguments to the discussion on\\npossible uberization scenarios of telecommunications networks.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12868</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12868</id><submitter>Tong Niu</submitter><version version=\"v1\"><date>Fri, 27 Sep 2019 18:40:23 GMT</date><size>2295kb</size><source_type>D</source_type></version><title>Automatically Learning Data Augmentation Policies for Dialogue Tasks</title><authors>Tong Niu, Mohit Bansal</authors><categories>cs.CL cs.AI cs.LG</categories><comments>7 pages (EMNLP 2019)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic data augmentation (AutoAugment) (Cubuk et al., 2019) searches for\\noptimal perturbation policies via a controller trained using performance\\nrewards of a sampled policy on the target task, hence reducing data-level model\\nbias. While being a powerful algorithm, their work has focused on computer\\nvision tasks, where it is comparatively easy to apply imperceptible\\nperturbations without changing an image\\'s semantic meaning. In our work, we\\nadapt AutoAugment to automatically discover effective perturbation policies for\\nnatural language processing (NLP) tasks such as dialogue generation. We start\\nwith a pool of atomic operations that apply subtle semantic-preserving\\nperturbations to the source inputs of a dialogue task (e.g., different POS-tag\\ntypes of stopword dropout, grammatical errors, and paraphrasing). Next, we\\nallow the controller to learn more complex augmentation policies by searching\\nover the space of the various combinations of these atomic operations.\\nMoreover, we also explore conditioning the controller on the source inputs of\\nthe target task, since certain strategies may not apply to inputs that do not\\ncontain that strategy\\'s required linguistic features. Empirically, we\\ndemonstrate that both our input-agnostic and input-aware controllers discover\\nuseful data augmentation policies, and achieve significant improvements over\\nthe previous state-of-the-art, including trained on manually-designed policies.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12874</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12874</id><submitter>Zhiang Chen</submitter><version version=\"v1\"><date>Fri, 27 Sep 2019 18:57:28 GMT</date><size>9199kb</size><source_type>D</source_type></version><title>Geomorphological Analysis Using Unpiloted Aircraft Systems, Structure\\n  from Motion, and Deep Learning</title><authors>Zhiang Chen, Tyler R. Scott, Sarah Bearman, Harish Anand, Chelsea\\n  Scott, J Ramon Arrowsmith, Jnaneshwar Das</authors><categories>cs.RO astro-ph.EP cs.LG physics.geo-ph</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We present a pipeline for geomorphological analysis that uses structure from\\nmotion (SfM) and deep learning on close-range aerial imagery to estimate\\nspatial distributions of rock traits (diameter and orientation), along a\\ntectonic fault scarp. Unpiloted aircraft systems (UAS) have enabled acquisition\\nof high-resolution imagery at close range, revolutionizing domains such as\\ninfrastructure inspection, precision agriculture, and disaster response. Our\\npipeline leverages UAS-based imagery to help scientists gain a better\\nunderstanding of surface processes. Our pipeline uses SfM on aerial imagery to\\nproduce a georeferenced orthomosaic with 2 cm/pixel resolution. A human expert\\nannotates rocks on a set of image tiles sampled from the orthomosaic, and these\\nannotations are used to train a deep neural network to detect and segment\\nindividual rocks in the whole site. Our pipeline, in effect, automatically\\nextracts semantic information (rock boundaries) on large volumes of unlabeled\\nhigh-resolution aerial imagery, and subsequent structural analysis and shape\\ndescriptors result in estimates of rock diameter and orientation. We present\\nresults of our analysis on imagery collected along a fault scarp in the\\nVolcanic Tablelands in eastern California. Although presented in the context of\\ngeology, our pipeline can be extended to a variety of morphological analysis\\ntasks in other domains.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12877</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12877</id><submitter>Jens Stoye</submitter><version version=\"v1\"><date>Fri, 27 Sep 2019 19:13:02 GMT</date><size>120kb</size></version><title>Computing the Inversion-Indel Distance</title><authors>Eyla Willing and Jens Stoye and Mar\\\\\\'ilia D. V. Braga</authors><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The inversion distance, that is the distance between two unichromosomal\\ngenomes with the same content allowing only inversions of DNA segments, can be\\nexactly computed thanks to a pioneering approach of Hannenhalli and Pevzner\\nfrom 1995. In 2000, El-Mabrouk extended the inversion model to perform the\\ncomparison of unichromosomal genomes with unequal contents, combining\\ninversions with insertions and deletions (indels) of DNA segments, giving rise\\nto the inversion-indel distance. However, only a heuristic was provided for its\\ncomputation. In 2005, Yancopoulos, Attie and Friedberg started a new branch of\\nresearch by introducing the generic double cut and join (DCJ) operation, that\\ncan represent several genome rearrangements (including inversions). In 2006,\\nBergeron, Mixtacki and Stoye showed that the DCJ distance can be computed in\\nlinear time with a very simple procedure. As a consequence, in 2010 we gave a\\nlinear-time algorithm to compute the DCJ-indel distance. This result allowed\\nthe inversion-indel model to be revisited from another angle. In 2013, we could\\nshow that, when the diagram that represents the relation between the two\\ncompared genomes has no bad components, the inversion-indel distance is equal\\nto the DCJ-indel distance. In the present work we complete the study of the\\ninversion-indel distance by giving the first algorithm to compute it exactly\\neven in the presence of bad components.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12887</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12887</id><submitter>Zudi Lin</submitter><version version=\"v1\"><date>Fri, 27 Sep 2019 19:42:20 GMT</date><size>5944kb</size><source_type>D</source_type></version><title>A Topological Nomenclature for 3D Shape Analysis in Connectomics</title><authors>Abhimanyu Talwar, Zudi Lin, Donglai Wei, Yuesong Wu, Bowen Zheng,\\n  Jinglin Zhao, Won-Dong Jang, Xueying Wang, Jeff W. Lichtman, and Hanspeter\\n  Pfister</authors><categories>cs.CV</categories><comments>Technical report</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  An essential task in nano-scale connectomics is the morphology analysis of\\nneurons and organelles like mitochondria to shed light on their biological\\nproperties. However, these biological objects often have tangled parts or\\ncomplex branching patterns, which makes it hard to abstract, categorize, and\\nmanipulate their morphology. Here we propose a topological nomenclature to name\\nthese objects like chemical compounds for neuroscience analysis. To this end,\\nwe convert the volumetric representation into the topology-preserving reduced\\ngraph, develop nomenclature rules for pyramidal neurons and mitochondria from\\nthe reduced graph, and learn the feature embedding for shape manipulation. In\\nablation studies, we show that the proposed reduced graph extraction method\\nyield graphs better in accord with the perception of experts. On 3D shape\\nretrieval and decomposition tasks, we show that the encoded topological\\nnomenclature features achieve better results than state-of-the-art shape\\ndescriptors. To advance neuroscience, we will release a 3D mesh dataset of\\nmitochondria and pyramidal neurons reconstructed from a 100{\\\\mu}m cube electron\\nmicroscopy (EM) volume. Code is publicly available at\\nhttps://github.com/donglaiw/ibexHelper.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12892</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12892</id><submitter>S\\\\\\'ebastien Racani\\\\`ere</submitter><version version=\"v1\"><date>Fri, 27 Sep 2019 20:11:12 GMT</date><size>3978kb</size><source_type>D</source_type></version><title>Automated curricula through setter-solver interactions</title><authors>Sebastien Racaniere, Andrew K. Lampinen, Adam Santoro, David P.\\n  Reichert, Vlad Firoiu, Timothy P. Lillicrap</authors><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reinforcement learning algorithms use correlations between policies and\\nrewards to improve agent performance. But in dynamic or sparsely rewarding\\nenvironments these correlations are often too small, or rewarding events are\\ntoo infrequent to make learning feasible. Human education instead relies on\\ncurricula--the breakdown of tasks into simpler, static challenges with dense\\nrewards--to build up to complex behaviors. While curricula are also useful for\\nartificial agents, hand-crafting them is time consuming. This has lead\\nresearchers to explore automatic curriculum generation. Here we explore\\nautomatic curriculum generation in rich, dynamic environments. Using a\\nsetter-solver paradigm we show the importance of considering goal validity,\\ngoal feasibility, and goal coverage to construct useful curricula. We\\ndemonstrate the success of our approach in rich but sparsely rewarding 2D and\\n3D environments, where an agent is tasked to achieve a single goal selected\\nfrom a set of possible goals that varies between episodes, and identify\\nchallenges for future work. Finally, we demonstrate the value of a novel\\ntechnique that guides agents towards a desired goal distribution. Altogether,\\nthese results represent a substantial step towards applying automatic task\\ncurricula to learn complex, otherwise unlearnable goals, and to our knowledge\\nare the first to demonstrate automated curriculum generation for\\ngoal-conditioned agents in environments where the possible goals vary between\\nepisodes.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12894</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12894</id><submitter>Kostas Hatalis</submitter><version version=\"v1\"><date>Fri, 27 Sep 2019 20:14:47 GMT</date><size>394kb</size><source_type>D</source_type></version><title>Modeling and Detection of Future Cyber-Enabled DSM Data Attacks using\\n  Supervised Learning</title><authors>Kostas Hatalis, Parv Venkitasubramaniam, Shalinee Kishore</authors><categories>eess.SP cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Demand-Side Management (DSM) is a vital tool that can be used to ensure power\\nsystem reliability and stability. In future smart grids, certain portions of a\\ncustomers load usage could be under automatic control with a cyber-enabled DSM\\nprogram which selectively schedules loads as a function of electricity prices\\nto improve power balance and grid stability. In such a case, the security of\\nDSM cyberinfrastructure will be critical as advanced metering infrastructure,\\nand communication systems are susceptible to hacking, cyber-attacks. Such\\nattacks, in the form of data injection, can manipulate customer load profiles\\nand cause metering chaos and energy losses in the grid. These attacks are also\\nexacerbated by the feedback mechanism between load management on the consumer\\nside and dynamic price schemes by independent system operators. This work\\nprovides a novel methodology for modeling and simulating the nonlinear\\nrelationship between load management and real-time pricing. We then investigate\\nthe behavior of such a feedback loop under intentional cyber-attacks using our\\nfeedback model. We simulate and examine load-price data under different levels\\nof DSM participation with three types of additive attacks: ramp, sudden, and\\npoint attacks. We apply change point and supervised learning methods for\\ndetection of DSM attacks. Results conclude that while higher levels of DSM\\nparticipation can exacerbate attacks they also lead to better detection of such\\nattacks. Further analysis of results shows that point attacks are the hardest\\nto detect and supervised learning methods produce results on par or better than\\nsequential detectors.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12898</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12898</id><submitter>Mahsa Ghasemi</submitter><version version=\"v1\"><date>Fri, 27 Sep 2019 20:28:44 GMT</date><size>1348kb</size><source_type>D</source_type></version><title>Identifying Low-Dimensional Structures in Markov Chains: A Nonnegative\\n  Matrix Factorization Approach</title><authors>Mahsa Ghasemi, Abolfazl Hashemi, Haris Vikalo, Ufuk Topcu</authors><categories>cs.LG cs.SY eess.SY stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A variety of queries about stochastic systems boil down to study of Markov\\nchains and their properties. If the Markov chain is large, as is typically true\\nfor discretized continuous spaces, such analysis may be computationally\\nintractable. Nevertheless, in many scenarios, Markov chains have underlying\\nstructural properties that allow them to admit a low-dimensional\\nrepresentation. For instance, the transition matrix associated with the model\\nmay be low-rank and hence, representable in a lower-dimensional space. We\\nconsider the problem of learning low-dimensional representations for\\nlarge-scale Markov chains. To that end, we formulate the task of representation\\nlearning as that of mapping the state space of the model to a low-dimensional\\nstate space, referred to as the kernel space. The kernel space contains a set\\nof meta states which are desired to be representative of only a small subset of\\noriginal states. To promote this structural property, we constrain the number\\nof nonzero entries of the mappings between the state space and the kernel\\nspace. By imposing the desired characteristics of the structured\\nrepresentation, we cast the problem as the task of nonnegative matrix\\nfactorization. To compute the solution, we propose an efficient block\\ncoordinate gradient descent and theoretically analyze its convergence\\nproperties. Our extensive simulation results demonstrate the efficacy of the\\nproposed algorithm in terms of the quality of the low-dimensional\\nrepresentation as well as its computational cost.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12901</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12901</id><submitter>Feifan Wang</submitter><version version=\"v1\"><date>Sun, 15 Sep 2019 15:59:37 GMT</date><size>901kb</size><source_type>D</source_type></version><title>Brain-wise Tumor Segmentation and Patient Overall Survival Prediction</title><authors>Feifan Wang, Runzhou Jiang, Liqin Zheng, Bharat Biswal, and Chun Meng</authors><categories>eess.IV cs.CV</categories><comments>10 pages, 5 figures, 2 tables, pre-proceedings paper for Multimodal\\n  Brain Tumor Segmentation Challenge 2019 [BraTS\\n  2019](https://www.med.upenn.edu/cbica/brats2019.html)</comments><msc-class>68T45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Past few years have witnessed the prevalence of deep learning in many\\napplication scenarios, among which is medical image processing. Diagnosis and\\ntreatment of brain tumors require a delicate segmentation of brain tumors as a\\nprerequisite. However, such kind of work conventionally costs cerebral surgeons\\na lot of precious time. Computer vision techniques could provide surgeons a\\nrelief from the tedious marking procedure. In this paper, a 3D U-net based deep\\nlearning model has been trained with the help of brain-wise normalization and\\npatching strategies for the brain tumor segmentation task in BraTS 2019\\ncompetition. Dice coefficients for enhancing tumor, tumor core, and the whole\\ntumor are 0.737, 0.807 and 0.894 respectively on validation dataset.\\nFurthermore, numerical features extracted from predicted tumor labels have been\\nused for the overall survival days prediction task. The prediction accuracy on\\nvalidation dataset is 0.448.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12902</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12902</id><submitter>Denys Dutykh</submitter><version version=\"v1\"><date>Fri, 20 Sep 2019 07:48:26 GMT</date><size>7269kb</size><source_type>D</source_type></version><title>Interpreting Distortions in Dimensionality Reduction by Superimposing\\n  Neighbourhood Graphs</title><authors>Beno\\\\^it Colange and Laurent Vuillon and Sylvain Lespinats and Denys\\n  Dutykh</authors><categories>cs.CV cs.IR cs.LG</categories><comments>5 pages, 6 figures, 22 references. Paper presented at IEEE VIS 2019\\n  Conference. Other author\\'s papers can be downloaded at\\n  http://www.denys-dutykh.com/</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  To perform visual data exploration, many dimensionality reduction methods\\nhave been developed. These tools allow data analysts to represent\\nmultidimensional data in a 2D or 3D space, while preserving as much relevant\\ninformation as possible. Yet, they cannot preserve all structures\\nsimultaneously and they induce some unavoidable distortions. Hence, many\\ncriteria have been introduced to evaluate a map\\'s overall quality, mostly based\\non the preservation of neighbourhoods. Such global indicators are currently\\nused to compare several maps, which helps to choose the most appropriate\\nmapping method and its hyperparameters. However, those aggregated indicators\\ntend to hide the local repartition of distortions. Thereby, they need to be\\nsupplemented by local evaluation to ensure correct interpretation of maps. In\\nthis paper, we describe a new method, called MING, for `Map Interpretation\\nusing Neighbourhood Graphs\\'. It offers a graphical interpretation of pairs of\\nmap quality indicators, as well as local evaluation of the distortions. This is\\ndone by displaying on the map the nearest neighbours graphs computed in the\\ndata space and in the embedding. Shared and unshared edges exhibit reliable and\\nunreliable neighbourhood information conveyed by the mapping. By this mean,\\nanalysts may determine whether proximity (or remoteness) of points on the map\\nfaithfully represents similarity (or dissimilarity) of original data, within\\nthe meaning of a chosen map quality criteria. We apply this approach to two\\npairs of widespread indicators: precision/recall and\\ntrustworthiness/continuity, chosen for their wide use in the community, which\\nwill allow an easy handling by users.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12903</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12903</id><submitter>Shupeng Gui</submitter><version version=\"v1\"><date>Wed, 25 Sep 2019 18:35:03 GMT</date><size>455kb</size></version><title>PINE: Universal Deep Embedding for Graph Nodes via Partial Permutation\\n  Invariant Set Functions</title><authors>Shupeng Gui, Xiangliang Zhang, Pan Zhong, Shuang Qiu, Mingrui Wu,\\n  Jieping Ye, Zhengdao Wang, and Ji Liu</authors><categories>cs.LG stat.ML</categories><comments>24 pages, 4 figures, 3 tables. arXiv admin note: text overlap with\\n  arXiv:1805.11182</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph node embedding aims at learning a vector representation for all nodes\\ngiven a graph. It is a central problem in many machine learning tasks (e.g.,\\nnode classification, recommendation, community detection). The key problem in\\ngraph node embedding lies in how to define the dependence to neighbors.\\nExisting approaches specify (either explicitly or implicitly) certain\\ndependencies on neighbors, which may lead to loss of subtle but important\\nstructural information within the graph and other dependencies among neighbors.\\nThis intrigues us to ask the question: can we design a model to give the\\nmaximal flexibility of dependencies to each node\\'s neighborhood. In this paper,\\nwe propose a novel graph node embedding (named PINE) via a novel notion of\\npartial permutation invariant set function, to capture any possible dependence.\\nOur method 1) can learn an arbitrary form of the representation function from\\nthe neighborhood, withour losing any potential dependence structures, and 2) is\\napplicable to both homogeneous and heterogeneous graph embedding, the latter of\\nwhich is challenged by the diversity of node types. Furthermore, we provide\\ntheoretical guarantee for the representation capability of our method for\\ngeneral homogeneous and heterogeneous graphs. Empirical evaluation results on\\nbenchmark data sets show that our proposed PINE method outperforms the\\nstate-of-the-art approaches on producing node vectors for various learning\\ntasks of both homogeneous and heterogeneous graphs.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12905</identifier>\\n <datestamp>2019-10-02</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12905</id><submitter>Eric Clark</submitter><version version=\"v1\"><date>Fri, 20 Sep 2019 20:04:09 GMT</date><size>7291kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Tue, 1 Oct 2019 12:00:48 GMT</date><size>7291kb</size><source_type>D</source_type></version><title>Using Digital Field Experiments To Elicit Risk Mitigation Behavioral\\n  Strategies For Disease Management Across Agricultural Production Systems</title><authors>Eric M. Clark, Scott C. Merrill, Luke Trinity, Gabriela Bucini,\\n  Nicholas Cheney, Ollin Langle-Chimal, Trisha Shrum, Christopher Koliba, Asim\\n  Zia, and Julia M. Smith</authors><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Failing to mitigate propagation of disease spread can result in dire economic\\nconsequences for agricultural networks. Pathogens like Porcine Epidemic\\nDiarrhea virus, can quickly spread among producers. Biosecurity is designed to\\nprevent infection transmission. When considering biosecurity investments,\\nmanagement must balance the cost of protection versus the consequences of\\ncontracting an infection. Thus, an examination of the decision making processes\\nassociated with investment in biosecurity is important for enhancing system\\nwide biosecurity. Data gathered from digital field experiments can provide\\ninsights into behavioral strategies and inform the development of decision\\nsupport systems. We created an online digital experiment to simulate outbreak\\nscenarios among swine production supply chains, where participants were tasked\\nwith making biosecurity investment decisions. In Experiment One, we quantified\\nthe risk associated with each participant\\'s decisions and delineated three\\ndominant categories of risk attitudes: risk averse, risk tolerant, and\\nopportunistic. Each risk class exhibited unique approaches in reaction to risk\\nand disease information. We also tested how information uncertainty affects\\nrisk aversion, by varying the amount of visibility of the infection as well as\\nthe amount of biosecurity implemented across the system. We found evidence that\\nmore visibility in the number of infected sites increases risk averse\\nbehaviors, while more visibility in the amount of neighboring biosecurity\\nincreased risk taking behaviors. In Experiment Two, we were surprised to find\\nno evidence for differences in behavior of livestock specialists compared to\\nAmazon Mechanical Turk participants. Our findings provide support for using\\ndigital field experiments to study how risk communication affects behavior,\\nwhich can provide insights towards more effective messaging strategies.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12906</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12906</id><submitter>Karol Arndt</submitter><version version=\"v1\"><date>Mon, 16 Sep 2019 11:59:40 GMT</date><size>5583kb</size><source_type>D</source_type></version><title>Meta Reinforcement Learning for Sim-to-real Domain Adaptation</title><authors>Karol Arndt, Murtaza Hazara, Ali Ghadirzadeh, Ville Kyrki</authors><categories>cs.CV cs.RO</categories><comments>Submitted to ICRA 2020</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Modern reinforcement learning methods suffer from low sample efficiency and\\nunsafe exploration, making it infeasible to train robotic policies entirely on\\nreal hardware. In this work, we propose to address the problem of sim-to-real\\ndomain transfer by using meta learning to train a policy that can adapt to a\\nvariety of dynamic conditions, and using a task-specific trajectory generation\\nmodel to provide an action space that facilitates quick exploration. We\\nevaluate the method by performing domain adaptation in simulation and analyzing\\nthe structure of the latent space during adaptation. We then deploy this policy\\non a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a\\nhockey puck to a target. Our method shows more consistent and stable domain\\nadaptation than the baseline, resulting in better overall performance.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12907</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12907</id><submitter>Xiaoyang Guo</submitter><version version=\"v1\"><date>Mon, 30 Sep 2019 13:29:04 GMT</date><size>5862kb</size><source_type>D</source_type></version><title>A Quotient Space Formulation for Statistical Analysis of Graphical Data</title><authors>Xiaoyang Guo, Anuj Srivastava, Sudeep Sarkar</authors><categories>cs.CV cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex analyses involving multiple, dependent random quantities often lead\\nto graphical models: a set of nodes denoting variables of interest, and\\ncorresponding edges denoting statistical interactions between nodes. To develop\\nstatistical analyses for graphical data, one needs mathematical representations\\nand metrics for matching and comparing graphs, and other geometrical tools,\\nsuch as geodesics, means, and covariances, on representation spaces of graphs.\\nThis paper utilizes a quotient structure to develop efficient algorithms for\\ncomputing these quantities, leading to useful statistical tools, including\\nprincipal component analysis, linear dimension reduction, and analytical\\nstatistical modeling. The efficacy of this framework is demonstrated using\\ndatasets taken from several problem areas, including alphabets, video\\nsummaries, social networks, and biochemical structures.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12908</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12908</id><submitter>Jens Lundell</submitter><version version=\"v1\"><date>Sun, 15 Sep 2019 15:12:14 GMT</date><size>6628kb</size><source_type>D</source_type></version><title>Beyond Top-Grasps Through Scene Completion</title><authors>Jens Lundell, Francesco Verdoja, Ville Kyrki</authors><categories>cs.CV cs.RO</categories><comments>Submitted to IEEE Conference on Robotics and Automation 2020 (ICRA)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current end-to-end grasp planning methods propose grasps in the order of\\n(milli)seconds that attain high grasp success rates on a diverse set of\\nobjects, but often by constraining the workspace to top-grasps. In this work,\\nwe present a method that allows end-to-end top grasp planning methods to\\ngenerate full six-degree-of-freedom grasps using a single RGB-D view as input.\\nThis is achieved by estimating the complete shape of the object to be grasped,\\nthen simulating different viewpoints of the object, passing the simulated\\nviewpoints to an end-to-end grasp generation method, and finally executing the\\noverall best grasp. The method was experimentally validated on a Franka Emika\\nPanda by comparing 429 grasps generated by the state-of-the-art Fully\\nConvolutional Grasp Quality CNN, both on simulated and real camera viewpoints.\\nThe results show statistically significant improvements in terms of grasp\\nsuccess rate when using simulated viewpoints over real camera viewpoints,\\nespecially when the real camera viewpoint is angled.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12909</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12909</id><submitter>Yubao Liu</submitter><version version=\"v1\"><date>Thu, 19 Sep 2019 18:15:10 GMT</date><size>837kb</size></version><title>Deeply Matting-based Dual Generative Adversarial Network for Image and\\n  Document Label Supervision</title><authors>Yubao Liu, Kai Lin</authors><categories>cs.CV cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although many methods have been proposed to deal with nature image\\nsuper-resolution (SR) and get impressive performance, the text images SR is not\\ngood due to their ignorance of document images. In this paper, we propose a\\nmatting-based dual generative adversarial network (mdGAN) for document image\\nSR. Firstly, the input image is decomposed into document text, foreground and\\nbackground layers using deep image matting. Then two parallel branches are\\nconstructed to recover text boundary information and color information\\nrespectively. Furthermore, in order to improve the restoration accuracy of\\ncharacters in output image, we use the input image\\'s corresponding ground truth\\ntext label as extra supervise information to refine the two-branch networks\\nduring training. Experiments on real text images demonstrate that our method\\noutperforms several state-of-the-art methods quantitatively and qualitatively.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12911</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12911</id><submitter>Luisa Polania</submitter><version version=\"v1\"><date>Thu, 19 Sep 2019 00:22:36 GMT</date><size>5067kb</size><source_type>D</source_type></version><title>Graph Neural Networks for Image Understanding Based on Multiple Cues:\\n  Group Emotion Recognition and Event Recognition as Use Cases</title><authors>Xin Guo, Luisa F. Polania, Bin Zhu, Charles Boncelet, Kenneth E.\\n  Barner</authors><categories>cs.CV</categories><comments>Paper accepted for publication at the 2020 IEEE Winter Conference on\\n  Applications of Computer Vision (WACV)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph neural network (GNN) for image understanding based on multiple cues\\nis proposed in this paper. Compared to traditional feature and decision fusion\\napproaches that neglect the fact that features can interact and exchange\\ninformation, the proposed GNN is able to pass information among features\\nextracted from different models. Two image understanding tasks, namely\\ngroup-level emotion recognition (GER) and event recognition, which are highly\\nsemantic and require the interaction of several deep models to synthesize\\nmultiple cues, were selected to validate the performance of the proposed\\nmethod. It is shown through experiments that the proposed method achieves\\nstate-of-the-art performance on the selected image understanding tasks. In\\naddition, a new group-level emotion recognition database is introduced and\\nshared in this paper.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12912</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12912</id><submitter>Andre Pacheco</submitter><version version=\"v1\"><date>Mon, 16 Sep 2019 14:27:12 GMT</date><size>5348kb</size><source_type>D</source_type></version><title>The impact of patient clinical information on automated skin cancer\\n  detection</title><authors>Andre G. C. Pacheco and Renato A. Krohling</authors><categories>eess.IV cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Skin cancer is one of the most common types of cancer around the world. For\\nthis reason, over the past years, different approaches have been proposed to\\nassist detect it. Nonetheless, most of them are based only on dermoscopy images\\nand do not take into account the patient clinical information. In this work,\\nfirst, we present a new dataset that contains clinical images, acquired from\\nsmartphones, and patient clinical information of the skin lesions. Next, we\\nintroduce a straightforward approach to combine the clinical data and the\\nimages using different well-known deep learning models. These models are\\napplied to the presented dataset using only the images and combining them with\\nthe patient clinical information. We present a comprehensive study to show the\\nimpact of the clinical data on the final predictions. The results obtained by\\ncombining both sets of information show a general improvement of around 7% in\\nthe balanced accuracy for all models. In addition, the statistical test\\nindicates significant differences between the models with and without\\nconsidering both data. The improvement achieved shows the potential of using\\npatient clinical information in skin cancer detection and indicates that this\\npiece of information is important to leverage skin cancer detection systems.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12913</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12913</id><submitter>Prabin Sharma</submitter><version version=\"v1\"><date>Wed, 18 Sep 2019 15:46:48 GMT</date><size>865kb</size></version><title>Student Engagement Detection Using Emotion Analysis, Eye Tracking and\\n  Head Movement with Machine Learning</title><authors>Prabin Sharma, Shubham Joshi, Subash Gautam, Vitor Filipe, Manuel J.\\n  C. S. Reis</authors><categories>cs.CV cs.CY cs.LG</categories><comments>8 pages, 9 Figures, 2 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  With the increase of distance learning, in general, and e-learning, in\\nparticular, having a system capable of determining the engagement of students\\nis of primordial importance, and one of the biggest challenges, both for\\nteachers, researchers and policy makers. Here, we present a system to detect\\nthe engagement level of the students. It uses only information provided by the\\ntypical built-in web-camera present in a laptop computer, and was designed to\\nwork in real time. We combine information about the movements of the eyes and\\nhead, and facial emotions to produce a concentration index with three classes\\nof engagement: &quot;very engaged&quot;, &quot;nominally engaged&quot; and &quot;not engaged at all&quot;.\\nThe system was tested in a typical e-learning scenario, and the results show\\nthat it correctly identifies each period of time where students were &quot;very\\nengaged&quot;, &quot;nominally engaged&quot; and &quot;not engaged at all&quot;. Additionally, the\\nresults also show that the students with best scores also have higher\\nconcentration indexes.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12914</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12914</id><submitter>David Isele</submitter><version version=\"v1\"><date>Fri, 27 Sep 2019 20:44:41 GMT</date><size>1658kb</size><source_type>D</source_type></version><title>Interactive Decision Making for Autonomous Vehicles in Dense Traffic</title><authors>David Isele</authors><categories>cs.AI cs.RO</categories><journal-ref>ITSC 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dense urban traffic environments can produce situations where accurate\\nprediction and dynamic models are insufficient for successful autonomous\\nvehicle motion planning. We investigate how an autonomous agent can safely\\nnegotiate with other traffic participants, enabling the agent to handle\\npotential deadlocks. Specifically we consider merges where the gap between cars\\nis smaller than the size of the ego vehicle. We propose a game theoretic\\nframework capable of generating and responding to interactive behaviors. Our\\nmain contribution is to show how game-tree decision making can be executed by\\nan autonomous vehicle, including approximations and reasoning that make the\\ntree-search computationally tractable. Additionally, to test our model we\\ndevelop a stochastic rule-based traffic agent capable of generating interactive\\nbehaviors that can be used as a benchmark for simulating traffic participants\\nin a crowded merge setting.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12916</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12916</id><submitter>Lucas Pascal</submitter><version version=\"v1\"><date>Fri, 13 Sep 2019 14:49:38 GMT</date><size>2829kb</size><source_type>D</source_type></version><title>Semantic and Visual Similarities for Efficient Knowledge Transfer in CNN\\n  Training</title><authors>Lucas Pascal, Xavier Bost (LIA), Beno\\\\^it Huet</authors><categories>cs.CV cs.LG cs.MM</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, representation learning approaches have disrupted many\\nmultimedia computing tasks. Among those approaches, deep convolutional neural\\nnetworks (CNNs) have notably reached human level expertise on some constrained\\nimage classification tasks. Nonetheless, training CNNs from scratch for new\\ntask or simply new data turns out to be complex and time-consuming. Recently,\\ntransfer learning has emerged as an effective methodology for adapting\\npre-trained CNNs to new data and classes, by only retraining the last\\nclassification layer. This paper focuses on improving this process, in order to\\nbetter transfer knowledge between CNN architectures for faster trainings in the\\ncase of fine tuning for image classification. This is achieved by combining and\\ntransfering supplementary weights, based on similarity considerations between\\nsource and target classes. The study includes a comparison between semantic and\\ncontent-based similarities, and highlights increased initial performances and\\ntraining speed, along with superior long term performances when limited\\ntraining samples are available.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12917</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12917</id><submitter>Mansaf Alam Dr</submitter><version version=\"v1\"><date>Fri, 20 Sep 2019 10:16:34 GMT</date><size>538kb</size></version><title>A Lightweight Deep Learning Model for Human Activity Recognition on Edge\\n  Devices</title><authors>Preeti Agarwal and Mansaf Alam</authors><categories>eess.SP cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human Activity Recognition (HAR) using wearable and mobile sensors has gained\\nmomentum in last few years, in various fields, such as, healthcare,\\nsurveillance, education, entertainment. Nowadays, Edge Computing has emerged to\\nreduce communication latency and network traffic.Edge devices are resource\\nconstrained devices and cannot support high computation. In literature, various\\nmodels have been developed for HAR. In recent years, deep learning algorithms\\nhave shown high performance in HAR, but these algorithms require lot of\\ncomputation making them inefficient to be deployed on edge devices. This paper,\\nproposes a Lightweight Deep Learning Model for HAR requiring less computational\\npower, making it suitable to be deployed on edge devices. The performance of\\nproposed model is tested on the participants six daily activities data. Results\\nshow that the proposed model outperforms many of the existing machine learning\\nand deep learning techniques.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12919</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12919</id><submitter>Madhura Ingalhalikar</submitter><version version=\"v1\"><date>Mon, 23 Sep 2019 13:47:12 GMT</date><size>3032kb</size></version><title>HR-CAM: Precise Localization of Pathology Using Multi-level Learning in\\n  CNNs</title><authors>Sumeet Shinde, Tanay Chougule, Jitender Saini and Madhura Ingalhalikar</authors><categories>cs.CV</categories><comments>Medical Image Computing and Computer Assisted Intervention, 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a CNN based technique that aggregates feature maps from its\\nmultiple layers that can localize abnormalities with greater details as well as\\npredict pathology under consideration. Existing class activation mapping (CAM)\\ntechniques extract feature maps from either the final layer or a single\\nintermediate layer to create the discriminative maps and then interpolate to\\nupsample to the original image resolution. In this case, the subject specific\\nlocalization is coarse and is unable to capture subtle abnormalities. To\\nmitigate this, our method builds a novel CNN based discriminative localization\\nmodel that we call high resolution CAM (HR-CAM), which accounts for layers from\\neach resolution, therefore facilitating a comprehensive map that can delineate\\nthe pathology for each subject by combining low-level, intermediate as well as\\nhigh-level features from the CNN. Moreover, our model directly provides the\\ndiscriminative map in the resolution of the original image facilitating finer\\ndelineation of abnormalities. We demonstrate the working of our model on a\\nsimulated abnormalities data where we illustrate how the model captures finer\\ndetails in the final discriminative maps as compared to current techniques. We\\nthen apply this technique: (1) to classify ependymomas from grade IV\\nglioblastoma on T1-weighted contrast enhanced (T1-CE) MRI and (2) to predict\\nParkinson\\'s disease from neuromelanin sensitive MRI. In all these cases we\\ndemonstrate that our model not only predicts pathologies with high accuracies,\\nbut also creates clinically interpretable subject specific high resolution\\ndiscriminative localizations. Overall, the technique can be generalized to any\\nCNN and carries high relevance in a clinical setting.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12921</identifier>\\n <datestamp>2019-10-08</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12921</id><submitter>Benjamin Renoust</submitter><version version=\"v1\"><date>Tue, 17 Sep 2019 06:22:32 GMT</date><size>4797kb</size><source_type>D</source_type></version><version version=\"v2\"><date>Sun, 6 Oct 2019 14:07:58 GMT</date><size>4795kb</size><source_type>D</source_type></version><title>Historical and Modern Features for Buddha Statue Classification</title><authors>Benjamin Renoust, Matheus Oliveira Franca, Jacob Chan, Noa Garcia, Van\\n  Le, Ayaka Uesaka, Yuta Nakashima, Hajime Nagahara, Jueren Wang, Yutaka\\n  Fujioka</authors><categories>cs.CV cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While Buddhism has spread along the Silk Roads, many pieces of art have been\\ndisplaced. Only a few experts may identify these works, subjectively to their\\nexperience. The construction of Buddha statues was taught through the\\ndefinition of canon rules, but the applications of those rules greatly varies\\nacross time and space. Automatic art analysis aims at supporting these\\nchallenges. We propose to automatically recover the proportions induced by the\\nconstruction guidelines, in order to use them and compare between different\\ndeep learning features for several classification tasks, in a medium size but\\nrich dataset of Buddha statues, collected with experts of Buddhism art history.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12922</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12922</id><submitter>Han Li</submitter><version version=\"v1\"><date>Mon, 16 Sep 2019 09:47:47 GMT</date><size>977kb</size><source_type>D</source_type></version><title>Encoding CT Anatomy Knowledge for Unpaired Chest X-ray Image\\n  Decomposition</title><authors>Zeju Li, Han Li, Hu Han, Gonglei Shi, Jiannan Wang, and S. Kevin Zhou</authors><categories>eess.IV cs.CV</categories><comments>9 pages with 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although chest X-ray (CXR) offers a 2D projection with overlapped anatomies,\\nit is widely used for clinical diagnosis. There is clinical evidence supporting\\nthat decomposing an X-ray image into different components (e.g., bone, lung and\\nsoft tissue) improves diagnostic value. We hereby propose a decomposition\\ngenerative adversarial network (DecGAN) to anatomically decompose a CXR image\\nbut with unpaired data. We leverage the anatomy knowledge embedded in CT, which\\nfeatures a 3D volume with clearly visible anatomies. Our key idea is to embed\\nCT priori decomposition knowledge into the latent space of unpaired CXR\\nautoencoder. Specifically, we train DecGAN with a decomposition loss,\\nadversarial losses, cycle-consistency losses and a mask loss to guarantee that\\nthe decomposed results of the latent space preserve realistic body structures.\\nExtensive experiments demonstrate that DecGAN provides superior unsupervised\\nCXR bone suppression results and the feasibility of modulating CXR components\\nby latent space disentanglement. Furthermore, we illustrate the diagnostic\\nvalue of DecGAN and demonstrate that it outperforms the state-of-the-art\\napproaches in terms of predicting 11 out of 14 common lung diseases.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12923</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12923</id><submitter>Iv\\\\\\'an L\\\\\\'opez-Espejo</submitter><version version=\"v1\"><date>Sun, 15 Sep 2019 15:48:29 GMT</date><size>152kb</size><source_type>D</source_type></version><title>End-to-End Deep Residual Learning with Dilated Convolutions for\\n  Myocardial Infarction Detection and Localization</title><authors>Iv\\\\\\'an L\\\\\\'opez-Espejo</authors><categories>eess.IV cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, I investigate the use of end-to-end deep residual learning\\nwith dilated convolutions for myocardial infarction (MI) detection and\\nlocalization from electrocardiogram (ECG) signals. Although deep residual\\nlearning has already been applied to MI detection and localization, I propose a\\nmore accurate system that distinguishes among a higher number (i.e., six) of MI\\nlocations. Inspired by speech waveform processing with neural networks, I found\\na more robust front-end than directly arranging the multi-lead ECG signal into\\nan input matrix consisting of the use of a single one-dimensional convolutional\\nlayer per ECG lead to extract a pseudo-time-frequency representation and create\\na compact and discriminative input feature volume. As a result, I end up with a\\nsystem achieving an MI detection and localization accuracy of 99.99% on the\\nwell-known Physikalisch-Technische Bundesanstalt (PTB) database.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12925</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12925</id><submitter>David Isele</submitter><version version=\"v1\"><date>Fri, 27 Sep 2019 20:49:05 GMT</date><size>1774kb</size><source_type>D</source_type></version><title>Interaction-Aware Multi-Agent Reinforcement Learning for Mobile Agents\\n  with Individual Goals</title><authors>Anahita Mohseni-Kabir, David Isele, and Kikuo Fujimura</authors><categories>cs.AI cs.LG</categories><journal-ref>ICRA 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a multi-agent setting, the optimal policy of a single agent is largely\\ndependent on the behavior of other agents. We investigate the problem of\\nmulti-agent reinforcement learning, focusing on decentralized learning in\\nnon-stationary domains for mobile robot navigation. We identify a cause for the\\ndifficulty in training non-stationary policies: mutual adaptation to\\nsub-optimal behaviors, and we use this to motivate a curriculum-based strategy\\nfor learning interactive policies. The curriculum has two stages. First, the\\nagent leverages policy gradient algorithms to learn a policy that is capable of\\nachieving multiple goals. Second, the agent learns a modifier policy to learn\\nhow to interact with other agents in a multi-agent setting. We evaluated our\\napproach on both an autonomous driving lane-change domain and a robot\\nnavigation domain.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12926</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12926</id><submitter>Dave Cliff</submitter><version version=\"v1\"><date>Wed, 18 Sep 2019 07:20:49 GMT</date><size>1240kb</size></version><title>A Cloud-Native Globally Distributed Financial Exchange Simulator for\\n  Studying Real-World Trading-Latency Issues at Planetary Scale</title><authors>Bradley Miles and Dave Cliff</authors><categories>cs.CY q-fin.TR</categories><comments>10 pages, 5 figures. To be presented at the European Modelling and\\n  Simulation Symposium (EMSS2019) Lisbon, Portugal, 18th-20th September 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a new public-domain open-source simulator of an electronic\\nfinancial exchange, and of the traders that interact with the exchange, which\\nis a truly distributed and cloud-native system that been designed to run on\\nwidely available commercial cloud-computing services, and in which various\\ncomponents can be placed in specified geographic regions around the world,\\nthereby enabling the study of planetary-scale latencies in contemporary\\nautomated trading systems. Our simulator allows an exchange server to be\\nlaunched in the cloud, specifying a particular geographic zone for the cloud\\nhosting service; automated-trading clients which attach to the exchange can\\nthen also be launched in the cloud, in the same geographic zone and/or in\\ndifferent zones anywhere else on the planet, and those clients are then subject\\nto the real-world latencies introduced by planetary-scale cloud communication\\ninterconnections. In this paper we describe the design and implementation of\\nour simulator, called DBSE, which is based on a previous public-domain\\nsimulator, extended in ways that are partly inspired by the architecture of the\\nreal-world Jane Street Exchange. DBSE relies fundamentally on UDP and TCP\\nnetwork communications protocols and implements a subset of the FIX de facto\\nstandard protocol for financial information exchange. We show results from an\\nexample in which the exchange server is remotely launched on a cloud facility\\nlocated in London (UK), with trader clients running in Ohio (USA) and Sydney\\n(Australia). We close with discussion of how our simulator could be further\\nused to study planetary-scale latency arbitrage in financial markets.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12927</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12927</id><submitter>Basemah Alshemali</submitter><version version=\"v1\"><date>Thu, 19 Sep 2019 21:08:13 GMT</date><size>26kb</size></version><title>Toward Robust Image Classification</title><authors>Basemah Alshemali, Alta Graham, Jugal Kalita</authors><categories>cs.CV cs.LG eess.IV</categories><comments>2019 Intelligent Systems Conference, pp 483-489</comments><journal-ref>Intelligent Systems and Applications. IntelliSys 2019. Advances in\\n  Intelligent Systems and Computing, vol 1038. Springer, Cham</journal-ref><doi>10.1007/978-3-030-29513-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural networks are frequently used for image classification, but can be\\nvulnerable to misclassification caused by adversarial images. Attempts to make\\nneural network image classification more robust have included variations on\\npreprocessing (cropping, applying noise, blurring), adversarial training, and\\ndropout randomization. In this paper, we implemented a model for adversarial\\ndetection based on a combination of two of these techniques: dropout\\nrandomization with preprocessing applied to images within a given Bayesian\\nuncertainty. We evaluated our model on the MNIST dataset, using adversarial\\nimages generated using Fast Gradient Sign Method (FGSM), Jacobian-based\\nSaliency Map Attack (JSMA) and Basic Iterative Method (BIM) attacks. Our model\\nachieved an average adversarial image detection accuracy of 97%, with an\\naverage image classification accuracy, after discarding images flagged as\\nadversarial, of 99%. Our average detection accuracy exceeded that of recent\\npapers using similar techniques.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12928</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12928</id><submitter>Ivan P Yamshchikov</submitter><version version=\"v1\"><date>Thu, 26 Sep 2019 11:34:04 GMT</date><size>600kb</size><source_type>D</source_type></version><title>Decomposing Textual Information For Style Transfer</title><authors>Ivan P. Yamshchikov, Viacheslav Shibaev, Aleksander Nagaev, J\\\\&quot;urgen\\n  Jost and Alexey Tikhonov</authors><categories>cs.CL</categories><comments>arXiv admin note: substantial text overlap with arXiv:1908.06809</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on latent representations that could effectively decompose\\ndifferent aspects of textual information. Using a framework of style transfer\\nfor texts, we propose several empirical methods to assess information\\ndecomposition quality. We validate these methods with several state-of-the-art\\ntextual style transfer methods. Higher quality of information decomposition\\ncorresponds to higher performance in terms of bilingual evaluation understudy\\n(BLEU) between output and human-written reformulations.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12929</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12929</id><submitter>Yumeng Zhang</submitter><version version=\"v1\"><date>Mon, 16 Sep 2019 16:03:08 GMT</date><size>598kb</size><source_type>D</source_type></version><title>Self-Paced Video Data Augmentation with Dynamic Images Generated by\\n  Generative Adversarial Networks</title><authors>Yumeng Zhang, Gaoguo Jia, Li Chen, Mingrui Zhang, Junhai Yong</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is an urgent need for an effective video classification method by means\\nof a small number of samples. The deficiency of samples could be effectively\\nalleviated by generating samples through Generative Adversarial Networks (GAN),\\nbut the generation of videos on a typical category remains to be underexplored\\nsince the complex actions and the changeable viewpoints are difficult to\\nsimulate. In this paper, we propose a generative data augmentation method for\\ntemporal stream of the Temporal Segment Networks with the dynamic image. The\\ndynamic image compresses the motion information of video into a still image,\\nremoving the interference factors such as the background. Thus it is easier to\\ngenerate images with categorical motion information using GAN. We use the\\ngenerated dynamic images to enhance the features, with regularization achieved\\nas well, thereby to achieve the effect of video augmentation. In order to deal\\nwith the uneven quality of generated images, we propose a Self-Paced Selection\\n(SPS) method, which automatically selects the high-quality generated samples to\\nbe added to the network training. Our method is verified on two benchmark\\ndatasets, HMDB51 and UCF101. The experimental results show that the method can\\nimprove the accuracy of video classification under the circumstance of sample\\ninsufficiency and sample imbalance.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12930</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12930</id><submitter>Eric Ambrose</submitter><version version=\"v1\"><date>Fri, 27 Sep 2019 20:52:12 GMT</date><size>6647kb</size><source_type>D</source_type></version><title>Improved Performance on Moving-Mass Hopping Robots with Parallel\\n  Elasticity</title><authors>Eric Ambrose and Aaron D. Ames</authors><categories>cs.RO</categories><comments>7 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robotic Hopping is challenging from the perspective of both modeling the\\ndynamics as well as the mechanical design due to the short period of ground\\ncontact in which to actuate on the world. Previous work has demonstrated stable\\nhopping on a moving-mass robot, wherein a single spring was utilized below the\\nbody of the robot. This paper finds that the addition of a spring in parallel\\nto the actuator greatly improves the performance of moving mass hopping robots.\\nThis is demonstrated through the design of a novel one-dimensional hopping\\nrobot. For this robot, a rigorous trajectory optimization method is developed\\nusing hybrid systems models with experimentally tuned parameters. Simulation\\nresults are used to study the effects of a parallel spring on energetic\\nefficiency, stability and hopping effort. We find that the double-spring model\\nhad 2.5x better energy efficiency than the single-spring model, and was able to\\nhop using 40% less peak force from the actuator. Furthermore, the double-spring\\nmodel produces stable hopping without the need for stabilizing controllers.\\nThese concepts are demonstrated experimentally on a novel hopping robot,\\nwherein hop heights up to 40cm were achieved.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12931</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12931</id><submitter>L\\\\\\'aszl\\\\\\'o Csat\\\\\\'o</submitter><version version=\"v1\"><date>Wed, 25 Sep 2019 17:05:32 GMT</date><size>95kb</size><source_type>D</source_type></version><title>A revenue allocation scheme based on pairwise comparisons</title><authors>D\\\\\\'ora Gr\\\\\\'eta Petr\\\\\\'oczy, L\\\\\\'aszl\\\\\\'o Csat\\\\\\'o</authors><categories>econ.GN cs.AI q-fin.EC</categories><comments>14 pages, 3 figures, 2 tables</comments><msc-class>62F07, 90B50, 91B08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A model of sharing revenues among groups when group members are ranked\\nseveral times is presented. The methodology is based on pairwise comparison\\nmatrices, allows for the use of any weighting method, and makes possible to\\ntune the level of inequality. Our proposal is demonstrated on the example of\\nFormula One prize money allocation among the constructors. We introduce an\\naxiom called scale invariance, which requires the ranking of teams to be\\nindependent of the parameter controlling inequality. The eigenvector method is\\nrevealed to violate this condition in our dataset, while the row geometric mean\\nmethod always satisfies it. The revenue allocation is not influenced by the\\narbitrary valuation given to the race prizes in the official points scoring\\nsystem of Formula One.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12932</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12932</id><submitter>Benjamin Renoust</submitter><version version=\"v1\"><date>Tue, 17 Sep 2019 06:35:24 GMT</date><size>2227kb</size><source_type>D</source_type></version><title>BUDA.ART: A Multimodal Content-Based Analysis and Retrieval System for\\n  Buddha Statues</title><authors>Benjamin Renoust, Matheus Oliveira Franca, Jacob Chan, Van Le, Ayaka\\n  Uesaka, Yuta Nakashima, Hajime Nagahara, Jueren Wang, Yutaka Fujioka</authors><categories>cs.CV cs.HC cs.IR cs.MM</categories><comments>Demo video at: https://www.youtube.com/watch?v=3XJvLjSWieY</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce BUDA.ART, a system designed to assist researchers in Art\\nHistory, to explore and analyze an archive of pictures of Buddha statues. The\\nsystem combines different CBIR and classical retrieval techniques to assemble\\n2D pictures, 3D statue scans and meta-data, that is focused on the Buddha\\nfacial characteristics. We build the system from an archive of 50,000 Buddhism\\npictures, identify unique Buddha statues, extract contextual information, and\\nprovide specific facial embedding to first index the archive. The system allows\\nfor mobile, on-site search, and to explore similarities of statues in the\\narchive. In addition, we provide search visualization and 3D analysis of the\\nstatues\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12933</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12933</id><submitter>Hamid Tizhoosh</submitter><version version=\"v1\"><date>Sun, 15 Sep 2019 01:13:41 GMT</date><size>1078kb</size><source_type>D</source_type></version><title>Subtractive Perceptrons for Learning Images: A Preliminary Report</title><authors>H.R.Tizhoosh, Shivam Kalra, Shalev Lifshitz, Morteza Babaie</authors><categories>cs.CV</categories><comments>To appear in the 9th Intern. Conf. on Image Processing Theory, Tools\\n  and Applications (IPTA 2019), Istanbul, Turkey</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, artificial neural networks have achieved tremendous success\\nfor many vision-based tasks. However, this success remains within the paradigm\\nof \\\\emph{weak AI} where networks, among others, are specialized for just one\\ngiven task. The path toward \\\\emph{strong AI}, or Artificial General\\nIntelligence, remains rather obscure. One factor, however, is clear, namely\\nthat the feed-forward structure of current networks is not a realistic\\nabstraction of the human brain. In this preliminary work, some ideas are\\nproposed to define a \\\\textit{subtractive Perceptron} (s-Perceptron), a\\ngraph-based neural network that delivers a more compact topology to learn one\\nspecific task. In this preliminary study, we test the s-Perceptron with the\\nMNIST dataset, a commonly used image archive for digit recognition. The\\nproposed network achieves excellent results compared to the benchmark networks\\nthat rely on more complex topologies.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12935</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12935</id><submitter>Yi Zeng</submitter><version version=\"v1\"><date>Thu, 19 Sep 2019 15:27:06 GMT</date><size>174kb</size></version><title>Responsible Facial Recognition and Beyond</title><authors>Yi Zeng, Enmeng Lu, Yinqian Sun, Ruochen Tian</authors><categories>cs.CV cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Facial recognition is changing the way we live in and interact with our\\nsociety. Here we discuss the two sides of facial recognition, summarizing\\npotential risks and current concerns. We introduce current policies and\\nregulations in different countries. Very importantly, we point out that the\\nrisks and concerns are not only from facial recognition, but also realistically\\nvery similar to other biometric recognition technology, including but not\\nlimited to gait recognition, iris recognition, fingerprint recognition, voice\\nrecognition, etc. To create a responsible future, we discuss possible\\ntechnological moves and efforts that should be made to keep facial recognition\\n(and biometric recognition in general) developing for social good.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12936</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12936</id><submitter>Yi Cheng</submitter><version version=\"v1\"><date>Tue, 24 Sep 2019 04:12:50 GMT</date><size>1748kb</size><source_type>D</source_type></version><title>6D Pose Estimation with Correlation Fusion</title><authors>Yi Cheng, Hongyuan Zhu, Cihan Acar, Wei Jing, Yan Wu, Liyuan Li,\\n  Cheston Tan, Joo-Hwee Lim</authors><categories>cs.CV cs.RO</categories><comments>Submitted to conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  6D object pose estimation is widely applied in robotic tasks such as grasping\\nand manipulation. Prior methods using RGB-only images are vulnerable to heavy\\nocclusion and poor illumination, so it is important to complement them with\\ndepth information. However, existing methods using RGB-D data don\\'t adequately\\nexploit consistent and complementary information between two modalities. In\\nthis paper, we present a novel method to effectively consider the correlation\\nwithin and across RGB and depth modalities with attention mechanism to learn\\ndiscriminative multi-modal features. Then, effective fusion strategies for\\nintra- and inter-correlation modules are explored to ensure efficient\\ninformation flow between RGB and depth. To the best of our knowledge, this is\\nthe first work to explore effective intra- and inter-modality fusion in 6D pose\\nestimation and experimental results show that our method can help achieve the\\nstate-of-the-art performance on LineMOD and YCB-Video datasets as well as\\nbenefit robot grasping task.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12937</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12937</id><submitter>Manel Mart\\\\\\'inez-Ram\\\\\\'on</submitter><version version=\"v1\"><date>Wed, 18 Sep 2019 17:19:17 GMT</date><size>2423kb</size><source_type>D</source_type></version><title>Unsupervised Segmentation of Fire and Smoke from Infra-Red Videos</title><authors>Meenu Ajith and Manel Mart\\\\\\'inez-Ram\\\\\\'on</authors><categories>cs.CV cs.LG eess.IV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a vision-based fire and smoke segmentation system which\\nuse spatial, temporal and motion information to extract the desired regions\\nfrom the video frames. The fusion of information is done using multiple\\nfeatures such as optical flow, divergence and intensity values. These features\\nextracted from the images are used to segment the pixels into different classes\\nin an unsupervised way. A comparative analysis is done by using multiple\\nclustering algorithms for segmentation. Here the Markov Random Field performs\\nmore accurately than other segmentation algorithms since it characterizes the\\nspatial interactions of pixels using a finite number of parameters. It builds a\\nprobabilistic image model that selects the most likely labeling using the\\nmaximum a posteriori (MAP) estimation. This unsupervised approach is tested on\\nvarious images and achieves a frame-wise fire detection rate of 95.39%. Hence\\nthis method can be used for early detection of fire in real-time and it can be\\nincorporated into an indoor or outdoor surveillance system.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12938</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12938</id><submitter>Akhil Gupta</submitter><version version=\"v1\"><date>Thu, 19 Sep 2019 20:20:04 GMT</date><size>519kb</size></version><title>Time Series Modeling for Dream Team in Fantasy Premier League</title><authors>Akhil Gupta</authors><categories>cs.CY cs.LG</categories><comments>International Conference on Sports Engineering (ICSE\\'17)</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The performance of football players in English Premier League varies largely\\nfrom season to season and for different teams. It is evident that a method\\ncapable of forecasting and analyzing the future of these players on-field\\nantics shall assist the management to a great extent. In a simulated\\nenvironment like the Fantasy Premier League, enthusiasts from all over the\\nworld participate and manage the players catalogue for the entire season. Due\\nto the dynamic nature of points system, there is no known approach for the\\nformulation of a dream team. This study aims to tackle this problem by using a\\nhybrid of Autoregressive Integrated Moving Average (ARIMA) and Recurrent Neural\\nNetworks (RNNs) for time series prediction of player points and subsequent\\nmaximization of total points using Linear Programming (LPP). Given the player\\npoints for the past three seasons, the predictions have been made for the\\ncurrent season by modeling differently for ARIMA and RNN, and then creating an\\nensemble of the same. Prior to that, proper data preprocessing techniques were\\ndeployed to enhance the efficacy of the prepared model. Constraints on the type\\nof players like goalkeepers, defenders, midfielders and forwards along with the\\ntotal budget were effectively optimized using LPP approach. The validation of\\nthe proposed team was done with the performance in upcoming season, where the\\nplayers outperform as expected, and helped in strengthening the feasibility of\\nthe solution. Likewise, the proposed approach can be extended to English\\nPremier League by official managers on-field.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12939</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12939</id><submitter>Xiaonan Zhao</submitter><version version=\"v1\"><date>Fri, 27 Sep 2019 20:54:42 GMT</date><size>1026kb</size><source_type>D</source_type></version><title>A weakly supervised adaptive triplet loss for deep metric learning</title><authors>Xiaonan Zhao, Huan Qi, Rui Luo and Larry Davis</authors><categories>cs.CV</categories><comments>4 pages, ICCV Fashion Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of distance metric learning in visual similarity\\nsearch, defined as learning an image embedding model which projects images into\\nEuclidean space where semantically and visually similar images are closer and\\ndissimilar images are further from one another. We present a weakly supervised\\nadaptive triplet loss (ATL) capable of capturing fine-grained semantic\\nsimilarity that encourages the learned image embedding models to generalize\\nwell on cross-domain data. The method uses weakly labeled product description\\ndata to implicitly determine fine grained semantic classes, avoiding the need\\nto annotate large amounts of training data. We evaluate on the Amazon fashion\\nretrieval benchmark and DeepFashion in-shop retrieval data. The method boosts\\nthe performance of triplet loss baseline by 10.6% on cross-domain data and\\nout-performs the state-of-art model on all evaluation metrics.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12940</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12940</id><submitter>Ashiqur KhudaBukhsh Ashiqur Rahman KhudaBukhsh</submitter><version version=\"v1\"><date>Wed, 11 Sep 2019 04:22:20 GMT</date><size>302kb</size></version><title>Kashmir: A Computational Analysis of the Voice of Peace</title><authors>Shriphani Palakodety, Ashiqur R. KhudaBukhsh, Jaime G. Carbonell</authors><categories>cs.CY cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent Pulwama terror attack (February 14, 2019, Pulwama, Kashmir)\\ntriggered a chain of escalating events between India and Pakistan adding\\nanother episode to their 70-year-old dispute over Kashmir. The present era of\\nubiquitious social media has never seen nuclear powers closer to war. In this\\npaper, we analyze this evolving international crisis via a substantial corpus\\nconstructed using comments on YouTube videos (921,235 English comments posted\\nby 392,460 users out of 2.04 million overall comments by 791,289 users on 2,890\\nvideos). Our main contributions in the paper are three-fold. First, we present\\nan observation that polyglot word-embeddings reveal precise and accurate\\nlanguage clusters, and subsequently construct a document\\nlanguage-identification technique with negligible annotation requirements. We\\ndemonstrate the viability and utility across a variety of data sets involving\\nseveral low-resource languages. Second, we present an extensive analysis on\\ntemporal trends of pro-peace and pro-war intent through a manually constructed\\npolarity phrase lexicon. We observe that when tensions between the two nations\\nwere at their peak, pro-peace intent in the corpus was at its highest point.\\nFinally, in the context of heated discussions in a politically tense situation\\nwhere two nations are at the brink of a full-fledged war, we argue the\\nimportance of automatic identification of user-generated web content that can\\ndiffuse hostility and address this prediction task, dubbed \\\\emph{hope-speech\\ndetection}.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12942</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12942</id><submitter>Zheyu Yang</submitter><version version=\"v1\"><date>Sun, 15 Sep 2019 14:59:53 GMT</date><size>1817kb</size><source_type>D</source_type></version><title>DashNet: A Hybrid Artificial and Spiking Neural Network for High-speed\\n  Object Tracking</title><authors>Zheyu Yang, Yujie Wu, Guanrui Wang, Yukuan Yang, Guoqi Li, Lei Deng,\\n  Jun Zhu, Luping Shi</authors><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer-science-oriented artificial neural networks (ANNs) have achieved\\ntremendous success in a variety of scenarios via powerful feature extraction\\nand high-precision data operations. It is well known, however, that ANNs\\nusually suffer from expensive processing resources and costs. In contrast,\\nneuroscience-oriented spiking neural networks (SNNs) are promising for\\nenergy-efficient information processing benefit from the event-driven spike\\nactivities, whereas, they are yet be evidenced to achieve impressive\\neffectiveness on real complicated tasks. How to combine the advantage of these\\ntwo model families is an open question of great interest. Two significant\\nchallenges need to be addressed: (1) lack of benchmark datasets including both\\nANN-oriented (frames) and SNN-oriented (spikes) signal resources; (2) the\\ndifficulty in jointly processing the synchronous activation from ANNs and\\nevent-driven spikes from SNNs. In this work, we proposed a hybrid paradigm,\\nnamed as DashNet, to demonstrate the advantages of combining ANNs and SNNs in a\\nsingle model. A simulator and benchmark dataset NFS-DAVIS is built, and a\\ntemporal complementary filter (TCF) and attention module are designed to\\naddress the two mentioned challenges, respectively. In this way, it is shown\\nthat DashNet achieves the record-breaking speed of 2083FPS on neuromorphic\\nchips and the best tracking performance on NFS-DAVIS and PRED18 datasets. To\\nthe best of our knowledge, DashNet is the first framework that can integrate\\nand process ANNs and SNNs in a hybrid paradigm, which provides a novel solution\\nto achieve both effectiveness and efficiency for high-speed object tracking.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<record>\\n<header>\\n <identifier>oai:arXiv.org:1909.12943</identifier>\\n <datestamp>2019-10-01</datestamp>\\n <setSpec>cs</setSpec>\\n</header>\\n<metadata>\\n <arXivRaw xmlns=\"http://arxiv.org/OAI/arXivRaw/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd\">\\n <id>1909.12943</id><submitter>Mesay Samuel</submitter><version version=\"v1\"><date>Mon, 23 Sep 2019 21:12:22 GMT</date><size>614kb</size><source_type>D</source_type></version><title>Handwritten Amharic Character Recognition Using a Convolutional Neural\\n  Network</title><authors>Mesay Samuel Gondere, Lars Schmidt-Thieme, Abiot Sinamo Boltena, Hadi\\n  Samer Jomaa</authors><categories>cs.CV cs.LG stat.ML</categories><comments>ECDA2019 Conference Oral Presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Amharic is the official language of the Federal Democratic Republic of\\nEthiopia. There are lots of historic Amharic and Ethiopic handwritten documents\\naddressing various relevant issues including governance, science, religious,\\nsocial rules, cultures and art works which are very reach indigenous knowledge.\\nThe Amharic language has its own alphabet derived from Ge\\'ez which is currently\\nthe liturgical language in Ethiopia. Handwritten character recognition for non\\nLatin scripts like Amharic is not addressed especially using the advantages of\\nthe state of the art techniques. This research work designs for the first time\\na model for Amharic handwritten character recognition using a convolutional\\nneural network. The dataset was organized from collected sample handwritten\\ndocuments and data augmentation was applied for machine learning. The model was\\nfurther enhanced using multi-task learning from the relationships of the\\ncharacters. Promising results are observed from the later model which can\\nfurther be applied to word prediction.\\n</abstract></arXivRaw>\\n</metadata>\\n</record>\\n<resumptionToken cursor=\"0\" completeListSize=\"2525\">3996692|1001</resumptionToken>\\n</ListRecords>\\n</OAI-PMH>\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds] *",
   "language": "python",
   "name": "conda-env-ds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
